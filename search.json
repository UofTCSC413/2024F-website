[
  {
    "objectID": "course-grading.html",
    "href": "course-grading.html",
    "title": "Grading Scheme",
    "section": "",
    "text": "Assignment 1\n15%\n\n\nAssignment 2\n15%\n\n\nPanel Discussion\n10%\n\n\nProject Proposal\n10%\n\n\nProject Presentation\n25%\n\n\nFinal Project Report\n25%\n\n\n\n\nMarking rubric for panel discussion\nEvery week we will reserve 20-30 mins for a panel discussion based on the assigned reading for that day (4-5 papers). This discussion will include three types of roles: panel members, audience members, and a moderator. Each panel discussion will include 4 panel members, 1 moderator, and audience members. Panel members are responsible for answering questions, the audience is responsible for asking questions, and the moderator is responsible for steering the discussion and having backup questions if the audience is not asking any.\n\nPanel member evaluation\n\nAnswering questions from the moderator and the audience correctly / well (6 pts)\nEngaging with points of other panelists (1 pts)\nKeeping answers brief / allowing other people time to speak (2 pts)\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (1 pts)\n\n\n\nAudience member evaluation\n\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (10 pts)\n\n\n\nModerator evaluation\n\nSteering the discussion in terms of groups / themes of questions (2 pts)\nEnsuring there is time for every panel member to speak (4 pts)\nEngaging the audience / ensuring the audience has enough time to ask questions (3 pts)\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (1 pts)\n\n\n\n\nMarking rubric for the project proposal\n\nIntroduction (1 pts), which states the proposed problem being solved and any applications / implications.\nFigure or diagram (1 pts), showing an overview of your proposed solution, i.e. shows the overall idea in a way that is easily understandable without even reading the rest of the report.\nRelated work (1 pts) and bibliography. Highlight how your method is different from other approaches. Present other approaches in the proper light without diminishing their contributions.\nMethodology (2 pts). Describe your proposed methodology as well as any assumptions it relies on. Explain prerequisite concepts clearly and succinctly.\nEvaluation (2 pts). What experiments are you planning to do and why? What are the questions you want to answer through these experiments?\nTimeline (1 pts). What are the milestones required to complete your project and by when do you plan to complete them?\nAnticipated risks and mitigation plan (2 pts). What issues might arise with your proposed project and timeline and how will you address these issues if they occur?\n\n\n\nMarking rubric for the project presentation\n\nQuality of presentation\n\nSlide design (2 pts)\nDelivery of presentation (3 pts)\nRespecting time constraints (2 pts)\nResponse to questions (3 pts)\n\n\n\nTechnical content\n\nMotivation and definition of the problem (2 pts)\nPutting prior work into context (3 pts)\nMethodology explanation (3 pts)\nDiscussion of experiments (5 pts)\nDiscussion of limitations (2 pts)\n\n\n\n\nMarking rubric for the final project report\n\nAbstract (2 pts) that summarizes the main idea of the project and your contributions.\nIntroduction (3 pts) that states the problem being solved and any applications / implications.\nFigure or diagram (2 pts) that shows the overall idea in a way that is easily understandable.\nRelated work (2 pts) and bibliography. Highlight how your method is different from other approaches. Present other approaches in the proper light without diminishing their contributions.\nMethodology (7 pts). Describe your method in detail as well as any assumptions it relies on. Explain prerequisite concepts clearly and succinctly. Include algorithm descriptions, figures, and equations as you wish.\nEvaluation (8 pts). Include any figures or tables that illustrate your experimental results. Do not forget to include error bars if applicable. Analyze your findings, and comment on their statistical significance. In your evaluation please take into account Joelle Pineau’s ML reproducibility checklist.\nLimitations (2 pts). Describe some settings in which your approach performs poorly, and list a few ideas for how to adddress them. Describe opportunities for future work, as well as open problems.\nConclusions (1 pts). A summary of your contributions and results."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Mine Çetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STA 210” in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "href": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you’ll upload your PDF and them mark the page(s) where each question can be found. It’s OK if a question spans multiple pages, just mark them all. It’s also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fall 2024 - CSC 413: Neural Networks and Deep Learning",
    "section": "",
    "text": "Course Overview\nIt is very hard to hand-design programs to solve many real-world problems, e.g. distinguishing images of cats v.s. dogs. Machine learning algorithms allow computers to learn from example data, and produce a program that does the job. Neural networks are a class of machine learning algorithms originally inspired by the brain, but which have recently have seen a lot of success at practical applications. They’re at the heart of production systems at companies like Google and Facebook for image processing, speech-to-text, and language understanding.\nThis course gives an overview of both the foundational ideas and the recent advances in neural net algorithms. Roughly the first 2/3 of the course focuses on supervised learning - training the network to produce a specified behavior when one has lots of labeled examples of that behavior. The last 1/3 focuses on unsupervised learning.\n\n\nPrerequisites\nPrerequisite: CSC311H5 or CSC411H5\nExclusion: CSC321H5 or CSC321H1 or CSC413H1 or CSC421H1 (SCI)\nDistribution Requirement: SCI\nStudents who lack a pre/co-requisite can be removed at any time unless they have received an explicit waiver from the department. The waiver form can be downloaded from here.\n\n\nCourse Delivery Details\n\n\n\n\n\n\n\n\n\n\nLectures\nProf\nDay\nTime\nLocation\n\n\n\n\nLEC0101\nIgor Gilitschenski\nTuesday\n5:00 pm - 7:00 pm\nMN3190\n\n\nLEC0102\nFlorian Shkurti\nWednesday\n11:00 am - 1:00 pm\nMN3190\n\n\n\n\n\n\nTutorials/Labs\nDay\nTime\nLocation\n\n\n\n\nPRA0101\nFriday\n10:00 am - 11:00 am\nDH2020\n\n\nPRA0102\nFriday\n11:00 am - 12:00 pm\nDH2020\n\n\nPRA0103\nFriday\n12:00 pm - 1:00 pm\nDH2020\n\n\n\n\nAnnouncements will be posted on Quercus\nDiscussions will take place on Piazza\nZoom recordings will be posted on Quercus after lectures",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, you’ll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance you’ll use for the course."
  },
  {
    "objectID": "ex/w06/questions/prob-variance-sol.html",
    "href": "ex/w06/questions/prob-variance-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First, we use the definition of variance and rewrite the left hand side as \\[\n\\Var(aX+bY) = \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2.\n\\] Next, we expand the squares for each of the terms on the right hand side: \\[\\begin{align*}\n\\E\\li[ (aX+bY)^2 \\ri]\n  &= \\E\\li[ a^2X^2 + 2abXY + b^2Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ XY ] + b^2\\E\\li[ Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri] ,\\\\\n\\E[aX+bY]^2\n  &= \\li(a\\E[X] + b\\E[Y]\\ri)^2 \\\\\n  &= a^2\\E[X]^2 + 2ab\\E[X]\\,\\E[Y] + b^2\\E[Y]^2.\n\\end{align*}\\] Subtracting the two terms, we get \\[\\begin{align*}\n& \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri]\n    - a^2\\E[X]^2 - 2ab\\E[X]\\,\\E[Y] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] - a^2\\E[X]^2\n   + b^2\\E\\li[ Y^2 \\ri] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2(\\E\\li[ X^2 \\ri] - \\E[X]^2)\n   + b^2\\li(\\E\\li[ Y^2 \\ri] - \\E[Y]^2\\ri) \\\\\n& \\quad = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w06/questions/ml-variance_bias_decomposition.html",
    "href": "ex/w06/questions/ml-variance_bias_decomposition.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Let \\(D=\\lbrace (x_i,y_i) | i=1 \\ldots n\\rbrace\\) be a dataset obtained from the true underlying data distribution \\(P\\), i.e. \\(D\\sim P^n\\). And let \\(h_D(\\cdot)\\) be a classifier trained on \\(D\\). Show the variance bias decomposition \\[\n\\underbrace{\\mathbb{E}_{D,x,y} \\li[ (h_D(x) - y)^2 \\ri]}_{\\text{Expected test error}}  \n  = \\underbrace{\\mathbb{E}_{D,x} \\li[ (h_D(x) - \\hat{h}(x))^2 \\ri]}_{\\text{Variance}} +\n  \\underbrace{\\mathbb{E}_{x,y} \\li[ (\\hat{y}(x) - y)^2 \\ri]}_{\\text{Noise}}  +  \\underbrace{\\mathbb{E}_{x} \\li[ (\\hat{h}(x) - \\hat{y}(x))^2 \\ri]}_{\\text{Bias}^2}\n\\] where \\(\\hat{h}(x) = \\mathbb{E}_{D \\sim P^n}[h_D(x)]\\) is the expected regressor over possible training sets, given the learning algorithm \\(\\mathcal{A}\\) and \\(\\hat{y}(x) = \\mathbb{E}_{y|x}[y]\\) is the expected label given \\(x\\). As mentioned in the lecture, labels might not be deterministic given x. To carry out the proof, proceed in the following steps:\n\nShow that the following identity holds \\[\\begin{align}\n\\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n= \\E_{D, x}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri].\n   \\end{align}\\]\nNext, show \\[\\begin{align}\nE_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri]\n=E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri]\n\\end{align}\\] which completes the proof by substituting (2) into (1)."
  },
  {
    "objectID": "ex/w06/exercises06-ensembling.html",
    "href": "ex/w06/exercises06-ensembling.html",
    "title": "Ensembling",
    "section": "",
    "text": "Tre training code below (based on a tutorial from here) trains a simple MNIS classifier.\nYour task is to adjust it to use an ensemble of 5 models instead of a single one.\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom torchvision import datasets, transforms\nfrom torch import nn, optim\ntransform = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5,), (0.5,)),\n                              ])\nSetup the dataset and dataloaders.\ntrainset = datasets.MNIST(\n    'datasets', download=True, train=True, transform=transform)\nvalset = datasets.MNIST(\n    'datasets', download=True, train=False, transform=transform)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=64, shuffle=True)\nvalloader = torch.utils.data.DataLoader(\n    valset, batch_size=64, shuffle=True)\nDefine the model.\ninput_size = 784\nhidden_sizes = [128, 64]\noutput_size = 10\n\nmodel = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[1], output_size),\n                      nn.LogSoftmax(dim=1))\ncriterion = nn.NLLLoss()\nimages, labels = next(iter(trainloader))\nimages = images.view(images.shape[0], -1)\n\nlogps = model(images) #log probabilities\nloss = criterion(logps, labels) #calculate the NLL loss\nRun the main training loop.\noptimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\ntime0 = time()\nepochs = 15\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in trainloader:\n        # Flatten MNIST images into a 784 long vector\n        images = images.view(images.shape[0], -1)\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    else:\n        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\nprint(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\ncorrect_count, all_count = 0, 0\nfor images,labels in valloader:\n  for i in range(len(labels)):\n    img = images[i].view(1, 784)\n    with torch.no_grad():\n        logps = model(img)\n\n    \n    ps = torch.exp(logps)\n    probab = list(ps.numpy()[0])\n    pred_label = probab.index(max(probab))\n    true_label = labels.numpy()[i]\n    if(true_label == pred_label):\n      correct_count += 1\n    all_count += 1\n\nprint(\"Number Of Images Tested =\", all_count)\nprint(\"\\nModel Accuracy =\", (correct_count/all_count))"
  },
  {
    "objectID": "ex/w06/exercises06-ensembling.html#version-with-ensembles",
    "href": "ex/w06/exercises06-ensembling.html#version-with-ensembles",
    "title": "Ensembling",
    "section": "Version with Ensembles",
    "text": "Version with Ensembles\nTODO: Add your code below."
  },
  {
    "objectID": "ex/w08/questions/rnn-addition.html",
    "href": "ex/w08/questions/rnn-addition.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "In this problem, you will implement a recurrent neural network which implements binary addition. The inputs are given as binary sequences, starting with the significant binary digit. (It is easier to start from the least significant bit, just like how you did addition in grade school.) The sequences will be padded with at least one zero as the most significant digit, so that the output length is the same as the input length. For example, the problem \\(100111 + 110010\\), whose target output value is \\(1011001\\), will be represented as follows: \\[\\begin{align*}\n\\bf{x}^{(1)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(2)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(3)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(4)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(5)} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(6)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(7)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{align*}\\]\nWith the target output: \\[\\begin{align*}\ny^{(1)} = 1,\ny^{(2)} = 0,\ny^{(3)} = 0,\ny^{(4)} = 1,\ny^{(5)} = 1,\ny^{(6)} = 0,\ny^{(7)} = 1,\n\\end{align*}\\]\nThere are two input units corresponding to the two inputs, and one output unit. Therefore, the pattern of inputs and outputs for this example would be:\n\nDesign, by hand, the weights and biases for an RNN which has two input units, three hidden units, and one output unit, which implements binary addition as discussed above. All of the units use the hard threshold activation function (\\(f(x) = 1\\) if \\(x &gt; 0\\) and \\(0\\) otherwise). In particular, specify weight matrices \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{W}\\), bias vector \\(\\mathbf{b}_{\\mathbf{h}}\\), and scalar bias \\(b_y\\) for the following architecture: \\[\\begin{align*}\nh^{(t)} &= f(\\bf{W}h^{(t-1)} + \\bf{U}\\bf{x}^{(t)} + \\bf{b_h}) \\\\\ny^{(t)} &= f(\\bf{v}^T h^{(t)} + b_y)\n\\end{align*}\\]\n\nWhat are the shapes of \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\)?\nCome up with values for \\(\\mathbf{U}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\). Justify your answer. Hint: When performing binary addition, in addition to adding up two digits in a column, we need to track whether there is a digit from the previous column. We will choose one of the three units in \\(\\bf{h}^{(t)}\\), say \\(\\bf{h}_2^{(t)}\\), to represent this carry digit. You may also find it helpful to set \\(\\bf{h}_1\\) to activate if the sum of the 3 digits is at least 1, \\(\\bf{h}_2\\) to activate if the sum is at least 2, and \\(\\bf{h}_3\\) to activate if the sum is at least 3.\nCome up with the values of \\(\\bf{v}\\) and \\(b_y\\). Justify your answer."
  },
  {
    "objectID": "ex/w08/questions/rnn-sentiment-sol.html",
    "href": "ex/w08/questions/rnn-sentiment-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "We will need to compute \\(h^{(t)}\\) for \\(t = 1, \\ldots, T\\). Each of this computation requires applying the weight matrices \\(W\\) and \\(T\\) once. The matrix \\(V\\) is only applied once at the end. Therefore, we need to apply \\(W\\) and \\(U\\) \\(T\\) times each and \\(V\\) once.\nThe shape of \\(U\\) is \\(d_h \\times d_x\\), the shape of \\(W\\) is \\(d_h \\times d_h\\), and the shape of \\(V\\) is \\(d_y \\times d_h\\), where \\(d_h\\) is the dimensionality of the \\(h^{(i)}\\) (i.e. \\(h^{(i)}\\in\\R^{d_h}\\)), \\(d_x\\) is the dimensionality of the inputs \\(x^{(i)}\\), and \\(d_y\\) is the dimensionality of the ouput \\(y\\).\nFor each of the \\(T\\) steps, we need to perform two matrix-vector multiplications (one for \\(Ux^{(i)}\\) and one for \\(Uh^{(i)}\\)) and two vector additions. To compute the output, we need one additional matrix-vector multiplication and one vector addition."
  },
  {
    "objectID": "ex/w08/questions/rnn-scalar.html",
    "href": "ex/w08/questions/rnn-scalar.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Suppose we have the following vanilla RNN network, where the inputs and hidden units are scalars. \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(w \\cdot h^{(t-1)} + u \\cdot x^{(t-1)} + b_h\\ri) \\\\\ny &= \\sigma\\li(v \\cdot h^{(T)} + b_y\\ri)\n\\end{align*}\\]\n\nShow that if \\(|w| &lt; 1\\), and the number of time steps \\(T\\) is large, then the gradient \\(\\frac{\\partial y}{\\partial x^{(0)}}\\) vanishes.\nWhy is the result from Part (a) troubling?"
  },
  {
    "objectID": "ex/w08/exercises08.html",
    "href": "ex/w08/exercises08.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/rnn-sentiment.md"
  },
  {
    "objectID": "ex/w08/exercises08.html#exercise-1---rnn-for-sentiment-analysis",
    "href": "ex/w08/exercises08.html#exercise-1---rnn-for-sentiment-analysis",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/rnn-sentiment.md"
  },
  {
    "objectID": "ex/w08/exercises08.html#exercise-2---scalar-rnn",
    "href": "ex/w08/exercises08.html#exercise-2---scalar-rnn",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Scalar RNN",
    "text": "Exercise 2 - Scalar RNN\n!include questions/rnn-scalar.md"
  },
  {
    "objectID": "ex/w08/exercises08.html#exercise-3---rnn-addition",
    "href": "ex/w08/exercises08.html#exercise-3---rnn-addition",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - RNN Addition",
    "text": "Exercise 3 - RNN Addition\n!include questions/rnn-addition.md"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers-sol.html",
    "href": "ex/w09/questions/attn-transformers-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The softmax function is applied row-wise and the shape of the result is \\(n_q\\times n_k\\). One way to see this is by looking at the shape of the dot product \\(QK^\\top\\) which is \\(n_q\\times n_k\\). Each row represents the pre-softmax scores of all keys and a given query. Because we need to normalize our attention weights per query, the normalization happens along the rows.\nThe value of \\(d\\) is \\(d_k\\). It is needed to scale the dot product so that the gradient of the softmax function does not vanish.\nTo obtain the computational complexity, let’s look at all the operations individually:\n\n\\(QK^\\top\\) requires \\(n_q n_k d_k\\) multiplications and \\(n_qn_k(d_k-1)\\) additions.\nDividing by \\(\\sqrt{d_k}\\) needs to be carried out \\(n_q n_k\\) times.\nApplying the softmax function can be implemented in \\(n_q n_k\\) divisions and \\(n_q(n_k-1)\\) additions.\nThe final matrix multiplication requires \\(n_qd_vn_k\\) multiplications and \\(n_q d_v (n_k-1)\\) additions.\n\nThe masking matrix is a triangular matrix with \\(-\\infty\\) on its top right half. This results in softmax weights being \\(0\\) for all key-query combinations to which \\(-\\infty\\) is added."
  },
  {
    "objectID": "ex/w09/questions/attn-transformers-notes.html",
    "href": "ex/w09/questions/attn-transformers-notes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\n\n\nQ = torch.tensor([[1, 2], [3, 1]]).float()\n\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\n\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\n\n\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])\n\n\nLazy version\n\nF.scaled_dot_product_attention(Q, K, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers_by_hand.html",
    "href": "ex/w09/questions/attn-transformers_by_hand.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Consider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 2\\\\\n3 & 1\n\\epmat,\\quad\nK = \\bpmat\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 2 & -2\\\\\n1 & 1 & 2 \\\\\n0 & 1 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w09/questions/attn-transformers_by_hand-notes.html",
    "href": "ex/w09/questions/attn-transformers_by_hand-notes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\n\n\nQ = torch.tensor([[1, 2], [3, 1]]).float()\n\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\n\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\n\n\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])\n\n\nLazy version\n\nF.scaled_dot_product_attention(Q, K, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers.html",
    "href": "ex/w09/questions/attn-transformers.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Transformers use a scaled dot product attention mechanism given by \\[\nC\n= \\text{attention}(Q, K, V)\n= \\text{softmax}\\left(\\fr{QK^\\top}{\\sqrt{d}}\\right) V,\n\\] where \\(Q\\in\\R^{n_q\\times d_k}\\), \\(K\\in\\R^{n_k\\times d_k}\\), \\(V\\in\\R^{n_k\\times d_v}\\).\n\nIs the softmax function here applied row-wise or column-wise? What is the shape of the result?\nWhat is the value of \\(d\\)? Why is it needed?\nWhat is the computational complexity of this attention mechanism? How many additions and multiplications are required? Assume the canonical matrix multiplcation and not counting \\(\\exp(x)\\) towards computational cost.\nIn the masked variant of the module, a masking matrix is added before the softmax function is applied. What are its values and its shape? For simplicity, assume \\(n_q=n_k\\)."
  },
  {
    "objectID": "ex/w09/exercises09.html",
    "href": "ex/w09/exercises09.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/attn-dot_product_attention.md"
  },
  {
    "objectID": "ex/w09/exercises09.html#exercise-1---dot-product-attention",
    "href": "ex/w09/exercises09.html#exercise-1---dot-product-attention",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/attn-dot_product_attention.md"
  },
  {
    "objectID": "ex/w09/exercises09.html#exercise-2---attention-in-transformers",
    "href": "ex/w09/exercises09.html#exercise-2---attention-in-transformers",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Attention in Transformers",
    "text": "Exercise 2 - Attention in Transformers\n!include questions/attn-transformers.md"
  },
  {
    "objectID": "ex/w09/exercises09.html#exercise-3---scaled-dot-product-attention-by-hand",
    "href": "ex/w09/exercises09.html#exercise-3---scaled-dot-product-attention-by-hand",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Scaled Dot-Product Attention by Hand",
    "text": "Exercise 3 - Scaled Dot-Product Attention by Hand\n!include questions/attn-transformers_by_hand.md"
  },
  {
    "objectID": "ex/w07/questions/cnn-by_hand.html",
    "href": "ex/w07/questions/cnn-by_hand.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Consider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 2 & -1 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 2 \\\\\n2 & 1 & 0 & -1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 0 \\\\\n2, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w07/questions/opt-influence_functions-sol.html",
    "href": "ex/w07/questions/opt-influence_functions-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First, let’s recall the definitions of \\(\\te\\) and \\(\\hte(\\epsilon)\\): \\[\\begin{align*}\n\\hat{\\theta}\n&= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] \\\\\n\\hte({\\epsilon})\n  &= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] + \\epsilon L(x,y; \\theta)\n\\end{align*}\\] The first order taylor series expansion around \\(\\epsilon = 0\\) is given by \\[\n\\hte(0) + \\epsilon \\fr{d\\hte(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0}\n\\] From the definitions above, it can directly be seen that \\(\\hte(0) = \\hte\\) which completes the proof."
  },
  {
    "objectID": "ex/w07/questions/prob-mle_exp_dist-sol.html",
    "href": "ex/w07/questions/prob-mle_exp_dist-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters).\nAs mentioned in our previous exercise, we usually work with the log-likelihood in practice. In this particular case, the log likelihood is given by \\[\\begin{aligned}\nl(\\la | x_1, \\ldots, x_N)\n  & := \\sum_{i=1}^N \\ln f(\\fx_i | \\la) \\\\\n  & = \\sum_{i=1}^N \\ln\\li(\\la \\exp(-\\la x_i) \\ri) \\\\\n  & = \\sum_{i=1}^N \\ln(\\la) + \\ln\\li( \\exp(-\\la x_i) \\ri) \\\\\n  & =  N \\ln(\\la) - \\sum_{i=1}^N \\la x_i .\n\\end{aligned}\\] The derivative with respect to \\(\\la\\) is \\[\n\\fr{\\partial l(\\la | x_1, \\ldots, x_N)}{\\partial \\la}\n  = \\fr{N}{\\la} - \\sum_{i=1}^N x_i .\n\\] The MLE is obtained by setting it to 0 and solving for \\(\\la\\) as \\[\n\\hla_{MLE} = N \\li( \\sum_{i=1}^N x_i\\ri)^{-1}.\n\\]"
  },
  {
    "objectID": "ex/w07/questions/opt-influence_functions.html",
    "href": "ex/w07/questions/opt-influence_functions.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Let \\(\\hte\\) and \\(\\hte(\\ve)\\) be as defined in class. Show that the first order Taylor expansion of \\(\\hte(\\ve)\\) around \\(\\ve=0\\) is given by the equation given in class, i.e. by \\[\\begin{align*}\n\\hat{\\theta}({\\epsilon}) \\approx \\hat{\\theta} + \\epsilon\\frac{d\\hat{\\theta}(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0} .\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html",
    "href": "ex/w07/exercises07_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The exercises this week involve some old material so you can check your learning and understanding."
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-1---maximum-likelihood-estimator",
    "href": "ex/w07/exercises07_solution.html#exercise-1---maximum-likelihood-estimator",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 1 - Maximum Likelihood Estimator",
    "text": "Exercise 1 - Maximum Likelihood Estimator\n!include questions/prob-mle_exp_dist.md\n\nSolution\n!include questions/prob-mle_exp_dist-sol.md"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-2---convolutional-layers",
    "href": "ex/w07/exercises07_solution.html#exercise-2---convolutional-layers",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\n!include questions/cnn-by_hand.md\n\nSolution\n!include questions/cnn-by_hand-sol.md"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-3---computational-parameter-counting",
    "href": "ex/w07/exercises07_solution.html#exercise-3---computational-parameter-counting",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Computational Parameter Counting",
    "text": "Exercise 3 - Computational Parameter Counting\n!include questions/pytorch-parameter_count.md\n\nSolution\n!include questions/pytorch-parameter_count-sol.md"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-4---influence-functions",
    "href": "ex/w07/exercises07_solution.html#exercise-4---influence-functions",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - Influence Functions",
    "text": "Exercise 4 - Influence Functions\n!include questions/opt-influence_functions.md\n\nSolution\n!include questions/opt-influence_functions-sol.md"
  },
  {
    "objectID": "ex/w10/questions/tconv-params.html",
    "href": "ex/w10/questions/tconv-params.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "What is the number of learnable parameters for each of the following transposed convolution layers defined in PyTorch. Try to calculate those by hand first and use pytorch later to verify your results.\n\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=1)\nnn.ConvTranspose2d(in_channels=3, out_channels=10, kernel_size=3, stride=2)\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=4, stride=5)\nnn.ConvTranspose2d(in_channels=3, out_channels=4, kernel_size=3, stride=23)"
  },
  {
    "objectID": "ex/w10/questions/tconv-sizes-sol.html",
    "href": "ex/w10/questions/tconv-sizes-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Output tensor size: \\(2\\times 4\\times 4\\)\nOutput tensor size: \\(4\\times 10\\times 10\\)\nThe first dimension is always the number of input channels, i.e. \\(c_{out}\\). The output for the first value of the input tensor is of size \\(3\\times 3\\). For all remaining values, the stride determines the additional values of the ouput. Stride 2 means, that we need two values for each additional value. Thus, the output height \\(h_{out}\\) and width are \\(w_{out}\\): \\[\n\\begin{aligned}\nh_{out} &= 3 + (h-1)\\cdot 2 = 2h+1\\\\\nw_{out} &= 3 + (w-1)\\cdot 2 = 2w+1\\\\\n\\end{aligned}\n\\] Thus, the resulting size is \\(2\\times (2h+1) \\times (2w+1)\\).\nThis generalization requires a similar consideration and we can basically reuse the equations from above: \\[\n\\begin{aligned}\nh_{out} &= h_k + (h-1)\\cdot s\\\\\nw_{out} &= w_k + (w-1)\\cdot s\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "ex/w10/questions/tconv-by_hand-sol.html",
    "href": "ex/w10/questions/tconv-by_hand-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The resulting output tensor is of shape \\(6 \\times 6\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  0 &  0 &  2 &  0\\\\\n1 &  2 &  0 &  0 &  2 &  4\\\\\n2 &  0 &  3 &  0 &  0 &  0\\\\\n2 &  4 &  3 &  6 &  0 &  0\\\\\n-1 &  0 &  0 &  0 &  3 &  0\\\\\n-1 & -2 &  0 &  0 &  3 &  6\n\\epmat\n\\]\nThe resulting output tensor is of shape \\(4 \\times 4\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  2 &  0 \\\\\n3 &  5 &  2 &  4 \\\\\n1 &  7 &  9 &  0 \\\\\n-1 & -2 &  3 &  6\n\\epmat\n\\]\nWe can verify our answers with PyTorch with the following commands:\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=2)\n\nnn.functional.conv_transpose2d(X.unsqueeze(0), K)\nThe call to .unsqueeze(0) can be left out if X has a batch dimension and is already a 4D tensor.\nA potential implementaiton (that isn’t optimized in any way) of transposed convolutions looks like this:\n\n\ndef conv_transpose2d(input, weight, stride=1):\n  # input - input of shape (C_in, H, W)\n  # weight - kernel of shape (C_in, C_out, K, K)\n  # stride - stride of the transposed convolutio\n  # RETURNS\n  # output - output of shape (C_out, H_out, W_out)\n  (c_in, h_in, w_in) = X.size()\n  (c2_in, c_out, k, k2) = K.size()\n\n  assert c_in == c2_in, \"Number of input channels must match\"\n  assert k == k2, \"Kernel must be square\"\n\n  h_out = (h_in - 1) * stride + k\n  w_out = (w_in - 1) * stride + k\n  output = torch.zeros((c_out, h_out, w_out))\n\n  for c_cur_in in range(c_in):\n    for c_cur_out in range(c_out):\n      for h in range(0, h_in):\n        for w in range(0, w_in):\n          output[c_cur_out, h*stride:h*stride+k, w*stride:w*stride+k] \\\n              += weight[c_cur_in, c_cur_out,:,:] * input[c_cur_in,h,w]\n\n  return output"
  },
  {
    "objectID": "ex/w10/questions/tconv-params-sol.html",
    "href": "ex/w10/questions/tconv-params-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The number of parameters is the product of input channels, output channels and kernel size. For each output channel, there is one bias parameter (if bias=True which is default in PyTorch). The stride does not matter for the number of paramters.\n\n\\(3\\cdot 2 \\cdot 9 + 2=56\\)\n\\(3\\cdot 10 \\cdot 9 + 10=280\\)\n\\(3\\cdot 2 \\cdot 16 + 2=98\\)\n\\(3\\cdot 4 \\cdot 9 + 4=112\\)"
  },
  {
    "objectID": "ex/w10/questions/tcons-by_hand-notes.html",
    "href": "ex/w10/questions/tcons-by_hand-notes.html",
    "title": "Solutions to (a), (b), and (c)",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nX = torch.tensor(\n    [[1, 0, 2], \n     [2, 3, 0], \n     [-1, 0, 3]]).unsqueeze(0)\n\nK = torch.tensor(\n    [[1, 0],\n     [1, 2]]).unsqueeze(0).unsqueeze(0)\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=2)\n\ntensor([[[[ 1,  0,  0,  0,  2,  0],\n          [ 1,  2,  0,  0,  2,  4],\n          [ 2,  0,  3,  0,  0,  0],\n          [ 2,  4,  3,  6,  0,  0],\n          [-1,  0,  0,  0,  3,  0],\n          [-1, -2,  0,  0,  3,  6]]]])\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=1)\n\ntensor([[[[ 1,  0,  2,  0],\n          [ 3,  5,  2,  4],\n          [ 1,  7,  9,  0],\n          [-1, -2,  3,  6]]]])"
  },
  {
    "objectID": "ex/w10/questions/tcons-by_hand-notes.html#solution-to-d",
    "href": "ex/w10/questions/tcons-by_hand-notes.html#solution-to-d",
    "title": "Solutions to (a), (b), and (c)",
    "section": "Solution to (d)",
    "text": "Solution to (d)\nBelow are some computations for verification.\n\ndef conv_transpose2d(input, weight, stride=1):\n  # input - input of shape (C_in, H, W)\n  # weight - kernel of shape (C_in, C_out, K, K)\n  # stride - stride of the transposed convolutio\n  # RETURNS\n  # output - output of shape (C_out, H_out, W_out)\n  (c_in, h_in, w_in) = X.size()\n  (c2_in, c_out, k, k2) = K.size()\n\n  assert c_in == c2_in, \"Number of input channels must match\"\n  assert k == k2, \"Kernel must be square\"\n\n  h_out = (h_in - 1) * stride + k\n  w_out = (w_in - 1) * stride + k\n  output = torch.zeros((c_out, h_out, w_out))\n\n  for c_cur_in in range(c_in):\n    for c_cur_out in range(c_out):\n      for h in range(0, h_in):\n        for w in range(0, w_in):\n          output[c_cur_out, h*stride:h*stride+k, w*stride:w*stride+k] \\\n              += weight[c_cur_in, c_cur_out,:,:] * input[c_cur_in,h,w]\n\n  return output\n\n\nconv_transpose2d(X, K, stride=2)\n\ntensor([[[ 1.,  0.,  0.,  0.,  2.,  0.],\n         [ 1.,  2.,  0.,  0.,  2.,  4.],\n         [ 2.,  0.,  3.,  0.,  0.,  0.],\n         [ 2.,  4.,  3.,  6.,  0.,  0.],\n         [-1.,  0.,  0.,  0.,  3.,  0.],\n         [-1., -2.,  0.,  0.,  3.,  6.]]])\n\n\n\nconv_transpose2d(X, K, stride=1)\n\ntensor([[[ 1.,  0.,  2.,  0.],\n         [ 3.,  5.,  2.,  4.],\n         [ 1.,  7.,  9.,  0.],\n         [-1., -2.,  3.,  6.]]])\n\n\n\nX = torch.randn((2,3,3))\nK = torch.randn((2,1,2,2))\n\n\nconv_transpose2d(X, K, stride=1)\n\ntensor([[[ 2.5196, -3.0373, -1.7501,  1.1531],\n         [-0.2329, -1.2616, -1.2943,  0.5111],\n         [-1.0364, -0.8051, -2.0395, -1.4085],\n         [ 1.9276,  0.1688, -3.1863, -1.4459]]])\n\n\n\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=1)\n\ntensor([[[[ 2.5196, -3.0373, -1.7501,  1.1531],\n          [-0.2329, -1.2616, -1.2943,  0.5111],\n          [-1.0364, -0.8051, -2.0395, -1.4085],\n          [ 1.9276,  0.1688, -3.1863, -1.4459]]]])"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html",
    "href": "ex/w10/exercises10_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/tconv-sizes.md\n\n\n!include questions/tconv-sizes-sol.md"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html#exercise-1---transposed-convolution-output-sizes",
    "href": "ex/w10/exercises10_solution.html#exercise-1---transposed-convolution-output-sizes",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/tconv-sizes.md\n\n\n!include questions/tconv-sizes-sol.md"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html#exercise-2---transposed-convolution-parameter-sizes",
    "href": "ex/w10/exercises10_solution.html#exercise-2---transposed-convolution-parameter-sizes",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Transposed Convolution Parameter Sizes",
    "text": "Exercise 2 - Transposed Convolution Parameter Sizes\n!include questions/tconv-params.md\n\nSolution\n!include questions/tconv-params-sol.md"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html#exercise-3---transposed-convolution-by-hand",
    "href": "ex/w10/exercises10_solution.html#exercise-3---transposed-convolution-by-hand",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Transposed Convolution by Hand",
    "text": "Exercise 3 - Transposed Convolution by Hand\n!include questions/tconv-by_hand.md\n\nSolution\n!include questions/tconv-by_hand-sol.md"
  },
  {
    "objectID": "ex/w11/questions/cnn-by_hand.html",
    "href": "ex/w11/questions/cnn-by_hand.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Consider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & 1 & -1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 3 & 0 & 1 \\\\\n1 & -1 & 0 & 1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 2 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w11/questions/pytorch-parameter_count-sol.html",
    "href": "ex/w11/questions/pytorch-parameter_count-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First, we hvae to load the alexnet model which is part of torchvision:\nimport torchvision\nalexnet = torchvision.models.alexnet()\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in alexnet.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the alexnet model. The structure is similar to vgg and the forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in alexnet.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over alexnet.classifier instead of alexnet.features."
  },
  {
    "objectID": "ex/w11/questions/linalg-evs_to_mat-sol.html",
    "href": "ex/w11/questions/linalg-evs_to_mat-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\nIn our case, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 1.5 & -0.5 \\\\\n    -0.5 & 1.5\n    \\epmat\n  \\]"
  },
  {
    "objectID": "ex/w11/questions/attn-transformers_by_hand.html",
    "href": "ex/w11/questions/attn-transformers_by_hand.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Consider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 3\\\\\n0 & 1\n\\epmat,\\quad\nK = \\bpmat\n1 & 1\\\\\n1 & 2\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 0 & -2\\\\\n2 & 1 & 2 \\\\\n0 & 3 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w11/questions/attn-transformers_by_hand-sol.html",
    "href": "ex/w11/questions/attn-transformers_by_hand-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n1.80 & 1.00 & 1.44\\\\\n1.26 & 1.25 & 0.26\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w11/questions/cnn-by_hand-sol.html",
    "href": "ex/w11/questions/cnn-by_hand-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Here, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 &  3 &  -1 \\\\\n4 &  2 &  2 \\\\\n5 &  3 & 3\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 & -1 \\\\\n5 & 3\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 &  0 & 0 &  0 & 0 \\\\\n0 & 1 &  0 & 1 & -1 & 0 \\\\\n0 & 1 &  0 & 1 &  0 & 0 \\\\\n0 & 0 &  3 & 0 &  1 & 0 \\\\\n0 & 1 & -1 & 0 &  1 & 0 \\\\\n0 & 0 &  0 & 0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 1 & 0 \\\\\n2 & 2 & 0 \\\\\n2 & -1 & 1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html",
    "href": "ex/w11/exercises11_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/linalg-evs_to_mat.md\n\n\n!include questions/linalg-evs_to_mat-sol.md"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-1---eigenvalues-and-eigenvectors",
    "href": "ex/w11/exercises11_solution.html#exercise-1---eigenvalues-and-eigenvectors",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/linalg-evs_to_mat.md\n\n\n!include questions/linalg-evs_to_mat-sol.md"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-2---parameter-counting",
    "href": "ex/w11/exercises11_solution.html#exercise-2---parameter-counting",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Parameter Counting",
    "text": "Exercise 2 - Parameter Counting\n!include questions/pytorch-parameter_count.md\n\nSolution\n!include questions/pytorch-parameter_count-sol.md"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-3---convolutional-layers",
    "href": "ex/w11/exercises11_solution.html#exercise-3---convolutional-layers",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Convolutional Layers",
    "text": "Exercise 3 - Convolutional Layers\n!include questions/cnn-by_hand.md\n\nSolution\n!include questions/cnn-by_hand-sol.md"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-4---scaled-dot-product-attention",
    "href": "ex/w11/exercises11_solution.html#exercise-4---scaled-dot-product-attention",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - Scaled Dot-Product Attention",
    "text": "Exercise 4 - Scaled Dot-Product Attention\n!include questions/attn-transformers_by_hand.md\n\nSolution\n!include questions/attn-transformers_by_hand-sol.md"
  },
  {
    "objectID": "ex/w02/questions/linalg-evs.html",
    "href": "ex/w02/questions/linalg-evs.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue."
  },
  {
    "objectID": "ex/w02/questions/linalg-evs-sol.html",
    "href": "ex/w02/questions/linalg-evs-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Because \\(\\la_1=\\la_2\\), we will write \\(\\la\\) for simplicity. The result is obtained by applying the definition of eigenvalues and distributivity via \\[\\begin{equation*}\n\\fA\\fu\n  = \\fA (\\al \\fu_1 + \\be \\fu_2)\n  =   \\al \\fA \\fu_1 + \\be \\fA \\fu_2\n  =   \\al \\la \\fu_1 + \\be \\la \\fu_2\n  =   \\la (\\al\\fu_1 + \\be \\fu_2)\n  =   \\la \\fu .\n\\end{equation*}\\]"
  },
  {
    "objectID": "ex/w02/questions/nn-compgraph-sol.html",
    "href": "ex/w02/questions/nn-compgraph-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Applying the chain rule, we have\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial w_j} \\]\nLooking at each term individually yields \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial y}\n  &= \\frac{\\partial}{\\partial y} [-t \\log(y) - (1 - t) \\log(1 - y)]\n  = - \\frac{t}{y} + \\frac{1 - t}{1 - y}\\\\\n\\frac{\\partial y}{\\partial z}\n  &= \\frac{\\partial \\sigma(z)}{\\partial z}\n  = \\sigma(z) (1 - \\sigma(z))\n  = y (1 - y)\\\\\n\\frac{\\partial z}{\\partial w_j}\n  &= \\frac{\\partial}{\\partial w_j} (w^\\top x) = x_j\n\\end{aligned}\n\\]\nBringing it all together yields: \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial w_j}\n  &= \\left( - \\frac{t}{y} + \\frac{1 - t}{1 - y} \\right) \\cdot y (1 - y) \\cdot x_j \\\\\n  &= (-t + ty + 1 - t - y + ty) x_j \\\\\n  &= (y - t) x_j\n\\end{aligned}\n\\]\nThe computation graph is given in the figure below.\n\n\n\n\nComputation graph for exercise 4 (b)"
  },
  {
    "objectID": "ex/w02/questions/prob-evvar-sol.html",
    "href": "ex/w02/questions/prob-evvar-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The key idea here is to compute the gradient of the objective function and solve for \\(\\fmu\\). The gradient is obtained by applying the chain rule resulting in \\[0=\\nabla_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2 = -2 \\sum_i (\\fx_i - \\fmu) .\\] Now, we solve this for \\(\\fmu\\) to obtain \\[\\fmu = \\frac{1}{N} \\sum_i \\fx_i .\\]\nHere, we simply need to apply some algebraic manipulations to show that the two definitions are equivalent. We start with the first definition and expand the square: \\[\\begin{align*}\n\\E[(X - \\E[X])^2]\n  &= \\E\\li[X^2 - 2X\\E[X]+\\E[X]^2\\ri] \\\\\n  &= \\E\\li[X^2\\ri] - \\E[2X\\E[X]]+\\E\\li[\\E[X]^2\\ri]\\\\\n  &= \\E\\li[X^2\\ri] - 2\\E[X]\\E[X]+\\E[X]^2\\\\\n  &= \\E\\li[X^2\\ri] - \\E[X]^2\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w02/exercises02.html",
    "href": "ex/w02/exercises02.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/linalg-evs.md"
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-1---eigenvectors-and-eigenvalues",
    "href": "ex/w02/exercises02.html#exercise-1---eigenvectors-and-eigenvalues",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/linalg-evs.md"
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-2---variance-and-expectation",
    "href": "ex/w02/exercises02.html#exercise-2---variance-and-expectation",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Variance and Expectation",
    "text": "Exercise 2 - Variance and Expectation\n!include questions/prob-evvar.md"
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-3---linear-regression",
    "href": "ex/w02/exercises02.html#exercise-3---linear-regression",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Linear Regression",
    "text": "Exercise 3 - Linear Regression\n!include questions/linreg.md"
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-4---gradients-and-computation-graphs",
    "href": "ex/w02/exercises02.html#exercise-4---gradients-and-computation-graphs",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - Gradients and Computation Graphs",
    "text": "Exercise 4 - Gradients and Computation Graphs\n!include questions/nn-compgraph.md"
  },
  {
    "objectID": "ex/w05/questions/opt-momentum-sol.html",
    "href": "ex/w05/questions/opt-momentum-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "An example implementation could look like this. First we deinfe the objective function and its gradient:\ndef objective(x):\n    return x**2\n\ndef obj_grad(x):\n    return 2*x\nThen, we implement the momentum optimizer itself:\ndef sgd_with_momentum(obj, grad, x_init, learning_rate, momentum, max_iter):\n  x = x_init\n  update = 0\n  for i in range(max_iter):\n    update = momentum * update - learning_rate * grad(x)\n    x = x + update\n\n    print('&gt;%d f(%s) = %.5f' % (i, x, obj(x)))\n  return x\nIt can now be invoked directly via:\nsgd_with_momentum(\n    objective, obj_grad, x_init=3.0, learning_rate=0.1, momentum=0.5, \n    max_iter=20\n    )"
  },
  {
    "objectID": "ex/w05/questions/linalg-evs_to_mat.html",
    "href": "ex/w05/questions/linalg-evs_to_mat.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "You are given the sets of eigevalues and eigenvectors. Compute the corresponding matrix.\n\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,0)^\\top\\), \\(\\fv_2 = (0,1)^\\top\\).\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,1)^\\top\\), \\(\\fv_2 = (1,-1)^\\top\\)."
  },
  {
    "objectID": "ex/w05/questions/calc-taylor-sol.html",
    "href": "ex/w05/questions/calc-taylor-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "There are different ways to write the second order taylor series expansion a at a point \\(\\fa\\) for multivariate functions \\(f\\). We will use the following form\n\\[T(x) = f(\\fa) + \\nabla f(\\fa)^\\top (\\fx-\\fa) + \\frac{1}{2}(\\fx-\\fa)^\\top \\fH (\\fx-\\fa)\\]\nwhere \\(\\nabla f(\\fa)\\) is the gradient of \\(f\\) at \\(\\fa\\) and \\(\\fH\\) is the Hessian of \\(f\\) at \\(\\fa\\). As a reminder, the Hessian is the matrix of second order partial derivatives. So, all we need to do for all of the exercises is evaluate \\(f(\\fa)\\) and compute its gradient and Hessian.\n\n\\(T(x) = 5 + 15 (x-1) + 15 (x-1)^2\\)\nFirst, we simplify \\(f(x,y)=x^2 (y^3+1)\\) and compute \\(f(3,2)=81\\) and then we compute the gradient and Hessian of \\(f\\) resulting in \\[\n\\nabla f(x,y) = \\bpmat 2x (y^3-1) \\\\ x^2(3y^2-1) \\epmat,\\quad\n\\fH = \\bpmat 2(y^3-1) & 6 xy^2 \\\\ 6xy^2 & 6x^2y \\epmat.\n\\]\nThis one is similar to (b) but we have to be careful with the logarithm. First, we have \\(f(\\fx_0)=0\\) because \\(\\log(1)=0\\). Then, we compute the gradient and Hessian of \\(f\\) resulting in \\[\n    \\nabla f(\\fx) = \\bpmat\n   3x_1^2 x_2 \\log(x_2) \\\\ x_1^3 (1+\\log(x_2))\n   \\epmat,\\quad\n    \\fH = \\bpmat\n   6x_1 x_2 \\log(x_2)   & 3x_1^2 (1+\\log(x_2)) \\\\\n   3x_1^2 (1+\\log(x_2)) & x_1^3 x_2^{-1}\n    \\epmat\n    \\]\nEvaluating the trigonometric functions here is simpler than it seems because they are evaluated at \\(\\pi\\) and \\(-\\pi\\). We get \\(f(\\fx_0) = \\sin(-\\pi) + \\cos(\\pi) = 0 - 1 = -1\\) and \\[\n\\nabla f(\\fx) = \\bpmat \\cos(x_1) \\\\ -\\sin(x_2) \\epmat, \\quad\n\\fH = \\bpmat -\\sin(x_1) & 0 \\\\ 0 & -\\cos(x_2) \\epmat.\n\\]"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html",
    "href": "ex/w05/exercises05_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/calc-taylor.md\n\n\n!include questions/calc-taylor-sol.md"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html#exercise-1---taylor-series",
    "href": "ex/w05/exercises05_solution.html#exercise-1---taylor-series",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/calc-taylor.md\n\n\n!include questions/calc-taylor-sol.md"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html#exercise-2---eigenvalues-eigenvectors",
    "href": "ex/w05/exercises05_solution.html#exercise-2---eigenvalues-eigenvectors",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Eigenvalues, Eigenvectors",
    "text": "Exercise 2 - Eigenvalues, Eigenvectors\n!include questions/linalg-evs_to_mat.md\n\nSolution\n!include questions/linalg-evs_to_mat-sol.md"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html#exercise-3---sgd-with-momentum",
    "href": "ex/w05/exercises05_solution.html#exercise-3---sgd-with-momentum",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - SGD with Momentum",
    "text": "Exercise 3 - SGD with Momentum\n!include questions/opt-momentum.md\n\nSolution\n!include questions/opt-momentum-sol.md"
  },
  {
    "objectID": "ex/w04/questions/cnn-sizes-sol.html",
    "href": "ex/w04/questions/cnn-sizes-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The outputs dimensions after each layer are:\n\n1. 98 x 98 x 5 \n2. 49 x 49 x 5\n3. 47 x 47 x 10\n4. 23 x 23 x 10\n5. 21 x 21 x 5 \n6. 2205\n7. 20\n8. 10\n\nThe number of parameters for each layer is:\n\n1. 140\n2. 0\n3. 460\n4. 0\n5. 455\n6. 0\n7. 44120\n8. 210"
  },
  {
    "objectID": "ex/w04/questions/nlp-co_occurrence-sol.html",
    "href": "ex/w04/questions/nlp-co_occurrence-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The solution is not unique because we can change the order of the reference entries. In our case we will simply follow the intuitive ordering resulting in the following co-occurence matrix:\n\n\n\nReference\nA\nbird\nin\nthe\nhand\nis\nworth\ntwo\nbush\n\n\n\n\nA\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nbird\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nin\n0\n0\n0\n2\n0\n0\n0\n0\n0\n\n\nthe\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nhand\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nis\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nworth\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\ntwo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nbush\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "ex/w04/questions/nlp-co_occurrence.html",
    "href": "ex/w04/questions/nlp-co_occurrence.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?"
  },
  {
    "objectID": "ex/w04/questions/cnn-by_hand-sol.html",
    "href": "ex/w04/questions/cnn-by_hand-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Here, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -1 & -3 \\\\\n2 & 3 &  3 \\\\\n5 & 2 &  1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -3 \\\\\n5 &  1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -2 & 1 & 0\\\\\n0 & 0 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 1 & 0\\\\\n0 & -3 & 4 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & -2 & 0 \\\\\n0 & 3  &  0 \\\\\n-3 & 8 & 0\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w04/exercises04.html",
    "href": "ex/w04/exercises04.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/nlp-co_occurrence.md"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-1---co-occurrence-matrix",
    "href": "ex/w04/exercises04.html#exercise-1---co-occurrence-matrix",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/nlp-co_occurrence.md"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-2---convolutional-layers",
    "href": "ex/w04/exercises04.html#exercise-2---convolutional-layers",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\n!include questions/cnn-by_hand.md"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-3---sizes-in-mlps-refresher",
    "href": "ex/w04/exercises04.html#exercise-3---sizes-in-mlps-refresher",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Sizes in MLPs Refresher",
    "text": "Exercise 3 - Sizes in MLPs Refresher\n!include questions/mlp-sizes.md"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-4---sizes-in-cnns",
    "href": "ex/w04/exercises04.html#exercise-4---sizes-in-cnns",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - Sizes in CNNs",
    "text": "Exercise 4 - Sizes in CNNs\n!include questions/cnn-sizes.md"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html",
    "href": "ex/w04/exercises04_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/nlp-co_occurrence.md\n\n\n!include questions/nlp-co_occurrence-sol.md"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-1---co-occurrence-matrix",
    "href": "ex/w04/exercises04_solution.html#exercise-1---co-occurrence-matrix",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/nlp-co_occurrence.md\n\n\n!include questions/nlp-co_occurrence-sol.md"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-2---convolutional-layers",
    "href": "ex/w04/exercises04_solution.html#exercise-2---convolutional-layers",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\n!include questions/cnn-by_hand.md\n\nSolution\n!include questions/cnn-by_hand-sol.md"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-3---sizes-in-mlps-refresher",
    "href": "ex/w04/exercises04_solution.html#exercise-3---sizes-in-mlps-refresher",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Sizes in MLPs Refresher",
    "text": "Exercise 3 - Sizes in MLPs Refresher\n!include questions/mlp-sizes.md\n\nSolution\n!include questions/mlp-sizes-sol.md"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-4---sizes-in-cnns",
    "href": "ex/w04/exercises04_solution.html#exercise-4---sizes-in-cnns",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - Sizes in CNNs",
    "text": "Exercise 4 - Sizes in CNNs\n!include questions/cnn-sizes.md\n\nSolution\n!include questions/cnn-sizes-sol.md"
  },
  {
    "objectID": "ex/w03/questions/autodiff-modes.html",
    "href": "ex/w03/questions/autodiff-modes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Consider the function \\(F(x) = f_3(f_2(f_1(x))\\) and assume you also know the derivatives \\(f_i'\\) for all \\(f_i\\).\n\nApply the chain rule to express \\(F'(x)\\) in terms of \\(f_i'\\)s and \\(f_i\\).\nWrite down the pseudocode for computing \\(F'(x)\\) using the forward mode and the reverse mode respectively. Assume all functions to be scalar functions of a scalar variable, i.e. \\(f_i: \\R \\rightarrow \\R\\).\nIf you simply ask your interpreter / compiler to evaluate the expression in (a), will the computation be in forward mode, reverse mode, or neither of the modes? Why? You can assume that your interpreter / compiler does not do any caching or optimization and simply evaluates the expression from left to right. Does anything change if you assume that your interpreter caches results that have been computed before?"
  },
  {
    "objectID": "ex/w03/questions/prob-mle.html",
    "href": "ex/w03/questions/prob-mle.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean."
  },
  {
    "objectID": "ex/w03/questions/prob-mle-sol.html",
    "href": "ex/w03/questions/prob-mle-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters). The product on the right hand side is also known as the likelihood function. In practice, we usually work with the log-likelihood function instead. Because the logarithm is monotonously increasing, the resulting estimators are the same, i.e. \\[\n\\hte_{\\mathrm{MLE}}\n= \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n= \\argmax_{\\te \\in \\Te} \\sum_{i=1}^N \\log \\li( f(\\fx_i | \\te)\\ri).\n\\]\nSecond, let’s remember that the probability density function of a multivariate Gaussian distribution is given by \\[\nf(\\fx_i | \\mu, \\Si)\n= \\frac{1}{(2 \\pi)^{n/2} |\\Si|^{1/2}}\n   \\exp \\li(\n     - \\frac{1}{2}\n     (\\fx_i - \\mu)^\\top\n     \\Si^{-1}\n     (\\fx_i - \\mu)\n   \\ri)\n\\] with parameters \\(\\te=(\\mu, \\Si)\\), where \\(\\mu\\in\\R^n\\) is the mean vector and \\(\\Si\\in\\R^{n\\times n}\\) is the covariance matrix. Moreover, the notation \\(|\\Si|\\) denotes the determinant of \\(\\Si\\).\nOur goal is to maximize the log-likelihood function which in this case is \\[\\begin{aligned}\nl(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n  & := \\sum_{i=1}^N \\log f(\\fx_i | \\mu, \\Si) \\\\\n  & = \\sum_{i=1}^N \\li(\n    - \\frac{n}{2} \\log (2 \\pi)\n    - \\frac{1}{2} \\log |\\Si|  \n    - \\frac{1}{2}  (\\fx_i - \\mu)^\\top \\Si^{-1} (\\fx_i - \\mu)\n  \\ri).\n\\end{aligned}\\] For obtaining the MLE of \\(\\mu\\), we can simply take the gradient of \\(l\\) with respect to \\(\\mu\\) and set it to zero resulting in: \\[\n\\nabla_\\mu l(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n= \\sum_{i=1}^N  \\Si^{-1} ( \\fx_i - \\mu )\n= 0\n\\] Since \\(\\Si\\) is a covariance matrix, it is positive definite. Thus, we can multiply both sides of the equation by \\(\\Si\\) and obtain \\[\n\\begin{aligned}\n0 & = N \\mu - \\sum_{i=1}^N  \\fx_i,\n\\\\\n\\Rightarrow \\hmu_{\\mathrm{MLE}} &=  \\frac{1}{N} \\sum_{i=1}^N \\fx_i .\n\\end{aligned}\n\\] Conveniently, \\(\\Si\\) disappears and thus we do not have to worry about it."
  },
  {
    "objectID": "ex/w03/exercises03.html",
    "href": "ex/w03/exercises03.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/prob-mle.md"
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-1---maximum-likelihood-estimation-refresher",
    "href": "ex/w03/exercises03.html#exercise-1---maximum-likelihood-estimation-refresher",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/prob-mle.md"
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-2---more-gradients",
    "href": "ex/w03/exercises03.html#exercise-2---more-gradients",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - More Gradients",
    "text": "Exercise 2 - More Gradients\n!include questions/backprop-batchnorm.md"
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-3---autodiff-modes",
    "href": "ex/w03/exercises03.html#exercise-3---autodiff-modes",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Autodiff Modes",
    "text": "Exercise 3 - Autodiff Modes\n!include questions/autodiff-modes.md"
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-4---glove-embeddings",
    "href": "ex/w03/exercises03.html#exercise-4---glove-embeddings",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - GloVe Embeddings",
    "text": "Exercise 4 - GloVe Embeddings\nOpen the notebook presented in class and work through it by trying some of the ideas presented therein for different word combinations."
  },
  {
    "objectID": "course-syllabus-BE4.html",
    "href": "course-syllabus-BE4.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus."
  },
  {
    "objectID": "course-syllabus-BE4.html#course-info",
    "href": "course-syllabus-BE4.html#course-info",
    "title": "Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\n\n\n\n\n\n\nLectures\nProf\nDay\nTime\nLocation\n\n\n\n\nLEC0101\nIgor Gilitschenski\nTuesday\n5:00 pm - 7:00 pm\nMN3190\n\n\nLEC0102\nFlorian Shkurti\nWednesday\n11:00 am - 1:00 pm\nMN3190\n\n\n\n\n\n\nTutorials/Labs\nDay\nTime\nLocation\n\n\n\n\nPRA0101\nFriday\n10:00 am - 11:00 am\nDH2020\n\n\nPRA0102\nFriday\n11:00 am - 12:00 pm\nDH2020\n\n\nPRA0103\nFriday\n12:00 pm - 1:00 pm\nDH2020"
  },
  {
    "objectID": "course-syllabus-BE4.html#textbooks-other-materials",
    "href": "course-syllabus-BE4.html#textbooks-other-materials",
    "title": "Syllabus",
    "section": "Textbooks & Other Materials",
    "text": "Textbooks & Other Materials\nThere is no required textbook for the class. We will primarily use the CSC413 Course Notes written by Prof. Roger Grosse, to be posted on the course website. A few small readings may be assigned if the need arises. These required readings will all be available on the web, for free. There are also some relevant resources which are freely available online. We will try to provide links on a lecture-by-lecture basis\n\nVideo lectures for UofT Professor Geoffrey Hinton’s Coursera course. Professor Hinton is one of the fathers of the field, so think of these as the Feynman Lectures of neural nets.\nDeep Learning, a textbook by Yoshua Bengio, Ian Goodfellow, and Aaron Courville.\nAndrej Karpathy’s lecture notes on convolutional networks. These are very readable and cover the material in roughly the first half of the course.\nRichard Socher’s lecture notes, focusing on RNNs.\nVideo lectures for Hugo Larochelle’s neural networks course. These are similar to Professor Hinton’s lectures but a bit more mathematical.\nDavid MacKay’s textbook, Information Theory, Inference, and Learning Algorithms. This isn’t focused on neural nets per se, but it has some overlap with this course, especially the lectures on Bayesian models.\nDavid Barber’s textbook, Bayesian Reasoning and Machine Learning. This book also isn’t focused on neural nets, but also covers a lot of materials on Bayesian methods.\nDive Into Deep Learning An online deep learning textbook with sample code"
  },
  {
    "objectID": "course-syllabus-BE4.html#assessments-deadlines",
    "href": "course-syllabus-BE4.html#assessments-deadlines",
    "title": "Syllabus",
    "section": "Assessments & Deadlines",
    "text": "Assessments & Deadlines\n\nGrading\n\n\n\n\n\n\n\n\n\nType\nDescription\nDue Date\nWeight\n\n\n\n\nLab\nTutorial/Lab Activities (7x 4% and 1x 2%)\nOn-going\n30%\n\n\nAssignment\nMath assignment 1 (done individually)\n2024-10-11\n10%\n\n\nAssignment\nMath assignment 2 (done individually)\n2024-11-01\n10%\n\n\nTerm Test\nMidterm test 1 (during tutorials)\n2024-10-18\n10%\n\n\nTerm Test\nMidterm test 2 (during tutorials)\n2024-11-15\n10%\n\n\nAssignment\nFinal Project Proposal (done in groups of 2-4)\n2024-11-06\n10%\n\n\nAssignment\nFinal Project (done in groups of 2-4)\n2024-12-05\n20%\n\n\n\n\n\nAssignments\nThere are two assignments consisting of theoretical/math problems. In addition to correctness, you are graded on design, proof quality, and the quality of your written explanations. All assignments are to be submitted on Markus by 6pm ET on the specified date.\n\n\nMidterm Tests\nThere are two midterm tests to be held during the tutorials, each weighted equally. Each term test will be 50 minutes long. You must write these tests during your assigned tutorial time, or speak to the course coordinator in advance to write during a different time.\n\n\nTutorial / Lab Activities\nThese are practical lab activities where coding exercises will be submitted for credit, each worth 4% (except for the first one, which is worth 2%). These are to be completed individually. However, you may receive assistantce from the TA during the tutorial / lab.\n\n\nFinal Project\nThe final group project will is an implementation project that will replace the final exam. Students are expected to build and analyze a Recurrent Neural Network (RNN) or Transformer model that solves a deep learning problem related to sequences (e.g. classifying or generating a sequence). In the final project report members will declare what each of them contributed to the project (who implemented what).\n\n\nRemark Requests\nRe-mark requests must be submitted within 7 days of receiving the graded work and must describe the exact issue with the grading. We want to fix mistakes while discouraging frivolous requests, so please read the entire policy before sending a request:\n\nIf there is a clear typo unrelated to the course material (e.g. the TA made an arithmetic error), then we will fix the mistake without re-reading the entire work. The request may be processed before the 7 days deadline.\nIf you think the TA did not follow the marking scheme, your entire work will be reviewed. Mistakes missed by the original grader may be caught by the new grader, so your grade may go up, down, or stay the same. We will generally wait until the 7 days have passed to process all remark requests at the same time.\nIf you would like us to grade using a different marking scheme, that is not possible. We use the same marking scheme so that we can be as consistent as we can and be fair to every student."
  },
  {
    "objectID": "course-syllabus-BE4.html#penalties-for-lateness",
    "href": "course-syllabus-BE4.html#penalties-for-lateness",
    "title": "Syllabus",
    "section": "Penalties for Lateness",
    "text": "Penalties for Lateness\nAll work in this course are due 6pm ET and are to be submitted on Markus. Assignments submitted electronically will be timestamped based on the server time, not the student’s local/PC time.\nWe will be using a grace token system. Each student will begin with 12 tokens. Each token can be used to extend the assignment deadline by 12 hours. Additionally:\n\nAssignments: You can use a maximum of 6 tokens to extend the deadline of an assignment.\nTutorial / Lab Activities: You can use a maximum of 2 tokens to extend the deadline of a tutorial activity.\nProject: You can use as many tokens as you have remaining to extend the deadline of the final project.\n\nMarkUs automatically deducts grace tokens when you submit an assignment late—you do not need to explicitly say you are using a grace token, just submit your work within the grace token period. If you work with a partner or a group, grace tokens are deducted from all group members, not just one of you. For example, if Alice and Bob are working together, and wish to submit a project 24 hours late, they must both have at least two grace tokens remaining."
  },
  {
    "objectID": "course-syllabus-BE4.html#procedures-and-rules",
    "href": "course-syllabus-BE4.html#procedures-and-rules",
    "title": "Syllabus",
    "section": "Procedures and Rules",
    "text": "Procedures and Rules\n\nMissed Term Work\nFor missed term work such as a missed in-person midterm or in-person tutorial/lab activity, students must provide valid documentation such as the UTM Verification of Illness or Injury. The documentation must be sent to florian@cs.toronto.edu and i.gilitschenski@utoronto.ca within 72 hours of the missed activity.\nOnce per semester, at any one time except for the in-person midterms and tutorial/lab activities, each student is allowed to miss work without any documentation. In that case you must fill out the ACORN absence declaration form. The form can be used at most ONCE per semester (once in total for all of your courses, not once per course). The absence you declare can be for a maximum of 7 consecutive days. If you use the ACORN absence declaration form, you do not need to submit any documentation for missed work during that absence. Note that the ACORN absence declaration can not be used for in-person midterms or tutorial/lab activities. If you miss the in-person midterms or tutorial/lab activities you must submit documentation such as the UTM Verification of Illness or Injury.\nFor missed in-person term work with valid documentation or declaration, your term work will be shifted to one of the midterms. For missed assignments or projects, which you had a week or more to complete, no accommodations will be given, except if you have highly extenuating circumstances (for example if had to be hospitalized for a long period of time).\nNote: You should check all your course outlines carefully because different courses may have different policies."
  },
  {
    "objectID": "course-syllabus-BE4.html#academic-integrity",
    "href": "course-syllabus-BE4.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAcademic integrity is essential to the pursuit of learning and scholarship in a university, and to ensuring that a degree from the University of Toronto Mississauga is a strong signal of each student’s individual academic achievement. As a result, UTM treats cases of cheating and plagiarism very seriously. The University of Toronto’s Code of Behaviour on Academic Matters outlines the behaviours that constitute academic dishonesty and the process for addressing academic offences. Potential offences include, but are not limited to:\nIn papers and assignments:\n\nUsing someone else’s ideas or words without appropriate acknowledgement.\nSubmitting your own work in more than one course, or more than once in the same course, without the permission of the instructor.\nMaking up sources or facts.\nObtaining or providing unauthorized assistance on any assignment.\n\nOn tests and exams:\n\nUsing or possessing unauthorized aids.\nLooking at someone else’s answers during an exam or test.\nMisrepresenting your identity.\n\nIn academic work:\n\nFalsifying institutional documents or grades.\nFalsifying or altering any documentation required, including (but not limited to) doctor’s notes.\n\nKeep in mind that the department uses software that compares programs for evidence of similar code. Below are some tips to help you avoid committing an academic offence, like plagiarism. - Never look at another student’s lab/assignment solution(s). Never show another student your lab/assignment solution. This applies to all drafts of a solution and to incomplete and even incorrect solutions. - Keep discussions with other students focused on concepts and examples. Never discuss labs/assignments before the due date with anyone but your Instructors and your TAs. - Do not discuss your solution publicly on the discussion board or publicly in the lab rooms/office hours.\nAll suspected cases of academic dishonesty will be investigated following procedures outlined in the Code of Behaviour on Academic Matters. If you have questions or concerns about what constitutes appropriate academic behaviour or appropriate research and citation methods, you are expected to seek out additional information on academic integrity from your instructor or from other institutional resources.\nYou may not discuss the assignments with anyone other than your instructors, your teaching assistants, and your partners. The only exceptions are:\n\nAsking and answering questions on Piazza. Please do not share partial solutions or code on Piazza.\nDuring office hours, under the supervision of an instructor or teaching assistant.\nDuring certain tutorials, under the supervision of an instructor or teaching assistant.\n\nStudents are encouraged to make use of technology, including generative artificial intelligence tools, to contribute to their understanding of course materials.\n\nStudents may use artificial intelligence tools, including generative AI, in this course as learning aids or to help produce assignments. However, students are ultimately accountable for the work they submit.\nStudents must submit, as an appendix with their assignments, any content produced by an artificial intelligence tool, and the prompt used to generate the content.\nAny content produced by an artificial intelligence tool must be cited appropriately. Many organizations that publish standard citation formats are now providing information on citing generative AI (e.g., MLA: https://style.mla.org/citing-generative-ai/ ).\nStudents may choose to use generative artificial intelligence tools as they work through the assignments in this course; this use must be documented in an appendix for each assignment. The documentation should include what tool(s) were used, how they were used, and how the results from the AI were incorporated into the submitted work.\nCourse instructors reserve the right to ask students to explain their process for creating their assignment.\n\n\nPlagiarism Detection\nNormally, students will be required to submit their course essays to the University’s plagiarism detection tool for a review of textual similarity and detection of possible plagiarism. In doing so, students will allow their essays to be included as source documents in the tool’s reference database, where they will be used solely for the purpose of detecting plagiarism. The terms that apply to the University’s use of this tool are described on the Centre for Teaching Support & Innovation web site (https://uoft.me/pdtfaq).\nStudents may wish to opt out of using the plagiarism detection tool. In order to opt out, contact your instructor by email no later than two (2) weeks after the start of classes. If you have opted out, then specific information on an alternative method to submit your assignment can be found below.\n\n\nInformed Consent – Email Lists\nAs a student enrolled in this course, you understand that you are providing your implicit consent to be included in an email list for the department to send you non-essential information from time to time. If you do not wish to be included in such an email list, please request to be removed by contacting one of the Academic Advisors & Undergraduate Program Administrators. Their information can be found on the MCS Website Contact Us page."
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "Important\n\n\n\nDue dates: Friday, April 15 - Project peer review of drafts"
  },
  {
    "objectID": "weeks/week-14.html#participate",
    "href": "weeks/week-14.html#participate",
    "title": "Week 14",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 26 - MultiLR: Predictive models (cont.)\n🖥️ Lecture 27 - Exam 3 Review"
  },
  {
    "objectID": "weeks/week-14.html#practice",
    "href": "weeks/week-14.html#practice",
    "title": "Week 14",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 12 - Exam 3 Review"
  },
  {
    "objectID": "weeks/week-14.html#perform",
    "href": "weeks/week-14.html#perform",
    "title": "Week 14",
    "section": "Perform",
    "text": "Perform\n✍️ Project - Peer review of drafts\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\nDue date: Lab 4 - Friday, Feb 25"
  },
  {
    "objectID": "weeks/week-8.html#prepare",
    "href": "weeks/week-8.html#prepare",
    "title": "Week 8",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Tidy Modeling in R Chp 10: Resampling for evaluating performance"
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 14 - MLR: Cross validation\n🖥️ Lecture 15 - Exam 2 review"
  },
  {
    "objectID": "weeks/week-8.html#practice",
    "href": "weeks/week-8.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\nApplication Exercise 6 - The office - CV\nApplication Exercise 7 - Exam 2 Review"
  },
  {
    "objectID": "weeks/week-8.html#perform",
    "href": "weeks/week-8.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 4 - The Office, another look\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important\n\n\n\nDue dates: None."
  },
  {
    "objectID": "weeks/week-12.html#prepare",
    "href": "weeks/week-12.html#prepare",
    "title": "Week 12",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Chp 26: Inference for logistic regression"
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 22 - LR: Inference + conditions\n🖥️ Lecture 23 - Multinomial logistic regression (MultiLR)"
  },
  {
    "objectID": "weeks/week-12.html#practice",
    "href": "weeks/week-12.html#practice",
    "title": "Week 12",
    "section": "Practice",
    "text": "Practice\nNo application exercises this week."
  },
  {
    "objectID": "weeks/week-12.html#perform",
    "href": "weeks/week-12.html#perform",
    "title": "Week 12",
    "section": "Perform",
    "text": "Perform\n✍️ HW 4 - Multinomial logistic regression\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nLab 5 - Friday, Mar 25\nHW 3 - Friday, Mar 25"
  },
  {
    "objectID": "weeks/week-11.html#prepare",
    "href": "weeks/week-11.html#prepare",
    "title": "Week 11",
    "section": "Prepare",
    "text": "Prepare\nNo additional readings this week. Catch up with previously assigned readings if you’ve fallen behind."
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 20 - LR: Prediction / classification\n🖥️ Lecture 21 - LR: Model validation"
  },
  {
    "objectID": "weeks/week-11.html#practice",
    "href": "weeks/week-11.html#practice",
    "title": "Week 11",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 10 - Flight delays"
  },
  {
    "objectID": "weeks/week-11.html#perform",
    "href": "weeks/week-11.html#perform",
    "title": "Week 11",
    "section": "Perform",
    "text": "Perform\n✍️ HW 3 - Logistic regression and log transformation\n⌨️ Lab 5 - General Social Survey\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nClasses are virtual this week. Find Zoom links here.\nDue dates:\n\nLab 1: Fri, Jan 14, 5pm ET\nAE 1: Sun, Jan 16, 11:59pm ET"
  },
  {
    "objectID": "weeks/week-2.html#prepare",
    "href": "weeks/week-2.html#prepare",
    "title": "Week 2",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Chp 7: Linear regression with a single predictor"
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\n🖥️ Lab 1 - Meet the toolkit\n🖥️ Lecture 2 - Simple linear regression\n🖥️ Lecture 3 - Model fitting in R with tidymodels"
  },
  {
    "objectID": "weeks/week-2.html#practice",
    "href": "weeks/week-2.html#practice",
    "title": "Week 2",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 1 - Bike rentals in DC (Post-class note: complete only Part 1 - Daily counts and temperature)"
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 1 - Meet the toolkit\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\nClasses are virtual this week. Find Zoom links here."
  },
  {
    "objectID": "weeks/week-1.html#prepare",
    "href": "weeks/week-1.html#prepare",
    "title": "Week 1",
    "section": "Prepare",
    "text": "Prepare\n📖 Read the syllabus\n📖 Read the support resources"
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\n🖥️ Lab 0 -Meet+ greet\n🖥️ Lecture 1 - Welcome to STA 210"
  },
  {
    "objectID": "weeks/week-1.html#practice",
    "href": "weeks/week-1.html#practice",
    "title": "Week 1",
    "section": "Practice",
    "text": "Practice\n📋 AE 0 - Movies"
  },
  {
    "objectID": "weeks/week-1.html#perform",
    "href": "weeks/week-1.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 0 - Meet + greet\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 2 - Fri, Feb 4, 5pm ET\nExam 1 released on Fri, Feb 4, due Mon, Feb 7 at 11:59pm"
  },
  {
    "objectID": "weeks/week-5.html#prepare",
    "href": "weeks/week-5.html#prepare",
    "title": "Week 5",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Sec 8.1: Indicator and categorical predictors\n📖 Read Introduction to Modern Statistics, Sec 8.2: Many predictors in a model\n📖 Read Introduction to Modern Statistics, Sec 8.3: Adjusted R-squared"
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\n🖥️ Lab 3 - Coffee ratings\n🖥️ Lecture 8 - Multiple linear regression (MLR)\n🖥️ Lecture 9 - Exam 1 review"
  },
  {
    "objectID": "weeks/week-5.html#practice",
    "href": "weeks/week-5.html#practice",
    "title": "Week 5",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 4 - Exam 1 Review"
  },
  {
    "objectID": "weeks/week-5.html#perform",
    "href": "weeks/week-5.html#perform",
    "title": "Week 5",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 3 - Coffee ratings\n✅ Exam 1\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\nDue dates: Exam 1 - Mon, Feb 7 at 11:59pm"
  },
  {
    "objectID": "weeks/week-6.html#prepare",
    "href": "weeks/week-6.html#prepare",
    "title": "Week 6",
    "section": "Prepare",
    "text": "Prepare\nNo reading (take a break after the exam! 😴)"
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 10 - MLR: Types of predictors\n🖥️ Lecture 11 - MLR: Model comparison"
  },
  {
    "objectID": "weeks/week-6.html#practice",
    "href": "weeks/week-6.html#practice",
    "title": "Week 6",
    "section": "Practice",
    "text": "Practice\nNo application exercises"
  },
  {
    "objectID": "weeks/week-6.html#perform",
    "href": "weeks/week-6.html#perform",
    "title": "Week 6",
    "section": "Perform",
    "text": "Perform\nNo lab\n✅ Exam 1\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful Resources and Links",
    "section": "",
    "text": "Recommended Simulators\nYou are encouraged to use the simplest possible simulator to accomplish the task you are interested in. You can submit links to simulators not included here by opening a github issue.\n\n\n\nSimulately\nA detailed wiki comparing various widely-used robotics simulators. Read this first. The rest of this table shows simulators and environments not mentioned by Simulately.\n\n\nIsaac Lab (formerly Isaac Orbit / Isaac Gym)\nLayer of abstraction and tools to make using Isaac Sim easier.\n\n\nDrake Simulator\nA framework of simulation, analysis and control tools for robotics.\n\n\nDeepmind Control Suite\nSet of robotics environments on top of Mujoco.\n\n\nMujoco Menagerie\nHigh-quality description files and assets for robots, built on top of Mujoco.\n\n\nOpenAI Gym\nAtari, Mujoco, classic control, and third-party environments for RL.\n\n\nRoboSuite\nRobotics simulation environments on top of Mujoco. Also a benchmark.\n\n\nKlampt\nModeling, simulating, planning, and optimization for complex robots, particularly for manipulation and locomotion tasks.\n\n\nDART\nPhysics simulator for robotics and animation.\n\n\nCARLA\nSelf-driving environment and benchmarks on top of the Unreal simulation engine.\n\n\nAirSim\nRobotics simulation environments for flying and driving, built on top of Unreal engine.\n\n\ngym-pybullet-drones\nRobotics simulation environments and tools for quadrotors on top of PyBullet.\n\n\nHabitat 3.0\nSimulation of indoor scenes, humans, and robots. Good for visual navigation and social navigation tasks.\n\n\nGPUDrive\nGPU-accelerated multi-agent driving simulator.\n\n\nProcGen\nProcedurally generated simulation environments (not robotics, but useful).\n\n\nRaiSim\nRigid body physics engine. Supports biomechanics of human motion, as well as quadrupeds.\n\n\nFlightmare\nSimulation environment for flying vehicles built on top of the Unity simulation engine.\n\n\nIKEA Furniture Assembly\nIKEA furniture assembly environment.\n\n\nFurnitureBench\nSimulators, datasets, and real environments for furniture assembly\n\n\nRLBench\nSimulation environments for manipulation, built on top of the CoppeliaSim simulator.\n\n\nALFRED\nSimulation environments for visual and language-based navigation and manipulation tasks.\n\n\nMyoSuite\nMuscosceletal simulation environments for biomechanics, based on Mujoco.\n\n\nMetaWorld\nMulti-task RL environments and benchmarks.\n\n\nBimanual Manipulation Gym\nBimanual manipulation environments\n\n\n\n\n\nRecommended datasets\n\n\n\nSimulately\nA detailed wiki comparing various widely-used robotics datasets. Read this first. The rest of this table shows datasets not mentioned by Simulately.\n\n\nD4RL\nManipulation and navigation datasets for offline RL\n\n\nRoboMimic\nManipulation datasets and imitation learning algorithms\n\n\nMimicGen\nAutomatic augmentation of manipulation datasets starting from human demonstrations\n\n\nOptimus\nAutomatically generating long-horizon manipulation dataset from Task and Motion Planners.\n\n\nDROID\nManipulation dataset across various labs and robots\n\n\nD4RL\nManipulation and navigation datasets for offline RL\n\n\n\n\n\nRecommended RL, IL, trajectory optimization, and motion planning libraries\n\n\n\nSimulately\nA detailed wiki comparing various widely-used RL libaries. Read this first. The rest of this table shows libraries not mentioned by Simulately.\n\n\nRSL RL\nRL library used for training quadrupeds at the RSL lab at ETHZ. Used in Isaac Lab.\n\n\nSTORM\nMPC motion planner on the GPU\n\n\nOMPL\nOpen motion planning library\n\n\nMink\nInverse kinematics library, built on top of pink and pinocchio\n\n\nPureJaxRL\nRL library in JAX, with training and environments running fully on GPU\n\n\nCleanRL\nClean implementations of Online RL baselines\n\n\nClean Offline RL\nClean implementations of Offline RL baselines\n\n\nrliable\nMethod and library for reliable evaluation of RL algorithms\n\n\nDiffusion policy\nImplementation of diffusion policy in action space for imitation learning\n\n\nImplicit behavior cloning\nImplementation of behavior cloning with energy based models\n\n\nTheseus\nA library for differentiable nonlinear optimization in Pytorch\n\n\nModel-based RL algorithms\nList of model-based RL algorithms"
  },
  {
    "objectID": "labs/lab02.html",
    "href": "labs/lab02.html",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "",
    "text": "In this lab, we will build a neural network that can predict the next word in a sentence given the previous three. We will apply an idea called weight sharing to go beyond the multi-layer perceptrons that we discussed in class.\nWe will also solve this problem problem twice: once in numpy, and once using PyTorch. When using numpy, you’ll implement the backpropagation computation manually.\nThe prediction task is not very interesting on its own, but in learning to predict subsequent words given the previous three, our neural networks will learn about how to represent words. In the last part of the lab, we’ll explore the vector representations of words that our model produces, and analyze these representations.\nAcknowledgements:\n\nBased on an assignment by George Dahl, Jing Yao Li, and Roger Grosse\nModification by Lisa Zhang\n\n\n\nClick “Working Alone” on Markus.\nSubmit the ipynb file lab02.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your implementation of convert_words_to_indices (1 point)\nPart 2. Your implementation of do_forward_pass (4 points)\nPart 3. Your implementation of do_backward_pass (3 points)\nPart 3. The output of the gradient checking code (1 point)\nPart 4. Your explanation of why each row of model.Ww corresponds to a word representation (1 point)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nWe will begin by downloading the data onto Google Colab.\n# Download lab data file\n!wget https://www.cs.toronto.edu/~lczhang/413/raw_sentences.txt\nWith any machine learning problem, the first thing that we would want to do is to get an intuitive understanding of what our data looks like. The following code reads the sentences in our file, split each sentence into its individual words, and stores the sentences (list of words) in the variable sentences.\nsentences = []\nfor line in open('raw_sentences.txt'):\n    words = line.split()\n    sentence = [word.lower() for word in words]\n    sentences.append(sentence)\nThere are 97,162 sentences in total, and these sentences are composed of 250 distinct words.\nvocab = set([w for s in sentences for w in s])\nprint(len(sentences)) # 97162\nprint(len(vocab)) # 250\nWe will separate our data into training, validation, and test. We will use 10,000 sentences for test, 10,000 for validation, and the remaining for training.\n# First, randomly shuffle the sentences in case these sentences are\n# temporally correlated (i.e., so that our train/val/test sets have\n# equal probability of getting the earlier vs later sentences)\nimport random\nrandom.seed(10)\nrandom.shuffle(sentences)\n\ntest, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]\nTask: To get an understanding of the data set that we are working with, start by printing 10 sentences in the training set. How are punctuated treated in this word representation? What about words with apostrophes?\n# TODO: Your code goes here\nfor i, s in enumerate(sentences): # SOLUTION\n    print(s)            # SOLUTION\n    if i &gt;= 10:         # SOLUTION\n        break           # SOLUTION\nIt is also a good idea to explore the distributional properties of the data.\nTask: The length of the sentences affects the types of modeling we can perform on the data. If the sentences are too short, then using a model that depends on many previous words would not make sense. Run the below code, which computes the length of the shortest, average, and longest sentences in the training set.\nsentence_lengths = [len(s) for s in train]\nprint(\"Shortest Sentence\", np.min(sentence_lengths))\nprint(\"Average Sentence\", np.mean(sentence_lengths))\nprint(\"Longest Sentence\", np.max(sentence_lengths))\nTask: How many words are in the training set? In general, there may be words in the validation/test sets that are not in training!\n# TODO: Write code to perform the computation\nvocab = set([w for s in train for w in s]) # SOLUTION\nprint(len(vocab)) # SOLUTION\nTask: What is the most common word in the training set? How often does this word appear in the training set? This information is useful to know since it helps us understand the difficult of the word prediction problem. In other words, this figure represents the accuracy of a “baseline” model that simply returns the most common word as the prediction for what the next word should be!\nfrom collections import Counter\n\ntotal_words = 0        # count number of words in the training set\nword_count = Counter() # count the occurrence of each word\nfor s in train:\n    for w in s:\n        word_count[w] += 1\n        total_words += 1\n\n# TODO: find the most common word\nprint(word_count.most_common(1)) # ('.', 64303) # SOLUTION\nprint(64303/total_words)                        # SOLUTION\nNow that we understand a bit about the distributional properties of our data, we can move on to representing our data numerically in a way that a neural network can use.\nWe will use a one-hot encoding to represent words. Alternatively, you can think of what we’re doing as assigning each word to a unique integer index. We will need some functions that converts sentences into the corresponding word indices.\nGraded Task: Complete the helper functions convert_words_to_indices. The functions generate_4grams and process_data have been written for you. The process_data function will take a list of sentences (i.e. list of list of words), and generate an \\(N \\times 4\\) numpy matrix containing indices of 4 words that appear next to each other. You can use the constants vocab, vocab_itos, and vocab_stoi in your code.\n# A list of all the words in the data set. We will assign a unique \n# identifier for each of these words.\nvocab = sorted(list(set([w for s in train for w in s])))\n# A mapping of index =&gt; word (string)\nvocab_itos = dict(enumerate(vocab))\n# A mapping of word =&gt; its index\nvocab_stoi = {word:index for index, word in vocab_itos.items()}\n\ndef convert_words_to_indices(sents):\n    \"\"\"\n    This function takes a list of sentences (list of list of words)\n    and returns a new list with the same structure, but where each word\n    is replaced by its index in `vocab_stoi`.\n\n    Example:\n    &gt;&gt;&gt; convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'],\n                                  ['other', 'one', 'since', 'yesterday'],\n                                  ['you']])\n    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n    \"\"\"\n    indices = []\n    # TODO: Write your code here\n    for s in sents:                                # SOLUTION\n        indices.append([vocab_stoi[w] for w in s]) # SOLUTION\n    return indices\n\ndef generate_4grams(seqs):\n    \"\"\"\n    This function takes a list of sentences (list of lists) and returns\n    a new list containing the 4-grams (four consecutively occurring words)\n    that appear in the sentences. Note that a unique 4-gram can appear multiple\n    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n\n    Example:\n\n    &gt;&gt;&gt; generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n    &gt;&gt;&gt; generate_4grams([[1, 1, 1, 1, 1]])\n    [[1, 1, 1, 1], [1, 1, 1, 1]]\n    \"\"\"\n    grams = []\n    for seq in seqs:\n        for i in range(len(seq) - 4):\n            grams.append(seq[i:i+4])\n    return grams\n\ndef process_data(sents):\n    \"\"\"\n    This function takes a list of sentences (list of lists), and generates an\n    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n    \"\"\"\n    indices = convert_words_to_indices(sents)\n    fourgrams = generate_4grams(indices)\n    return np.array(fourgrams)\n\ntrain4grams = process_data(train)\nvalid4grams = process_data(valid)\ntest4grams = process_data(test)\nTask: We are almost ready to discuss the model. Review the following helper functions, which has been written for you:\n\nmake_onehot, which converts indices\n\ndef make_onehot(indicies, total=250):\n    \"\"\"\n    Convert indicies into one-hot vectors.\n\n    Parameters:\n        `indices` - a numpy array of any shape (e.g. `[N, 3]` where `N`\n                    is the batch size)\n        `total` - an integer describing the total number of possible classes\n                  (maximum possible value in `indicies`)\n\n    Returns: a one-hot representation of the input numpy array \n             (If the input is of shape `[X, Y]`, then the output would\n             be of shape `[X, Y, total]` and consists of 0's and 1's)\n    \"\"\"\n    # create an identity matrix of shape [total, total]\n    I = np.eye(total)\n    # index the appropriate columns of that identity matrix\n    return I[indicies]\n\ndef softmax(x):\n    \"\"\"\n    Compute the softmax of vector x, or row-wise for a matrix x.\n    We subtract x.max(axis=0) from each row for numerical stability.\n\n    Parameters:\n        `x` - a numpy array shape `[N, num_classes]`\n\n    Returns: a numpy array of the same shape as the input.\n    \"\"\"\n    x = x.T\n    exps = np.exp(x - x.max(axis=0))\n    probs = exps / np.sum(exps, axis=0)\n    return probs.T\nThere is one more data processing function that we need, which turns the four-grams into inputs (consisting of the one-hot representations of the first 3 words), and the target output (the index of the 4th word).\nSince the one-hot representation is not memory efficient, we will only convert data into this representation when required, and only do so at a minibatch level.\ndef get_batch(data, range_min, range_max, onehot=True):\n    \"\"\"\n    Convert one batch of data (specifically, `data[range_min:range_max]`)\n    in the form of 4-grams into input and output data and return the\n    training data.\n\n    Parameters:\n        `data` - a numpy array of shape [N, 4] produced by a call\n                 to the function `process_data`\n        `range_min` - the starting index of the minibatch\n        `range_max` - the ending index of the minibatch, with\n                      range_max &gt; range_min and\n                      batch_size = range_max - range_min\n        `onehot` - boolean value, if `True` the targets are also made\n                   to be one-hot vectors rather than indices\n\n    Returns: a tuple `(x, t)` where\n     - `x` is an numpy array of one-hot vectors of shape [batch_size, 3, 250]\n     - `t` is either\n            - a numpy array of shape [batch_size, 250] if onehot is True,\n            - a numpy array of shape [batch_size] containing indicies otherwise\n    \"\"\"\n    x = data[range_min:range_max, :3]\n    x = make_onehot(x)\n    x = x.reshape(-1, 750)\n    t = data[range_min:range_max, 3]\n    if onehot:\n        t = make_onehot(t).reshape(-1, 250)\n    return x, t\n\n# some testing code for illustrative purposes\nx_, t_ = get_batch(train4grams, 0, 10, onehot=False)\nprint(train4grams[0])\npos_index = train4grams[0][0]\nprint(x_[0, pos_index-1]) # should be 0\nprint(x_[0, pos_index])   # should be 1\nprint(x_[0, pos_index+1]) # should be 0\npos_index = train4grams[1][0]\nprint(x_[0, 250 + pos_index-1]) # should be 0\nprint(x_[0, 250 + pos_index])   # should be 1\nprint(x_[0, 250 + pos_index+1]) # should be 0\npos_index = train4grams[2][0]\nprint(x_[0, 500 + pos_index-1]) # should be 0\nprint(x_[0, 500 + pos_index])   # should be 1\nprint(x_[0, 500 + pos_index+1]) # should be 0\nFinally, the estimate_accuracy function has been provided to you as well. This function is similar to the accuracy function from lab 1.\ndef estimate_accuracy(model, data, batch_size=5000, max_N=10000):\n    \"\"\"\n    Estimate the accuracy of the model on the data. To reduce\n    computation time, use at least `max_N` elements of `data` to\n    produce the estimate.\n\n    Parameters:\n        `model` - an object (e.g. `NNModel`, see below) with a forward()\n                  method that produces predictions for an input\n        `data` - a dataset of 4grams (produced by `process_data`) over\n                 which we compute accuracy\n        `batch_size` - integer batch size to use to produce predictions\n        `max_N` - integer value describing the minimum number of predictions\n                  to make to produce the accuracy estimate\n\n    Returns: a floating point value between 0 and 1\n    \"\"\"\n    num_correct = 0\n    num_preds = 0\n    for i in range(0, data.shape[0], batch_size):\n        xs, ts = get_batch(data, i, i + batch_size, onehot=False)\n        z = model.forward(xs)\n        pred = np.argmax(z, axis=1)\n        num_correct += np.sum(ts == pred)\n        num_preds += ts.shape[0]\n\n        if num_preds &gt;= max_N: # at least max_N predictions have been made\n            break\n    return num_correct / num_preds\n\n\n\nIn this section, we will build our deep learning model. As we did in the previous lab, we begin by understanding how to make predictions with this model. So, in this part of the lab, we will write the functions required to perform the forward pass operation. We will write the backward-pass and train the model in Part 3 and 4.\nTask: Consider the following two models architectures:\nModel 1: \nModel 2: \nIn Model 1, the input \\(\\bf{x}\\) consists of three one-hot vectors concatenated together. We can think of \\(\\bf{h}\\) as a representation of those three words (all together). However, the model architecture treat the three one-hot vectors from the three words distinctly. However, \\(\\bf{W^{(1)}}\\) needs to learn about the first word separately from the second and third word. In other words, the deep learning model treats these three sets of one-hot features as if they have no semantic connection in common.\nIn Model 2, we use an idea called weight sharing, where we use the sample set of weights \\(\\bf{W}^{(word)}\\) to map the one-hot vectors into a vector representation. This allows us to learn the weights \\(\\bf{W}^{(word)}\\) from informatino from all three words. This model architecture encodes our knowledge that the three sets of one-hot vectors share something in common.\nWe will use model 2 in the rest of this lab. For clarity, here is the forward-pass computation to be performed. (Note that this is not vectorized!)\n\\[\\begin{align*}\n\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\nL &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n\\end{align*}\\]\nThe class NNModel represents this above neural network model. This class stores the weights and biases of our model. Moreover, this class will also have methods that use and modify these weights.\nMost of the class has been implemented for you, including these methods:\n\nThe initializeParams() method, which randomly initializes the weights\nThe loss() method, which computes the average cross-entropy loss\nThe update() method, which performs the gradient updates\nThe cleanup() method, which clears the member variables used in the computation\n\nThe implementation for these methods are incomplete:\n\nThe forward method computes the prediction given a data matrix X. These computations are known as the forward pass. This method also saves some of the intermediate values in the neural network computation, to make gradient computation easier.\nThe backward method computes the gradient of the average loss with respect to various quantities (i.e. the error signals). These computations are known as the backward pass.\n\nYou may assume that during an iteration of gradient descent, the following methods will be called in order:\n\nThe cleanup method to clear information stored from the previous computation\nThe forward method to compute the predictions\nThe backward method to compute the error signals\n(Possibly the loss method to compute the average loss)\nThe update method to move the weights\n\nYou might recognize that the way we set up the class correspond to what PyTorch does.\nclass NNModel(object):\n    def __init__(self, vocab_size=250, emb_size=150, num_hidden=100):\n        \"\"\"\n        Initialize the weights and biases of this two-layer MLP.\n        \"\"\"\n        # information about the model architecture\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.num_hidden = num_hidden\n\n        # weights for the embedding layer of the model\n        self.Ww = np.zeros([vocab_size, emb_size])\n\n        # weights and biases for the first layer of the MLP\n        self.W1 = np.zeros([emb_size * 3, num_hidden])\n        self.b1 = np.zeros([num_hidden])\n\n        # weights and biases for the second layer of the MLP\n        self.W2 = np.zeros([num_hidden, vocab_size])\n        self.b2 = np.zeros([vocab_size])\n\n        # initialize the weights and biases\n        self.initializeParams()\n\n        # set all values of intermediate variables (to be used in the\n        # forward/backward passes) to None\n        self.cleanup()\n\n    def initializeParams(self):\n        \"\"\"\n        Initialize the weights and biases of this two-layer MLP to be random.\n        This random initialization is necessary to break the symmetry in the\n        gradient descent update for our hidden weights and biases. If all our\n        weights were initialized to the same value, then their gradients will\n        all be the same!\n        \"\"\"\n        self.Ww = np.random.normal(0, 2/self.vocab_size, self.Ww.shape)\n        self.W1 = np.random.normal(0, 2/(3*self.emb_size), self.W1.shape)\n        self.b1 = np.random.normal(0, 2/(3*self.emb_size), self.b1.shape)\n        self.W2 = np.random.normal(0, 2/self.num_hidden, self.W2.shape)\n        self.b2 = np.random.normal(0, 2/self.num_hidden, self.b2.shape)\n\n    def forward(self, X):\n        \"\"\"\n        Compute the forward pass to produce prediction logits.\n\n        Parameters:\n            `X` - A numpy array of shape (N, self.vocab_size * 3)\n\n        Returns: A numpy array of logit predictions of shape\n                 (N, self.vocab_size)\n        \"\"\"\n        return do_forward_pass(self, X) # To be implemented below\n\n    def backward(self, ts):\n        \"\"\"\n        Compute the backward pass, given the ground-truth, one-hot targets.\n\n        You may assume that the `forward()` method has been called for the\n        corresponding input `X`, so that the quantities computed in the\n        `forward()` method is accessible.\n\n        Parameters:\n            `ts` - A numpy array of shape (N, self.vocab_size)\n        \"\"\"\n        return do_backward_pass(self, ts)\n\n    def loss(self, ts):\n        \"\"\"\n        Compute the average cross-entropy loss, given the ground-truth, one-hot targets.\n\n        You may assume that the `forward()` method has been called for the\n        corresponding input `X`, so that the quantities computed in the\n        `forward()` method is accessible.\n\n        Parameters:\n            `ts` - A numpy array of shape (N, self.num_classes)\n        \"\"\"\n        return np.sum(-ts * np.log(self.y)) / ts.shape[0]\n\n    def update(self, alpha):\n        \"\"\"\n        Compute the gradient descent update for the parameters of this model.\n\n        Parameters:\n            `alpha` - A number representing the learning rate\n        \"\"\"\n        self.Ww = self.Ww - alpha * self.Ww_bar\n        self.W1 = self.W1 - alpha * self.W1_bar\n        self.b1 = self.b1 - alpha * self.b1_bar\n        self.W2 = self.W2 - alpha * self.W2_bar\n        self.b2 = self.b2 - alpha * self.b2_bar\n\n    def cleanup(self):\n        \"\"\"\n        Erase the values of the variables that we use in our computation.\n        \"\"\"\n        # To be filled in during the forward pass\n        self.N = None # Number of data points in the batch\n        self.xa = None # word (a)'s one-hot encoding\n        self.xb = None # word (b)'s one-hot encoding\n        self.xc = None # word (c)'s one-hot encoding\n        self.va = None # word (a)'s embedding\n        self.vb = None # word (b)'s embedding\n        self.vc = None # word (c)'s embedding\n        self.v = None  # concatenated embedding\n        self.m = None  # pre-activation hidden state\n        self.h = None  # post-activation hidden state\n        self.z = None  # prediction logit\n        self.y = None  # prediction softmax\n\n        # To be filled in during the backward pass\n        self.z_bar  = None # The error signal for self.z\n        self.W2_bar = None # The error signal for self.W2\n        self.b2_bar = None # The error signal for self.b2\n        self.h_bar  = None # The error signal for self.h\n        self.m_bar  = None # The error signal for self.z1\n        self.W1_bar = None # The error signal for self.W1\n        self.b1_bar = None # The error signal for self.b1\n        self.v_bar  = None # The error signal for self.v\n        self.va_bar = None # The error signal for self.va\n        self.vb_bar = None # The error signal for self.vb\n        self.vc_bar = None # The error signal for self.vc\n        self.Ww_bar = None # The error signal for self.Ww\nGraded Task: Complete the implementation of the do_forward_pass method, which computes the predictions given a NNModel and a batch of input data.\nWe recommend that you reason about your approach on paper before writing any numpy code. Track the shapes of your quantities carefully! When you finally write your numpy code, print out the shapes of your quantities as you go along, and reason about whether these shapes match your initial expectations.\ndef do_forward_pass(model, X):\n    \"\"\"\n    Compute the forward pass to produce prediction logits.\n\n    This function also keeps some of the intermediate values in\n    the neural network computation, to make computing gradients easier.\n\n    For the ReLU activation, you may find the function `np.maximum` helpful\n\n    Parameters:\n        `model` - An instance of the class NNModel\n        `X` - A numpy array of shape (N, model.vocab_size)\n\n    Returns: A numpy array of logit predictions of shape\n             (N, model.vocab_size)\n    \"\"\"\n    # populate the input attributes necessary for the\n    # backward pass\n    model.N = X.shape[0]\n    model.X = X\n\n    # for xa, xb, xc, we index the appropriate range of X\n    # (recall that the tensor X has shape [batch_size, 3*vocab_size])\n    model.xa = X[:, :model.vocab_size]\n    model.xb = X[:, model.vocab_size:model.vocab_size*2]\n    model.xc = X[:, model.vocab_size*2:]\n\n    # compute the embeddings\n    model.va = None # TODO\n    model.vb = None # TODO\n    model.vc = None # TODO\n    model.va = model.xa @ model.Ww # SOLUTION\n    model.vb = model.xb @ model.Ww # SOLUTION\n    model.vc = model.xc @ model.Ww # SOLUTION\n    model.v = np.concatenate([model.va, model.vb, model.vc], axis=1)\n\n    # compute the remaining part of the forward pass\n    model.m = None # TODO - the hidden state value (pre-activation)\n    model.m = model.v @ model.W1 + model.b1 # SOLUTION\n    model.h = None # TODO - the hidden state value (post ReLU activation)\n    model.h = np.maximum(model.m, 0) # SOLUTION\n    model.z = None # TODO - the logit scores (pre-activation)\n    model.z = model.h @ model.W2 + model.b2 # SOLUTION\n    model.y = None # TODO - the class probabilities (post-activation)\n    model.y = softmax(model.z) # SOLUTION\n    return model.z\nTask: One way important way to check your implementation is to run the forward() method to ensure that the shapes of your quantities are correct. Run the below code. If you run into shape mismatch issues, print out the shapes of the quantities that you are working with (e.g. print(model.va.shape)) and ensure that these shapes are what you expect them to be.\n# Create a batch of data that we will use for gradient checking\n# we will use a small batch size of 8. This number is chosen\n# because it is small, but also because this shape does not\n# appear elsewhere in our architecture (e.g. vocab size, num hidden)\n# so that shape mismatch issues are easier to identify.\nx_, t_ = get_batch(train4grams, 0, 8)\nmodel = NNModel()\ny = model.forward(x_)\n\n# TODO: Check that these shapes are correct. What should these shapes be?\nprint(model.va.shape, model.vb.shape, model.vc.shape)\nprint(model.v.shape)\nprint(model.m.shape, model.h.shape)\nprint(model.z.shape, model.y.shape)\nAt this point, we can work with a pre-trained model by loading weights that are provided to you via the link below. If you would like, you can jump to part 4 first and explore the interesting properties of this model before tackling backpropagation and model training.\n!wget https://www.cs.toronto.edu/~lczhang/413/sentence_pretrained.pk\n\n\ndef load_pretrained(model):\n    import pickle\n    assert(model.vocab_size == 250)\n    assert(model.emb_size   == 150)\n    assert(model.num_hidden == 100)\n    Ww, W1, b1, W2, b2 = pickle.load(open(\"sentence_pretrained.pk\", \"rb\"))\n    model.Ww = Ww\n    model.W1 = W1\n    model.b1 = b1\n    model.W2 = W2\n    model.b2 = b2\n    model.cleanup()\n    return model\n\nmodel = load_pretrained(NNModel())\n\n\n\nWe are ready to complete the function that computes the backward pass of our model!\nYou should start by reviewing the lecture slides on backpropagation. One difference between the slides and our implementation here is that the slides express the required computations for computing the gradients of the loss for a single data point. However, our implementation of backpropagation is further vectorized to compute gradients of the loss for a batch consisting of multiple data points.\nWe begin with applying the backpropagation algorithm on our forward pass steps from earlier. Recall that our model’s forward pass is as follows:\n\\[\\begin{align*}\n\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\nL &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n\\end{align*}\\]\nFollowing the steps discussed in this week’s lecture, we should get the following backward-pass computation (verify this yourself!): \\[\\begin{align*}\n\\overline{{\\bf z}}  &= {\\bf y} - {\\bf t} \\\\\n\\overline{W^{(2)}}  &= \\overline{{\\bf z}}{\\bf h}^T \\\\\n\\overline{{\\bf b^{(2)}}}  &= \\overline{{\\bf z}} \\\\\n\\overline{{\\bf h}}  &= {W^{(2)}}^T\\overline{z} \\\\\n\\overline{W^{(1)}} &= \\overline{{\\bf m}} {\\bf v}^T \\\\\n\\overline{{\\bf b}^{(1)}} &= \\overline{{\\bf m}} \\\\\n\\overline{{\\bf m}}  &= \\overline{{\\bf h}}\\circ \\textrm{ReLU}'({\\bf m}) \\\\\n\\overline{{\\bf v}} &=  {W^{(1)}}^T \\overline{{\\bf m}} \\\\\n\\overline{{\\bf v_a}} &= \\dots \\\\\n\\overline{{\\bf v_b}} &= \\dots \\\\\n\\overline{{\\bf v_c}} &= \\dots \\\\\n\\overline{{\\bf W^{(word)}}} &= \\dots \\\\\n\\end{align*}\\]\nTask: What is the error signal \\(\\overline{{\\bf v_a}}\\)? How does this quantity relate to \\(\\overline{{\\bf v}}\\)? To answer this question, reason about the scalars that make up the elements of \\(\\overline{{\\bf v}}\\). Which of these scalars also appear in \\(\\overline{{\\bf v_a}}\\)?\nExpress your answer by computing va_bar (representing the quantity \\(\\overline{{\\bf v_a}}\\)) given v_bar (representing the quantity \\(\\overline{{\\bf v}}\\)).\nN = 10\nemb_size = 100\nv_bar = np.random.rand(N, emb_size * 3)\n\nva_bar = None # TODO\nvb_bar = None # TODO\nvc_bar = None # TODO\nva_bar = v_bar [:, :emb_size] # SOLUTION\nTask: What is the derivative \\(\\overline{{\\bf W^{(word)}}}\\)? You may find it helpful to draw a computation graph, and then remember the multivariate chain rule. If \\(\\overline{{\\bf W^{(word)}}}\\) affects the loss in 3 different paths, what do we do with those 3 gradients?\n# TODO: Work out the derivative on paper.\nWe are still not done: the gradient computation is for a single input \\({\\bf x}\\). We will need to vectorize each of these computations so that they work for an entire batch of inputs \\({\\bf X}\\) of shape \\(N \\times 3\\textrm{vocab_size}\\).\nFor some quantities, vectorizing the backward-pass computation is just as straightforward as the forward-pass computation, requiring the same techniques. For example, each input \\({\\bf x}\\) in a batch will have its own corresponding value of \\({\\bf z}\\) and thus $. (If this sentence is confusing, check that your description of the shape for z_bar from Part 2 has the batch size N in there somewhere.)\nFor other quantities, vectorizing requires the use of the multivariate chain rule. For example, there is a single weight matrix \\(W^{(2)}\\), used for all inputs in a batch. Thus, a change in \\(W^{(2)}\\) will affect the predictions for all inputs. (If this sentence is confusing, check that your description of the shape for W2_bar from Part 2 does not have batch size N in there.)\nThe vectorization for the quantities consistent with those of a MLP is already provided to you in the do_backward_pass function. However, the rest of this function is incomplete.\nGraded Task: Complete the implementation of the do_backward_pass function, which performs backpropagation given a NNModel, given the ground-truth one-hot targets ts. This function assumes that the forward pass method had been called on the input X corresponding to those one-hot targets.\nOnce again, we recommend that you reason about your approach on paper before writing any numpy code! In particular, understand the vectorization strategies discussed in the previous weeks and above before proceeding. Track the shapes of your quantities carefully! When you finally write your numpy code, print out the shapes of your quantities as you go along, and reason about whether these shapes match your initial expectations.\ndef do_backward_pass(model, ts):\n    \"\"\"\n    Compute the backward pass, given the ground-truth, one-hot targets.\n\n    You may assume that `model.forward()` has been called for the\n    corresponding input `X`, so that the quantities computed in the\n    `forward()` method is accessible.\n\n    The member variables you store here will be used in the `update()`\n    method. Check that the shapes match what you wrote in Part 2.\n\n    Parameters:\n        `model` - An instance of the class NNModel\n        `ts` - A numpy array of shape (N, model.num_classes)\n    \"\"\"\n    # The gradient signal for the MLP part of this is given\n    # to you (or worked out together from above, TODO)\n    model.z_bar = (model.y - ts) / model.N\n    model.W2_bar = np.dot(model.h.T, model.z_bar)\n    model.b2_bar = np.dot(np.ones(model.N).T, model.z_bar)\n    model.h_bar = np.matmul(model.z_bar, model.W2.T)\n    model.m_bar = model.h_bar * (model.m &gt; 0)\n    model.W1_bar = np.dot(model.v.T, model.m_bar)\n    model.b1_bar = np.dot(np.ones(model.N).T, model.m_bar)\n    model.v_bar = np.matmul(model.m_bar, model.W1.T)\n\n    # Refer to your answer above\n    model.va_bar = None # TODO\n    model.vb_bar = None # TODO\n    model.vc_bar = None # TODO\n    model.va_bar = model.v_bar[:, :model.emb_size]                 # SOLUTION\n    model.vb_bar = model.v_bar[:, model.emb_size:model.emb_size*2] # SOLUTION\n    model.vc_bar = model.v_bar[:, model.emb_size*2:]               # SOLUTION\n\n    # Refer to your answer above\n    model.Ww_bar = None\n    model.Ww_bar = np.dot(model.xa.T, model.va_bar) + np.dot(model.xb.T, model.vb_bar) + np.dot(model.xc.T, model.vc_bar) # SOLUTION\nAs we saw in CSC311, debugging machine learning code can be extremely challenging. It helps to be systematic about testing, and to test every helper function as we write it. It is important to test do_backward_pass before using it for training, so that we can isolates issues related to computing gradients vs. other training issues (e.g. those related to poor hyperparameter choices).\nTask: As in the forward pass, start by making sure that the shapes match. Again, If you run into shape mismatch issues, print out the shapes of the quantities that you are working with.\nx_, t_ = get_batch(train4grams, 0, 8)\nmodel = NNModel()\n\nmodel.forward(x_)\nmodel.backward(t_)\nmodel.update(0.001)\nThe above step checks that the shapes match. But we also saw, in CSC311, that one way to check the gradient computation is through finite difference. Recall the definition of a derivative. For a function \\(g(w): \\mathbb{R} \\rightarrow \\mathbb{R}\\),\n\\[g'(w) = \\lim_{h \\rightarrow 0} \\frac{g(w+h) - g(w)}{h}\\]\nThis above rule tells us that if we have a way to evaluate g and would like to test our implementation of \\(g'\\), we can choose an \\(h\\) small enough, and check if:\n\\[g'(w) \\approx \\frac{g(w+h) - g(w)}{h}\\]\nIn our case, we have that for any parameter \\(w_j\\) and an \\(h\\) small enough, we should have for our loss \\[\\mathcal{E}\\]:\n\\[\\frac{\\partial \\mathcal{E}}{\\partial w_j} \\approx \\frac{\\mathcal{E}(w_0, w_1, \\dots, w_{j-1}, w_j + h, w_{j+1}, \\dots, w_D) - \\mathcal{E}(w_0, w_1, \\dots, w_D)}{h}\\]\n(A word about notation: here we are enumerating over all scalar weights \\(w_0 \\dots w_D\\) in our model. You will often see this in machine learning textbooks and papers, where we ignore the fact that these scalar weights come from several different weight matrices and bias vectors. This notation might feel strange/imprecise as first, but keep in mind that mathematical notations is a form of language whose purpose is to communicate ideas. Practitioners choose different notations, and even introduce new notation, with the goal of clearly communicating a specific idea. Here, the idea is that we should be able to test the gradient computation or a single scalar weight by computing the loss function twice: once with a slight perturbation on that scalar weight.)\nGraded Task: Run the below code to spot test that the gradients Ww_bar is computed correctly. Include the output of the code in your submission.\n# We will opt to use a large batch size to test the gradients `Ww_bar`\n# with a large batch size. Why do you think this is? (Why might we\n# be more likely to have gradients of value 0 if the batch size is\n# small?)\nx_, t_ = get_batch(train4grams, 0, 800)\n\nmodel = NNModel()\nmodel.forward(x_)\n\n# Check the gradient for Ww_bar[3, 10]. \n# You should spot check other indices too!\nmodel.backward(t_)\ngradient = model.Ww_bar[3, 10]\n\n# we should have \n# gradient ~= (loss_perturbed - loss_initial) / h\n# where loss_perturbed is the loss if we perturb \n# model.Ww_bar[3, 10] by a small value h\n\nloss_initial = model.loss(t_)\n\nh = 0.01\nmodel.Ww[3, 10] += h \n\nmodel.cleanup()\nmodel.forward(x_)\nloss_perturbed = model.loss(t_)\n\n# These two values should be close\nprint(gradient)\nprint((loss_perturbed - loss_initial) / h) \nIf gradient checking succeeds, we are ready to train our model. The function train_model is written for you. Run the code below with the default hyperparameters. Although hyperparameter tuning is an important step in machine learning, we have chosen reasonable hyperparameters to you to keep this lab a reasonable size.\ndef train_model(model,\n                train_data=train4grams,\n                validation_data=valid4grams,\n                batch_size=50,\n                learning_rate=0.3,\n                max_iters=20000,\n                plot_every=1000):\n    \"\"\"\n    Use gradient descent to train the numpy model on the dataset train4grams.\n    \"\"\"\n    iters, train_loss, train_acc, val_acc = [], [], [], [] # for the training curve\n    iter_count = 0  # count the number of iterations\n    try:\n        while iter_count &lt; max_iters:\n            # shuffle the training data, and break early if we don't have\n            # enough data to remaining in the batch\n            np.random.shuffle(train_data)\n            for i in range(0, train_data.shape[0], batch_size):\n                if (i + batch_size) &gt; train_data.shape[0]:\n                    break\n\n                # get the input and targets of a minibatch\n                xs, ts = get_batch(train_data, i, i + batch_size, onehot=True)\n\n                # erase any accumulated gradients\n                model.cleanup()\n\n                # forward pass: compute prediction\n                ys = model.forward(xs)\n\n                # backward pass: compute error \n                model.backward(ts)\n                model.update(learning_rate)\n\n                # increment the iteration count\n                iter_count += 1\n\n                # compute and plot the *validation* loss and accuracy\n                if (iter_count % plot_every == 0):\n                    iters.append(iter_count)\n                    train_loss.append(model.loss(ts))\n                    train_acc.append(estimate_accuracy(model, train_data))\n                    val_acc.append(estimate_accuracy(model, validation_data))\n                    model.cleanup()\n                    print(f\"Iter {iter_count}. Acc [val:{val_acc[-1]}, train:{train_acc[-1]}] Loss {train_loss[-1]}]\")\n                if iter_count &gt;= max_iters:\n                    break\n    finally:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel= NNModel()\ntrain_model(model)\n\n\n\nIn this section, we will use apply the model for sentence completion, and to explore model embeddings. If you do not have a trained model, you may use the trained weights provided as part of the assignment.\n# model = load_pretrained(NNModel())\nTask: The function make_prediction has been written for you. It takes as parameters a NNModel model and sentence (a list of words), and produces a prediction for the next word in the sentence.\nRun the following code to predict what the next word should be in each of the following sentences:\ndef make_prediction(model, sentence):\n    \"\"\"\n    Use the model to make a prediction for the next word in the\n    sentence using the last 3 words (sentence[-3:])\n    \"\"\"\n    global vocab_itos\n    indices = convert_words_to_indices([sentence[-3:]])\n    X = make_onehot(indices).reshape(-1, 750)\n    z = model.forward(X)\n    i = np.argmax(z)\n    return vocab_itos[i]\n\nprint(make_prediction(model, ['you', 'are', 'a']))\nprint(make_prediction(model, ['there', 'are', 'no']))\nprint(make_prediction(model, ['yesterday', 'the', 'federal']))\nDo your predictions make sense? (If all of your predictions are the same, train your model for more iterations, or change the hyper parameters in your model.\n# TODO: Your analysis goes here\nWhile training the NNModel, we trained the weight model.Ww, which takes a one-hot representation of a word in our vocabulary, and returns a low-dimensional vector representation of that word. These representations, also called word embeddings have interesting properties.\nGraded Task: Explain why each row of model.Ww contains the vector representing of a word. For example model.Ww[vocab_stoi[\"any\"],:] contains the vector representation of the word “any”.\n# TODO: Write your explanation here\nOne interesting thing about these word embeddings is that distances in these vector representations of words make some sense! To show this, we have provided code below that computes the cosine similarity of every pair of words in our vocabulary.\nnorms = np.linalg.norm(model.Ww, axis=1)\nword_emb_norm = (model.Ww.T / norms).T\nsimilarities = np.matmul(word_emb_norm, word_emb_norm.T)\n\n# Some example distances. The first one should be larger than the second\nprint(similarities[vocab_stoi['any'], vocab_stoi['many']])\nprint(similarities[vocab_stoi['any'], vocab_stoi['government']])\nTask: Run the below code, which computes the 5 closest words to each of the following words. Replace these words with words of your choice to explore the distances in the word embeddings.\ndef get_closest(word):\n    dst = [(w, similarities[vocab_stoi[word], idx])\n           for w, idx in vocab_stoi.items()] \n    dst = sorted(dst, key=lambda x: x[1], reverse=True)\n    return dst[1:6]\n\nprint(get_closest(\"four\"))\nprint(get_closest(\"go\"))\nprint(get_closest(\"should\"))\nprint(get_closest(\"yesterday\"))\nNotice that similar words provided above tend to occur in similar surrounding words in a sentence. Why do you think this might be? Consider the architecture used in this model, and what this model is trained to do. (How would replacing a word with another word with a similar embedding change the neural network prediction?)\nWe can also visualize the word embeddings by reducing the dimensionality of the word vectors to 2D. There are many dimensionality reduction techniques that we could use, and we will use an algorithm called t-SNE. (You don’t need to know what this is for the lab). Nearby points in this 2-D space are meant to correspond to nearby points in the original, high-dimensional space.\nThe following code runs the t-SNE algorithm and plots the result. Look at the plot and find two clusters of related words. What do the words in each cluster have in common?\nNote that there is randomness in the initialization of the t-SNE algorithm. If you re-run this code, you may get a different image.\nimport sklearn.manifold\ntsne = sklearn.manifold.TSNE()\nY = tsne.fit_transform(word_emb_norm)\n\nplt.figure(figsize=(10, 10))\nplt.xlim(Y[:,0].min(), Y[:, 0].max())\nplt.ylim(Y[:,1].min(), Y[:, 1].max())\nfor i, w in enumerate(vocab):\n    plt.text(Y[i, 0], Y[i, 1], w)\nplt.show()\n# For instructor use # SOLUTION\ndef save_model(model):  # SOLUTION\n    import pickle# SOLUTION\n    weights = (model.Ww, model.W1, model.b1, model.W2, model.b2) # SOLUTION\n    pickle.dump(weights, open(\"sentence_pretrained.pk\", \"wb\"))# SOLUTION\nsave_model(model) # SOLUTION"
  },
  {
    "objectID": "labs/lab02.html#submission",
    "href": "labs/lab02.html#submission",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "",
    "text": "Click “Working Alone” on Markus.\nSubmit the ipynb file lab02.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your implementation of convert_words_to_indices (1 point)\nPart 2. Your implementation of do_forward_pass (4 points)\nPart 3. Your implementation of do_backward_pass (3 points)\nPart 3. The output of the gradient checking code (1 point)\nPart 4. Your explanation of why each row of model.Ww corresponds to a word representation (1 point)\n\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "labs/lab02.html#part-1.-data",
    "href": "labs/lab02.html#part-1.-data",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "",
    "text": "We will begin by downloading the data onto Google Colab.\n# Download lab data file\n!wget https://www.cs.toronto.edu/~lczhang/413/raw_sentences.txt\nWith any machine learning problem, the first thing that we would want to do is to get an intuitive understanding of what our data looks like. The following code reads the sentences in our file, split each sentence into its individual words, and stores the sentences (list of words) in the variable sentences.\nsentences = []\nfor line in open('raw_sentences.txt'):\n    words = line.split()\n    sentence = [word.lower() for word in words]\n    sentences.append(sentence)\nThere are 97,162 sentences in total, and these sentences are composed of 250 distinct words.\nvocab = set([w for s in sentences for w in s])\nprint(len(sentences)) # 97162\nprint(len(vocab)) # 250\nWe will separate our data into training, validation, and test. We will use 10,000 sentences for test, 10,000 for validation, and the remaining for training.\n# First, randomly shuffle the sentences in case these sentences are\n# temporally correlated (i.e., so that our train/val/test sets have\n# equal probability of getting the earlier vs later sentences)\nimport random\nrandom.seed(10)\nrandom.shuffle(sentences)\n\ntest, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]\nTask: To get an understanding of the data set that we are working with, start by printing 10 sentences in the training set. How are punctuated treated in this word representation? What about words with apostrophes?\n# TODO: Your code goes here\nfor i, s in enumerate(sentences): # SOLUTION\n    print(s)            # SOLUTION\n    if i &gt;= 10:         # SOLUTION\n        break           # SOLUTION\nIt is also a good idea to explore the distributional properties of the data.\nTask: The length of the sentences affects the types of modeling we can perform on the data. If the sentences are too short, then using a model that depends on many previous words would not make sense. Run the below code, which computes the length of the shortest, average, and longest sentences in the training set.\nsentence_lengths = [len(s) for s in train]\nprint(\"Shortest Sentence\", np.min(sentence_lengths))\nprint(\"Average Sentence\", np.mean(sentence_lengths))\nprint(\"Longest Sentence\", np.max(sentence_lengths))\nTask: How many words are in the training set? In general, there may be words in the validation/test sets that are not in training!\n# TODO: Write code to perform the computation\nvocab = set([w for s in train for w in s]) # SOLUTION\nprint(len(vocab)) # SOLUTION\nTask: What is the most common word in the training set? How often does this word appear in the training set? This information is useful to know since it helps us understand the difficult of the word prediction problem. In other words, this figure represents the accuracy of a “baseline” model that simply returns the most common word as the prediction for what the next word should be!\nfrom collections import Counter\n\ntotal_words = 0        # count number of words in the training set\nword_count = Counter() # count the occurrence of each word\nfor s in train:\n    for w in s:\n        word_count[w] += 1\n        total_words += 1\n\n# TODO: find the most common word\nprint(word_count.most_common(1)) # ('.', 64303) # SOLUTION\nprint(64303/total_words)                        # SOLUTION\nNow that we understand a bit about the distributional properties of our data, we can move on to representing our data numerically in a way that a neural network can use.\nWe will use a one-hot encoding to represent words. Alternatively, you can think of what we’re doing as assigning each word to a unique integer index. We will need some functions that converts sentences into the corresponding word indices.\nGraded Task: Complete the helper functions convert_words_to_indices. The functions generate_4grams and process_data have been written for you. The process_data function will take a list of sentences (i.e. list of list of words), and generate an \\(N \\times 4\\) numpy matrix containing indices of 4 words that appear next to each other. You can use the constants vocab, vocab_itos, and vocab_stoi in your code.\n# A list of all the words in the data set. We will assign a unique \n# identifier for each of these words.\nvocab = sorted(list(set([w for s in train for w in s])))\n# A mapping of index =&gt; word (string)\nvocab_itos = dict(enumerate(vocab))\n# A mapping of word =&gt; its index\nvocab_stoi = {word:index for index, word in vocab_itos.items()}\n\ndef convert_words_to_indices(sents):\n    \"\"\"\n    This function takes a list of sentences (list of list of words)\n    and returns a new list with the same structure, but where each word\n    is replaced by its index in `vocab_stoi`.\n\n    Example:\n    &gt;&gt;&gt; convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'],\n                                  ['other', 'one', 'since', 'yesterday'],\n                                  ['you']])\n    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n    \"\"\"\n    indices = []\n    # TODO: Write your code here\n    for s in sents:                                # SOLUTION\n        indices.append([vocab_stoi[w] for w in s]) # SOLUTION\n    return indices\n\ndef generate_4grams(seqs):\n    \"\"\"\n    This function takes a list of sentences (list of lists) and returns\n    a new list containing the 4-grams (four consecutively occurring words)\n    that appear in the sentences. Note that a unique 4-gram can appear multiple\n    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n\n    Example:\n\n    &gt;&gt;&gt; generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n    &gt;&gt;&gt; generate_4grams([[1, 1, 1, 1, 1]])\n    [[1, 1, 1, 1], [1, 1, 1, 1]]\n    \"\"\"\n    grams = []\n    for seq in seqs:\n        for i in range(len(seq) - 4):\n            grams.append(seq[i:i+4])\n    return grams\n\ndef process_data(sents):\n    \"\"\"\n    This function takes a list of sentences (list of lists), and generates an\n    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n    \"\"\"\n    indices = convert_words_to_indices(sents)\n    fourgrams = generate_4grams(indices)\n    return np.array(fourgrams)\n\ntrain4grams = process_data(train)\nvalid4grams = process_data(valid)\ntest4grams = process_data(test)\nTask: We are almost ready to discuss the model. Review the following helper functions, which has been written for you:\n\nmake_onehot, which converts indices\n\ndef make_onehot(indicies, total=250):\n    \"\"\"\n    Convert indicies into one-hot vectors.\n\n    Parameters:\n        `indices` - a numpy array of any shape (e.g. `[N, 3]` where `N`\n                    is the batch size)\n        `total` - an integer describing the total number of possible classes\n                  (maximum possible value in `indicies`)\n\n    Returns: a one-hot representation of the input numpy array \n             (If the input is of shape `[X, Y]`, then the output would\n             be of shape `[X, Y, total]` and consists of 0's and 1's)\n    \"\"\"\n    # create an identity matrix of shape [total, total]\n    I = np.eye(total)\n    # index the appropriate columns of that identity matrix\n    return I[indicies]\n\ndef softmax(x):\n    \"\"\"\n    Compute the softmax of vector x, or row-wise for a matrix x.\n    We subtract x.max(axis=0) from each row for numerical stability.\n\n    Parameters:\n        `x` - a numpy array shape `[N, num_classes]`\n\n    Returns: a numpy array of the same shape as the input.\n    \"\"\"\n    x = x.T\n    exps = np.exp(x - x.max(axis=0))\n    probs = exps / np.sum(exps, axis=0)\n    return probs.T\nThere is one more data processing function that we need, which turns the four-grams into inputs (consisting of the one-hot representations of the first 3 words), and the target output (the index of the 4th word).\nSince the one-hot representation is not memory efficient, we will only convert data into this representation when required, and only do so at a minibatch level.\ndef get_batch(data, range_min, range_max, onehot=True):\n    \"\"\"\n    Convert one batch of data (specifically, `data[range_min:range_max]`)\n    in the form of 4-grams into input and output data and return the\n    training data.\n\n    Parameters:\n        `data` - a numpy array of shape [N, 4] produced by a call\n                 to the function `process_data`\n        `range_min` - the starting index of the minibatch\n        `range_max` - the ending index of the minibatch, with\n                      range_max &gt; range_min and\n                      batch_size = range_max - range_min\n        `onehot` - boolean value, if `True` the targets are also made\n                   to be one-hot vectors rather than indices\n\n    Returns: a tuple `(x, t)` where\n     - `x` is an numpy array of one-hot vectors of shape [batch_size, 3, 250]\n     - `t` is either\n            - a numpy array of shape [batch_size, 250] if onehot is True,\n            - a numpy array of shape [batch_size] containing indicies otherwise\n    \"\"\"\n    x = data[range_min:range_max, :3]\n    x = make_onehot(x)\n    x = x.reshape(-1, 750)\n    t = data[range_min:range_max, 3]\n    if onehot:\n        t = make_onehot(t).reshape(-1, 250)\n    return x, t\n\n# some testing code for illustrative purposes\nx_, t_ = get_batch(train4grams, 0, 10, onehot=False)\nprint(train4grams[0])\npos_index = train4grams[0][0]\nprint(x_[0, pos_index-1]) # should be 0\nprint(x_[0, pos_index])   # should be 1\nprint(x_[0, pos_index+1]) # should be 0\npos_index = train4grams[1][0]\nprint(x_[0, 250 + pos_index-1]) # should be 0\nprint(x_[0, 250 + pos_index])   # should be 1\nprint(x_[0, 250 + pos_index+1]) # should be 0\npos_index = train4grams[2][0]\nprint(x_[0, 500 + pos_index-1]) # should be 0\nprint(x_[0, 500 + pos_index])   # should be 1\nprint(x_[0, 500 + pos_index+1]) # should be 0\nFinally, the estimate_accuracy function has been provided to you as well. This function is similar to the accuracy function from lab 1.\ndef estimate_accuracy(model, data, batch_size=5000, max_N=10000):\n    \"\"\"\n    Estimate the accuracy of the model on the data. To reduce\n    computation time, use at least `max_N` elements of `data` to\n    produce the estimate.\n\n    Parameters:\n        `model` - an object (e.g. `NNModel`, see below) with a forward()\n                  method that produces predictions for an input\n        `data` - a dataset of 4grams (produced by `process_data`) over\n                 which we compute accuracy\n        `batch_size` - integer batch size to use to produce predictions\n        `max_N` - integer value describing the minimum number of predictions\n                  to make to produce the accuracy estimate\n\n    Returns: a floating point value between 0 and 1\n    \"\"\"\n    num_correct = 0\n    num_preds = 0\n    for i in range(0, data.shape[0], batch_size):\n        xs, ts = get_batch(data, i, i + batch_size, onehot=False)\n        z = model.forward(xs)\n        pred = np.argmax(z, axis=1)\n        num_correct += np.sum(ts == pred)\n        num_preds += ts.shape[0]\n\n        if num_preds &gt;= max_N: # at least max_N predictions have been made\n            break\n    return num_correct / num_preds"
  },
  {
    "objectID": "labs/lab02.html#part-2.-model-building-forward-pass",
    "href": "labs/lab02.html#part-2.-model-building-forward-pass",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "",
    "text": "In this section, we will build our deep learning model. As we did in the previous lab, we begin by understanding how to make predictions with this model. So, in this part of the lab, we will write the functions required to perform the forward pass operation. We will write the backward-pass and train the model in Part 3 and 4.\nTask: Consider the following two models architectures:\nModel 1: \nModel 2: \nIn Model 1, the input \\(\\bf{x}\\) consists of three one-hot vectors concatenated together. We can think of \\(\\bf{h}\\) as a representation of those three words (all together). However, the model architecture treat the three one-hot vectors from the three words distinctly. However, \\(\\bf{W^{(1)}}\\) needs to learn about the first word separately from the second and third word. In other words, the deep learning model treats these three sets of one-hot features as if they have no semantic connection in common.\nIn Model 2, we use an idea called weight sharing, where we use the sample set of weights \\(\\bf{W}^{(word)}\\) to map the one-hot vectors into a vector representation. This allows us to learn the weights \\(\\bf{W}^{(word)}\\) from informatino from all three words. This model architecture encodes our knowledge that the three sets of one-hot vectors share something in common.\nWe will use model 2 in the rest of this lab. For clarity, here is the forward-pass computation to be performed. (Note that this is not vectorized!)\n\\[\\begin{align*}\n\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\nL &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n\\end{align*}\\]\nThe class NNModel represents this above neural network model. This class stores the weights and biases of our model. Moreover, this class will also have methods that use and modify these weights.\nMost of the class has been implemented for you, including these methods:\n\nThe initializeParams() method, which randomly initializes the weights\nThe loss() method, which computes the average cross-entropy loss\nThe update() method, which performs the gradient updates\nThe cleanup() method, which clears the member variables used in the computation\n\nThe implementation for these methods are incomplete:\n\nThe forward method computes the prediction given a data matrix X. These computations are known as the forward pass. This method also saves some of the intermediate values in the neural network computation, to make gradient computation easier.\nThe backward method computes the gradient of the average loss with respect to various quantities (i.e. the error signals). These computations are known as the backward pass.\n\nYou may assume that during an iteration of gradient descent, the following methods will be called in order:\n\nThe cleanup method to clear information stored from the previous computation\nThe forward method to compute the predictions\nThe backward method to compute the error signals\n(Possibly the loss method to compute the average loss)\nThe update method to move the weights\n\nYou might recognize that the way we set up the class correspond to what PyTorch does.\nclass NNModel(object):\n    def __init__(self, vocab_size=250, emb_size=150, num_hidden=100):\n        \"\"\"\n        Initialize the weights and biases of this two-layer MLP.\n        \"\"\"\n        # information about the model architecture\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.num_hidden = num_hidden\n\n        # weights for the embedding layer of the model\n        self.Ww = np.zeros([vocab_size, emb_size])\n\n        # weights and biases for the first layer of the MLP\n        self.W1 = np.zeros([emb_size * 3, num_hidden])\n        self.b1 = np.zeros([num_hidden])\n\n        # weights and biases for the second layer of the MLP\n        self.W2 = np.zeros([num_hidden, vocab_size])\n        self.b2 = np.zeros([vocab_size])\n\n        # initialize the weights and biases\n        self.initializeParams()\n\n        # set all values of intermediate variables (to be used in the\n        # forward/backward passes) to None\n        self.cleanup()\n\n    def initializeParams(self):\n        \"\"\"\n        Initialize the weights and biases of this two-layer MLP to be random.\n        This random initialization is necessary to break the symmetry in the\n        gradient descent update for our hidden weights and biases. If all our\n        weights were initialized to the same value, then their gradients will\n        all be the same!\n        \"\"\"\n        self.Ww = np.random.normal(0, 2/self.vocab_size, self.Ww.shape)\n        self.W1 = np.random.normal(0, 2/(3*self.emb_size), self.W1.shape)\n        self.b1 = np.random.normal(0, 2/(3*self.emb_size), self.b1.shape)\n        self.W2 = np.random.normal(0, 2/self.num_hidden, self.W2.shape)\n        self.b2 = np.random.normal(0, 2/self.num_hidden, self.b2.shape)\n\n    def forward(self, X):\n        \"\"\"\n        Compute the forward pass to produce prediction logits.\n\n        Parameters:\n            `X` - A numpy array of shape (N, self.vocab_size * 3)\n\n        Returns: A numpy array of logit predictions of shape\n                 (N, self.vocab_size)\n        \"\"\"\n        return do_forward_pass(self, X) # To be implemented below\n\n    def backward(self, ts):\n        \"\"\"\n        Compute the backward pass, given the ground-truth, one-hot targets.\n\n        You may assume that the `forward()` method has been called for the\n        corresponding input `X`, so that the quantities computed in the\n        `forward()` method is accessible.\n\n        Parameters:\n            `ts` - A numpy array of shape (N, self.vocab_size)\n        \"\"\"\n        return do_backward_pass(self, ts)\n\n    def loss(self, ts):\n        \"\"\"\n        Compute the average cross-entropy loss, given the ground-truth, one-hot targets.\n\n        You may assume that the `forward()` method has been called for the\n        corresponding input `X`, so that the quantities computed in the\n        `forward()` method is accessible.\n\n        Parameters:\n            `ts` - A numpy array of shape (N, self.num_classes)\n        \"\"\"\n        return np.sum(-ts * np.log(self.y)) / ts.shape[0]\n\n    def update(self, alpha):\n        \"\"\"\n        Compute the gradient descent update for the parameters of this model.\n\n        Parameters:\n            `alpha` - A number representing the learning rate\n        \"\"\"\n        self.Ww = self.Ww - alpha * self.Ww_bar\n        self.W1 = self.W1 - alpha * self.W1_bar\n        self.b1 = self.b1 - alpha * self.b1_bar\n        self.W2 = self.W2 - alpha * self.W2_bar\n        self.b2 = self.b2 - alpha * self.b2_bar\n\n    def cleanup(self):\n        \"\"\"\n        Erase the values of the variables that we use in our computation.\n        \"\"\"\n        # To be filled in during the forward pass\n        self.N = None # Number of data points in the batch\n        self.xa = None # word (a)'s one-hot encoding\n        self.xb = None # word (b)'s one-hot encoding\n        self.xc = None # word (c)'s one-hot encoding\n        self.va = None # word (a)'s embedding\n        self.vb = None # word (b)'s embedding\n        self.vc = None # word (c)'s embedding\n        self.v = None  # concatenated embedding\n        self.m = None  # pre-activation hidden state\n        self.h = None  # post-activation hidden state\n        self.z = None  # prediction logit\n        self.y = None  # prediction softmax\n\n        # To be filled in during the backward pass\n        self.z_bar  = None # The error signal for self.z\n        self.W2_bar = None # The error signal for self.W2\n        self.b2_bar = None # The error signal for self.b2\n        self.h_bar  = None # The error signal for self.h\n        self.m_bar  = None # The error signal for self.z1\n        self.W1_bar = None # The error signal for self.W1\n        self.b1_bar = None # The error signal for self.b1\n        self.v_bar  = None # The error signal for self.v\n        self.va_bar = None # The error signal for self.va\n        self.vb_bar = None # The error signal for self.vb\n        self.vc_bar = None # The error signal for self.vc\n        self.Ww_bar = None # The error signal for self.Ww\nGraded Task: Complete the implementation of the do_forward_pass method, which computes the predictions given a NNModel and a batch of input data.\nWe recommend that you reason about your approach on paper before writing any numpy code. Track the shapes of your quantities carefully! When you finally write your numpy code, print out the shapes of your quantities as you go along, and reason about whether these shapes match your initial expectations.\ndef do_forward_pass(model, X):\n    \"\"\"\n    Compute the forward pass to produce prediction logits.\n\n    This function also keeps some of the intermediate values in\n    the neural network computation, to make computing gradients easier.\n\n    For the ReLU activation, you may find the function `np.maximum` helpful\n\n    Parameters:\n        `model` - An instance of the class NNModel\n        `X` - A numpy array of shape (N, model.vocab_size)\n\n    Returns: A numpy array of logit predictions of shape\n             (N, model.vocab_size)\n    \"\"\"\n    # populate the input attributes necessary for the\n    # backward pass\n    model.N = X.shape[0]\n    model.X = X\n\n    # for xa, xb, xc, we index the appropriate range of X\n    # (recall that the tensor X has shape [batch_size, 3*vocab_size])\n    model.xa = X[:, :model.vocab_size]\n    model.xb = X[:, model.vocab_size:model.vocab_size*2]\n    model.xc = X[:, model.vocab_size*2:]\n\n    # compute the embeddings\n    model.va = None # TODO\n    model.vb = None # TODO\n    model.vc = None # TODO\n    model.va = model.xa @ model.Ww # SOLUTION\n    model.vb = model.xb @ model.Ww # SOLUTION\n    model.vc = model.xc @ model.Ww # SOLUTION\n    model.v = np.concatenate([model.va, model.vb, model.vc], axis=1)\n\n    # compute the remaining part of the forward pass\n    model.m = None # TODO - the hidden state value (pre-activation)\n    model.m = model.v @ model.W1 + model.b1 # SOLUTION\n    model.h = None # TODO - the hidden state value (post ReLU activation)\n    model.h = np.maximum(model.m, 0) # SOLUTION\n    model.z = None # TODO - the logit scores (pre-activation)\n    model.z = model.h @ model.W2 + model.b2 # SOLUTION\n    model.y = None # TODO - the class probabilities (post-activation)\n    model.y = softmax(model.z) # SOLUTION\n    return model.z\nTask: One way important way to check your implementation is to run the forward() method to ensure that the shapes of your quantities are correct. Run the below code. If you run into shape mismatch issues, print out the shapes of the quantities that you are working with (e.g. print(model.va.shape)) and ensure that these shapes are what you expect them to be.\n# Create a batch of data that we will use for gradient checking\n# we will use a small batch size of 8. This number is chosen\n# because it is small, but also because this shape does not\n# appear elsewhere in our architecture (e.g. vocab size, num hidden)\n# so that shape mismatch issues are easier to identify.\nx_, t_ = get_batch(train4grams, 0, 8)\nmodel = NNModel()\ny = model.forward(x_)\n\n# TODO: Check that these shapes are correct. What should these shapes be?\nprint(model.va.shape, model.vb.shape, model.vc.shape)\nprint(model.v.shape)\nprint(model.m.shape, model.h.shape)\nprint(model.z.shape, model.y.shape)\nAt this point, we can work with a pre-trained model by loading weights that are provided to you via the link below. If you would like, you can jump to part 4 first and explore the interesting properties of this model before tackling backpropagation and model training.\n!wget https://www.cs.toronto.edu/~lczhang/413/sentence_pretrained.pk\n\n\ndef load_pretrained(model):\n    import pickle\n    assert(model.vocab_size == 250)\n    assert(model.emb_size   == 150)\n    assert(model.num_hidden == 100)\n    Ww, W1, b1, W2, b2 = pickle.load(open(\"sentence_pretrained.pk\", \"rb\"))\n    model.Ww = Ww\n    model.W1 = W1\n    model.b1 = b1\n    model.W2 = W2\n    model.b2 = b2\n    model.cleanup()\n    return model\n\nmodel = load_pretrained(NNModel())"
  },
  {
    "objectID": "labs/lab02.html#part-3.-model-building-backwards-pass",
    "href": "labs/lab02.html#part-3.-model-building-backwards-pass",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "",
    "text": "We are ready to complete the function that computes the backward pass of our model!\nYou should start by reviewing the lecture slides on backpropagation. One difference between the slides and our implementation here is that the slides express the required computations for computing the gradients of the loss for a single data point. However, our implementation of backpropagation is further vectorized to compute gradients of the loss for a batch consisting of multiple data points.\nWe begin with applying the backpropagation algorithm on our forward pass steps from earlier. Recall that our model’s forward pass is as follows:\n\\[\\begin{align*}\n\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\nL &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n\\end{align*}\\]\nFollowing the steps discussed in this week’s lecture, we should get the following backward-pass computation (verify this yourself!): \\[\\begin{align*}\n\\overline{{\\bf z}}  &= {\\bf y} - {\\bf t} \\\\\n\\overline{W^{(2)}}  &= \\overline{{\\bf z}}{\\bf h}^T \\\\\n\\overline{{\\bf b^{(2)}}}  &= \\overline{{\\bf z}} \\\\\n\\overline{{\\bf h}}  &= {W^{(2)}}^T\\overline{z} \\\\\n\\overline{W^{(1)}} &= \\overline{{\\bf m}} {\\bf v}^T \\\\\n\\overline{{\\bf b}^{(1)}} &= \\overline{{\\bf m}} \\\\\n\\overline{{\\bf m}}  &= \\overline{{\\bf h}}\\circ \\textrm{ReLU}'({\\bf m}) \\\\\n\\overline{{\\bf v}} &=  {W^{(1)}}^T \\overline{{\\bf m}} \\\\\n\\overline{{\\bf v_a}} &= \\dots \\\\\n\\overline{{\\bf v_b}} &= \\dots \\\\\n\\overline{{\\bf v_c}} &= \\dots \\\\\n\\overline{{\\bf W^{(word)}}} &= \\dots \\\\\n\\end{align*}\\]\nTask: What is the error signal \\(\\overline{{\\bf v_a}}\\)? How does this quantity relate to \\(\\overline{{\\bf v}}\\)? To answer this question, reason about the scalars that make up the elements of \\(\\overline{{\\bf v}}\\). Which of these scalars also appear in \\(\\overline{{\\bf v_a}}\\)?\nExpress your answer by computing va_bar (representing the quantity \\(\\overline{{\\bf v_a}}\\)) given v_bar (representing the quantity \\(\\overline{{\\bf v}}\\)).\nN = 10\nemb_size = 100\nv_bar = np.random.rand(N, emb_size * 3)\n\nva_bar = None # TODO\nvb_bar = None # TODO\nvc_bar = None # TODO\nva_bar = v_bar [:, :emb_size] # SOLUTION\nTask: What is the derivative \\(\\overline{{\\bf W^{(word)}}}\\)? You may find it helpful to draw a computation graph, and then remember the multivariate chain rule. If \\(\\overline{{\\bf W^{(word)}}}\\) affects the loss in 3 different paths, what do we do with those 3 gradients?\n# TODO: Work out the derivative on paper.\nWe are still not done: the gradient computation is for a single input \\({\\bf x}\\). We will need to vectorize each of these computations so that they work for an entire batch of inputs \\({\\bf X}\\) of shape \\(N \\times 3\\textrm{vocab_size}\\).\nFor some quantities, vectorizing the backward-pass computation is just as straightforward as the forward-pass computation, requiring the same techniques. For example, each input \\({\\bf x}\\) in a batch will have its own corresponding value of \\({\\bf z}\\) and thus $. (If this sentence is confusing, check that your description of the shape for z_bar from Part 2 has the batch size N in there somewhere.)\nFor other quantities, vectorizing requires the use of the multivariate chain rule. For example, there is a single weight matrix \\(W^{(2)}\\), used for all inputs in a batch. Thus, a change in \\(W^{(2)}\\) will affect the predictions for all inputs. (If this sentence is confusing, check that your description of the shape for W2_bar from Part 2 does not have batch size N in there.)\nThe vectorization for the quantities consistent with those of a MLP is already provided to you in the do_backward_pass function. However, the rest of this function is incomplete.\nGraded Task: Complete the implementation of the do_backward_pass function, which performs backpropagation given a NNModel, given the ground-truth one-hot targets ts. This function assumes that the forward pass method had been called on the input X corresponding to those one-hot targets.\nOnce again, we recommend that you reason about your approach on paper before writing any numpy code! In particular, understand the vectorization strategies discussed in the previous weeks and above before proceeding. Track the shapes of your quantities carefully! When you finally write your numpy code, print out the shapes of your quantities as you go along, and reason about whether these shapes match your initial expectations.\ndef do_backward_pass(model, ts):\n    \"\"\"\n    Compute the backward pass, given the ground-truth, one-hot targets.\n\n    You may assume that `model.forward()` has been called for the\n    corresponding input `X`, so that the quantities computed in the\n    `forward()` method is accessible.\n\n    The member variables you store here will be used in the `update()`\n    method. Check that the shapes match what you wrote in Part 2.\n\n    Parameters:\n        `model` - An instance of the class NNModel\n        `ts` - A numpy array of shape (N, model.num_classes)\n    \"\"\"\n    # The gradient signal for the MLP part of this is given\n    # to you (or worked out together from above, TODO)\n    model.z_bar = (model.y - ts) / model.N\n    model.W2_bar = np.dot(model.h.T, model.z_bar)\n    model.b2_bar = np.dot(np.ones(model.N).T, model.z_bar)\n    model.h_bar = np.matmul(model.z_bar, model.W2.T)\n    model.m_bar = model.h_bar * (model.m &gt; 0)\n    model.W1_bar = np.dot(model.v.T, model.m_bar)\n    model.b1_bar = np.dot(np.ones(model.N).T, model.m_bar)\n    model.v_bar = np.matmul(model.m_bar, model.W1.T)\n\n    # Refer to your answer above\n    model.va_bar = None # TODO\n    model.vb_bar = None # TODO\n    model.vc_bar = None # TODO\n    model.va_bar = model.v_bar[:, :model.emb_size]                 # SOLUTION\n    model.vb_bar = model.v_bar[:, model.emb_size:model.emb_size*2] # SOLUTION\n    model.vc_bar = model.v_bar[:, model.emb_size*2:]               # SOLUTION\n\n    # Refer to your answer above\n    model.Ww_bar = None\n    model.Ww_bar = np.dot(model.xa.T, model.va_bar) + np.dot(model.xb.T, model.vb_bar) + np.dot(model.xc.T, model.vc_bar) # SOLUTION\nAs we saw in CSC311, debugging machine learning code can be extremely challenging. It helps to be systematic about testing, and to test every helper function as we write it. It is important to test do_backward_pass before using it for training, so that we can isolates issues related to computing gradients vs. other training issues (e.g. those related to poor hyperparameter choices).\nTask: As in the forward pass, start by making sure that the shapes match. Again, If you run into shape mismatch issues, print out the shapes of the quantities that you are working with.\nx_, t_ = get_batch(train4grams, 0, 8)\nmodel = NNModel()\n\nmodel.forward(x_)\nmodel.backward(t_)\nmodel.update(0.001)\nThe above step checks that the shapes match. But we also saw, in CSC311, that one way to check the gradient computation is through finite difference. Recall the definition of a derivative. For a function \\(g(w): \\mathbb{R} \\rightarrow \\mathbb{R}\\),\n\\[g'(w) = \\lim_{h \\rightarrow 0} \\frac{g(w+h) - g(w)}{h}\\]\nThis above rule tells us that if we have a way to evaluate g and would like to test our implementation of \\(g'\\), we can choose an \\(h\\) small enough, and check if:\n\\[g'(w) \\approx \\frac{g(w+h) - g(w)}{h}\\]\nIn our case, we have that for any parameter \\(w_j\\) and an \\(h\\) small enough, we should have for our loss \\[\\mathcal{E}\\]:\n\\[\\frac{\\partial \\mathcal{E}}{\\partial w_j} \\approx \\frac{\\mathcal{E}(w_0, w_1, \\dots, w_{j-1}, w_j + h, w_{j+1}, \\dots, w_D) - \\mathcal{E}(w_0, w_1, \\dots, w_D)}{h}\\]\n(A word about notation: here we are enumerating over all scalar weights \\(w_0 \\dots w_D\\) in our model. You will often see this in machine learning textbooks and papers, where we ignore the fact that these scalar weights come from several different weight matrices and bias vectors. This notation might feel strange/imprecise as first, but keep in mind that mathematical notations is a form of language whose purpose is to communicate ideas. Practitioners choose different notations, and even introduce new notation, with the goal of clearly communicating a specific idea. Here, the idea is that we should be able to test the gradient computation or a single scalar weight by computing the loss function twice: once with a slight perturbation on that scalar weight.)\nGraded Task: Run the below code to spot test that the gradients Ww_bar is computed correctly. Include the output of the code in your submission.\n# We will opt to use a large batch size to test the gradients `Ww_bar`\n# with a large batch size. Why do you think this is? (Why might we\n# be more likely to have gradients of value 0 if the batch size is\n# small?)\nx_, t_ = get_batch(train4grams, 0, 800)\n\nmodel = NNModel()\nmodel.forward(x_)\n\n# Check the gradient for Ww_bar[3, 10]. \n# You should spot check other indices too!\nmodel.backward(t_)\ngradient = model.Ww_bar[3, 10]\n\n# we should have \n# gradient ~= (loss_perturbed - loss_initial) / h\n# where loss_perturbed is the loss if we perturb \n# model.Ww_bar[3, 10] by a small value h\n\nloss_initial = model.loss(t_)\n\nh = 0.01\nmodel.Ww[3, 10] += h \n\nmodel.cleanup()\nmodel.forward(x_)\nloss_perturbed = model.loss(t_)\n\n# These two values should be close\nprint(gradient)\nprint((loss_perturbed - loss_initial) / h) \nIf gradient checking succeeds, we are ready to train our model. The function train_model is written for you. Run the code below with the default hyperparameters. Although hyperparameter tuning is an important step in machine learning, we have chosen reasonable hyperparameters to you to keep this lab a reasonable size.\ndef train_model(model,\n                train_data=train4grams,\n                validation_data=valid4grams,\n                batch_size=50,\n                learning_rate=0.3,\n                max_iters=20000,\n                plot_every=1000):\n    \"\"\"\n    Use gradient descent to train the numpy model on the dataset train4grams.\n    \"\"\"\n    iters, train_loss, train_acc, val_acc = [], [], [], [] # for the training curve\n    iter_count = 0  # count the number of iterations\n    try:\n        while iter_count &lt; max_iters:\n            # shuffle the training data, and break early if we don't have\n            # enough data to remaining in the batch\n            np.random.shuffle(train_data)\n            for i in range(0, train_data.shape[0], batch_size):\n                if (i + batch_size) &gt; train_data.shape[0]:\n                    break\n\n                # get the input and targets of a minibatch\n                xs, ts = get_batch(train_data, i, i + batch_size, onehot=True)\n\n                # erase any accumulated gradients\n                model.cleanup()\n\n                # forward pass: compute prediction\n                ys = model.forward(xs)\n\n                # backward pass: compute error \n                model.backward(ts)\n                model.update(learning_rate)\n\n                # increment the iteration count\n                iter_count += 1\n\n                # compute and plot the *validation* loss and accuracy\n                if (iter_count % plot_every == 0):\n                    iters.append(iter_count)\n                    train_loss.append(model.loss(ts))\n                    train_acc.append(estimate_accuracy(model, train_data))\n                    val_acc.append(estimate_accuracy(model, validation_data))\n                    model.cleanup()\n                    print(f\"Iter {iter_count}. Acc [val:{val_acc[-1]}, train:{train_acc[-1]}] Loss {train_loss[-1]}]\")\n                if iter_count &gt;= max_iters:\n                    break\n    finally:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel= NNModel()\ntrain_model(model)"
  },
  {
    "objectID": "labs/lab02.html#part-4.-applying-the-model",
    "href": "labs/lab02.html#part-4.-applying-the-model",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "",
    "text": "In this section, we will use apply the model for sentence completion, and to explore model embeddings. If you do not have a trained model, you may use the trained weights provided as part of the assignment.\n# model = load_pretrained(NNModel())\nTask: The function make_prediction has been written for you. It takes as parameters a NNModel model and sentence (a list of words), and produces a prediction for the next word in the sentence.\nRun the following code to predict what the next word should be in each of the following sentences:\ndef make_prediction(model, sentence):\n    \"\"\"\n    Use the model to make a prediction for the next word in the\n    sentence using the last 3 words (sentence[-3:])\n    \"\"\"\n    global vocab_itos\n    indices = convert_words_to_indices([sentence[-3:]])\n    X = make_onehot(indices).reshape(-1, 750)\n    z = model.forward(X)\n    i = np.argmax(z)\n    return vocab_itos[i]\n\nprint(make_prediction(model, ['you', 'are', 'a']))\nprint(make_prediction(model, ['there', 'are', 'no']))\nprint(make_prediction(model, ['yesterday', 'the', 'federal']))\nDo your predictions make sense? (If all of your predictions are the same, train your model for more iterations, or change the hyper parameters in your model.\n# TODO: Your analysis goes here\nWhile training the NNModel, we trained the weight model.Ww, which takes a one-hot representation of a word in our vocabulary, and returns a low-dimensional vector representation of that word. These representations, also called word embeddings have interesting properties.\nGraded Task: Explain why each row of model.Ww contains the vector representing of a word. For example model.Ww[vocab_stoi[\"any\"],:] contains the vector representation of the word “any”.\n# TODO: Write your explanation here\nOne interesting thing about these word embeddings is that distances in these vector representations of words make some sense! To show this, we have provided code below that computes the cosine similarity of every pair of words in our vocabulary.\nnorms = np.linalg.norm(model.Ww, axis=1)\nword_emb_norm = (model.Ww.T / norms).T\nsimilarities = np.matmul(word_emb_norm, word_emb_norm.T)\n\n# Some example distances. The first one should be larger than the second\nprint(similarities[vocab_stoi['any'], vocab_stoi['many']])\nprint(similarities[vocab_stoi['any'], vocab_stoi['government']])\nTask: Run the below code, which computes the 5 closest words to each of the following words. Replace these words with words of your choice to explore the distances in the word embeddings.\ndef get_closest(word):\n    dst = [(w, similarities[vocab_stoi[word], idx])\n           for w, idx in vocab_stoi.items()] \n    dst = sorted(dst, key=lambda x: x[1], reverse=True)\n    return dst[1:6]\n\nprint(get_closest(\"four\"))\nprint(get_closest(\"go\"))\nprint(get_closest(\"should\"))\nprint(get_closest(\"yesterday\"))\nNotice that similar words provided above tend to occur in similar surrounding words in a sentence. Why do you think this might be? Consider the architecture used in this model, and what this model is trained to do. (How would replacing a word with another word with a similar embedding change the neural network prediction?)\nWe can also visualize the word embeddings by reducing the dimensionality of the word vectors to 2D. There are many dimensionality reduction techniques that we could use, and we will use an algorithm called t-SNE. (You don’t need to know what this is for the lab). Nearby points in this 2-D space are meant to correspond to nearby points in the original, high-dimensional space.\nThe following code runs the t-SNE algorithm and plots the result. Look at the plot and find two clusters of related words. What do the words in each cluster have in common?\nNote that there is randomness in the initialization of the t-SNE algorithm. If you re-run this code, you may get a different image.\nimport sklearn.manifold\ntsne = sklearn.manifold.TSNE()\nY = tsne.fit_transform(word_emb_norm)\n\nplt.figure(figsize=(10, 10))\nplt.xlim(Y[:,0].min(), Y[:, 0].max())\nplt.ylim(Y[:,1].min(), Y[:, 1].max())\nfor i, w in enumerate(vocab):\n    plt.text(Y[i, 0], Y[i, 1], w)\nplt.show()\n# For instructor use # SOLUTION\ndef save_model(model):  # SOLUTION\n    import pickle# SOLUTION\n    weights = (model.Ww, model.W1, model.b1, model.W2, model.b2) # SOLUTION\n    pickle.dump(weights, open(\"sentence_pretrained.pk\", \"wb\"))# SOLUTION\nsave_model(model) # SOLUTION"
  },
  {
    "objectID": "labs/lab03.html",
    "href": "labs/lab03.html",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "",
    "text": "MedMNIST’s PneumoniaMNIST data set. We will now transition fully to using PyTorch for our labs going forward.\nBy the end of this lab, you will be able to:\n\nBuild and train an MLP using PyTorch\nDefine the following performance metrics for evaluating machine learning models: true positive, true negative, false positive, false negative, precision, recall, ROC curve, and AUC.\nInterpret the confusion matrix.\nExplain the advantages of the AUC metrics over accuracy metrics.\nPerform grid search to find hyperparameters.\n\nAcknowledgements:\n\nThe MedMNIST data is from https://medmnist.com/\nThis assignment is written by Mahdi Haghifam, Sonya Allin, Lisa Zhang, Mike Pawliuk and Rutwa Engineer\n\nPlease work in groups of 1-2 during the lab.\n\n\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab03.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 2. Your expression that computes the number of trainable parameters in the MLPModel (1 point)\nPart 2. Your implementation of accuracy. (1 point)\nPart 2. Your implementation of train_model. (2 points)\nPart 3. Your implementation of precision and recall. (2 points)\nPart 3. Your interpretation of the confusion matrix for m_once (1 point)\nPart 4. Your completion of the grid search, along with the output (2 point)\nPart 4. Your description of why a model with high AUC may still perform poorly for some groups (1 point)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nWe will be using the medmnist data set, which is available as a Python package. Recall that on Google Colab, we use “!” to run shell commands.\n!pip install medmnist\n\n\n\nWe will use the MedMNIST data set, which is described here: https://medmnist.com/. We will use the PneumoniaMNIST images, which are greyscale chest X-ray images that has been resized to 28x28. The task is to predict, given one of these X-ray images, whether the patient has pneumonia or not—a binary classification task. We chose this dataset both because it is lightweight, and because it allows us to discuss the sensitive nature of biomedical images.\nLet’s begin by printing some information about the PneumoniaMNIST data set:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport medmnist\nfrom medmnist import PneumoniaMNIST\n\nmedmnist.INFO['pneumoniamnist']\nTask: The dataset providers already split the data into training, validation, and test sets. How many samples are there in the training, validation, and test sets?\n# TODO: Write your answer here.\nLet’s visually inspect the first element of the training data:\ntrain_data_imgs = PneumoniaMNIST(split='train', download=True)\n\nfor img, target in train_data_imgs:\n    plt.imshow(img, cmap='gray')\n    print(np.array(img)) # img is a numpy array of shape 28x28 , with integer values between 0-255\n    print(target)        # the target\n    break\nTask: Based on the code above, what is the type of the data structure train_data?\n# TODO: Write your answer here.\nTask: The code below plots 5 images from each class: normal and pneumonia. Do you notice qualitative differences between these two sets of images? It is always important to qualitatively assess your data prior to training, so that you can develop intuition as to what features may or may not be important for your model. Understanding your data also helps to estimate how challenging the classification problem may be and identify incorrect implementations (e.g., a surprisingly high model accuracy could indicate issues with training set leakage into the test set).\n# normal images\nplt.figure()\nn = 0\nfor img, target in train_data_imgs:\n    if int(target) == 0:\n      plt.subplot(1, 5, n+1)\n      plt.title(\"normal\")\n      plt.imshow(img, cmap='gray')\n      n += 1\n    if n &gt;= 5:\n      break\n# pneumonia images\nplt.figure()\nn = 0\nfor img, target in train_data_imgs:\n    if int(target) == 1:\n      plt.subplot(1, 5, n+1)\n      plt.title(\"pneumonia\")\n      plt.imshow(img, cmap='gray')\n      n += 1\n    if n &gt;= 5:\n      break\n# TODO: Write your explanation here.\nPyTorch makes it easy to apply pre-processing transformations to the data, for example to normalize the data prior to using for training. We will use the standard preprocessing functions to transform the images into tensors for PyTorch to be able to use. This transformation also changes the values to be floating-point numbers between 0 and 1.\nimport torchvision.transforms as transforms # contains a collection of transformations\n\ntrain_data = PneumoniaMNIST(split='train', download=True, transform=transforms.ToTensor())\nval_data = PneumoniaMNIST(split='val', download=True, transform=transforms.ToTensor())\ntest_data = PneumoniaMNIST(split='test', download=True, transform=transforms.ToTensor())\n\nfor img, target in train_data:\n    print(img)    # img is a PyTorch tensor fo shape 1x28x28\n    print(target) # the target\n    break\nTask: How many X-ray images are in the training set with pneumonia? What about without pneumonia? What about the validation/test sets? What does your answer say about the data balance?\n# TODO: Write code to find the answer here.\n\nfrom collections import Counter # SOLUTION\nprint(\"train\", Counter([int(target) for img, target in train_data]))   # SOLUTION\nprint(\"val\", Counter([int(target) for img, target in val_data]))   # SOLUTION\nprint(\"test\", Counter([int(target) for img, target in test_data]))   # SOLUTION\n\n\n\nWe will build our own PyTorch model, which will be a subclass of nn.Module. This subclass provides the important methods that we used in the training loop in lab 1, including the methods that allow us to compute the forward pass by calling the model object, and other methods used under the hood to compute the backward pass.\nOur model will be a three-layer MLP with the following architecture: ACTUALTODO—the model architecture may change!\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLPModel(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=100):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        return out\nGraded Task: How many trainable parameters are in this model? Express your answer in terms of input_dim and num_hidden.\n# TODO: Compute the number of trainable parameters in MLPModel\nIn order to assess model performance, we will begin by implementing the accuracy function, which computes the accuracy of the model across a dataset.\nGraded Task: Complete the accuracy function. Keep in mind that this function will be slightly different from the accuracy function in lab 1, since we are working on a binary classification problem and prediction here is a single logit value (rather than a vector).\ndef accuracy(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    correct, total = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n        y = (z &gt; 0).to(torch.int) # SOLUTION\n\n        correct += int(torch.sum(t == y))\n        total   += t.shape[0]\n    return correct / total\nBecause we are working with binary classification, we will be using a different implementation of the cross-entropy loss function, implemented via PyTorch in a class called BCEWithLogitsLoss (short for Binary Cross Entropy with Logits loss).\ncriterion = nn.BCEWithLogitsLoss()\nThis loss function takes a predicted logit (pre-softmax activation) and the ground-truth label. The use of pre-softmax logits rather than prediction probabilities is due to numerical stability reasons.\nprint(criterion(torch.tensor([2.5]),  # predicted\n                torch.tensor([1.])))  # actual\n\nprint(criterion(torch.tensor([-2.5]), # predicted\n                torch.tensor([1.])))  # actual\nTask: Explain why the second printed value above is larger than the first. In other words, why does it make sense that we think of the second prediction (logit of z=-2.5) as “worse” than the first (logit of z=2.5)?\n# TODO: Your explanation goes here.\nGraded Task: Complete the following code to be used for training.\ndef train_model(model,                # an instance of MLPModel\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=0.1,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=50,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (images, labels) in enumerate(train_loader):\n                z = None # TODO\n                z = model(images.reshape(-1, 784)) # SOLUTION\n  \n                loss = None # TODO\n                loss = criterion(z, labels.to(torch.float)) # SOLUTION\n  \n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n  \n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    ta = accuracy(model, train_data)\n                    va = accuracy(model, val_data)\n                    train_loss.append(float(loss))\n                    train_acc.append(ta)\n                    val_acc.append(va)\n                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        if plot:\n            plt.figure()\n            plt.plot(iters[:len(train_loss)], train_loss)\n            plt.title(\"Loss over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n\n            plt.figure()\n            plt.plot(iters[:len(train_acc)], train_acc)\n            plt.plot(iters[:len(val_acc)], val_acc)\n            plt.title(\"Accuracy over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Accuracy\")\n            plt.legend([\"Train\", \"Validation\"])\n\n# Please include the output of this cell for grading\nmodel = MLPModel()\ntrain_model(model, train_data, val_data)\nTask: Suppose that a model has a validation accuracy of 74% for this binary classification task. Why would this model be considered a very bad model? Your answer should illustrate why accuracy may not be an excellent tool to use.\n# TODO: Write your explanation here\n# SOLUTION: Data imbalance!\n\n\n\nWe often use accuracy as a go-to metric when evaluating the performance of a classification model. However, the accuracy measure weighs all errors equally. A deeper look into the types of errors made can provide a more complete picture of model performance, especially when there is data imbalance and—when applying models in real situations—when some errors may be associated with more serious impacts to users than others.\nTo start our explorations, we’ll look at the decisions we made well, i.e. the:\n\nTrue Positives (TP), or positive outcomes that were correctly predicted as positive.\nTrue Negatives (TN), or negative outcomes that were correctly predicted as negative.\n\nThen we will look at our mistakes, i.e. the:\n\nFalse Positives (FP, or Type I errors), or negative outcomes that were predicted as positive. In our case, this occurs when our model predicts that a person has heart disease, but they do not.\nFalse Negatives (FN, or Type II errors), or positive outcomes that were predicted as negative. In our case, this occurs when our model predicts that a person does not have heart disease, but they do.\n\nWe can then use the metrics above to calculate:\n\nPrecision (or True Positive Rate, or Positive Predicive Value): \\(\\frac{TP}{TP + FP}\\). The answers the question: out of all the examples that we predicted as positive, how many are really positive?\nRecall (or Sensitivity): \\(\\frac{TP}{TP + FN}\\). The answers the question: out of all the positive examples in the data set, how many did we predict as positive?\nFalse Positive Rate (or Negative Predicive Value): \\(\\frac{TN}{TN + FN}\\). The answers the question: out of all the examples that we predicted as negative, how many are really negative?\n\nGraded Task: Complete the functions precision and recall:\ndef precision(model, dataset):\n    \"\"\"\n    Compute the precision of `model` over the `dataset`.  We will take the\n    **most probable class** as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    true_pos, total_pred_pos = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n        y = (z &gt; 0).to(torch.int) # SOLUTION\n\n        # TODO: update total_pred_pos and true_pos\n        total_pred_pos += torch.sum(y)      # SOLUTION\n        true_pos += torch.sum((y + t) == 2) # SOLUTION\n    return true_pos / total_pred_pos\n\n\ndef recall(model, dataset):\n    \"\"\"\n    Compute the recall (or sensitivity) of `model` over the `dataset`.  We will\n    take the **most probable class** as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    true_pos, total_actual_pos = 0, 0 # track the true and false positive\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n        y = (z &gt; 0).to(torch.int) # SOLUTION\n\n        # TODO: update total_pos and true_pos\n        total_actual_pos += torch.sum(t)    # SOLUTION\n        true_pos += torch.sum((y + t) == 2) # SOLUTION\n    return true_pos / total_actual_pos\n\nprint(\"Precision(Training)\", precision(model, train_data))\nprint(\"Recall(Training)\", recall(model, train_data))\nprint(\"Precision(Validation)\", precision(model, val_data))\nprint(\"Recall(Validation)\", recall(model, val_data))\nA confusion matrix is a table that shows the number of TP, TN, FP, and FN. A confusion matrix can be a valuable tool in understanding why a model makes the mistake that it makes.\nTask Run the code below to display the confusion matrix for your model for the validation data.\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef get_prediction(model, data, sample=1000):\n    loader = torch.utils.data.DataLoader(data, batch_size=sample, shuffle=True)\n    for X, t in loader:\n        z = model(X.view(-1, 784))\n        y = torch.sigmoid(z)\n        break\n    y = y.detach().numpy()\n    t = t.detach().numpy()\n    return y, t\n\ny, t = get_prediction(model, val_data)\ny = y &gt; 0.5\ncm = confusion_matrix(t, y)\ncmp = ConfusionMatrixDisplay(cm, display_labels=[\"0\", \"1\"])\ncmp.plot()\nplt.title(\"Confusion Matrix (Val Data)\")\nTask: The code below trains a MLPModel for a very few number of iterations. You should see that this model achieves a 74% accuracy. Display the confusion matrix for this model by running the code below.\nm_once = MLPModel()\ntrain_model(m_once, train_data, val_data, learning_rate=0.5, batch_size=500, num_epochs=1)\nprint(\"Training Accuracy:\", accuracy(m_once, train_data))\nprint(\"Validation Accuracy:\", accuracy(m_once, val_data))\n\ny, t = get_prediction(m_once, val_data)\ny = y &gt; 0.5\nConfusionMatrixDisplay(confusion_matrix(t, y), display_labels=[\"0\", \"1\"]).plot()\nplt.title(\"Confusion Matrix (Val Data)\")\nGraded Task: What does the confusion matrix tell you about how the m_once model is achieving 74% accuracy?\n# TODO: Your explanation goes here.\nWe have been choosing a threshold of 0.5 for turning our continuous predicted probabilities into a discrete prediction. However, this can be an arbitrary choice.\nTask: Explain why, in practical application, it may be reasonable to use a different threshold value. In what situation might you want the threshold to be set very high in order to make a positive prediction? What about a negative prediction?\n# TODO: Your explanation goes here.\nA receiver operating characteristic curve (or ROC) shows how the True Positive Rate and False Positive Rate vary based on our choice of the decision making threshold used to binarize predictions. By default, this threshold is 0.5, but it can be changed to any value between 0 and 1. Different thresholds will result in different TP and FP rates, all of which are illustrated on our graph. we can calculate the area underneath this curve in order to get a sense as to how our classifiers might work across a wide range of different thresholds. This calcution of area can also be used as a metric of our model’s “goodness”, and it is called AUC (or “Area Under Curve”).\nThe AUC metric is particularly useful for machine learning practitioners because it does not depend on the choice of the threshold value used for making discrete predicions. The metric is also resistant to measurement.\nTask: Is it better for the AUC to be larger or smaller? Explain why.\n# TODO: Your explanation goes here\nThe code below plots the ROC curve for a model.\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, auc\n\ny, t = get_prediction(model, val_data)\n\nfpr, tpr, thresholds = roc_curve(t, y)\nroc_auc = auc(fpr, tpr)\nrocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\nrocp.plot()\nplt.title(\"Validation ROC Curve\")\n\n\ny, t = get_prediction(model, train_data)\n\nfpr, tpr, thresholds = roc_curve(t, y)\nroc_auc = auc(fpr, tpr)\nrocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\nrocp.plot()\nplt.title(\"Training ROC Curve\")\nHere is a function you can use to estimate the auc:\ndef get_auc(model, data):\n    y, t = get_prediction(model, data)\n    fpr, tpr, thresholds = roc_curve(t, y)\n    return auc(fpr, tpr)\n\n\n\nAs we mentioned in lab 1, hyperparameter choices matter significantly, and these hyperparameter choices interact with one another. Practitioners use a strategy called grid search to try all variations of hyperparameters from a set of hyperparameters.\nOne very important hyperparameter is the number of hidden units in our MLPModel. This setting affects the number of parameters (weights/biases) used in our model.\nThe use of ReLU vs sigmoid activation function is another hyperparameter that we will explore.\nFinally, optimization parameters like the batch size and the learning rate can also significantly affect the learning process.\nclass MLPModelSigmoid(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=100):\n        super(MLPModelSigmoid, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sig(out)\n        out = self.fc2(out)\n        out = self.sig(out)\n        out = self.fc3(out)\n        return out\nGraded Task: Complete the code below, which performs grid search over the following hyperparameter values of the:\n\nhidden size\nactivation function (ReLu vs sigmoid activation)\nbatch size\nlearning rate\n\nDo so by creating a new model and train it with the appropriate settings, then assessing the final training/validation accuracy, precision, recall, and AUC score. You may use to use the flag plot=False when calling train_model. You might also set plot_every to a large value and visualize the training curve as a separate step for hyperparameter values that you’re interested in.\nPlease include all your output in your submission.\n(There is one more graded task below that you can complete while the hyperparameter tuning is running.)\ngridsearch = {}\nfor num_hidden in [25, 100, 250]:\n    for act in [\"relu\", \"sigmoid\"]:\n        for bs in [10, 100, 500]:\n            for lr in [0.01, 0.1]:\n                # Adjust num_epoch based on the batch size, so that we \n                # train for the same number of iterations irrespective\n                # of batch size\n                ne = int(20 * (bs/100))\n\n                modelname = f\"num_hidden: {num_hidden}, activation: {act}, batch_size: {bs}, learning_rate: {lr}\"\n                print(f\"========={modelname}\")\n\n                # TODO: create and train the model with the appropriate settings\n                if act == \"relu\":                   # SOLUTION\n                    m = MLPModel(num_hidden=num_hidden)  # SOLUTION\n                else:                               # SOLUTION\n                    m = MLPModelSigmoid(num_hidden=num_hidden) # SOLUTION\n                train_model(m, train_data, val_data,  # SOLUTION\n                            learning_rate=lr,         # SOLUTION\n                            batch_size=bs,            # SOLUTION\n                            num_epochs=ne,            # SOLUTION\n                            plot=False)            # SOLUTION\n\n                # Update and display metrics. This part is done for you.\n                metrics = {\n                    \"acc_train\": accuracy(m, train_data),\n                    \"acc_val\": accuracy(m, val_data),\n                    \"precision_train\": precision(m, train_data),\n                    \"precision_val\": precision(m, val_data),\n                    \"recall_train\": recall(m, train_data),\n                    \"recall_val\": recall(m, val_data),\n                    \"auc_train\": get_auc(m, train_data),\n                    \"auc_val\": get_auc(m, val_data),\n                }\n                gridsearch[modelname] = metrics\n                print(f'Accuracy (train):{metrics[\"acc_train\"]} (val):{metrics[\"acc_val\"]}')\n                print(f'Precision (train):{metrics[\"precision_train\"]} (val):{metrics[\"precision_val\"]}')\n                print(f'Recall (train):{metrics[\"recall_train\"]} (val):{metrics[\"recall_val\"]}')\n                print(f'AUC (train):{metrics[\"auc_train\"]} (val):{metrics[\"auc_val\"]}')\nPlease include the below output in your submission\nprint(gridsearch)\nTask: Which hyperparameter choice is the “best”? You should base this answer on the validation AUC. Use the other metrics as a guide to understand the kinds of predictions and mistakes that your model is likely make. Train a final model with those hyperparameter values.\n# TODO\nTask: Report the test accuracy and AUC for this model, and plot the confusion matrix over the test set.\n# TODO\nGraded Task: Explain why a model with high AUC may still produce consistently poor predictions for a subset of the population. You might find this article interesting: Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis; in particular, Figure 1 shows how test AUC differs male/female patients depending on the training set used.\n# TODO"
  },
  {
    "objectID": "labs/lab03.html#submission",
    "href": "labs/lab03.html#submission",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "",
    "text": "If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab03.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 2. Your expression that computes the number of trainable parameters in the MLPModel (1 point)\nPart 2. Your implementation of accuracy. (1 point)\nPart 2. Your implementation of train_model. (2 points)\nPart 3. Your implementation of precision and recall. (2 points)\nPart 3. Your interpretation of the confusion matrix for m_once (1 point)\nPart 4. Your completion of the grid search, along with the output (2 point)\nPart 4. Your description of why a model with high AUC may still perform poorly for some groups (1 point)\n\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "labs/lab03.html#google-colab-setup",
    "href": "labs/lab03.html#google-colab-setup",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "",
    "text": "We will be using the medmnist data set, which is available as a Python package. Recall that on Google Colab, we use “!” to run shell commands.\n!pip install medmnist"
  },
  {
    "objectID": "labs/lab03.html#part-1.-data",
    "href": "labs/lab03.html#part-1.-data",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "",
    "text": "We will use the MedMNIST data set, which is described here: https://medmnist.com/. We will use the PneumoniaMNIST images, which are greyscale chest X-ray images that has been resized to 28x28. The task is to predict, given one of these X-ray images, whether the patient has pneumonia or not—a binary classification task. We chose this dataset both because it is lightweight, and because it allows us to discuss the sensitive nature of biomedical images.\nLet’s begin by printing some information about the PneumoniaMNIST data set:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport medmnist\nfrom medmnist import PneumoniaMNIST\n\nmedmnist.INFO['pneumoniamnist']\nTask: The dataset providers already split the data into training, validation, and test sets. How many samples are there in the training, validation, and test sets?\n# TODO: Write your answer here.\nLet’s visually inspect the first element of the training data:\ntrain_data_imgs = PneumoniaMNIST(split='train', download=True)\n\nfor img, target in train_data_imgs:\n    plt.imshow(img, cmap='gray')\n    print(np.array(img)) # img is a numpy array of shape 28x28 , with integer values between 0-255\n    print(target)        # the target\n    break\nTask: Based on the code above, what is the type of the data structure train_data?\n# TODO: Write your answer here.\nTask: The code below plots 5 images from each class: normal and pneumonia. Do you notice qualitative differences between these two sets of images? It is always important to qualitatively assess your data prior to training, so that you can develop intuition as to what features may or may not be important for your model. Understanding your data also helps to estimate how challenging the classification problem may be and identify incorrect implementations (e.g., a surprisingly high model accuracy could indicate issues with training set leakage into the test set).\n# normal images\nplt.figure()\nn = 0\nfor img, target in train_data_imgs:\n    if int(target) == 0:\n      plt.subplot(1, 5, n+1)\n      plt.title(\"normal\")\n      plt.imshow(img, cmap='gray')\n      n += 1\n    if n &gt;= 5:\n      break\n# pneumonia images\nplt.figure()\nn = 0\nfor img, target in train_data_imgs:\n    if int(target) == 1:\n      plt.subplot(1, 5, n+1)\n      plt.title(\"pneumonia\")\n      plt.imshow(img, cmap='gray')\n      n += 1\n    if n &gt;= 5:\n      break\n# TODO: Write your explanation here.\nPyTorch makes it easy to apply pre-processing transformations to the data, for example to normalize the data prior to using for training. We will use the standard preprocessing functions to transform the images into tensors for PyTorch to be able to use. This transformation also changes the values to be floating-point numbers between 0 and 1.\nimport torchvision.transforms as transforms # contains a collection of transformations\n\ntrain_data = PneumoniaMNIST(split='train', download=True, transform=transforms.ToTensor())\nval_data = PneumoniaMNIST(split='val', download=True, transform=transforms.ToTensor())\ntest_data = PneumoniaMNIST(split='test', download=True, transform=transforms.ToTensor())\n\nfor img, target in train_data:\n    print(img)    # img is a PyTorch tensor fo shape 1x28x28\n    print(target) # the target\n    break\nTask: How many X-ray images are in the training set with pneumonia? What about without pneumonia? What about the validation/test sets? What does your answer say about the data balance?\n# TODO: Write code to find the answer here.\n\nfrom collections import Counter # SOLUTION\nprint(\"train\", Counter([int(target) for img, target in train_data]))   # SOLUTION\nprint(\"val\", Counter([int(target) for img, target in val_data]))   # SOLUTION\nprint(\"test\", Counter([int(target) for img, target in test_data]))   # SOLUTION"
  },
  {
    "objectID": "labs/lab03.html#part-2.-model-and-training",
    "href": "labs/lab03.html#part-2.-model-and-training",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "",
    "text": "We will build our own PyTorch model, which will be a subclass of nn.Module. This subclass provides the important methods that we used in the training loop in lab 1, including the methods that allow us to compute the forward pass by calling the model object, and other methods used under the hood to compute the backward pass.\nOur model will be a three-layer MLP with the following architecture: ACTUALTODO—the model architecture may change!\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLPModel(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=100):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        return out\nGraded Task: How many trainable parameters are in this model? Express your answer in terms of input_dim and num_hidden.\n# TODO: Compute the number of trainable parameters in MLPModel\nIn order to assess model performance, we will begin by implementing the accuracy function, which computes the accuracy of the model across a dataset.\nGraded Task: Complete the accuracy function. Keep in mind that this function will be slightly different from the accuracy function in lab 1, since we are working on a binary classification problem and prediction here is a single logit value (rather than a vector).\ndef accuracy(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    correct, total = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n        y = (z &gt; 0).to(torch.int) # SOLUTION\n\n        correct += int(torch.sum(t == y))\n        total   += t.shape[0]\n    return correct / total\nBecause we are working with binary classification, we will be using a different implementation of the cross-entropy loss function, implemented via PyTorch in a class called BCEWithLogitsLoss (short for Binary Cross Entropy with Logits loss).\ncriterion = nn.BCEWithLogitsLoss()\nThis loss function takes a predicted logit (pre-softmax activation) and the ground-truth label. The use of pre-softmax logits rather than prediction probabilities is due to numerical stability reasons.\nprint(criterion(torch.tensor([2.5]),  # predicted\n                torch.tensor([1.])))  # actual\n\nprint(criterion(torch.tensor([-2.5]), # predicted\n                torch.tensor([1.])))  # actual\nTask: Explain why the second printed value above is larger than the first. In other words, why does it make sense that we think of the second prediction (logit of z=-2.5) as “worse” than the first (logit of z=2.5)?\n# TODO: Your explanation goes here.\nGraded Task: Complete the following code to be used for training.\ndef train_model(model,                # an instance of MLPModel\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=0.1,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=50,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (images, labels) in enumerate(train_loader):\n                z = None # TODO\n                z = model(images.reshape(-1, 784)) # SOLUTION\n  \n                loss = None # TODO\n                loss = criterion(z, labels.to(torch.float)) # SOLUTION\n  \n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n  \n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    ta = accuracy(model, train_data)\n                    va = accuracy(model, val_data)\n                    train_loss.append(float(loss))\n                    train_acc.append(ta)\n                    val_acc.append(va)\n                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        if plot:\n            plt.figure()\n            plt.plot(iters[:len(train_loss)], train_loss)\n            plt.title(\"Loss over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n\n            plt.figure()\n            plt.plot(iters[:len(train_acc)], train_acc)\n            plt.plot(iters[:len(val_acc)], val_acc)\n            plt.title(\"Accuracy over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Accuracy\")\n            plt.legend([\"Train\", \"Validation\"])\n\n# Please include the output of this cell for grading\nmodel = MLPModel()\ntrain_model(model, train_data, val_data)\nTask: Suppose that a model has a validation accuracy of 74% for this binary classification task. Why would this model be considered a very bad model? Your answer should illustrate why accuracy may not be an excellent tool to use.\n# TODO: Write your explanation here\n# SOLUTION: Data imbalance!"
  },
  {
    "objectID": "labs/lab03.html#part-3.-performance-metrics",
    "href": "labs/lab03.html#part-3.-performance-metrics",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "",
    "text": "We often use accuracy as a go-to metric when evaluating the performance of a classification model. However, the accuracy measure weighs all errors equally. A deeper look into the types of errors made can provide a more complete picture of model performance, especially when there is data imbalance and—when applying models in real situations—when some errors may be associated with more serious impacts to users than others.\nTo start our explorations, we’ll look at the decisions we made well, i.e. the:\n\nTrue Positives (TP), or positive outcomes that were correctly predicted as positive.\nTrue Negatives (TN), or negative outcomes that were correctly predicted as negative.\n\nThen we will look at our mistakes, i.e. the:\n\nFalse Positives (FP, or Type I errors), or negative outcomes that were predicted as positive. In our case, this occurs when our model predicts that a person has heart disease, but they do not.\nFalse Negatives (FN, or Type II errors), or positive outcomes that were predicted as negative. In our case, this occurs when our model predicts that a person does not have heart disease, but they do.\n\nWe can then use the metrics above to calculate:\n\nPrecision (or True Positive Rate, or Positive Predicive Value): \\(\\frac{TP}{TP + FP}\\). The answers the question: out of all the examples that we predicted as positive, how many are really positive?\nRecall (or Sensitivity): \\(\\frac{TP}{TP + FN}\\). The answers the question: out of all the positive examples in the data set, how many did we predict as positive?\nFalse Positive Rate (or Negative Predicive Value): \\(\\frac{TN}{TN + FN}\\). The answers the question: out of all the examples that we predicted as negative, how many are really negative?\n\nGraded Task: Complete the functions precision and recall:\ndef precision(model, dataset):\n    \"\"\"\n    Compute the precision of `model` over the `dataset`.  We will take the\n    **most probable class** as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    true_pos, total_pred_pos = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n        y = (z &gt; 0).to(torch.int) # SOLUTION\n\n        # TODO: update total_pred_pos and true_pos\n        total_pred_pos += torch.sum(y)      # SOLUTION\n        true_pos += torch.sum((y + t) == 2) # SOLUTION\n    return true_pos / total_pred_pos\n\n\ndef recall(model, dataset):\n    \"\"\"\n    Compute the recall (or sensitivity) of `model` over the `dataset`.  We will\n    take the **most probable class** as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    true_pos, total_actual_pos = 0, 0 # track the true and false positive\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n        y = (z &gt; 0).to(torch.int) # SOLUTION\n\n        # TODO: update total_pos and true_pos\n        total_actual_pos += torch.sum(t)    # SOLUTION\n        true_pos += torch.sum((y + t) == 2) # SOLUTION\n    return true_pos / total_actual_pos\n\nprint(\"Precision(Training)\", precision(model, train_data))\nprint(\"Recall(Training)\", recall(model, train_data))\nprint(\"Precision(Validation)\", precision(model, val_data))\nprint(\"Recall(Validation)\", recall(model, val_data))\nA confusion matrix is a table that shows the number of TP, TN, FP, and FN. A confusion matrix can be a valuable tool in understanding why a model makes the mistake that it makes.\nTask Run the code below to display the confusion matrix for your model for the validation data.\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef get_prediction(model, data, sample=1000):\n    loader = torch.utils.data.DataLoader(data, batch_size=sample, shuffle=True)\n    for X, t in loader:\n        z = model(X.view(-1, 784))\n        y = torch.sigmoid(z)\n        break\n    y = y.detach().numpy()\n    t = t.detach().numpy()\n    return y, t\n\ny, t = get_prediction(model, val_data)\ny = y &gt; 0.5\ncm = confusion_matrix(t, y)\ncmp = ConfusionMatrixDisplay(cm, display_labels=[\"0\", \"1\"])\ncmp.plot()\nplt.title(\"Confusion Matrix (Val Data)\")\nTask: The code below trains a MLPModel for a very few number of iterations. You should see that this model achieves a 74% accuracy. Display the confusion matrix for this model by running the code below.\nm_once = MLPModel()\ntrain_model(m_once, train_data, val_data, learning_rate=0.5, batch_size=500, num_epochs=1)\nprint(\"Training Accuracy:\", accuracy(m_once, train_data))\nprint(\"Validation Accuracy:\", accuracy(m_once, val_data))\n\ny, t = get_prediction(m_once, val_data)\ny = y &gt; 0.5\nConfusionMatrixDisplay(confusion_matrix(t, y), display_labels=[\"0\", \"1\"]).plot()\nplt.title(\"Confusion Matrix (Val Data)\")\nGraded Task: What does the confusion matrix tell you about how the m_once model is achieving 74% accuracy?\n# TODO: Your explanation goes here.\nWe have been choosing a threshold of 0.5 for turning our continuous predicted probabilities into a discrete prediction. However, this can be an arbitrary choice.\nTask: Explain why, in practical application, it may be reasonable to use a different threshold value. In what situation might you want the threshold to be set very high in order to make a positive prediction? What about a negative prediction?\n# TODO: Your explanation goes here.\nA receiver operating characteristic curve (or ROC) shows how the True Positive Rate and False Positive Rate vary based on our choice of the decision making threshold used to binarize predictions. By default, this threshold is 0.5, but it can be changed to any value between 0 and 1. Different thresholds will result in different TP and FP rates, all of which are illustrated on our graph. we can calculate the area underneath this curve in order to get a sense as to how our classifiers might work across a wide range of different thresholds. This calcution of area can also be used as a metric of our model’s “goodness”, and it is called AUC (or “Area Under Curve”).\nThe AUC metric is particularly useful for machine learning practitioners because it does not depend on the choice of the threshold value used for making discrete predicions. The metric is also resistant to measurement.\nTask: Is it better for the AUC to be larger or smaller? Explain why.\n# TODO: Your explanation goes here\nThe code below plots the ROC curve for a model.\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, auc\n\ny, t = get_prediction(model, val_data)\n\nfpr, tpr, thresholds = roc_curve(t, y)\nroc_auc = auc(fpr, tpr)\nrocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\nrocp.plot()\nplt.title(\"Validation ROC Curve\")\n\n\ny, t = get_prediction(model, train_data)\n\nfpr, tpr, thresholds = roc_curve(t, y)\nroc_auc = auc(fpr, tpr)\nrocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\nrocp.plot()\nplt.title(\"Training ROC Curve\")\nHere is a function you can use to estimate the auc:\ndef get_auc(model, data):\n    y, t = get_prediction(model, data)\n    fpr, tpr, thresholds = roc_curve(t, y)\n    return auc(fpr, tpr)"
  },
  {
    "objectID": "labs/lab03.html#part-4.-hyperparameter-tuning-via-grid-search",
    "href": "labs/lab03.html#part-4.-hyperparameter-tuning-via-grid-search",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "",
    "text": "As we mentioned in lab 1, hyperparameter choices matter significantly, and these hyperparameter choices interact with one another. Practitioners use a strategy called grid search to try all variations of hyperparameters from a set of hyperparameters.\nOne very important hyperparameter is the number of hidden units in our MLPModel. This setting affects the number of parameters (weights/biases) used in our model.\nThe use of ReLU vs sigmoid activation function is another hyperparameter that we will explore.\nFinally, optimization parameters like the batch size and the learning rate can also significantly affect the learning process.\nclass MLPModelSigmoid(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=100):\n        super(MLPModelSigmoid, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sig(out)\n        out = self.fc2(out)\n        out = self.sig(out)\n        out = self.fc3(out)\n        return out\nGraded Task: Complete the code below, which performs grid search over the following hyperparameter values of the:\n\nhidden size\nactivation function (ReLu vs sigmoid activation)\nbatch size\nlearning rate\n\nDo so by creating a new model and train it with the appropriate settings, then assessing the final training/validation accuracy, precision, recall, and AUC score. You may use to use the flag plot=False when calling train_model. You might also set plot_every to a large value and visualize the training curve as a separate step for hyperparameter values that you’re interested in.\nPlease include all your output in your submission.\n(There is one more graded task below that you can complete while the hyperparameter tuning is running.)\ngridsearch = {}\nfor num_hidden in [25, 100, 250]:\n    for act in [\"relu\", \"sigmoid\"]:\n        for bs in [10, 100, 500]:\n            for lr in [0.01, 0.1]:\n                # Adjust num_epoch based on the batch size, so that we \n                # train for the same number of iterations irrespective\n                # of batch size\n                ne = int(20 * (bs/100))\n\n                modelname = f\"num_hidden: {num_hidden}, activation: {act}, batch_size: {bs}, learning_rate: {lr}\"\n                print(f\"========={modelname}\")\n\n                # TODO: create and train the model with the appropriate settings\n                if act == \"relu\":                   # SOLUTION\n                    m = MLPModel(num_hidden=num_hidden)  # SOLUTION\n                else:                               # SOLUTION\n                    m = MLPModelSigmoid(num_hidden=num_hidden) # SOLUTION\n                train_model(m, train_data, val_data,  # SOLUTION\n                            learning_rate=lr,         # SOLUTION\n                            batch_size=bs,            # SOLUTION\n                            num_epochs=ne,            # SOLUTION\n                            plot=False)            # SOLUTION\n\n                # Update and display metrics. This part is done for you.\n                metrics = {\n                    \"acc_train\": accuracy(m, train_data),\n                    \"acc_val\": accuracy(m, val_data),\n                    \"precision_train\": precision(m, train_data),\n                    \"precision_val\": precision(m, val_data),\n                    \"recall_train\": recall(m, train_data),\n                    \"recall_val\": recall(m, val_data),\n                    \"auc_train\": get_auc(m, train_data),\n                    \"auc_val\": get_auc(m, val_data),\n                }\n                gridsearch[modelname] = metrics\n                print(f'Accuracy (train):{metrics[\"acc_train\"]} (val):{metrics[\"acc_val\"]}')\n                print(f'Precision (train):{metrics[\"precision_train\"]} (val):{metrics[\"precision_val\"]}')\n                print(f'Recall (train):{metrics[\"recall_train\"]} (val):{metrics[\"recall_val\"]}')\n                print(f'AUC (train):{metrics[\"auc_train\"]} (val):{metrics[\"auc_val\"]}')\nPlease include the below output in your submission\nprint(gridsearch)\nTask: Which hyperparameter choice is the “best”? You should base this answer on the validation AUC. Use the other metrics as a guide to understand the kinds of predictions and mistakes that your model is likely make. Train a final model with those hyperparameter values.\n# TODO\nTask: Report the test accuracy and AUC for this model, and plot the confusion matrix over the test set.\n# TODO\nGraded Task: Explain why a model with high AUC may still produce consistently poor predictions for a subset of the population. You might find this article interesting: Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis; in particular, Figure 1 shows how test AUC differs male/female patients depending on the training set used.\n# TODO"
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "Lab 01",
    "section": "",
    "text": "In this lab, we will review linear models for classification, which was discussed in depth in CSC311. We will use this as an opportunity to review key ideas, including the splitting of the dataset into train/validation/test sets, optimization methods using Stochastic Gradient Descent, and so on. This lab reviews Python libraries that you have used in CSC311, including numpy, matplotlib and others.\nBut the main aim of this lab is to introduce a new Python library that we will be using throughout this course: PyTorch. PyTorch provides automatic differentiation capabilities and other neural network tools. This means that we do not need to compute gradients ourselves! Instead, we rely on PyTorch to build the computation graph and compute gradients. PyTorch can do this because it knows how to compute gradients for simple operations like addition, multiplication, ReLU activation, and common functions like exponentials, logarithms, and so on. The neural networks we build require computation that are combinations of these simple operations.\nFor now, we will we solve a multi-class classification problem in two ways: first with numpy, and then with PyTorch.\nBy the end of this lab, you will be able to:\n\nImplement and train a multi-class logistic regression model using PyTorch.\nCompute the accuracy metric for a machine learning model.\nCompute numerically, via numpy, the gradient of a linear model.\nCheck that PyTorch correctly computes gradients for a linear model via automatic differentiation.\nIdentify and explain the elements of the training loop in a PyTorch implementation.\nIdentify optimization parameters and hyperparameters, and explain how hyperparameter choices impact training, underfitting and overfitting (i.e. bias/variance decomposition).\n\nPlease work in groups of 1-2 during the lab, but submit your own solution individually.\n\n\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab01.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your explanation of the purpose of the training and validation sets. (1 point)\nPart 2. Your implementation of accuracy_basic. (2 point)\nPart 2. Your implementation of accuracy_vectorized. (1 point)\nPart 2. Your implementation of accuracy. (1 point)\nPart 3. Your computation of model_bias_grad. (2 points)\nPart 4. Your completion of train_model. (1 point)\nPart 5. Your list of hyperparameters. (1 point)\nPart 5. Your explanation of what happens if the learning rate is too large. (1 point)\n\n\n\n\nWe will use Google Colab to open IPython Notebook (ipynb) file. This tool allows us to write and execute Python code through our browser, without any environmental setup.\nHere are the steps to open ipynb file on Google Colab.\n\nDownload lab01.ipynb, available from the Quercus course website.\nClick on the following link to open Google Colab: https://colab.research.google.com/\nClick “Upload”, then choose the file which has been downloaded in step 1.\n\nAnd that’s it! Now we can start writing the codes, creating the new code or text cell, etc.\nHere are some basic functionalities and features that you might find useful.\n\nRunning a cell\nClick the run button on the left side of the code cell (looks like a “play” button with a triangle in a circle)\nor\npress SHIFT + ENTER.\nInstalling libraries using Bash Commands\nAlthough most of the commonly used libraries (e.g. NumPy, Pandas, Matplotlib) are pre-installed, we may occasionally ask you to install new libraries or run other bash commands. Bash commands can be run by prefixing instructions in a code cell with ‘!’ in Google Colab (One exception: ‘cd’ command can be run by prefixing with ‘%’), e.g. !pip install [package name]\nMounting Google Drive\nYou may optionally mount Google Drive. Click the files button on the left pane, then click on ‘mount drive’ button (looks like a file icon with a google drive logo).\nor\nRun the following code snippet: from google.colab import drive     drive.mount('/content/drive') By mounting the drive, we can use any files or folders in our drive by using the path as follows: /content/drive/MyDrive/[folder name] For example, we can read the csv file uploaded in the drive using Pandas library as follows: pd.read_csv('/content/drive/MyDrive/myfolder/myfile.csv')\n\nNow, we are ready to import the necessary packages and begin our lab.\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nWe will use the MNIST data set, which consists of hand-written digits. This dataset is available within the torchvision.datasets library. The dataset creators divided the MNIST imgages into a training and test set, so that different researchers report test accuracy on a consistent set of images. (Recall that the test set is to be set aside and not used during training or to make any model decisions, and that it is used to estimate how well your models generalize to new data that it has never seen before.)\nfrom torchvision.datasets import MNIST\n\nmnist_train = MNIST(root=\".\",      # where on the disk to store the data\n                    download=True, # download the data if it does not already exist\n                    train=True)    # use the training set (rather than the test set)\nTask: If different practitioners are exploring machine learning models for the same task and data set, why is it important that they use these practitioners report their test performance (e.g., accuracy) on the same test set?\n# TODO: Write your answer here\nIt is always a good idea to visually inspect our data before working with it. First, let’s take a look at the first element of the training set:\nprint(mnist_train[0]) # a tuple consisting of the image, and the label (5)\nThe image can be displayed on Google Colab using matplotlib:\nplt.imshow(mnist_train[0][0], cmap='gray') # display the image\nIt is important to note that images are represented using numbers on your machine. Converting this image into a numpy array shows a representation of the image using 28x28 numbers, each representing a pixel value.\nnp.array(mnist_train[0][0])\nTask: What does the numerical value 0 (smallest possible value) mean in the image? What about the largest possible value, 255?\n# TODO: Write your answer here\nFor our purposes, we will only use the first 5000 elements of the training set. This is to make training faster.\nPyTorch also makes it easy to apply pre-processing transformations to the data, for example to normalize the data prior to using for training. We will use the standard preprocessing functions to transform the images into tensors for PyTorch to be able to use. This transformation also changes the values to be floating-point numbers between 0 and 1. Performing this transformation to PyTorch tensors now makes it easier to use PyTorch functionalities to help us create minibatches with this data.\nimport torchvision.transforms as transforms\n\nmnist_data = MNIST(root=\".\",      # where on the disk to store the data\n                   download=True, # download the data if it does not already exist\n                   train=True,    # use the canonical training set (rather than the test set)\n                   transform=transforms.ToTensor()) # transforms the images into PyTorch tensors\nmnist_data = list(mnist_data)[:5000]\n\nprint(mnist_data[0]) # a tuple consisting of a PyTorch tensor of shape (1, 28, 28) and an integer target label\nWe will split the data set into 3000 for training, 1000 for validation, and 1000 for test:\ntrain_data = mnist_data[:3000]\nval_data   = mnist_data[3000:4000]\ntest_data  = mnist_data[4000:]\nGraded Task: We described, above, that the purpose of the test set is to estimate how well our models would generalize to new data that it has never seen before. What are the purposes of the training and validation sets?\n# TODO: Write your answer here\n\n\n\nYou may recall from CSC311 that in machine learning, we often describe a model by first describing how to make predictions with a model (e.g., multi-class classification), and then describe how to find appropriate weights (e.g., an optimization method like gradient descent). We will follow that process here.\nRecall that, mathematically, the multi-class classification model can be written as follows:\n\\[{\\bf y} = \\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})\\]\nWhere the \\[{\\bf x}\\] vector represents the input (i.e., the vector representation of the MNIST image), and the \\[{\\bf y}\\] vector contains the predicted probably of the image being in each class (i.e., predicted probability of the image being of each digit 0-9).\nTask: In the MNIST image classification tas, what are the shapes of the quantities \\[{\\bf x}\\], \\[{\\bf W}\\], \\[{\\bf b}\\] and \\[{\\bf y}\\]?\n# TODO: Write your answer here\nThe matrix \\[{\\bf W}\\] and \\[{\\bf b}\\] are the parameters of the model. The matrix \\[{\\bf W}\\] is sometimes called the weight matrix and the vector \\[{\\bf b}\\] the bias vector, but these parameters taken together are often referred to collectively as the weights. “Good” values of the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are those that would produce values of the vector \\[{\\bf y}\\] that more accurately match the actual target label. We will need to discuss what “good” means and how to measure “good”ness and optimize it. For now, let’s begin by applying and analyzing a “bad” model: a model where the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are chosen randomly.\nNotice that there are two parts to the computation \\[\\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})\\]: there is a linear transformation on \\[{\\bf X}\\], and then there is a softmax operation. PyTorch models these two components separately.\nThe linear transformation is modeled as a Python class in PyTorch. Since the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are parameters of the linear transformation portion of the computation, the weights and biases are class attributes. The output of the linear transformation step is typically denoted using the symbol \\[{\\bf z}\\], and is called the logit (or unnormalized logits).\nimport torch.nn as nn\n\nmodel = nn.Linear(in_features=784,\n                  out_features=10)\n\nprint(model.weight) # the weight parameter, initialized to random values\nprint(model.bias)   # the bias parameter, initialized to random values\nThis model object can be called like a function to perform the computation \\[{\\bf W}{\\bf x} + {\\bf b}\\]:\n# reshape the input image into the shape [1, 784]\n# PyTorch always expects inputs of shape [batch_size, num_features]\nx = train_data[0][0].reshape(1, 784)\n\n# Computes z = Wx + b, also called the *logit*\nz = model(x) \nprint(z)\nThe softmax operation has no trainable parameters (i.e., no numbers that we tune that would effect the predictions of our model). Torch ahs a function torch.softmax that performs this operation, which normalizes the prediction so that the prediction represents a probability distribution. The dim parameter to the function tells PyTorch which dimension represent the different label classes (as opposed to, say, the dimension that represents different images in a batch).\ny = torch.softmax(z, dim=1)\nprint(y) # notice that this represents a probability distribution!\nIf we are looking for a discrete/point prediction rather than a probability distribution, we will typically choose the label that the model believes to be most probable:\npred = torch.argmax(y, axis=1)\nprint(pred) # a prediction of which class/digit the image belong to\nNote that if all we wanted was a discrete prediction, we need not compute the softmax! (Why is that? How can we prove this property mathematically, using the definition of the softmax operation?)\npred = torch.argmax(z, axis=1)\nprint(pred)\nGraded Task: Complete the function below, which computes the accuracy of a PyTorch model over a dataset. The accuracy metric is the proportion of predictions made that is correct, or:\n\\[\\frac{\\textrm{the number of correct predictions}}{\\textrm{total number of predictions made}}\\]\ndef accuracy_basic(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A torch.nn model. We will only be passing `nn.Linear` models.\n                  However, to make your code more generally useful, do not access\n                  `model.weight` and `model.bias` parameters directly. These\n                  class attributes may not exist for other kinds of models.\n        `dataset` - A list of 2-tuples of the form (x, t), where `x` is a PyTorch\n                  tensor of shape [1, 28, 28] representing an MNIST image,\n                  and `t` is the corresponding target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    total = 0      # count the total number of predictions made\n    correct = 0    # count the number of correct predictions made\n    for img, t in dataset:\n        x = None # TODO - what should the input be? Recall that \n        x = img.reshape(1, 784) # SOLUTION\n        z = None # TODO - how can we compute z = Wx + b using `model`?\n        z = model(x) # SOLUTION\n        pred = None # TODO - how can we obtain a point prediction?\n        pred = int(torch.argmax(z, axis=1)) # SOLUTION\n        if t == pred:\n            correct += 1\n        total += 1\n    return correct / total\n\n\nprint(\"Accuracy over the training set:\")\nprint(accuracy_basic(model, train_data))\nTask: Explain why we would expect the training accuracy above to be poor.\n# TODO: Your explanation goes here.\nOne other nice thing about our nn.Linear model is that PyTorch vectorizes computations for us: we can make predictions for many images at the same time. Here is a rudimentary example where we make predictions for the first three images of our training set.\nx1 = train_data[0][0].reshape(1, 784)\nx2 = train_data[1][0].reshape(1, 784)\nx3 = train_data[2][0].reshape(1, 784)\n\nX = torch.cat([x1, x2, x3]).reshape(-1, 784) # note the -1 value here, explained below\nprint(X.shape) # Pytorch figures out that the shape of this tensor needs to be [3, 784]\nThe above code uses a features of PyTorch’s reshape() method that allows you to use the size -1 as a placeholder value and let PyTorch figure out what the correct size should be.\nNow, we can make predictions for all 3 images simultaneously\nz = model(X)\ny = torch.softmax(z, dim=1)\nprint(y)\nTask Complete the function accuracy_vectorized that outputs the same result as accuracy_basic, but uses vectorization to compute predictions for all inputs in the dataset simultaneously.\ndef accuracy_vectorized(model, dataset):\n    \"\"\"\n    Same signature as the `accuracy_basic()` function, but the call to model() is vectorized\n    \"\"\"\n    X = torch.concat([x for x, t in dataset])\n    t = torch.Tensor([t for x, t in dataset])\n    z = None # TODO: use a single call to model() to compute the prediction for all inputs\n    z = model(X.reshape(-1, 784)) # SOLUTION\n    pred = None # TODO: `pred` should have the same shape as `t`\n    pred = torch.argmax(z, axis=1)  # SOLUTION\n    correct = int(torch.sum(t == pred)) # count the number of correct predictions\n    total = t.shape[0]                  # count the total number of predictions made\n    return correct / total\nTask: Compare the runtime of accuracy_basic and accuracy_vectorized by running the two cells below. The line %%time prints the amount of time that Colab takes to run the code in the cell. The function call is repeated 100 times so that we can more clearly see the difference in runtime. Using what you learned from CSC311, explain why accuracy_vectorized is faster.\nimport time\n\nstart_time = time.time()\nfor i in range(100):\n    accuracy_basic(model, train_data)\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Elapsed time: {elapsed_time:.4f} seconds\")\nimport time\n\nstart_time = time.time()\nfor i in range(100):\n    accuracy_vectorized(model, train_data)\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n# TODO: Your explanation goes here\nIf our data set is large, feeding all inputs into the model at the same time may result in an out of memory error. Thus, we may find PyTorch’s DataLoader useful. This class takes our data set and the desired batch size, and splits the data into mini-batches of that size. We can use a loop to iterate over the minibatches:\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=100)\nfor X, t in train_loader:\n    print(X.shape)\n    print(t.shape)\n    break\nTask: Complete the definition of the accuracy function below:\ndef accuracy(model, dataset):\n    \"\"\"\n    Same signature as the `accuracy_basic()` function, but we will use a DataLoader and process\n    100 images at a time\n    \"\"\"\n    correct, total = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for X, t in loader:\n        z = None # TODO: use a single call to `model()` here as before\n        z = model(X.reshape(-1, 784)) # SOLUTION\n        pred = None # TODO: `pred` should have the same shape as `t`\n        pred = torch.argmax(z, axis=1)  # SOLUTION\n        # TODO: update `correct` and `total`\n        correct += int(torch.sum(t == pred))  # SOLUTION\n        total   += t.shape[0]  # SOLUTION\n    return correct / total\n\naccuracy(model, train_data)\n\n\n\nNow that we understand how a linear model makes predictions, we can explore how to modify its parameters to produce a model that makes “better” predictions. To do this, we will define a measure of “good”ness (or rather, “bad”ness) of a model that is differentiable with respect to the parameters. Differentiability is important, because it allows us to compute derivatives with respect to these parameters, which tells us how to tune the parameters to decrease “bad”ness.\nThis “badness” metric is called a loss function. A loss function compares the model prediction against the ground-truth target and produces a value representing how different the prediction is from the target. Like in CSC311, we will use the Cross-Entropy Loss for multi-class classification. In statistics courses you may learn about the theoretical reasons why the cross-entropy loss is appropriate for the multi-class classification task.\n\\[\\mathcal{L}(y, t) = - t \\log(y) - (1-t) \\log(1-y)\\]\nYou can read more about PyTorch’s implementation of the Cross-entropy loss here: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html One key thing to note is that the cross entropy loss takes as input the unnormalized logits (the z’s) and not the post-softmax, normalized probabilities (the y’s).\ncriterion = nn.CrossEntropyLoss()\nBefore we go further, let’s get some more intuition about the cross-entropy loss. We first demonstrate the cross entropy loss in action by computing hte “badness”\nx = train_data[0][0].reshape(1, 784)        # input\nt = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\nz = model(x.reshape(-1, 784))               # prediction logit\n\nloss = criterion(z, t)\n\nprint(loss)\nGraded Task: Consider the code below. What value of label would produce the lowest cross-entropy loss? Why? To answer this question, start by exploring the arguments passed to the call to criterion, and form an understanding of what those arugments represent. (Why are there 3 possible values of label? What do the 3 floating-point values in the first argument to criterion represent?)\nlabel = None # 0, 1, or 2?\nlabel = 2 # SOLUTION\n\nloss = criterion(torch.Tensor([[1.5, 2.2, 3.2]]),\n                 torch.Tensor([label]).long())\nprint(loss)\n# TODO: Explain why the label choice produces the lowest loss.\nNow that we have an understanding of the cross-entropy loss, we can begin to understand how PyTorch computes gradients. Notice that when we print the variable loss above, the value printed is not only a numerical value, but also has other information attached. These information help PyTorch compute gradients, and these gradients can be propagated backwards using the loss.backward() method.\nUnder the hood, PyTorch performs backpropagation, which we discussed in CSC311 and will review again in the coming weeks. After loss.backward() is computed, tensors like model.bias and model.weights will have gradients.\n# recreate the model to clean up some hidden variables\nmodel = nn.Linear(in_features=784, out_features=10)\n\nx = train_data[0][0].reshape(1, 784)        # input\nt = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\nz = model(x.reshape(-1, 784))               # prediction logit\n\nloss = criterion(z, t)\nprint(loss)\n\nloss.backward() # propagate the gradients with respect to the loss\nprint(model.bias.grad) # the gradient of the loss with respect to the bias vector\nprint(model.weight.grad) # the gradient of the loss with respect to the weight matrix\nGraded Task: Verify that model.bias.grad is correct by computing this gradient explicitly in PyTorch. You may wish to begin by reviewing your past CSC311 notes, or by using calculus to find an expression to represent this quantity.\nt_onehot = torch.eye(10)[t] # You may find this useful. (Why? What does this quantity represent)\nmodel_bias_grad = None      # TODO\nmodel_bias_grad = (torch.softmax(z, dim=1) - t_onehot)   # SOLUTION\n\nprint(model_bias_grad) # should be the same as model.bias.grad\nThe gradient computation works in a vectorized setting as well.\n# create a data set with 3 inputs, 3 targets\n\nx1 = train_data[0][0].reshape(1, 784)\nx2 = train_data[1][0].reshape(1, 784)\nx3 = train_data[1][0].reshape(1, 784)\nX = torch.cat([x1, x2, x3]).reshape(-1, 784)\nt = torch.Tensor([train_data[0][1], train_data[1][1], train_data[2][1]]).long()\n                 \nprint(X.shape)\nprint(t)\nThe average loss (mean) across the data points are shown:\nmodel = nn.Linear(in_features=784, out_features=10)\nz = model(X)\nloss = criterion(z, t)\nprint(loss)\n\nloss.backward() # propagate the gradients with respect to the loss\nGraded Task: Verify that model.bias.grad is correct when using vectorized input by computing this gradient explicitly in PyTorch. Again, we recommend working this out by hand first. After you have done so, you may find the function torch.sum() or torch.mean() helpful.\nt_onehot = torch.eye(10)[t] # TODO: understand the shape of this quantity before using it\nmodel_bias_grad = None      # TODO\nmodel_bias_grad = torch.mean((torch.softmax(z, dim=1) - t_onehot), axis=0) # SOLUTION\n\nprint(model_bias_grad)\nprint(model.bias.grad) # should be the same as above\n\n\n\nWe are almost ready to use PyTorch to train the model. One last piece that we need is an optimizer that updates the model parameter based on the gradient. This update can be done in different ways, and the most basic approach discussed in CSC311 was using gradient descent.\n\\[{\\bf W} \\leftarrow {\\bf W} - \\lambda \\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}}\\]\nTODO: description of gradient descent here!!\nWhen we use the entire training data set to compute the gradient of the mean loss (with respect to each parameter), we call the approach full batch gradient descent. However, this approach is expensive if we have a large training set. Typically, we approximate this mean loss using a minibatch, or a small sample of the training set. Using gradient descent with this approximate gradient is called stochastic gradient descent.\nPyTorch has built-in classes inside the package torch.optim to perform these gradient update steps. We will use the SGD class. Initializing this class requires a few things, including the list of model parameters that we want to optimize. For us, the list of parameters to optimize include model.weight and model.bias, which we can obtain by calling model.parameters().\nimport torch.optim as optim\n\noptimizer = optim.SGD(model.parameters(), # the parameters to optimize\n                      lr=0.005)           # the learning rate\nThere are two important optimizer methods that we will use. First is the optimizer.zero_grad() method, which clears the .grad attribute of the parameters. Let’s see how it works:\nprint(model.bias.grad) # should be a nonzero value from above\n\noptimizer.zero_grad()\n\nprint(model.bias.grad) # should be cleared\nThe other method is the step() method, which performs the actual gradient descent update.\nmodel = nn.Linear(in_features=784, out_features=10)\noptimizer = optim.SGD(model.parameters(), lr=0.005)\n\nprint(model.bias)\n\nz = model(X)\nloss = criterion(z, t)\nloss.backward()\n\nprint(model.bias.grad) # gradient\nprint(model.bias - 0.005 * model.bias.grad) # this should be the updated value of model.bias\n\noptimizer.step()\n\nprint(model.bias) # should be different compared to above\nNow that we have everything in place, we are ready to write the training loop:\nGraded Task: Complete the function below, which trains the model.\ndef train_model(model,\n                train_data,\n                val_data,\n                learning_rate=0.005,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=10):\n    \"\"\"\n    Train the PyTorch model `model` using the training data `train_data` and the\n    corresponding hyperparameters. Report training loss, training accuracy, and\n    validation accuracy every `plot_every` iterations.\n    \"\"\"\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (images, labels) in enumerate(train_loader):\n                z = None # TODO\n                z = model(images.reshape(-1, 784)) # SOLUTION\n\n                loss = None # TODO\n                loss = criterion(z, labels) # SOLUTION\n\n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    train_loss.append(float(loss))\n                    train_acc.append(accuracy(model, train_data))\n                    val_acc.append(accuracy(model, val_data))\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel = nn.Linear(784, 10)\ntrain_model(model, train_data, val_data)\n\n\n\nOur training process is not yet complete. In general, the performance of machine learning models depend heavily on the hyperparameter settings used. Hyperparameters are settings that cannot be tuned via gradient descent in a straightforward way. These settings can affect the model architecture, but may also affect the optimization process.\nGraded Task: What are some examples of hyperparameters that affect the model architecture of a model? You may consider some hyperparameters that you learned about in CSC311. List at least 3 examples.\n# TODO: List at least 3 examples\nTask: What are some examples of hyperparameters that affect the optimization process?\n# TODO: List at least 2 examples\nModel architecture related hyperparameters are important to tune and should not be neglected in practical application. However, since we are working with a linear model right now, we are limited in this lab to exploring the optimization hyperparameters.\nTask: What happens if the learning rate is too small? Provide an example training curve by calling train_model with a low learning rate, and describe the features of the training curve that you see.\n# TODO\nGraded Task: What happens if the learning rate is too large? Provide an example training curve by calling train_model with a large learning rate, and describe the features of the training curve that you see.\n# TODO\nHyperparameter choices interact with one another. Thus, practitioners use a strategy called grid search to try all variations of hyperparameters from a set of hyperparameters. We will not do that here since linear models don’t yet have many hyperparameters to work with.\nTask: Choose the best model that you have trained. Typically we make this choice using the validation accuracy. To understand how well the model you choose would generalize to unseen data, we use the test data. Compute the test accuracy for this model by calling the function accuracy() on the model and the test data.\n# TODO: Compute the test accuracy"
  },
  {
    "objectID": "labs/lab01.html#submission",
    "href": "labs/lab01.html#submission",
    "title": "Lab 01",
    "section": "",
    "text": "If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab01.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your explanation of the purpose of the training and validation sets. (1 point)\nPart 2. Your implementation of accuracy_basic. (2 point)\nPart 2. Your implementation of accuracy_vectorized. (1 point)\nPart 2. Your implementation of accuracy. (1 point)\nPart 3. Your computation of model_bias_grad. (2 points)\nPart 4. Your completion of train_model. (1 point)\nPart 5. Your list of hyperparameters. (1 point)\nPart 5. Your explanation of what happens if the learning rate is too large. (1 point)"
  },
  {
    "objectID": "labs/lab01.html#google-colab-setup",
    "href": "labs/lab01.html#google-colab-setup",
    "title": "Lab 01",
    "section": "",
    "text": "We will use Google Colab to open IPython Notebook (ipynb) file. This tool allows us to write and execute Python code through our browser, without any environmental setup.\nHere are the steps to open ipynb file on Google Colab.\n\nDownload lab01.ipynb, available from the Quercus course website.\nClick on the following link to open Google Colab: https://colab.research.google.com/\nClick “Upload”, then choose the file which has been downloaded in step 1.\n\nAnd that’s it! Now we can start writing the codes, creating the new code or text cell, etc.\nHere are some basic functionalities and features that you might find useful.\n\nRunning a cell\nClick the run button on the left side of the code cell (looks like a “play” button with a triangle in a circle)\nor\npress SHIFT + ENTER.\nInstalling libraries using Bash Commands\nAlthough most of the commonly used libraries (e.g. NumPy, Pandas, Matplotlib) are pre-installed, we may occasionally ask you to install new libraries or run other bash commands. Bash commands can be run by prefixing instructions in a code cell with ‘!’ in Google Colab (One exception: ‘cd’ command can be run by prefixing with ‘%’), e.g. !pip install [package name]\nMounting Google Drive\nYou may optionally mount Google Drive. Click the files button on the left pane, then click on ‘mount drive’ button (looks like a file icon with a google drive logo).\nor\nRun the following code snippet: from google.colab import drive     drive.mount('/content/drive') By mounting the drive, we can use any files or folders in our drive by using the path as follows: /content/drive/MyDrive/[folder name] For example, we can read the csv file uploaded in the drive using Pandas library as follows: pd.read_csv('/content/drive/MyDrive/myfolder/myfile.csv')\n\nNow, we are ready to import the necessary packages and begin our lab.\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "labs/lab01.html#part-1.-data",
    "href": "labs/lab01.html#part-1.-data",
    "title": "Lab 01",
    "section": "",
    "text": "We will use the MNIST data set, which consists of hand-written digits. This dataset is available within the torchvision.datasets library. The dataset creators divided the MNIST imgages into a training and test set, so that different researchers report test accuracy on a consistent set of images. (Recall that the test set is to be set aside and not used during training or to make any model decisions, and that it is used to estimate how well your models generalize to new data that it has never seen before.)\nfrom torchvision.datasets import MNIST\n\nmnist_train = MNIST(root=\".\",      # where on the disk to store the data\n                    download=True, # download the data if it does not already exist\n                    train=True)    # use the training set (rather than the test set)\nTask: If different practitioners are exploring machine learning models for the same task and data set, why is it important that they use these practitioners report their test performance (e.g., accuracy) on the same test set?\n# TODO: Write your answer here\nIt is always a good idea to visually inspect our data before working with it. First, let’s take a look at the first element of the training set:\nprint(mnist_train[0]) # a tuple consisting of the image, and the label (5)\nThe image can be displayed on Google Colab using matplotlib:\nplt.imshow(mnist_train[0][0], cmap='gray') # display the image\nIt is important to note that images are represented using numbers on your machine. Converting this image into a numpy array shows a representation of the image using 28x28 numbers, each representing a pixel value.\nnp.array(mnist_train[0][0])\nTask: What does the numerical value 0 (smallest possible value) mean in the image? What about the largest possible value, 255?\n# TODO: Write your answer here\nFor our purposes, we will only use the first 5000 elements of the training set. This is to make training faster.\nPyTorch also makes it easy to apply pre-processing transformations to the data, for example to normalize the data prior to using for training. We will use the standard preprocessing functions to transform the images into tensors for PyTorch to be able to use. This transformation also changes the values to be floating-point numbers between 0 and 1. Performing this transformation to PyTorch tensors now makes it easier to use PyTorch functionalities to help us create minibatches with this data.\nimport torchvision.transforms as transforms\n\nmnist_data = MNIST(root=\".\",      # where on the disk to store the data\n                   download=True, # download the data if it does not already exist\n                   train=True,    # use the canonical training set (rather than the test set)\n                   transform=transforms.ToTensor()) # transforms the images into PyTorch tensors\nmnist_data = list(mnist_data)[:5000]\n\nprint(mnist_data[0]) # a tuple consisting of a PyTorch tensor of shape (1, 28, 28) and an integer target label\nWe will split the data set into 3000 for training, 1000 for validation, and 1000 for test:\ntrain_data = mnist_data[:3000]\nval_data   = mnist_data[3000:4000]\ntest_data  = mnist_data[4000:]\nGraded Task: We described, above, that the purpose of the test set is to estimate how well our models would generalize to new data that it has never seen before. What are the purposes of the training and validation sets?\n# TODO: Write your answer here"
  },
  {
    "objectID": "labs/lab01.html#part-2.-a-linear-model-in-pytorch",
    "href": "labs/lab01.html#part-2.-a-linear-model-in-pytorch",
    "title": "Lab 01",
    "section": "",
    "text": "You may recall from CSC311 that in machine learning, we often describe a model by first describing how to make predictions with a model (e.g., multi-class classification), and then describe how to find appropriate weights (e.g., an optimization method like gradient descent). We will follow that process here.\nRecall that, mathematically, the multi-class classification model can be written as follows:\n\\[{\\bf y} = \\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})\\]\nWhere the \\[{\\bf x}\\] vector represents the input (i.e., the vector representation of the MNIST image), and the \\[{\\bf y}\\] vector contains the predicted probably of the image being in each class (i.e., predicted probability of the image being of each digit 0-9).\nTask: In the MNIST image classification tas, what are the shapes of the quantities \\[{\\bf x}\\], \\[{\\bf W}\\], \\[{\\bf b}\\] and \\[{\\bf y}\\]?\n# TODO: Write your answer here\nThe matrix \\[{\\bf W}\\] and \\[{\\bf b}\\] are the parameters of the model. The matrix \\[{\\bf W}\\] is sometimes called the weight matrix and the vector \\[{\\bf b}\\] the bias vector, but these parameters taken together are often referred to collectively as the weights. “Good” values of the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are those that would produce values of the vector \\[{\\bf y}\\] that more accurately match the actual target label. We will need to discuss what “good” means and how to measure “good”ness and optimize it. For now, let’s begin by applying and analyzing a “bad” model: a model where the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are chosen randomly.\nNotice that there are two parts to the computation \\[\\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})\\]: there is a linear transformation on \\[{\\bf X}\\], and then there is a softmax operation. PyTorch models these two components separately.\nThe linear transformation is modeled as a Python class in PyTorch. Since the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are parameters of the linear transformation portion of the computation, the weights and biases are class attributes. The output of the linear transformation step is typically denoted using the symbol \\[{\\bf z}\\], and is called the logit (or unnormalized logits).\nimport torch.nn as nn\n\nmodel = nn.Linear(in_features=784,\n                  out_features=10)\n\nprint(model.weight) # the weight parameter, initialized to random values\nprint(model.bias)   # the bias parameter, initialized to random values\nThis model object can be called like a function to perform the computation \\[{\\bf W}{\\bf x} + {\\bf b}\\]:\n# reshape the input image into the shape [1, 784]\n# PyTorch always expects inputs of shape [batch_size, num_features]\nx = train_data[0][0].reshape(1, 784)\n\n# Computes z = Wx + b, also called the *logit*\nz = model(x) \nprint(z)\nThe softmax operation has no trainable parameters (i.e., no numbers that we tune that would effect the predictions of our model). Torch ahs a function torch.softmax that performs this operation, which normalizes the prediction so that the prediction represents a probability distribution. The dim parameter to the function tells PyTorch which dimension represent the different label classes (as opposed to, say, the dimension that represents different images in a batch).\ny = torch.softmax(z, dim=1)\nprint(y) # notice that this represents a probability distribution!\nIf we are looking for a discrete/point prediction rather than a probability distribution, we will typically choose the label that the model believes to be most probable:\npred = torch.argmax(y, axis=1)\nprint(pred) # a prediction of which class/digit the image belong to\nNote that if all we wanted was a discrete prediction, we need not compute the softmax! (Why is that? How can we prove this property mathematically, using the definition of the softmax operation?)\npred = torch.argmax(z, axis=1)\nprint(pred)\nGraded Task: Complete the function below, which computes the accuracy of a PyTorch model over a dataset. The accuracy metric is the proportion of predictions made that is correct, or:\n\\[\\frac{\\textrm{the number of correct predictions}}{\\textrm{total number of predictions made}}\\]\ndef accuracy_basic(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A torch.nn model. We will only be passing `nn.Linear` models.\n                  However, to make your code more generally useful, do not access\n                  `model.weight` and `model.bias` parameters directly. These\n                  class attributes may not exist for other kinds of models.\n        `dataset` - A list of 2-tuples of the form (x, t), where `x` is a PyTorch\n                  tensor of shape [1, 28, 28] representing an MNIST image,\n                  and `t` is the corresponding target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    total = 0      # count the total number of predictions made\n    correct = 0    # count the number of correct predictions made\n    for img, t in dataset:\n        x = None # TODO - what should the input be? Recall that \n        x = img.reshape(1, 784) # SOLUTION\n        z = None # TODO - how can we compute z = Wx + b using `model`?\n        z = model(x) # SOLUTION\n        pred = None # TODO - how can we obtain a point prediction?\n        pred = int(torch.argmax(z, axis=1)) # SOLUTION\n        if t == pred:\n            correct += 1\n        total += 1\n    return correct / total\n\n\nprint(\"Accuracy over the training set:\")\nprint(accuracy_basic(model, train_data))\nTask: Explain why we would expect the training accuracy above to be poor.\n# TODO: Your explanation goes here.\nOne other nice thing about our nn.Linear model is that PyTorch vectorizes computations for us: we can make predictions for many images at the same time. Here is a rudimentary example where we make predictions for the first three images of our training set.\nx1 = train_data[0][0].reshape(1, 784)\nx2 = train_data[1][0].reshape(1, 784)\nx3 = train_data[2][0].reshape(1, 784)\n\nX = torch.cat([x1, x2, x3]).reshape(-1, 784) # note the -1 value here, explained below\nprint(X.shape) # Pytorch figures out that the shape of this tensor needs to be [3, 784]\nThe above code uses a features of PyTorch’s reshape() method that allows you to use the size -1 as a placeholder value and let PyTorch figure out what the correct size should be.\nNow, we can make predictions for all 3 images simultaneously\nz = model(X)\ny = torch.softmax(z, dim=1)\nprint(y)\nTask Complete the function accuracy_vectorized that outputs the same result as accuracy_basic, but uses vectorization to compute predictions for all inputs in the dataset simultaneously.\ndef accuracy_vectorized(model, dataset):\n    \"\"\"\n    Same signature as the `accuracy_basic()` function, but the call to model() is vectorized\n    \"\"\"\n    X = torch.concat([x for x, t in dataset])\n    t = torch.Tensor([t for x, t in dataset])\n    z = None # TODO: use a single call to model() to compute the prediction for all inputs\n    z = model(X.reshape(-1, 784)) # SOLUTION\n    pred = None # TODO: `pred` should have the same shape as `t`\n    pred = torch.argmax(z, axis=1)  # SOLUTION\n    correct = int(torch.sum(t == pred)) # count the number of correct predictions\n    total = t.shape[0]                  # count the total number of predictions made\n    return correct / total\nTask: Compare the runtime of accuracy_basic and accuracy_vectorized by running the two cells below. The line %%time prints the amount of time that Colab takes to run the code in the cell. The function call is repeated 100 times so that we can more clearly see the difference in runtime. Using what you learned from CSC311, explain why accuracy_vectorized is faster.\nimport time\n\nstart_time = time.time()\nfor i in range(100):\n    accuracy_basic(model, train_data)\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Elapsed time: {elapsed_time:.4f} seconds\")\nimport time\n\nstart_time = time.time()\nfor i in range(100):\n    accuracy_vectorized(model, train_data)\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n# TODO: Your explanation goes here\nIf our data set is large, feeding all inputs into the model at the same time may result in an out of memory error. Thus, we may find PyTorch’s DataLoader useful. This class takes our data set and the desired batch size, and splits the data into mini-batches of that size. We can use a loop to iterate over the minibatches:\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=100)\nfor X, t in train_loader:\n    print(X.shape)\n    print(t.shape)\n    break\nTask: Complete the definition of the accuracy function below:\ndef accuracy(model, dataset):\n    \"\"\"\n    Same signature as the `accuracy_basic()` function, but we will use a DataLoader and process\n    100 images at a time\n    \"\"\"\n    correct, total = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for X, t in loader:\n        z = None # TODO: use a single call to `model()` here as before\n        z = model(X.reshape(-1, 784)) # SOLUTION\n        pred = None # TODO: `pred` should have the same shape as `t`\n        pred = torch.argmax(z, axis=1)  # SOLUTION\n        # TODO: update `correct` and `total`\n        correct += int(torch.sum(t == pred))  # SOLUTION\n        total   += t.shape[0]  # SOLUTION\n    return correct / total\n\naccuracy(model, train_data)"
  },
  {
    "objectID": "labs/lab01.html#part-3.-cross-entropy-loss-and-automatic-gradient-computation",
    "href": "labs/lab01.html#part-3.-cross-entropy-loss-and-automatic-gradient-computation",
    "title": "Lab 01",
    "section": "",
    "text": "Now that we understand how a linear model makes predictions, we can explore how to modify its parameters to produce a model that makes “better” predictions. To do this, we will define a measure of “good”ness (or rather, “bad”ness) of a model that is differentiable with respect to the parameters. Differentiability is important, because it allows us to compute derivatives with respect to these parameters, which tells us how to tune the parameters to decrease “bad”ness.\nThis “badness” metric is called a loss function. A loss function compares the model prediction against the ground-truth target and produces a value representing how different the prediction is from the target. Like in CSC311, we will use the Cross-Entropy Loss for multi-class classification. In statistics courses you may learn about the theoretical reasons why the cross-entropy loss is appropriate for the multi-class classification task.\n\\[\\mathcal{L}(y, t) = - t \\log(y) - (1-t) \\log(1-y)\\]\nYou can read more about PyTorch’s implementation of the Cross-entropy loss here: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html One key thing to note is that the cross entropy loss takes as input the unnormalized logits (the z’s) and not the post-softmax, normalized probabilities (the y’s).\ncriterion = nn.CrossEntropyLoss()\nBefore we go further, let’s get some more intuition about the cross-entropy loss. We first demonstrate the cross entropy loss in action by computing hte “badness”\nx = train_data[0][0].reshape(1, 784)        # input\nt = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\nz = model(x.reshape(-1, 784))               # prediction logit\n\nloss = criterion(z, t)\n\nprint(loss)\nGraded Task: Consider the code below. What value of label would produce the lowest cross-entropy loss? Why? To answer this question, start by exploring the arguments passed to the call to criterion, and form an understanding of what those arugments represent. (Why are there 3 possible values of label? What do the 3 floating-point values in the first argument to criterion represent?)\nlabel = None # 0, 1, or 2?\nlabel = 2 # SOLUTION\n\nloss = criterion(torch.Tensor([[1.5, 2.2, 3.2]]),\n                 torch.Tensor([label]).long())\nprint(loss)\n# TODO: Explain why the label choice produces the lowest loss.\nNow that we have an understanding of the cross-entropy loss, we can begin to understand how PyTorch computes gradients. Notice that when we print the variable loss above, the value printed is not only a numerical value, but also has other information attached. These information help PyTorch compute gradients, and these gradients can be propagated backwards using the loss.backward() method.\nUnder the hood, PyTorch performs backpropagation, which we discussed in CSC311 and will review again in the coming weeks. After loss.backward() is computed, tensors like model.bias and model.weights will have gradients.\n# recreate the model to clean up some hidden variables\nmodel = nn.Linear(in_features=784, out_features=10)\n\nx = train_data[0][0].reshape(1, 784)        # input\nt = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\nz = model(x.reshape(-1, 784))               # prediction logit\n\nloss = criterion(z, t)\nprint(loss)\n\nloss.backward() # propagate the gradients with respect to the loss\nprint(model.bias.grad) # the gradient of the loss with respect to the bias vector\nprint(model.weight.grad) # the gradient of the loss with respect to the weight matrix\nGraded Task: Verify that model.bias.grad is correct by computing this gradient explicitly in PyTorch. You may wish to begin by reviewing your past CSC311 notes, or by using calculus to find an expression to represent this quantity.\nt_onehot = torch.eye(10)[t] # You may find this useful. (Why? What does this quantity represent)\nmodel_bias_grad = None      # TODO\nmodel_bias_grad = (torch.softmax(z, dim=1) - t_onehot)   # SOLUTION\n\nprint(model_bias_grad) # should be the same as model.bias.grad\nThe gradient computation works in a vectorized setting as well.\n# create a data set with 3 inputs, 3 targets\n\nx1 = train_data[0][0].reshape(1, 784)\nx2 = train_data[1][0].reshape(1, 784)\nx3 = train_data[1][0].reshape(1, 784)\nX = torch.cat([x1, x2, x3]).reshape(-1, 784)\nt = torch.Tensor([train_data[0][1], train_data[1][1], train_data[2][1]]).long()\n                 \nprint(X.shape)\nprint(t)\nThe average loss (mean) across the data points are shown:\nmodel = nn.Linear(in_features=784, out_features=10)\nz = model(X)\nloss = criterion(z, t)\nprint(loss)\n\nloss.backward() # propagate the gradients with respect to the loss\nGraded Task: Verify that model.bias.grad is correct when using vectorized input by computing this gradient explicitly in PyTorch. Again, we recommend working this out by hand first. After you have done so, you may find the function torch.sum() or torch.mean() helpful.\nt_onehot = torch.eye(10)[t] # TODO: understand the shape of this quantity before using it\nmodel_bias_grad = None      # TODO\nmodel_bias_grad = torch.mean((torch.softmax(z, dim=1) - t_onehot), axis=0) # SOLUTION\n\nprint(model_bias_grad)\nprint(model.bias.grad) # should be the same as above"
  },
  {
    "objectID": "labs/lab01.html#part-4.-neural-network-training-via-stochastic-gradient-descent.",
    "href": "labs/lab01.html#part-4.-neural-network-training-via-stochastic-gradient-descent.",
    "title": "Lab 01",
    "section": "",
    "text": "We are almost ready to use PyTorch to train the model. One last piece that we need is an optimizer that updates the model parameter based on the gradient. This update can be done in different ways, and the most basic approach discussed in CSC311 was using gradient descent.\n\\[{\\bf W} \\leftarrow {\\bf W} - \\lambda \\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}}\\]\nTODO: description of gradient descent here!!\nWhen we use the entire training data set to compute the gradient of the mean loss (with respect to each parameter), we call the approach full batch gradient descent. However, this approach is expensive if we have a large training set. Typically, we approximate this mean loss using a minibatch, or a small sample of the training set. Using gradient descent with this approximate gradient is called stochastic gradient descent.\nPyTorch has built-in classes inside the package torch.optim to perform these gradient update steps. We will use the SGD class. Initializing this class requires a few things, including the list of model parameters that we want to optimize. For us, the list of parameters to optimize include model.weight and model.bias, which we can obtain by calling model.parameters().\nimport torch.optim as optim\n\noptimizer = optim.SGD(model.parameters(), # the parameters to optimize\n                      lr=0.005)           # the learning rate\nThere are two important optimizer methods that we will use. First is the optimizer.zero_grad() method, which clears the .grad attribute of the parameters. Let’s see how it works:\nprint(model.bias.grad) # should be a nonzero value from above\n\noptimizer.zero_grad()\n\nprint(model.bias.grad) # should be cleared\nThe other method is the step() method, which performs the actual gradient descent update.\nmodel = nn.Linear(in_features=784, out_features=10)\noptimizer = optim.SGD(model.parameters(), lr=0.005)\n\nprint(model.bias)\n\nz = model(X)\nloss = criterion(z, t)\nloss.backward()\n\nprint(model.bias.grad) # gradient\nprint(model.bias - 0.005 * model.bias.grad) # this should be the updated value of model.bias\n\noptimizer.step()\n\nprint(model.bias) # should be different compared to above\nNow that we have everything in place, we are ready to write the training loop:\nGraded Task: Complete the function below, which trains the model.\ndef train_model(model,\n                train_data,\n                val_data,\n                learning_rate=0.005,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=10):\n    \"\"\"\n    Train the PyTorch model `model` using the training data `train_data` and the\n    corresponding hyperparameters. Report training loss, training accuracy, and\n    validation accuracy every `plot_every` iterations.\n    \"\"\"\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (images, labels) in enumerate(train_loader):\n                z = None # TODO\n                z = model(images.reshape(-1, 784)) # SOLUTION\n\n                loss = None # TODO\n                loss = criterion(z, labels) # SOLUTION\n\n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    train_loss.append(float(loss))\n                    train_acc.append(accuracy(model, train_data))\n                    val_acc.append(accuracy(model, val_data))\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel = nn.Linear(784, 10)\ntrain_model(model, train_data, val_data)"
  },
  {
    "objectID": "labs/lab01.html#part-5.-hyperparameter-tuning",
    "href": "labs/lab01.html#part-5.-hyperparameter-tuning",
    "title": "Lab 01",
    "section": "",
    "text": "Our training process is not yet complete. In general, the performance of machine learning models depend heavily on the hyperparameter settings used. Hyperparameters are settings that cannot be tuned via gradient descent in a straightforward way. These settings can affect the model architecture, but may also affect the optimization process.\nGraded Task: What are some examples of hyperparameters that affect the model architecture of a model? You may consider some hyperparameters that you learned about in CSC311. List at least 3 examples.\n# TODO: List at least 3 examples\nTask: What are some examples of hyperparameters that affect the optimization process?\n# TODO: List at least 2 examples\nModel architecture related hyperparameters are important to tune and should not be neglected in practical application. However, since we are working with a linear model right now, we are limited in this lab to exploring the optimization hyperparameters.\nTask: What happens if the learning rate is too small? Provide an example training curve by calling train_model with a low learning rate, and describe the features of the training curve that you see.\n# TODO\nGraded Task: What happens if the learning rate is too large? Provide an example training curve by calling train_model with a large learning rate, and describe the features of the training curve that you see.\n# TODO\nHyperparameter choices interact with one another. Thus, practitioners use a strategy called grid search to try all variations of hyperparameters from a set of hyperparameters. We will not do that here since linear models don’t yet have many hyperparameters to work with.\nTask: Choose the best model that you have trained. Typically we make this choice using the validation accuracy. To understand how well the model you choose would generalize to unseen data, we use the test data. Compute the test accuracy for this model by calling the function accuracy() on the model and the test data.\n# TODO: Compute the test accuracy"
  },
  {
    "objectID": "labs/lab05.html",
    "href": "labs/lab05.html",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "In this lab, we will explore how training a neural network with some of the optimization methods discussed in the lecture can cause models to capture more information about the training data than we might intend. We will discuss why this may be problematic from a privacy perspective, and introduce the idea of differential privacy.\nFinally, this lab introduces an optimization strategy called DP-SGD or differentially private stochastic gradient descent. This strategy has some provable properties about the amount of information captured.\nThis lab also serves as an introductory guide to implementing optimization models that are presented in research papers. We hope that the techniques used in this lab build skills so that you can implement new techniques and ideas presented in other papers.\nBy the end of this lab, you will be able to:\n\nRecognize that typical methods of using SGD to train a neural network might capture too much information about the training data.\nArticulate the importance of privacy to stakeholders.\nExplain the components of the DP-SGD algorithm.\nCompare models trained using SGD and those trained with DP-SGD through the lens of differential privacy.\nImplement, from an algorithm description, optimization techniques like DP-SGD that requires manual gradient manipulation in Pytorch.\n\nPlease work in groups of 1-2 during the lab.\nAcknowledgements:\n\nThe MedMNIST data is from https://medmnist.com/\nThis assignment is written by Mahdi Haghifam, Sonya Allin, Lisa Zhang, Mike Pawliuk and Rutwa Engineer\n\nPlease work in groups of 1-2 during the lab.\n\n\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab05.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 2: Description of the difference between the non-dp model predictions over data in/out of training (1 point)\nPart 3: Explanation of why the “average height” model is not \\(\\epsilon\\)-DP (1 point)\nPart 4: Explanation of T_max in CosineAnnealingLR (1 point)\nPart 4: Explanation why max_grad_norm &gt;= 7.49 causes gradient clipping to remain unchanged in the example (1 point)\nPart 4: Implementation of dp_grads function (4 points)\nPart 4: Explanation of the difference in the histogram of the dp and non-dp models (1 point)\nPart 4: Analysis of the impact of privacy breach on a vulnerable individual (1 point)\n\n\n\n\nLike last week, we will be using the medmnist data set, which is available as a Python package. We will also be using opacus, which is a differential privacy library.\nRecall that on Google Colab, we use “!” to run shell commands. Below, we use such commands to install the Python packages.\n!pip install medmnist\n!pip install opacus\n\n\n\nWe will be using the same data and model as in lab 3, with modifications in the way that the training, validation, and test sets are split. These modifications are necessary to be able to showcase differential privacy issues using a small model and limited data set size to ensure that models do not take an overwhelming amount of time to train.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport opacus\nimport medmnist\nfrom medmnist import PneumoniaMNIST\nimport torchvision.transforms as transforms\n\nimport torch.utils.data as data_utils\n\nmedmnist.INFO['pneumoniamnist']\n\nTask Use code from lab 3 to re-acquaint yourself with the training data. What do the inputs look like? What about the targets? What is the distribution of the targets? Intuitively, how difficult is the classification problem?\n# TODO: Run/revise the data exploration code from lab 3 so\n# that you can describe the dataset and the difference between\n# the two classes\nTask: Using the standard train/validation/test split provided by the dataset, what percentage of the training set had the label 0? What about the validation set?\n# TODO: Write code to compute the figures here\nThese statistics differ significantly between the training, validation and test sets for our DP demonstration to work well with our small MLP model. Thus, we will perform our own split of the training, validation, and test sets.\nIn addition, we will split the data into four sets: training, validation, test, and a memorization assessment set. This data set will be the same size as our training set. Practitioners sometimes call this set a second or unused training set, even though this data set is not used for training. The idea is that we want to see if there is a difference between data that we actually used for training, vs another data that we could have used for training.\nTask: Run the following code to obtain the four datasets.\n# Load the training, validation, and test sets\n# We will normalize each data set to mean 0.5 and std 0.5: this\n# improves training speed\ndata_transform = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize(mean=[.5], std=[.5])])\ntrain_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=True)\nvalid_dataset = PneumoniaMNIST(split='val', transform=data_transform, download=True)\ntest_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=True)\n\n# Combine the training and validation\ncombined_data = train_dataset + valid_dataset\n\n# Re-split the data into training,  memory assessment\ntrain_dataset, mem_asses_dataset, valid_dataset = torch.utils.data.random_split(combined_data, [0.4, 0.4, 0.2])\nTask: What percentage of the training set had the label 0? What about the memorization assessment set? What about the validation set?\n# TODO: Run code to complete your solution here.\nNow that our data is ready, we can set up the model and training code similar to lab 3.\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLPModel(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=600):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        out = self.sigmoid(out)\n        out = self.fc3(out)\n        return out\nTo explore the distribution of predicted logits, the following code is written for you: it produces both the predictions and ground-truth labels across a dataset. There is also another utility function that can be used to measure the accuracy.\ndef get_predictions(model, data):\n    \"\"\"\n    Return the ground truth and predicted value across a dataset.\n    Unlike the get_prediction function in lab 3, this dataset\n    will produce the predictions for the *entire* dataset (no sampling)\n\n    Parameters:\n        `model` - A PyTorch model\n        `data` - A PyTorch dataset of MedMNIST images\n\n    Returns: A tuple `(ys, ts)` where:\n        `ys` is a list of prediction probabilities, same length as `data`\n        `ts` is a list of ground-truth labels, same length as `data`\n    \"\"\"\n    ys, ts = [], []\n    loader = torch.utils.data.DataLoader(data, batch_size=100)\n    for X, t in loader:\n        z = model(X.reshape(-1, 784))\n        ys += [float(y) for y in torch.sigmoid(z)]\n        ts += [float(t_) for t_ in t]\n    return ys, ts\n\ndef accuracy(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch model\n        `dataset` - A PyTorch dataset of MedMNIST images\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    ys, ts = get_predictions(model, dataset)\n    predicted = np.ndarray.round(np.array(ys))\n    return np.mean(predicted == ts)\nThe training code below is analogues to the training code used in lab 3, with some differences. One difference is that we use the Adam optimizer rather than SGD. The Adam optimizer combines ideas from momentum and RMSProp and is able to train our model to a suitable accuracy with much fewer iterations.\nTask: Run the code below to train our (non-private) model.\ndef train_model(model,                # a PyTorch model\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=1e-2,\n                batch_size=100,\n                num_epochs=45,\n                plot_every=20,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    for e in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n              z = model(images.reshape(-1, 784))\n              loss = criterion(z, labels.to(torch.float))\n              loss.backward()\n              optimizer.step()\n              optimizer.zero_grad()\n\n              iter_count += 1\n              if iter_count % plot_every == 0:\n                  iters.append(iter_count)\n                  ta = accuracy(model, train_data)\n                  va = accuracy(model, val_data)\n                  train_loss.append(float(loss))\n                  train_acc.append(ta)\n                  val_acc.append(va)\n                  print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n\n    if plot:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n \n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel_np = MLPModel()\ntrain_model(model_np, train_dataset, valid_dataset)\n\n\n\nIn this part of the lab, we will show that our trained model captures more information about the training data than we might intend. In particular, we show that the model predictions have different patterns for images used in training (compared to images that are not used in training). Specifically, the prediction logits follow a different distribution for training images and images not used for training.\nIn more general applications, the patterns in the logit distributions can be used to build classifiers that can predict whether an image was used in training a neural network.\nTask: Suppose that you are a patient in a medical study, who consented for their data to be used to train a machine learning model to detect the strains of certain diseases (i.e., strain A or B of the same disease). Explain why you might not want it be known that your data was used to build the model.\n# TODO: Your explanation goes here.\nRecall that we used train_dataset to train our model, but did not use the mem_asses_dataset. To show that our model behaves differently for data used in training (vs not), we will plot the histogram of prediction probabilities across these two datasets.\nTask: Run the code below, which produces cumulative histogram plots showing showing the log model predictions of negative and positive samples (truth label=0 vs true label=1). The predictions for data point in the training set is shown in blue. The predictions for data points not in the training set (in the memorization assessment set) is in red.\ndef plot_hist(model, in_dataset, out_dataset):\n    \"\"\"\n    Plots the histogram (cumulative, in the log space) of the predicted\n    probabilities for datasets that is in the training set vs out. The\n    histograms are separated by the true labels.\n\n    Parameters:\n        `model` - A PyTorch model\n        `in_dataset` - A PyTorch dataset used for training \n                       (i.e. *in* the training set)\n        `out_dataset` - A PyTorch dataset not used for training \n                       (i.e. *out* the training set)\n    \"\"\"\n    # Obtain the prediction for data points in both data sets\n    ys_in, ts_in  = get_predictions(model, in_dataset)\n    ys_out, ts_out = get_predictions(model, out_dataset)\n\n    # Compute the negative log() of these predictions, separated by the\n    # ground truth labels. An epsilon is added to the prediction for\n    # numerical stability\n    epsilon = 1e-10\n    conf_in_0 = [-np.log(y + epsilon) for t, y in zip(ts_in, ys_in) if t == 0]\n    conf_in_1 = [-np.log(1 - y + epsilon) for t, y in zip(ts_in, ys_in) if t == 1]\n    conf_out_0 = [-np.log(y + epsilon) for t, y in zip(ts_out, ys_out) if t == 0]\n    conf_out_1 = [-np.log(1 - y + epsilon) for t, y in zip(ts_out, ys_out) if t == 1]\n\n    # Bins used for the density/histogram\n    bins_0 = np.linspace(0,max(max(conf_in_0),max(conf_out_0)),500)\n    bins_1 = np.linspace(0,max(max(conf_in_1),max(conf_out_1)),500)\n\n    # Plot the histogram for the predicted probabilities for true label = 0\n    plt.subplot(2, 1, 1)\n    plt.hist(conf_out_0, bins_0, color='r', label='out', alpha=0.5,cumulative=True, density=True)\n    plt.hist(conf_in_0, bins_0, color='b', label='in', alpha=0.5,cumulative=True, density=True)\n    plt.legend()\n    plt.ylabel('CDF')\n    plt.xlabel('-log(Pr(pred=1))')\n    plt.title(\"True label=0\")\n\n    # Plot the histogram for the predicted probabilities for true label = 1\n    plt.subplot(2, 1, 2)\n    plt.hist(conf_out_1, bins_1, color='r', label='out', alpha=0.5,cumulative=True, density=True)\n    plt.hist(conf_in_1, bins_1, color='b', label='in', alpha=0.5,cumulative=True, density=True)\n    plt.legend()\n    plt.title(\"True label=1\")\n    plt.ylabel('CDF')\n    plt.xlabel('-log(Pr(pred=0))')\n\n    plt.subplots_adjust(hspace=1)\n    plt.show()\n\nplot_hist(model_np, train_dataset, mem_asses_dataset)\nGraded Task: What difference do you notice between the histograms of the data points in the training set, vs those not in the training set? Explain how this difference is indicative of overfitting.\n# TODO: Your answer goes here\n\n\n\nIn the previous section, we observed that a model’s prediction confidence for the samples inside its training set can be different from those outside the training set. We already know that this disparity has a negative impact on the model’s performance during test time. This phenomenon is known as overfitting and we know several techniques on how to measure and reduce the overfitting. In this part, we want to argue that this disparity has a negative impact on privacy of the training samples.\nAssume the designed model in the previous section is published for the public as a classification model for Pneumonia. Assume that the training set consists of individuals’ X-ray. Even though the participants have consented to participate in the research, their privacy should still be protected. If an attacker can determine whether a specific individual’s data is part of the research dataset, it might lead to unintended privacy breaches. Participants might not want their involvement in such a study to be public knowledge due to the stigma associated with certain medical conditions. This disparity may also help an attacker to reconstruct a participant’s data. This example shows that the disparity between the model’s confidence lets the adversary infer the membership of a sample from the predictions. This is alarming! In the next part, we discuss how to mitigate this risk by introducing the fundamental concept of Differential Privacy.\nDifferential privacy (DP) is a data privacy framework that aims to provide strong privacy guarantees when analyzing or sharing sensitive data. We say an ML algorithm satisfies Differential Privacy if changing one of the training samples does not change the output of the algorithm significantly. DP is an interesting property: assume you want to give a hospital access to your X-ray. If the hospital uses a DP ML algorithm, then, you are guaranteed that your presence does not affect the output significantly. This is promising and motivates people to give access to their data for the purpose of data analysis.\nNext, we discuss how to formalize DP.\nAssume we have a dataset \\(S=\\{(x_1,y_1),\\dots,(x_n,y_n)\\}\\) which consists of \\(n\\) individuals data. Consider the neighboring dataset \\(S'=\\{(x_1,y_1),\\dots,(x'_n, y'_n)\\}\\) which differs from \\(S\\) in only one sample. Then, we say a randomized algorithm \\(\\mathcal{A}\\) satisfies \\(\\epsilon\\)-DP if for all the output \\(y\\) in the range of \\(\\mathcal{A}\\) it satisfies\n\\[\n\\mathbb{Pr}\\left(\\mathcal{A}(S) =y \\right) \\leq \\exp(\\epsilon)  \\mathbb{Pr}\\left(\\mathcal{A}(S’) =y \\right).\n\\]\nBut what is the intuition behind this equation?\n\\(\\epsilon\\) is called the privacy budget. Privacy budget captures how strong our privacy guarantees are, by showing that the outcome is indistinguishable in two neighboring datasets. This can be shown by setting \\(\\epsilon = 0\\), the probability the analysis having an outcome is the same with or without you in the database. So if we set \\(\\epsilon\\) to some small value, we can get good guarantees that the output will not differ much.\nA common property of privacy-preserving algorithms is randomness. To see why it is the case assume we are interested in the average height of the students enrolled in CSC413while preserving their privacy. Consider a deterministic algorithm that reports the average height of the students. We argue that there exists no finite \\(\\epsilon\\) for which this algorithm is DP. For this example, it can be shown that by adding a Gaussian noise to the average height we can preserve privacy of the individuals.\nGraded Task: Explain why the algorithm that reports the average height of the students is not \\(\\epsilon\\)-DP for any finite \\(\\epsilon&gt;0\\).\n# TODO: Include your explanation here\nWe hope that the basic intuition behind differential privacy is now clear. DP is now a widely-used method to preserve privacy. It has been used in companies like Google and Apple to gather the user’s data. It is also recently used for the US Census. See the following [video]{https://www.youtube.com/watch?v=nVPE1dbA394} to get more information.\n\n\n\nIn this section, we discuss how we can make stochastic gradient descent differentially private and implement it. We will follow the algorithmic description of DP-SGD forom https://arxiv.org/pdf/1607.00133.pdf that we reproduce below. Start by reading the words/headings in the description, then we will discuss the algorithm line by line.\nAlgorithm Outline for Differentially Private SGD (DP-SGD)\nInput Examples \\(\\left\\{x_1, \\ldots, x_N\\right\\}\\), loss function \\(\\mathcal{L}(\\theta)=\\) \\(\\frac{1}{N} \\sum_i \\mathcal{L}\\left(\\theta, x_i\\right)\\).\nParameters: learning rate \\(\\eta_t\\), noise scale, \\(\\sigma\\), group size \\(L\\), gradient norm bound \\(C\\). \\\nInitialize \\(\\theta_0\\) randomly\nfor \\(t \\in[T]\\) do * Take a random sample \\(L_t\\) with sampling probability \\(L / N\\) * Compute gradient \\(\\quad\\) For each \\(i \\in L_t\\), compute \\(\\mathbf{g}_t\\left(x_i\\right) \\leftarrow \\nabla_{\\theta_t} \\mathcal{L}\\left(\\theta_t, x_i\\right)\\) * Clip gradient \\(\\overline{\\mathbf{g}}_t\\left(x_i\\right) \\leftarrow \\mathbf{g}_t\\left(x_i\\right) / \\max \\left(1, \\frac{\\left\\|\\mathbf{g}_t\\left(x_i\\right)\\right\\|_2}{C}\\right)\\) * Add noise \\(\\tilde{\\mathbf{g}}_t \\leftarrow \\frac{1}{L}\\left(\\sum_i \\overline{\\mathbf{g}}_t\\left(x_i\\right)+\\mathcal{N}\\left(0, \\sigma^2 C^2 \\mathbf{I}\\right)\\right)\\) \\ * Descent \\(\\theta_{t+1} \\leftarrow \\theta_t-\\eta_t \\tilde{\\mathbf{g}}_t\\)\nOutput \\(\\theta_T\\) and compute the overall privacy cost \\((\\varepsilon, \\delta)\\) using a privacy accounting method.\nTask: Read the algorithm above. Write down, for each symbol/notation used in the algorithm, what it represents. Pay particular attention to the symbol \\(\\mathbf{g}_t\\left(x_i\\right)\\) and its various modifications.\n# TODO: Make sure you understand the notation before moving on to the\n# detailed descriptions.\nNow, let’s discuss the algorithm line by line.\n\nSampling: The sampling mechanism used in DP-SGD is different from SGD. In non-private SGD, we choose a random permutation at the beginning of each epoch. However, in DP-SGD at each iteration we select a sample with probability (batchsize/number of samples) to be a member of the batch at the current iteration. This sampling mechanism is also called Poisson subsampling. We will not implement this sampling ourselves; we will use Opacus software package implement of it.\nGradient Computation: This step is analogous to gradient computation in SGD. However, the gradients of each sample in the batch is computed separately.\nClipping Gradients: You should be able to show that, mathematically, that clipping ensures that for each data point, the gradient vector of that data point has a maximum norm of \\(C\\). Why is this useful? Assume there is an outlier for which the gradient is very large. Without clipping, the impact of the outlier on the algorithm will be unbounded. DP-SGD performs clipping to each individual gradient point separately, which limits the contribution of each data point to the parameter update.\nAdding Noise: In order to achieve a specific level of privacy determined by \\(\\epsilon\\), we need to select the minimum amount of noise to be added in each iteration (\\(\\sigma\\) in the algorithm description). Since determining the exact amount requires a very technical calculation, there are software packages which can be used. Here, we use the Opacus software package from Meta research. It provides a function which takes as input the privacy level \\(\\epsilon\\), batchsize and the number of training points, and outputs the variance of the noise. There is another input, i.e., \\(\\delta\\). Do not make any changes to it. If you are interested to know what it means please read the following lecture note.\n\nTask: Notice that the scale of the noise added to the gradient is related to the clipping parameter. Does the amount of the noise added increase or decrease if we allow a larger maximum norm \\(C\\) during clipping? (Reasoning about these differences is one way to make sense of mathematical equations like these.)\n# TODO: Your answer goes here\nIn the next few tasks, we will describe the pieces of code that we will need to implement DP-SGD.\nTask: Run the following code to compare the batches produced by the usual Dataloader vs. via Poisson Sampling. What do you noitce?\n# Create a dataset with 20 numbers\nx = torch.arange(20)\nprint(x)\ndataset = torch.utils.data.TensorDataset(x)\n\nprint('PyTorch DataLoader')\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=4,shuffle=True)\nfor _ in range(2): # run for 2 epochs\n    for x_b in data_loader:\n        print(x_b)\n    print('-------------------')\n\nprint('Poisson Sampling')\ndp_data_loader = opacus.data_loader.DPDataLoader(dataset, sample_rate = 5/20)\nfor _ in range(2): # run for 2 epochs\n    for x_b in dp_data_loader:\n        print(x_b)\n    print('-------------------')\nIn addition to using a different sampling method, we will use CosineAnnealingLR learning rate scheduler in pytorch. You need to understand how this learning rate scheduler works and how it can be updated.\nGraded Task Read the PyTorch documentation on CosineAnnealingLR and explain what the parameter T_max represent.\n# TODO: Write your explanation here.\nThe most challenging part of implementing DP-SGD is that we will need to implement our own optimization process to modify the default gradient descent behaviour. However, Pytorch has a nice feature that we saw in lab 1: each parameter in a model stores its own gradient as an attribute. In particular, consider the following snippet which can be used to print the name and gradient of the parameters in a model.\nfor name, param in model_np.named_parameters(): \n    print(name)\n    print(param.grad)\nWhat we didn’t see in lab 1 is that we can anually change the gradient of each parameter!\nThis is powerful, because the optimizers in PyTorch uses the .grad attributes of each parameter to perform model updates. Thus, changing the .grad attributes provides a way to override the default gradient descent behaviour.\nTask: Run the below code, which demonstrates how the .grad attribute can be modified.\nmodel = nn.Linear(5, 1) # linear model with input dim = 5, and a single output\nprint(list(model.parameters())) # print the current parameters\n\n# manually set the optimizers\noptimizer = torch.optim.SGD(model.parameters(), 0.1)\nmodel.weight.grad = torch.nn.parameter.Parameter(torch.Tensor([[1, 2, 3, 4, 5.]]))\nmodel.bias.grad = torch.nn.parameter.Parameter(torch.Tensor([1.]))\noptimizer.step()\n\n# what would you expect the output to be?\nprint(list(model.parameters()))\nTo implement DP-SGD, We will need to manually modify the .grad attribute in a few ways. One of the steps to DP-SGD is gradient clipping. Fortunately, PyTorch actually comes with an implementation of gradient clipping through the function torch.nn.utils.clip_grad_norm_.\nTask: Run this code to see how gradient clipping works. Notice that the gradient direction is unchanged, only the magnitude.\nmodel = nn.Linear(5, 1) # linear model with input dim = 5, and a single output\nmodel.weight.grad = torch.Tensor([[1, 2, 3, 4, 5.]])\nmodel.bias.grad = torch.Tensor([1.])\nmax_grad_norm = 0.5\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\nprint(model.weight.grad, model.bias.grad)\nGraded Task: Explain why if we set max_grad_norm &gt;= 7.49 above, the gradient will be unchanged. Your explanation should demonstrate the calculation of where the number 7.49 comes from.\n# TODO: Include your explanation and calculation here.\nGraded Task: Now that we have the pieces we need to implement our DP-SGD gradient computation, Complete the code below, which performs one iteration of DP-SGD update for a batch of data. You may wish to look ahead to see how this function will be used in DP-SGD training.\ndef dp_grads(model, batch_data, criterion, max_grad_norm, noise_multiplier):\n    \"\"\"\n    Compute gradients for an iteration of DP-SGD training by setting the\n    .grad attribute of each parameter in model.named_parameters()\n    according to the DP-SGD algorithm.\n\n    Parameters:\n        - `model` - A PyTorch model\n        - `batch_data` - A list of tuples (x, t) representing a batch of data\n        - `criterion` - A PyTorch loss function\n        - `max_grad_norm` - The maximum gradient norm, used for gradient clipping\n                            (C in the algorithm description)\n        - `noise_multiplier` - The noise multiplier, used for adding noise\n                               (sigma in the algorithm description)\n\n    Returns: A dictionary `clipped_noisy_grads` that maps the names of each\n             parameter in `model.named_parameters()` to its modified gradient\n             computed according to DP-SGD\n    \"\"\"\n    # Create the mapping of each parameter in our model to\n    # what will evetually be the noisy gradients\n    clipped_noisy_grads = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n\n    # Iterate over the data points in each batch. This is unfortunately\n    # necessary so that we can perform gradient clipping separtely for each\n    # data point\n    for xi, ti in batch_data:\n        zi = None # TODO: compute the model prediction (logit)\n        zi = model(xi) # SOLUTION\n        lossi = None # TODO: compute the loss for this data point\n        lossi = criterion(zi.reshape(1), ti.to(torch.float).reshape(1))  # SOLUTION\n\n        # TODO: perform the backward pass\n        lossi.backward(retain_graph=True)  # SOLUTION\n\n        # TODO: perform gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # SOLUTION\n\n        # accumulate the clipped gradients in `clipped_noisy_grads`\n        for name, param in model.named_parameters():\n            clipped_noisy_grads[name] += param.grad\n\n        # TODO: clear the gradients in the model's computation graph\n        model.zero_grad() # SOLUTION\n\n    # Now, we iterate over the name parameters to add noise\n    for name, param in model.named_parameters():\n        # TODO: Read the equation in the \"Add Noise\" section of the\n        #      algorithm description, and implement it. You may find\n        #      the function `torch.normal` helpful.\n        clipped_noisy_grads[name] += 0 # TODO\n\n        clipped_noisy_grads[name] += torch.normal(mean=0.0, std=noise_multiplier * max_grad_norm, size=param.size()) # SOLUTION\n        clipped_noisy_grads[name] /= len(batch_data) # SOLUTION\n\n    return clipped_noisy_grads\nPlease include the output of the tests below in your submission. (What should the output of the test be?)\nmodel = nn.Linear(5, 1)\nmodel.weight = nn.Parameter(torch.Tensor([[1, 1, 0, 0, 0.]]))\nmodel.bias = nn.Parameter(torch.Tensor([0.]))\nbatch_data = [(torch.Tensor([[1, 1, 1, 0, 0.]]), torch.Tensor([1.])),\n              (torch.Tensor([[1, 0, 1, 0, 0.]]), torch.Tensor([0.]))]\ncriterion = nn.BCEWithLogitsLoss()\n\n# no noise and a large max_grad_norm\nprint(dp_grads(model, batch_data, criterion, max_grad_norm=1000, noise_multiplier=0))\nprint('-----------')\n\n# no noise and a small max_grad_norm\nprint(dp_grads(model, batch_data, criterion, max_grad_norm=0.5, noise_multiplier=3))\nprint('-----------')\n\n# small max_grad_norm and some noise (STD should be ~0.5x3/2)\nfor i in range(10):\n    print(dp_grads(model, batch_data, criterion, max_grad_norm=0.5, noise_multiplier=3))\nTask Now that we have DP-SGD in place, run the below code to train a differentially private model.\ndef train_model_private(model, traindata, valdata, learning_rate=2e-1,\n                        batch_size=500, num_epochs=25, plot_every=20,\n                        epsilon=0.5, max_grad_norm=6):\n    # Compute the noise multiplier\n    N = len(traindata)\n    noise_multiplier = opacus.accountants.utils.get_noise_multiplier(\n        target_epsilon=epsilon,\n        target_delta=1/N,\n        sample_rate=batch_size/N,\n        epochs=num_epochs)\n\n    # Use the differentially private data loader\n    train_loader = opacus.data_loader.DPDataLoader(\n        dataset=traindata,\n        sample_rate=batch_size/N)\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N//batch_size * num_epochs)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n \n    for e in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n            images = images.reshape(-1, 28*28)\n            # get the clipped noisy gradients from the function you wrote\n            clipped_noisy_grads = dp_grads(model,\n                                           batch_data=list(zip(images,labels)),\n                                           criterion=criterion,\n                                           max_grad_norm=max_grad_norm,\n                                           noise_multiplier=noise_multiplier)\n            # manually update the gradients\n            for name, param in model.named_parameters():\n                param.grad = clipped_noisy_grads[name]\n            optimizer.step() # update the parameters\n            scheduler.step() # update the learning rate scheduler\n            optimizer.zero_grad() # clean up accumualted gradients\n\n            iter_count += 1\n            if iter_count % plot_every == 0:\n                # forward pass to compute the loss\n                z = model(images.reshape(-1, 784))\n                loss = criterion(z, labels.to(torch.float))\n                optimizer.zero_grad()\n\n                iters.append(iter_count)\n                ta = accuracy(model, traindata)\n                va = accuracy(model, valdata)\n                train_loss.append(float(loss))\n                train_acc.append(ta)\n                val_acc.append(va)\n                print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n   \n    plt.figure()\n    plt.plot(iters[:len(train_loss)], train_loss)\n    plt.title(\"Loss over iterations\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n\n    plt.figure()\n    plt.plot(iters[:len(train_acc)], train_acc)\n    plt.plot(iters[:len(val_acc)], val_acc)\n    plt.title(\"Accuracy over iterations\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"Train\", \"Validation\"])\nmodel_priv = MLPModel()\ntrain_model_private(model_priv, train_dataset, valid_dataset)\nGraded Task: Plot the histogram of the model prediction for this differentially private model. How does this histogram differ from that of model_np from above?\nplot_hist(model_priv, train_dataset, mem_asses_dataset)\n# TODO: Explain how the histogram differs from that of model_np\nTask How does the accuracy differ from that of model_no mentioned above? Explain what you observe.\n# TODO: Your answer goes here\nGraded Task: Suppose that an attacker recognizes that your friend Taylor is in this data set, means that their X-ray was taken at some point during a hospitalization, and that Taylor provided researchers consent to be included in the study dataset. If this information is sold to a third-party (e.g., a credit reporting agency, an employer, or a landlord), how might this affect Taylor?\n# TODO: Your answer goes here\nIf you are interested in DP, we suggest performing hyperparameter tuning over batch size and max grad norm. In, DP-SGD usually larger batch size would help. So, for instance for two values of \\(\\varepsilon \\in \\{0.5,1,5\\}\\), try to find the best model for \\(\\text{batchsize}\\in \\{100,500\\}\\) and \\(\\text{gradnorm}\\in\\{4,8,16\\}\\).\n\n\n\n\n[Differential Privacy and the Census] https://www.youtube.com/watch?v=nVPE1dbA394\nMain DP-SGD Paper\nCS 860 - Algorithms for Private Data Analysis at UWaterloo"
  },
  {
    "objectID": "labs/lab05.html#submission",
    "href": "labs/lab05.html#submission",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab05.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 2: Description of the difference between the non-dp model predictions over data in/out of training (1 point)\nPart 3: Explanation of why the “average height” model is not \\(\\epsilon\\)-DP (1 point)\nPart 4: Explanation of T_max in CosineAnnealingLR (1 point)\nPart 4: Explanation why max_grad_norm &gt;= 7.49 causes gradient clipping to remain unchanged in the example (1 point)\nPart 4: Implementation of dp_grads function (4 points)\nPart 4: Explanation of the difference in the histogram of the dp and non-dp models (1 point)\nPart 4: Analysis of the impact of privacy breach on a vulnerable individual (1 point)"
  },
  {
    "objectID": "labs/lab05.html#google-colab-setup",
    "href": "labs/lab05.html#google-colab-setup",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "Like last week, we will be using the medmnist data set, which is available as a Python package. We will also be using opacus, which is a differential privacy library.\nRecall that on Google Colab, we use “!” to run shell commands. Below, we use such commands to install the Python packages.\n!pip install medmnist\n!pip install opacus"
  },
  {
    "objectID": "labs/lab05.html#part-1.-data-and-model",
    "href": "labs/lab05.html#part-1.-data-and-model",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "We will be using the same data and model as in lab 3, with modifications in the way that the training, validation, and test sets are split. These modifications are necessary to be able to showcase differential privacy issues using a small model and limited data set size to ensure that models do not take an overwhelming amount of time to train.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport opacus\nimport medmnist\nfrom medmnist import PneumoniaMNIST\nimport torchvision.transforms as transforms\n\nimport torch.utils.data as data_utils\n\nmedmnist.INFO['pneumoniamnist']\n\nTask Use code from lab 3 to re-acquaint yourself with the training data. What do the inputs look like? What about the targets? What is the distribution of the targets? Intuitively, how difficult is the classification problem?\n# TODO: Run/revise the data exploration code from lab 3 so\n# that you can describe the dataset and the difference between\n# the two classes\nTask: Using the standard train/validation/test split provided by the dataset, what percentage of the training set had the label 0? What about the validation set?\n# TODO: Write code to compute the figures here\nThese statistics differ significantly between the training, validation and test sets for our DP demonstration to work well with our small MLP model. Thus, we will perform our own split of the training, validation, and test sets.\nIn addition, we will split the data into four sets: training, validation, test, and a memorization assessment set. This data set will be the same size as our training set. Practitioners sometimes call this set a second or unused training set, even though this data set is not used for training. The idea is that we want to see if there is a difference between data that we actually used for training, vs another data that we could have used for training.\nTask: Run the following code to obtain the four datasets.\n# Load the training, validation, and test sets\n# We will normalize each data set to mean 0.5 and std 0.5: this\n# improves training speed\ndata_transform = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize(mean=[.5], std=[.5])])\ntrain_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=True)\nvalid_dataset = PneumoniaMNIST(split='val', transform=data_transform, download=True)\ntest_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=True)\n\n# Combine the training and validation\ncombined_data = train_dataset + valid_dataset\n\n# Re-split the data into training,  memory assessment\ntrain_dataset, mem_asses_dataset, valid_dataset = torch.utils.data.random_split(combined_data, [0.4, 0.4, 0.2])\nTask: What percentage of the training set had the label 0? What about the memorization assessment set? What about the validation set?\n# TODO: Run code to complete your solution here.\nNow that our data is ready, we can set up the model and training code similar to lab 3.\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLPModel(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=600):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        out = self.sigmoid(out)\n        out = self.fc3(out)\n        return out\nTo explore the distribution of predicted logits, the following code is written for you: it produces both the predictions and ground-truth labels across a dataset. There is also another utility function that can be used to measure the accuracy.\ndef get_predictions(model, data):\n    \"\"\"\n    Return the ground truth and predicted value across a dataset.\n    Unlike the get_prediction function in lab 3, this dataset\n    will produce the predictions for the *entire* dataset (no sampling)\n\n    Parameters:\n        `model` - A PyTorch model\n        `data` - A PyTorch dataset of MedMNIST images\n\n    Returns: A tuple `(ys, ts)` where:\n        `ys` is a list of prediction probabilities, same length as `data`\n        `ts` is a list of ground-truth labels, same length as `data`\n    \"\"\"\n    ys, ts = [], []\n    loader = torch.utils.data.DataLoader(data, batch_size=100)\n    for X, t in loader:\n        z = model(X.reshape(-1, 784))\n        ys += [float(y) for y in torch.sigmoid(z)]\n        ts += [float(t_) for t_ in t]\n    return ys, ts\n\ndef accuracy(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch model\n        `dataset` - A PyTorch dataset of MedMNIST images\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    ys, ts = get_predictions(model, dataset)\n    predicted = np.ndarray.round(np.array(ys))\n    return np.mean(predicted == ts)\nThe training code below is analogues to the training code used in lab 3, with some differences. One difference is that we use the Adam optimizer rather than SGD. The Adam optimizer combines ideas from momentum and RMSProp and is able to train our model to a suitable accuracy with much fewer iterations.\nTask: Run the code below to train our (non-private) model.\ndef train_model(model,                # a PyTorch model\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=1e-2,\n                batch_size=100,\n                num_epochs=45,\n                plot_every=20,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    for e in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n              z = model(images.reshape(-1, 784))\n              loss = criterion(z, labels.to(torch.float))\n              loss.backward()\n              optimizer.step()\n              optimizer.zero_grad()\n\n              iter_count += 1\n              if iter_count % plot_every == 0:\n                  iters.append(iter_count)\n                  ta = accuracy(model, train_data)\n                  va = accuracy(model, val_data)\n                  train_loss.append(float(loss))\n                  train_acc.append(ta)\n                  val_acc.append(va)\n                  print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n\n    if plot:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n \n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel_np = MLPModel()\ntrain_model(model_np, train_dataset, valid_dataset)"
  },
  {
    "objectID": "labs/lab05.html#part-2.-privacy-issues-in-our-model",
    "href": "labs/lab05.html#part-2.-privacy-issues-in-our-model",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "In this part of the lab, we will show that our trained model captures more information about the training data than we might intend. In particular, we show that the model predictions have different patterns for images used in training (compared to images that are not used in training). Specifically, the prediction logits follow a different distribution for training images and images not used for training.\nIn more general applications, the patterns in the logit distributions can be used to build classifiers that can predict whether an image was used in training a neural network.\nTask: Suppose that you are a patient in a medical study, who consented for their data to be used to train a machine learning model to detect the strains of certain diseases (i.e., strain A or B of the same disease). Explain why you might not want it be known that your data was used to build the model.\n# TODO: Your explanation goes here.\nRecall that we used train_dataset to train our model, but did not use the mem_asses_dataset. To show that our model behaves differently for data used in training (vs not), we will plot the histogram of prediction probabilities across these two datasets.\nTask: Run the code below, which produces cumulative histogram plots showing showing the log model predictions of negative and positive samples (truth label=0 vs true label=1). The predictions for data point in the training set is shown in blue. The predictions for data points not in the training set (in the memorization assessment set) is in red.\ndef plot_hist(model, in_dataset, out_dataset):\n    \"\"\"\n    Plots the histogram (cumulative, in the log space) of the predicted\n    probabilities for datasets that is in the training set vs out. The\n    histograms are separated by the true labels.\n\n    Parameters:\n        `model` - A PyTorch model\n        `in_dataset` - A PyTorch dataset used for training \n                       (i.e. *in* the training set)\n        `out_dataset` - A PyTorch dataset not used for training \n                       (i.e. *out* the training set)\n    \"\"\"\n    # Obtain the prediction for data points in both data sets\n    ys_in, ts_in  = get_predictions(model, in_dataset)\n    ys_out, ts_out = get_predictions(model, out_dataset)\n\n    # Compute the negative log() of these predictions, separated by the\n    # ground truth labels. An epsilon is added to the prediction for\n    # numerical stability\n    epsilon = 1e-10\n    conf_in_0 = [-np.log(y + epsilon) for t, y in zip(ts_in, ys_in) if t == 0]\n    conf_in_1 = [-np.log(1 - y + epsilon) for t, y in zip(ts_in, ys_in) if t == 1]\n    conf_out_0 = [-np.log(y + epsilon) for t, y in zip(ts_out, ys_out) if t == 0]\n    conf_out_1 = [-np.log(1 - y + epsilon) for t, y in zip(ts_out, ys_out) if t == 1]\n\n    # Bins used for the density/histogram\n    bins_0 = np.linspace(0,max(max(conf_in_0),max(conf_out_0)),500)\n    bins_1 = np.linspace(0,max(max(conf_in_1),max(conf_out_1)),500)\n\n    # Plot the histogram for the predicted probabilities for true label = 0\n    plt.subplot(2, 1, 1)\n    plt.hist(conf_out_0, bins_0, color='r', label='out', alpha=0.5,cumulative=True, density=True)\n    plt.hist(conf_in_0, bins_0, color='b', label='in', alpha=0.5,cumulative=True, density=True)\n    plt.legend()\n    plt.ylabel('CDF')\n    plt.xlabel('-log(Pr(pred=1))')\n    plt.title(\"True label=0\")\n\n    # Plot the histogram for the predicted probabilities for true label = 1\n    plt.subplot(2, 1, 2)\n    plt.hist(conf_out_1, bins_1, color='r', label='out', alpha=0.5,cumulative=True, density=True)\n    plt.hist(conf_in_1, bins_1, color='b', label='in', alpha=0.5,cumulative=True, density=True)\n    plt.legend()\n    plt.title(\"True label=1\")\n    plt.ylabel('CDF')\n    plt.xlabel('-log(Pr(pred=0))')\n\n    plt.subplots_adjust(hspace=1)\n    plt.show()\n\nplot_hist(model_np, train_dataset, mem_asses_dataset)\nGraded Task: What difference do you notice between the histograms of the data points in the training set, vs those not in the training set? Explain how this difference is indicative of overfitting.\n# TODO: Your answer goes here"
  },
  {
    "objectID": "labs/lab05.html#part-3.-differential-privacy",
    "href": "labs/lab05.html#part-3.-differential-privacy",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "In the previous section, we observed that a model’s prediction confidence for the samples inside its training set can be different from those outside the training set. We already know that this disparity has a negative impact on the model’s performance during test time. This phenomenon is known as overfitting and we know several techniques on how to measure and reduce the overfitting. In this part, we want to argue that this disparity has a negative impact on privacy of the training samples.\nAssume the designed model in the previous section is published for the public as a classification model for Pneumonia. Assume that the training set consists of individuals’ X-ray. Even though the participants have consented to participate in the research, their privacy should still be protected. If an attacker can determine whether a specific individual’s data is part of the research dataset, it might lead to unintended privacy breaches. Participants might not want their involvement in such a study to be public knowledge due to the stigma associated with certain medical conditions. This disparity may also help an attacker to reconstruct a participant’s data. This example shows that the disparity between the model’s confidence lets the adversary infer the membership of a sample from the predictions. This is alarming! In the next part, we discuss how to mitigate this risk by introducing the fundamental concept of Differential Privacy.\nDifferential privacy (DP) is a data privacy framework that aims to provide strong privacy guarantees when analyzing or sharing sensitive data. We say an ML algorithm satisfies Differential Privacy if changing one of the training samples does not change the output of the algorithm significantly. DP is an interesting property: assume you want to give a hospital access to your X-ray. If the hospital uses a DP ML algorithm, then, you are guaranteed that your presence does not affect the output significantly. This is promising and motivates people to give access to their data for the purpose of data analysis.\nNext, we discuss how to formalize DP.\nAssume we have a dataset \\(S=\\{(x_1,y_1),\\dots,(x_n,y_n)\\}\\) which consists of \\(n\\) individuals data. Consider the neighboring dataset \\(S'=\\{(x_1,y_1),\\dots,(x'_n, y'_n)\\}\\) which differs from \\(S\\) in only one sample. Then, we say a randomized algorithm \\(\\mathcal{A}\\) satisfies \\(\\epsilon\\)-DP if for all the output \\(y\\) in the range of \\(\\mathcal{A}\\) it satisfies\n\\[\n\\mathbb{Pr}\\left(\\mathcal{A}(S) =y \\right) \\leq \\exp(\\epsilon)  \\mathbb{Pr}\\left(\\mathcal{A}(S’) =y \\right).\n\\]\nBut what is the intuition behind this equation?\n\\(\\epsilon\\) is called the privacy budget. Privacy budget captures how strong our privacy guarantees are, by showing that the outcome is indistinguishable in two neighboring datasets. This can be shown by setting \\(\\epsilon = 0\\), the probability the analysis having an outcome is the same with or without you in the database. So if we set \\(\\epsilon\\) to some small value, we can get good guarantees that the output will not differ much.\nA common property of privacy-preserving algorithms is randomness. To see why it is the case assume we are interested in the average height of the students enrolled in CSC413while preserving their privacy. Consider a deterministic algorithm that reports the average height of the students. We argue that there exists no finite \\(\\epsilon\\) for which this algorithm is DP. For this example, it can be shown that by adding a Gaussian noise to the average height we can preserve privacy of the individuals.\nGraded Task: Explain why the algorithm that reports the average height of the students is not \\(\\epsilon\\)-DP for any finite \\(\\epsilon&gt;0\\).\n# TODO: Include your explanation here\nWe hope that the basic intuition behind differential privacy is now clear. DP is now a widely-used method to preserve privacy. It has been used in companies like Google and Apple to gather the user’s data. It is also recently used for the US Census. See the following [video]{https://www.youtube.com/watch?v=nVPE1dbA394} to get more information."
  },
  {
    "objectID": "labs/lab05.html#part-4.-differentially-private-sgd",
    "href": "labs/lab05.html#part-4.-differentially-private-sgd",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "In this section, we discuss how we can make stochastic gradient descent differentially private and implement it. We will follow the algorithmic description of DP-SGD forom https://arxiv.org/pdf/1607.00133.pdf that we reproduce below. Start by reading the words/headings in the description, then we will discuss the algorithm line by line.\nAlgorithm Outline for Differentially Private SGD (DP-SGD)\nInput Examples \\(\\left\\{x_1, \\ldots, x_N\\right\\}\\), loss function \\(\\mathcal{L}(\\theta)=\\) \\(\\frac{1}{N} \\sum_i \\mathcal{L}\\left(\\theta, x_i\\right)\\).\nParameters: learning rate \\(\\eta_t\\), noise scale, \\(\\sigma\\), group size \\(L\\), gradient norm bound \\(C\\). \\\nInitialize \\(\\theta_0\\) randomly\nfor \\(t \\in[T]\\) do * Take a random sample \\(L_t\\) with sampling probability \\(L / N\\) * Compute gradient \\(\\quad\\) For each \\(i \\in L_t\\), compute \\(\\mathbf{g}_t\\left(x_i\\right) \\leftarrow \\nabla_{\\theta_t} \\mathcal{L}\\left(\\theta_t, x_i\\right)\\) * Clip gradient \\(\\overline{\\mathbf{g}}_t\\left(x_i\\right) \\leftarrow \\mathbf{g}_t\\left(x_i\\right) / \\max \\left(1, \\frac{\\left\\|\\mathbf{g}_t\\left(x_i\\right)\\right\\|_2}{C}\\right)\\) * Add noise \\(\\tilde{\\mathbf{g}}_t \\leftarrow \\frac{1}{L}\\left(\\sum_i \\overline{\\mathbf{g}}_t\\left(x_i\\right)+\\mathcal{N}\\left(0, \\sigma^2 C^2 \\mathbf{I}\\right)\\right)\\) \\ * Descent \\(\\theta_{t+1} \\leftarrow \\theta_t-\\eta_t \\tilde{\\mathbf{g}}_t\\)\nOutput \\(\\theta_T\\) and compute the overall privacy cost \\((\\varepsilon, \\delta)\\) using a privacy accounting method.\nTask: Read the algorithm above. Write down, for each symbol/notation used in the algorithm, what it represents. Pay particular attention to the symbol \\(\\mathbf{g}_t\\left(x_i\\right)\\) and its various modifications.\n# TODO: Make sure you understand the notation before moving on to the\n# detailed descriptions.\nNow, let’s discuss the algorithm line by line.\n\nSampling: The sampling mechanism used in DP-SGD is different from SGD. In non-private SGD, we choose a random permutation at the beginning of each epoch. However, in DP-SGD at each iteration we select a sample with probability (batchsize/number of samples) to be a member of the batch at the current iteration. This sampling mechanism is also called Poisson subsampling. We will not implement this sampling ourselves; we will use Opacus software package implement of it.\nGradient Computation: This step is analogous to gradient computation in SGD. However, the gradients of each sample in the batch is computed separately.\nClipping Gradients: You should be able to show that, mathematically, that clipping ensures that for each data point, the gradient vector of that data point has a maximum norm of \\(C\\). Why is this useful? Assume there is an outlier for which the gradient is very large. Without clipping, the impact of the outlier on the algorithm will be unbounded. DP-SGD performs clipping to each individual gradient point separately, which limits the contribution of each data point to the parameter update.\nAdding Noise: In order to achieve a specific level of privacy determined by \\(\\epsilon\\), we need to select the minimum amount of noise to be added in each iteration (\\(\\sigma\\) in the algorithm description). Since determining the exact amount requires a very technical calculation, there are software packages which can be used. Here, we use the Opacus software package from Meta research. It provides a function which takes as input the privacy level \\(\\epsilon\\), batchsize and the number of training points, and outputs the variance of the noise. There is another input, i.e., \\(\\delta\\). Do not make any changes to it. If you are interested to know what it means please read the following lecture note.\n\nTask: Notice that the scale of the noise added to the gradient is related to the clipping parameter. Does the amount of the noise added increase or decrease if we allow a larger maximum norm \\(C\\) during clipping? (Reasoning about these differences is one way to make sense of mathematical equations like these.)\n# TODO: Your answer goes here\nIn the next few tasks, we will describe the pieces of code that we will need to implement DP-SGD.\nTask: Run the following code to compare the batches produced by the usual Dataloader vs. via Poisson Sampling. What do you noitce?\n# Create a dataset with 20 numbers\nx = torch.arange(20)\nprint(x)\ndataset = torch.utils.data.TensorDataset(x)\n\nprint('PyTorch DataLoader')\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=4,shuffle=True)\nfor _ in range(2): # run for 2 epochs\n    for x_b in data_loader:\n        print(x_b)\n    print('-------------------')\n\nprint('Poisson Sampling')\ndp_data_loader = opacus.data_loader.DPDataLoader(dataset, sample_rate = 5/20)\nfor _ in range(2): # run for 2 epochs\n    for x_b in dp_data_loader:\n        print(x_b)\n    print('-------------------')\nIn addition to using a different sampling method, we will use CosineAnnealingLR learning rate scheduler in pytorch. You need to understand how this learning rate scheduler works and how it can be updated.\nGraded Task Read the PyTorch documentation on CosineAnnealingLR and explain what the parameter T_max represent.\n# TODO: Write your explanation here.\nThe most challenging part of implementing DP-SGD is that we will need to implement our own optimization process to modify the default gradient descent behaviour. However, Pytorch has a nice feature that we saw in lab 1: each parameter in a model stores its own gradient as an attribute. In particular, consider the following snippet which can be used to print the name and gradient of the parameters in a model.\nfor name, param in model_np.named_parameters(): \n    print(name)\n    print(param.grad)\nWhat we didn’t see in lab 1 is that we can anually change the gradient of each parameter!\nThis is powerful, because the optimizers in PyTorch uses the .grad attributes of each parameter to perform model updates. Thus, changing the .grad attributes provides a way to override the default gradient descent behaviour.\nTask: Run the below code, which demonstrates how the .grad attribute can be modified.\nmodel = nn.Linear(5, 1) # linear model with input dim = 5, and a single output\nprint(list(model.parameters())) # print the current parameters\n\n# manually set the optimizers\noptimizer = torch.optim.SGD(model.parameters(), 0.1)\nmodel.weight.grad = torch.nn.parameter.Parameter(torch.Tensor([[1, 2, 3, 4, 5.]]))\nmodel.bias.grad = torch.nn.parameter.Parameter(torch.Tensor([1.]))\noptimizer.step()\n\n# what would you expect the output to be?\nprint(list(model.parameters()))\nTo implement DP-SGD, We will need to manually modify the .grad attribute in a few ways. One of the steps to DP-SGD is gradient clipping. Fortunately, PyTorch actually comes with an implementation of gradient clipping through the function torch.nn.utils.clip_grad_norm_.\nTask: Run this code to see how gradient clipping works. Notice that the gradient direction is unchanged, only the magnitude.\nmodel = nn.Linear(5, 1) # linear model with input dim = 5, and a single output\nmodel.weight.grad = torch.Tensor([[1, 2, 3, 4, 5.]])\nmodel.bias.grad = torch.Tensor([1.])\nmax_grad_norm = 0.5\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\nprint(model.weight.grad, model.bias.grad)\nGraded Task: Explain why if we set max_grad_norm &gt;= 7.49 above, the gradient will be unchanged. Your explanation should demonstrate the calculation of where the number 7.49 comes from.\n# TODO: Include your explanation and calculation here.\nGraded Task: Now that we have the pieces we need to implement our DP-SGD gradient computation, Complete the code below, which performs one iteration of DP-SGD update for a batch of data. You may wish to look ahead to see how this function will be used in DP-SGD training.\ndef dp_grads(model, batch_data, criterion, max_grad_norm, noise_multiplier):\n    \"\"\"\n    Compute gradients for an iteration of DP-SGD training by setting the\n    .grad attribute of each parameter in model.named_parameters()\n    according to the DP-SGD algorithm.\n\n    Parameters:\n        - `model` - A PyTorch model\n        - `batch_data` - A list of tuples (x, t) representing a batch of data\n        - `criterion` - A PyTorch loss function\n        - `max_grad_norm` - The maximum gradient norm, used for gradient clipping\n                            (C in the algorithm description)\n        - `noise_multiplier` - The noise multiplier, used for adding noise\n                               (sigma in the algorithm description)\n\n    Returns: A dictionary `clipped_noisy_grads` that maps the names of each\n             parameter in `model.named_parameters()` to its modified gradient\n             computed according to DP-SGD\n    \"\"\"\n    # Create the mapping of each parameter in our model to\n    # what will evetually be the noisy gradients\n    clipped_noisy_grads = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n\n    # Iterate over the data points in each batch. This is unfortunately\n    # necessary so that we can perform gradient clipping separtely for each\n    # data point\n    for xi, ti in batch_data:\n        zi = None # TODO: compute the model prediction (logit)\n        zi = model(xi) # SOLUTION\n        lossi = None # TODO: compute the loss for this data point\n        lossi = criterion(zi.reshape(1), ti.to(torch.float).reshape(1))  # SOLUTION\n\n        # TODO: perform the backward pass\n        lossi.backward(retain_graph=True)  # SOLUTION\n\n        # TODO: perform gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # SOLUTION\n\n        # accumulate the clipped gradients in `clipped_noisy_grads`\n        for name, param in model.named_parameters():\n            clipped_noisy_grads[name] += param.grad\n\n        # TODO: clear the gradients in the model's computation graph\n        model.zero_grad() # SOLUTION\n\n    # Now, we iterate over the name parameters to add noise\n    for name, param in model.named_parameters():\n        # TODO: Read the equation in the \"Add Noise\" section of the\n        #      algorithm description, and implement it. You may find\n        #      the function `torch.normal` helpful.\n        clipped_noisy_grads[name] += 0 # TODO\n\n        clipped_noisy_grads[name] += torch.normal(mean=0.0, std=noise_multiplier * max_grad_norm, size=param.size()) # SOLUTION\n        clipped_noisy_grads[name] /= len(batch_data) # SOLUTION\n\n    return clipped_noisy_grads\nPlease include the output of the tests below in your submission. (What should the output of the test be?)\nmodel = nn.Linear(5, 1)\nmodel.weight = nn.Parameter(torch.Tensor([[1, 1, 0, 0, 0.]]))\nmodel.bias = nn.Parameter(torch.Tensor([0.]))\nbatch_data = [(torch.Tensor([[1, 1, 1, 0, 0.]]), torch.Tensor([1.])),\n              (torch.Tensor([[1, 0, 1, 0, 0.]]), torch.Tensor([0.]))]\ncriterion = nn.BCEWithLogitsLoss()\n\n# no noise and a large max_grad_norm\nprint(dp_grads(model, batch_data, criterion, max_grad_norm=1000, noise_multiplier=0))\nprint('-----------')\n\n# no noise and a small max_grad_norm\nprint(dp_grads(model, batch_data, criterion, max_grad_norm=0.5, noise_multiplier=3))\nprint('-----------')\n\n# small max_grad_norm and some noise (STD should be ~0.5x3/2)\nfor i in range(10):\n    print(dp_grads(model, batch_data, criterion, max_grad_norm=0.5, noise_multiplier=3))\nTask Now that we have DP-SGD in place, run the below code to train a differentially private model.\ndef train_model_private(model, traindata, valdata, learning_rate=2e-1,\n                        batch_size=500, num_epochs=25, plot_every=20,\n                        epsilon=0.5, max_grad_norm=6):\n    # Compute the noise multiplier\n    N = len(traindata)\n    noise_multiplier = opacus.accountants.utils.get_noise_multiplier(\n        target_epsilon=epsilon,\n        target_delta=1/N,\n        sample_rate=batch_size/N,\n        epochs=num_epochs)\n\n    # Use the differentially private data loader\n    train_loader = opacus.data_loader.DPDataLoader(\n        dataset=traindata,\n        sample_rate=batch_size/N)\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N//batch_size * num_epochs)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n \n    for e in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n            images = images.reshape(-1, 28*28)\n            # get the clipped noisy gradients from the function you wrote\n            clipped_noisy_grads = dp_grads(model,\n                                           batch_data=list(zip(images,labels)),\n                                           criterion=criterion,\n                                           max_grad_norm=max_grad_norm,\n                                           noise_multiplier=noise_multiplier)\n            # manually update the gradients\n            for name, param in model.named_parameters():\n                param.grad = clipped_noisy_grads[name]\n            optimizer.step() # update the parameters\n            scheduler.step() # update the learning rate scheduler\n            optimizer.zero_grad() # clean up accumualted gradients\n\n            iter_count += 1\n            if iter_count % plot_every == 0:\n                # forward pass to compute the loss\n                z = model(images.reshape(-1, 784))\n                loss = criterion(z, labels.to(torch.float))\n                optimizer.zero_grad()\n\n                iters.append(iter_count)\n                ta = accuracy(model, traindata)\n                va = accuracy(model, valdata)\n                train_loss.append(float(loss))\n                train_acc.append(ta)\n                val_acc.append(va)\n                print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n   \n    plt.figure()\n    plt.plot(iters[:len(train_loss)], train_loss)\n    plt.title(\"Loss over iterations\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n\n    plt.figure()\n    plt.plot(iters[:len(train_acc)], train_acc)\n    plt.plot(iters[:len(val_acc)], val_acc)\n    plt.title(\"Accuracy over iterations\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"Train\", \"Validation\"])\nmodel_priv = MLPModel()\ntrain_model_private(model_priv, train_dataset, valid_dataset)\nGraded Task: Plot the histogram of the model prediction for this differentially private model. How does this histogram differ from that of model_np from above?\nplot_hist(model_priv, train_dataset, mem_asses_dataset)\n# TODO: Explain how the histogram differs from that of model_np\nTask How does the accuracy differ from that of model_no mentioned above? Explain what you observe.\n# TODO: Your answer goes here\nGraded Task: Suppose that an attacker recognizes that your friend Taylor is in this data set, means that their X-ray was taken at some point during a hospitalization, and that Taylor provided researchers consent to be included in the study dataset. If this information is sold to a third-party (e.g., a credit reporting agency, an employer, or a landlord), how might this affect Taylor?\n# TODO: Your answer goes here\nIf you are interested in DP, we suggest performing hyperparameter tuning over batch size and max grad norm. In, DP-SGD usually larger batch size would help. So, for instance for two values of \\(\\varepsilon \\in \\{0.5,1,5\\}\\), try to find the best model for \\(\\text{batchsize}\\in \\{100,500\\}\\) and \\(\\text{gradnorm}\\in\\{4,8,16\\}\\)."
  },
  {
    "objectID": "labs/lab05.html#appendix",
    "href": "labs/lab05.html#appendix",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "[Differential Privacy and the Census] https://www.youtube.com/watch?v=nVPE1dbA394\nMain DP-SGD Paper\nCS 860 - Algorithms for Private Data Analysis at UWaterloo"
  },
  {
    "objectID": "labs/lab11.html",
    "href": "labs/lab11.html",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "",
    "text": "In this lab, we will build a generative transformer to generate new lines that emulates the TV show Friends. In order to do so, we will leverage Andrej Karpathy’s implementation of GPT2 called the nanoGPT. This particular implementation uses a small number of components and focuses on the essential ideas behind the GPT2 model.\nInstead of training a GPT model from scratch, we will fine-tune a pre-trained model. This reduces training time necessary to achieve a reasonable measure of performance.\nBy the end of this lab, you will be able to:\n\nFine-tune a transformer model on a new data set.\nTrace the execution of a transformer model to explain its inner working.\nCompare the batching approach used here and in the pervious lab.\nExplain the ethical issues involved in applying large language models.\n\nAcknowledgements:\n\nThe nanoGPT implementation is from https://github.com/karpathy/nanoGPT\nData is from https://convokit.cornell.edu/documentation/friends.html\nThe Byte Pair Encoding tokenizer is from https://github.com/openai/tiktoken\nGPT2 Model was introduced in the paper Language Models are Unsupervised Multitask Learners\n\nPlease work in groups of 1-2 during the lab.\n\n\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab11.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your code to generate the list uts. (1 point)\nPart 1. Your explanation for why the tokenization method needs to be consistent. (1 point)\nPart 1. Your explanation of why the target tensor is an “offset” of the input tensor. (1 point)\nPart 2. Your explanation of the relationship between lm_head and wte. (1 point)\nPart 2. Your result for the shape of x in the GPT.forward() method. (1 point)\nPart 2. Your computation of the shape of (q @ k.transpose(-2, -1)) in the causal self attention module (1 point)\nPart 2. Your explantion of why masking makes sense intuitively for causal self attention. (1 point)\nPart 2. Your computation of the number of parameters in a GPT2 model. (3 points)\n\n\n\n\nThe “Friends Corpus” can be downloaded through the ConvKit package on Python. You can read more about the data here.\nLet’s install this package, and explore the data set.\n%pip install convokit\nimport convokit\ncorpus = convokit.Corpus(convokit.download('friends-corpus'))\nTask: Run the code below, which iterates through the first 10 utterances of the show. How is each utterance formatted? What do the speaker and text field mean?\nfor i, utterance in enumerate(corpus.iter_utterances()):\n    print(utterance)\n    if (i &gt;= 9): \n        break\nGraded Task: Create a list of strings called uts that contains all utterances made by your favourite (of the 6) main character of the show.\ncharacter = 'Monica Geller' # OR 'Chandler Bing' OR 'Phoebe Buffay' OR ...\n\nuts = []\n\nfor utterance in corpus.iter_utterances():\n    pass # TODO\n    uts.append(utterance.text) # SOLUTION - this needs to be changed to account for the character chosen\nPlease include the output of this next line in your solution\nprint(len(uts)) \n# {'Monica Geller': 8498, 'Ross Geller': 9161, 'Phoebe Buffay': 7539, 'Chandler Bing': 8568, 'Joey Tribbiani': 8215, 'Rachel Green': 9331} # SOLUTION\nTask: Run the below code. This code combines these lines into a large string for training, and a large string for validation. We will index ranges in this large string for use in a minibatch—i.e. a minibatch of data will consist of a substring in this large string. This substring may start in the middle of an utterance and may contain multiple utterances—and that turns out to be okay! Our neural network still manages to learn what an utterance looks like and emulate it.\nSince this approach is simpler to implement than the batching approach seen in the previous lab, it is more often used.\nWe will use 90% of the data for training, and 10% of the data for validation.\ntrain_split = 0.9\nn = len(uts)\n\ntrain_data_str = '\\n'.join(uts[:int(n*train_split)])\nval_data_str = '\\n'.join(uts[int(n*train_split):])\nTask: Notice that we split the utterances so that the earlier utterances are in the training set, and the later utterances (i.e., later in the TV show) are in the validation set. Why is this method preferable to randomly splitting the utterances into training and validation?\n# Include your explanation here\nTask: Why do we not set aside a test set?\n# Include your explanation here\nJust like in the previous lab, we will tokenize our text. Modern models use a tokenization strategy called Byte Pair Encoding, which tokenize text into common into sub-word tokens. Sub-word tokens split words into commonly occuring (and thus meaningful) parts. For example, the word “utterance” could be split into “utter” and “ance”. The model would learn about these constinuent tokens in different contexts, helping the model generalize better.\nWe will use the Byte Pair Encoding (BPE) tokenizer from the tiktoken library. Let’s install and import this library.\n%pip install tiktoken\nimport tiktoken\nGraded Task: We will be fine-tuning a pre-trained GPT2 model. Explain why it is important for us to use the same tokenization method as is used in the original GPT2 model whose weights we will be using.\n# Your explanation goes here\nNow, let’s retrieve the original GPT2 model.\nenc = tiktoken.get_encoding(\"gpt2\")\n\ntrain_ids = enc.encode_ordinary(train_data_str)\nval_ids = enc.encode_ordinary(val_data_str)\nTask: How many tokens are in the training and validation sets? How does this compare with the number of words in each data set (computed using the str.split() method)? What about the number of characters?\n# TODO\nprint(\"training data:\")              # SOLUTION\nprint(\"tokens: \", len(train_ids))   # SOLUTION\nprint(\"words: \", len(train_data_str.split()))   # SOLUTION\nprint(\"chars: \", len(train_data_str))   # SOLUTION\nprint() # SOLUTION\nprint(\"valdiation data:\")              # SOLUTION\nprint(\"tokens: \", len(val_ids))   # SOLUTION\nprint(\"words: \", len(val_data_str.split()))   # SOLUTION\nprint(\"chars: \", len(val_data_str))   # SOLUTION\n\n# SOLUTION: There are about 1.5 tokens per word, and 3.4 characters per token\nTask: Run the below code, which will save the above numpy arrays in a file. This way, we can use the np.memmap function, which creates a memory-map to an array stored in a binary file on disk. This approach is useful for accessing segments of a large file on disk, which we will be doing.\nimport numpy as np\nimport os\n\ndata_dir = 'friends_gpt2'\nos.makedirs(data_dir, exist_ok=True)\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile(os.path.join(data_dir, 'train.bin'))\nval_ids.tofile(os.path.join(data_dir, 'val.bin'))\n# create a memory map\ntrain_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\nval_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\nTask: Use the get_batch function below to extract a sample input/output from this data set. Here, we will be using the approach shown in the generative RNN lecture, where the model generates the next token given the previous context.\nimport torch\n\ndef get_batch(data, block_size, batch_size, device):\n    \"\"\"\n    Return a minibatch of data. This function is not deterministic.\n    Calling this function multiple times will result in multiple different\n    return values.\n\n    Parameters:\n        `data` - a numpy array (e.g., created via a call to np.memmap)\n        `block_size` - the length of each sequence\n        `batch_size` - the number of sequences in the batch\n        `device` - the device to place the returned PyTorch tensor\n\n    Returns: A tuple of PyTorch tensors (x, t), where\n        `x` - represents the input tokens, with shape (batch_size, block_size)\n        `y` - represents the target output tokens, with shape (batch_size, block_size)\n    \"\"\"\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    t = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if 'cuda' in device:\n        # pin arrays x,t, which allows us to move them to GPU asynchronously\n        #  (non_blocking=True)\n        x, t = x.pin_memory().to(device, non_blocking=True), t.pin_memory().to(device, non_blocking=True)\n    else:\n        x, t = x.to(device), t.to(device)\n    return x, t\ndevice = 'cuda' if torch.cuda.is_available()  else 'cpu'\n# TODO: get and print a single batch from the training set\nget_batch(train_data, 10, 12, device) # SOLUTION\nGraded Task: Once again, we will be using the approach shown in the generative RNN lecture, where the model’s goal is to generate the next token given the previous context. With that in mind, explain why the target output tokens is very similar to the input tokens, just offset by 1 along the block_size dimension.\n# TODO: Your explanation goes here.\n\n\n\nNow that we have our data set in mind, it is time to set up our GPT2 model. We will use the code provided in the nanoGPT repository, slightly modified here for succinctness. Thus, we will not re-implement the GPT2 model. Instead, let’s use the nanoGPT implementation to understand, step-by-step, what happens in a GPT model.\nWe will explore the components of the GPT2 model first in a top-down manner, to get an intuition as to how the pieces connect. Then, we will explore the same components in a bottom-up manner, so that we can fully understand the role of each component.\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport inspect\nWe begin with the GPTConfig class, which contains model architecture settings for our GPT2 model. The settings specify:\n\nblock_size: the input sequence length. Shorter sequences can be padded (with a padding token as seen in the previous lab), and longer sequences must be cut shorter. During training, we will generate batches with sequences that are exactly the block size\nvocab_size: the number of unique tokens in our vocabulary. This affects the size of the initial embedding layer.\nn_layer: the number of transformer layers.\nn_head: the number of attention heads to use in the causal self-attention layer.\nn_embd: the embedding size used throughout the model. You can think of each token position as being represented as a vector of length n_embd.\ndropout: for dropout.\nbias: A boolean to determine whether to use a bias parameter in certain layers or not.\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\nTask: Which of these settings do you think would affect the total number of trainable parameters in a GPT model? Which of them do you think have the largest impact on the number of trainable parameters? Please write down your guess before continuing, and we will check back here later.\n# TODO: Write down your thoughts here.\nWith the setting in mind, we can set up a GPT model. A GPT model will take a GPTConfig object as a parameter. Pay particular attention to the __init__() and forward() methods. These are the methods that we will study in more detail.\nThe code uses a more PyTorch features that we have not discussed, but these features are mostly cosmetic and do not provide significantly different functionality: the use of nn.ModuleDict allows us to access modules in the GPT class in a straightforward way, and nn.ModuleList allows us to create a list of modules. We have not yet defined the PyTorch neural network modules Block and LayerNorm, but we will do so soon.\nIf you see a PyTorch feature used that you don’t understand, you can always look it up in the PyTorch documentation. However, you don’t try to understand everything at one go. It is normal to read code in multiple “passes”, and focus on the big picture in the first pass.\nTask: Begin with a first pass read of the __init__() and forward() methods of the GPT module.\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t &lt;= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n\n        return logits, loss\n\n    def crop_block_size(self, block_size):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert block_size &lt;= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            if hasattr(block.attn, 'bias'):\n                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        override_args = override_args or {} # default to empty dict\n        # only dropout can be overridden see more notes below\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        config_args['bias'] = True # always True for GPT model checkpoints\n        # we can override the dropout rate, if desired\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        # start with all of the candidate parameters\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        # filter out those that do not require grad\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() &gt;= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() &lt; 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt) # per second\n        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) &lt;= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits &lt; v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\nDoing a first-pass read on both the __init__() and forward() methods, you should see that the GPT model has the following components (ignoring dropout):\n\ntransformer.wte, which is an embedding layer that maps tokens to a vector embedding.\ntransformer.wtp, is also an embedding layer, but this one maps token position indices to a vector embedding. This index is required to inject position information into the embedding—otherwise transformer computation would be invariant to the reordering of input tokens (i.e., the computation would not change if the order of the input tokens change). Since the length of a sequence is at most block_size, so there are at most block_size indices to embed.\nA sequence of Blocks — to be defined. The output from the previous block is taken as the input of the next block.\nA final LayerNorm layer after the last block. This layer is also yet to be defined, and the name suggests that this is a normalization layer (similar to batch normalization) that does not change the shape of the features (i.e., the output shape is the same as the input shape).\nlm_head, which is a linear layer that maps embeddings back to a distribution over the possible otuput tokens.\n\nTask: Compute the number of parameters in the wte embedding layer of the GPT2 model. (For these and other questions that specifically mention GPT2 model, please use the config settings above and provide an actual numbers.)\n# TODO: Perform this computation by hand.\nTask: Compute the number of parameters in the wtp embedding layer of the GPT2 model.\n# TODO: Perform this computation by hand.\nGraded Task: Explain why the linear layer lm_head has the same number of parameters as the embedding layer wte. Provide an intuitive explanation for why weight tying—i.e., using the same set of weights for both layers, just transposed—would be reasonable. The weight tying is done to reduce the total number of parameters in the GPT2 model.\n# TODO: Include your explanations here\nTask: Explain why it is that in the forward() method, the tensor tok_emb has the shape (b, t, n_embd), where b is the batch size, t is the sequence length (max block_size), and n_embd is the embedding size.\n# TODO: Include your explanations here\nTask: Notice that in the forward() method, the tensor pos_emb has the shape (t, n_embd). In other words, we embed the position only once for each batch, and then rely on PyTorch tensor broadcasting to perform the addition tok_emb + pos_emb. Why is this ok?\n# TODO: Include your explanations here\nTask: What is the shape of tok_emb + pos_emb in the forward() method in a GPT2 model? This question is not trivial because the two addend tensors are not of the same shape. Thus, the addition uses broadcasting. PyTorch broadcasting works similarly to that of Numpy’s. You can look up “PyTorch broadcasting” to find resources related to how broadcasting works.\n# TODO: Perform this computation by hand.\nGraded Task: What is the shape of x in the forward() method? This is an important shape to remember, since it is the shape of the feature map consistent in most of the transformer network.\n# TODO: Perform this computation by hand.\nTask: What is the shape of logits in the forward() method?\n# TODO: Perform this computation by hand.\nThese questions above should give you a clear idea of the main components of the transformer model, the expected input and output tensor shapes, and the shapes of intermediate tensors. With this in mind, let’s explore the two modules referenced by GPT.\nWe’ll start with the simple one. The LayerNorm layer is intended to be similar to PyTorch’s LayerNorm layer.\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\nTask: How many parameters are in a LayerNorm layer?\n# TODO: Perform this computation by hand.\nTask: Read the description of the LayerNorm layer in PyTorch at https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm. Then, explain how layer normalization differs from batch normalization.\n# TODO: Perform this computation by hand.\nLet’s move on to the Block module. Recall that here are several Block modules in a GPT model, and the output of one module is the input of the next.\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\nThis module is actually quite succinct, but it also refers to modules that are yet to be defined. It consists of:\n\nA layer normalization layer.\nA causal self attention layer (to be defined). This is the heart of the GPT model.\nAnother layer normalization layer.\nAn MLP layer (to be defined).\n\nTask: Judging by the Block.forward() method above, why must the CausalSelfAttention and the MLP layers preserve the shape of the features?\n# TODO: Include your explanation here.\nTask: How might the skip-connections in the Block.forward() method help with gradient flow? An intuitive explanation is sufficient here.\n# TODO: Include your explanation here.\nWith the GPT2 Block in mind, we will define the MLP module next.\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\nImmediately, we see that this MLP consists of two linear layers. The activation function used between these two layers is the Gaussian Error Linear Units function. You can read more about it in this paper.\nTask: Compute the number of parameters in a MLP layer in a GPT2 model.\n# TODO: Perform this computation by hand\nTask: Recall that the input of the MLP layer is a tensor with the usual dimension computed earlier. What is the shape of self.c_fc(x) in the MLP.forward() method? What about the shape of the return value in this method?\n# TODO: Perform this computation by hand\nTask: Explain why this MLP layer is also called the “pointwise feed forward” layer. (Hint: a “point” here refers to a single token or position in the input sequence)\n# TODO: Include your explanation here.\nFinally, let’s study the definition of the CausalSelfAttention layer. This is the heart of the GPT model and is also the most complex module.\nTask: Begin with a first pass read of the __init__() and forward() methods of CausalSelfAttention module. We will then trace through the case where self.flash is False, since the code provides more detailed explanation for the computation steps.\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\nTask: Compute the number of parameters in the c_attn layer in a GPT2 model.\n# TODO: Perform this computation by hand\nTask: Like the comment in the __init__() method suggests, we can think of the c_attn layer as a combination of three nn.Linear(config.n_embd, config.n_embd, bias=config.bias) modules. These three networks projects the input embedding into three parts: q (for query), k (for key), and v (for *value).\nWhat is the shape of self.c_attn(x) in the forward() method? Use this answer to show that self.c_attn(x).split(self.n_embd, dim=2) gives us the same q, k, v values had we used three separate networks.\n# TODO: Perform the computation by hand, then include your explaination.\nTask: Explain why config.n_head must be a factor of config.n_embd.\n# TODO: Your explanation goes here.\nWe will explore the manual implementation of attention in the next few questions. For this part, it helps to first consider the case where the batch size B=1, and n_head=1. For a larger batch size and number of heads, the attention computation is repeated for every sequence in the batch and every attention head. Thus, the shapes of the three important tensors are:\n\nq: (1, 1, T, n_embd)\nk: (1, 1, T, n_embd)\nv: (1, 1, T, n_embd)\n\nLike discussed in the lectures, you can think of the attention mechanism as a “soft” dictionary lookup. Instead of obtaining a single key/values for a given query, attention gives us a probability distribution over the possible keys/values. We can then use this probability distribution to obtain a weighted sum (akin to an expected value) of the lookup value. Moreover, instead of having strings, numbers, or other objects as keys/values, a key is a vector (of shape n_embd), and a value is also a vector (of shape n_embd). This is consistent with what we have seen in neural networks—everything is represented using a vector! The tensors k and v contains these keys and values, and there is one vector at every token position. The tensor q contains the queries— analogues to the item (a possible key) that we are searching for in a regular dictionary lookup. There is also one query vector for each token position: for each token position, we want to look up a corresponding (weighted sum of) values that contains information pertinent to understanding the meaning of the token in this position.\nWith that in mind, let’s go through the mathematical computations.\nGraded Task: What is the shape of (q @ k.transpose(-2, -1))? For this and the following questions, assume that q, k, v have the shape above, where we have assumed that batch size and num heads are both 1.\n# TODO: Perform this computation by hand\n# SOLUTION: (T, T)\nTask: What is the value of math.sqrt(k.size(-1))?\n# TODO: Perform this computation by hand\n# SOLUTION: n_embed\nTask: Argue that the line att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) is computing a “distance” or “similarity” metric between the query at each token position and the key at each token position.\n# TODO: Your explanation goes here\nThe following line of code references a self.bias parameter, which is defined in the last line of the __init__() method. Since block_size is quite large, we can understand what self.bias looks like by running a similar piece of code below with a smaller block_size value.\nbias = torch.tril(torch.ones(5, 5)).view(1, 1, 5, 5)\nbias\nTask: Explain what the above code returns. Explain how PyTorch broadcasting may be useful for computations involving this tensor—i.e., why is it okay that the first two dimensions of this tensor are 1, thus assuming that batch size = 1 and num heads = 1?\n# TODO: Your explanation goes here\nTask: We will use a similar technique of running a modified version of the next two lines of code in the forward() method to better understand what it does. Run the below code, and explain what the masked_fill function does.\nattn = torch.rand(1, 1, 5, 5)\nattn\nmasked = attn.masked_fill(bias[:,:,:5, :5] == 0, float('-inf'))\nmasked\nout = F.softmax(masked, dim=-1)\nout\n# Your explanation goes here\nGraded Task: This masking is in place so that query tokens cannot “look up” key/values that are at a position with a larger index. Explain why this limitation means our GPT model cannot use information in subsequent/later tokens to form an understand of what what is in the current token. (Note: this masking is the “Causal” part of Causal Self-Attention!)\n# Your explanation goes here\nTask: Your answer above explains which positions in the out tensor need to be set to zero. Explain why setting the corresponding value of pre-softmax tensor masked to -inf is necessary. Why can’t we set the value of masked to 0 in these positions?\n# Your argument goes here\nTask: Argue that out[0,0,0,0] must always be 1.\n# Your argument goes here\nTask: Now, out in our example is akin to the final value of att in the CausalSelfAttention.forward() method. Explain why the operation y = att @ v computes a weighted sum of values at each token position, where the weights are defined by att.\n# Your explanation goes here\nTask: The above explanation pertain to a single attention head. Explain why using multiple attention heads allows a token position to consider information from various other positions. Alternatively, explain why using multiple heads might help the network learn different ways in which the meaning at one token could depend on other tokens.\n# Your explanation goes here\nGraded Task: Compute the total number of parameters in a GPT2 model by computing the following. Please use actual numbers in each case, assuming the GPT2 configuration from above.\n\nThe number of parameters in a CausalSelfAttention model.\nThe number of parameters in a MLP module.\nThe number of parameters in a Block module.\nThe number of parameters in all Block modules in a GPT2 model.\nThe number of parameters in the wte embedding layer in a GPT2 model.\nThe total number of parameters in a GPT2 model.\n\nPlease perform the computation either by hand (and show your work), or with a function that clearly shows the computations.\nYou should see that approximately 30% of the GPT2 weight comes from the wte embedding layer. This is why weight tying is used in the GPT module!\n# TODO: Your work goes here\n\n# SOLUTION THAT I GET:\n# SOLUTION 1. 2360832\n# SOLUTION 2. 4722432\n# SOLUTION 3. 7086336\n# SOLUTION 4. 85036032\n# SOLUTION 5. 38633472\n# SOLUTION 6. 124457472\n\n\n\nWe are ready to finetune our GPT2 model on the “Friends” data set! There is no graded task in this section since training this model can take some time to achieve reasonable performance.\nTo run this part of the lab, you will need to use a GPU. On Google Colab, you can select a session with a GPU by navigating to the “Runtime” menu, selecting “Change runtime type”, and then selecting the “T4 GPU” option.\nWe will set up a config object to make it easier to store and use configs.\nimport easydict\nimport math\nimport time\n\nfinetune_config_dict = {\n  'gradient_accumulation_steps': 32,\n  'block_size': 256,\n  'dropout': 0.2,\n  'bias': False,\n  'learning_rate': 3e-5,\n  'weight_decay': 0.1,\n  'beta1': 0.9,\n  'beta2': 0.99,\n  'grad_clip': 1.0,\n  'decay_lr': False,\n  'warmup_iters': 100,\n  'lr_decay_iters': 5000,\n  'min_lr': 0.0001}\nconfig = easydict.EasyDict(finetune_config_dict)\nFirst, we need to load the GPT2 weights.\n# initialize from OpenAI GPT-2 weights\noverride_args = dict(dropout=config.dropout)\nmodel = GPT.from_pretrained('gpt2', override_args)\n\n# crop down the model block size using model surgery\nif config.block_size &lt; model.config.block_size:\n    model.crop_block_size(config.block_size)\n\ndevice = 'cuda' if torch.cuda.is_available()  else 'cpu'\nmodel.to(device)\nTask: Explain why reducing the block_size do not significantly reduce the number of parameters, but does significantly reduce memory usage.\n# TODO: Include your explanation here\nThere are some additional helpers to improve training.\n# initialize a GradScaler. If enabled=False scaler is a no-op\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# learning rate decay scheduler (cosine with warmup)\ndef get_lr(config, it):\n    # 1) linear warmup for warmup_iters steps\n    if it &lt; config.warmup_iters:\n        return config.learning_rate * it / config.warmup_iters\n    # 2) if it &gt; lr_decay_iters, return min learning rate\n    if it &gt; config.lr_decay_iters:\n        return config.min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n    assert 0 &lt;= decay_ratio &lt;= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n\n# helps estimate an arbitrarily accurate loss over either split using many batches\n@torch.no_grad()\ndef estimate_loss(model, train_dataset, val_dataset, block_size):\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        dataset = train_dataset if split == 'train' else val_dataset\n        for k in range(eval_iters):\n            X, Y = get_batch(dataset, block_size, batch_size, device)\n            with ctx:\n                logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\nNow we can begin the training loop. You may need to increase max_iter to obtain good results.\niter_num = 0\nbest_val_loss = 1e9\neval_interval = 10\nlog_interval = 10\neval_iters = 40\nmax_iters = 500\nbatch_size = 1\n\n# optimizer\noptimizer = model.configure_optimizers(config.weight_decay, config.learning_rate,\n   (config.beta1, config.beta2), device)\n\n# training loop\nX, Y = get_batch(train_data, config.block_size, batch_size, device) # fetch the very first batch\nt0 = time.time()\nlocal_iter_num = 0 # number of iterations in the lifetime of this process\nraw_model = model # unwrap DDP container if needed\nrunning_mfu = -1.0\nwhile True:\n    # determine and set the learning rate for this iteration\n    lr = get_lr(config, iter_num) if config.decay_lr else config.learning_rate\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    # evaluate the loss on train/val sets and write checkpoints\n    if iter_num % eval_interval == 0:\n        losses = estimate_loss(model, train_data, val_data, config.block_size)\n        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # forward backward update, with optional gradient accumulation to simulate larger batch size\n    # and using the GradScaler if data type is float16\n    for micro_step in range(config.gradient_accumulation_steps):\n        with ctx:\n            logits, loss = model(X, Y)\n            loss = loss / config.gradient_accumulation_steps # scale the loss to account for gradient accumulation\n        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n        X, Y = get_batch(train_data, config.block_size, batch_size, device)\n        # backward pass, with gradient scaling if training in fp16\n        scaler.scale(loss).backward()\n\n    # clip the gradient\n    if config.grad_clip != 0.0:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n\n    # step the optimizer and scaler if training in fp16\n    scaler.step(optimizer)\n    scaler.update()\n\n    # flush the gradients as soon as we can, no need for this memory anymore\n    optimizer.zero_grad(set_to_none=True)\n\n    # timing and logging\n    t1 = time.time()\n    dt = t1 - t0\n    t0 = t1\n    if iter_num % log_interval == 0:\n        # get loss as float. note: this is a CPU-GPU sync point\n        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n        lossf = loss.item() * config.gradient_accumulation_steps\n        if local_iter_num &gt;= 5: # let the training loop settle a bit\n            mfu = raw_model.estimate_mfu(batch_size * config.gradient_accumulation_steps, dt)\n            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n\n    iter_num += 1\n    local_iter_num += 1\n\n    # termination conditions\n    if iter_num &gt; max_iters:\n        break\nHere is some code you can use to generate a sequence using the fine-tuned GPT2 model.\ninit_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\nout_dir = 'out' # ignored if init_from is not 'resume'\nstart = \"\\n\" # or \"&lt;|endoftext|&gt;\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\nnum_samples = 10 # number of samples to draw\nmax_new_tokens = 500 # number of tokens generated in each sample\ntemperature = 0.8 # 1.0 = no change, &lt; 1.0 = less random, &gt; 1.0 = more random, in predictions\ntop_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n\n\nenc = tiktoken.get_encoding(\"gpt2\")\nencode = lambda s: enc.encode(s, allowed_special={\"&lt;|endoftext|&gt;\"})\ndecode = lambda l: enc.decode(l)\n\nstart_ids = encode(start)\nx = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n\n# model = finetuned_model\n# run generation\nwith torch.no_grad():\n    with ctx:\n        for k in range(num_samples):\n            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n            print(decode(y[0].tolist()))\n            print('---------------')"
  },
  {
    "objectID": "labs/lab11.html#submission",
    "href": "labs/lab11.html#submission",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "",
    "text": "If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab11.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your code to generate the list uts. (1 point)\nPart 1. Your explanation for why the tokenization method needs to be consistent. (1 point)\nPart 1. Your explanation of why the target tensor is an “offset” of the input tensor. (1 point)\nPart 2. Your explanation of the relationship between lm_head and wte. (1 point)\nPart 2. Your result for the shape of x in the GPT.forward() method. (1 point)\nPart 2. Your computation of the shape of (q @ k.transpose(-2, -1)) in the causal self attention module (1 point)\nPart 2. Your explantion of why masking makes sense intuitively for causal self attention. (1 point)\nPart 2. Your computation of the number of parameters in a GPT2 model. (3 points)"
  },
  {
    "objectID": "labs/lab11.html#part-1.-data",
    "href": "labs/lab11.html#part-1.-data",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "",
    "text": "The “Friends Corpus” can be downloaded through the ConvKit package on Python. You can read more about the data here.\nLet’s install this package, and explore the data set.\n%pip install convokit\nimport convokit\ncorpus = convokit.Corpus(convokit.download('friends-corpus'))\nTask: Run the code below, which iterates through the first 10 utterances of the show. How is each utterance formatted? What do the speaker and text field mean?\nfor i, utterance in enumerate(corpus.iter_utterances()):\n    print(utterance)\n    if (i &gt;= 9): \n        break\nGraded Task: Create a list of strings called uts that contains all utterances made by your favourite (of the 6) main character of the show.\ncharacter = 'Monica Geller' # OR 'Chandler Bing' OR 'Phoebe Buffay' OR ...\n\nuts = []\n\nfor utterance in corpus.iter_utterances():\n    pass # TODO\n    uts.append(utterance.text) # SOLUTION - this needs to be changed to account for the character chosen\nPlease include the output of this next line in your solution\nprint(len(uts)) \n# {'Monica Geller': 8498, 'Ross Geller': 9161, 'Phoebe Buffay': 7539, 'Chandler Bing': 8568, 'Joey Tribbiani': 8215, 'Rachel Green': 9331} # SOLUTION\nTask: Run the below code. This code combines these lines into a large string for training, and a large string for validation. We will index ranges in this large string for use in a minibatch—i.e. a minibatch of data will consist of a substring in this large string. This substring may start in the middle of an utterance and may contain multiple utterances—and that turns out to be okay! Our neural network still manages to learn what an utterance looks like and emulate it.\nSince this approach is simpler to implement than the batching approach seen in the previous lab, it is more often used.\nWe will use 90% of the data for training, and 10% of the data for validation.\ntrain_split = 0.9\nn = len(uts)\n\ntrain_data_str = '\\n'.join(uts[:int(n*train_split)])\nval_data_str = '\\n'.join(uts[int(n*train_split):])\nTask: Notice that we split the utterances so that the earlier utterances are in the training set, and the later utterances (i.e., later in the TV show) are in the validation set. Why is this method preferable to randomly splitting the utterances into training and validation?\n# Include your explanation here\nTask: Why do we not set aside a test set?\n# Include your explanation here\nJust like in the previous lab, we will tokenize our text. Modern models use a tokenization strategy called Byte Pair Encoding, which tokenize text into common into sub-word tokens. Sub-word tokens split words into commonly occuring (and thus meaningful) parts. For example, the word “utterance” could be split into “utter” and “ance”. The model would learn about these constinuent tokens in different contexts, helping the model generalize better.\nWe will use the Byte Pair Encoding (BPE) tokenizer from the tiktoken library. Let’s install and import this library.\n%pip install tiktoken\nimport tiktoken\nGraded Task: We will be fine-tuning a pre-trained GPT2 model. Explain why it is important for us to use the same tokenization method as is used in the original GPT2 model whose weights we will be using.\n# Your explanation goes here\nNow, let’s retrieve the original GPT2 model.\nenc = tiktoken.get_encoding(\"gpt2\")\n\ntrain_ids = enc.encode_ordinary(train_data_str)\nval_ids = enc.encode_ordinary(val_data_str)\nTask: How many tokens are in the training and validation sets? How does this compare with the number of words in each data set (computed using the str.split() method)? What about the number of characters?\n# TODO\nprint(\"training data:\")              # SOLUTION\nprint(\"tokens: \", len(train_ids))   # SOLUTION\nprint(\"words: \", len(train_data_str.split()))   # SOLUTION\nprint(\"chars: \", len(train_data_str))   # SOLUTION\nprint() # SOLUTION\nprint(\"valdiation data:\")              # SOLUTION\nprint(\"tokens: \", len(val_ids))   # SOLUTION\nprint(\"words: \", len(val_data_str.split()))   # SOLUTION\nprint(\"chars: \", len(val_data_str))   # SOLUTION\n\n# SOLUTION: There are about 1.5 tokens per word, and 3.4 characters per token\nTask: Run the below code, which will save the above numpy arrays in a file. This way, we can use the np.memmap function, which creates a memory-map to an array stored in a binary file on disk. This approach is useful for accessing segments of a large file on disk, which we will be doing.\nimport numpy as np\nimport os\n\ndata_dir = 'friends_gpt2'\nos.makedirs(data_dir, exist_ok=True)\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile(os.path.join(data_dir, 'train.bin'))\nval_ids.tofile(os.path.join(data_dir, 'val.bin'))\n# create a memory map\ntrain_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\nval_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\nTask: Use the get_batch function below to extract a sample input/output from this data set. Here, we will be using the approach shown in the generative RNN lecture, where the model generates the next token given the previous context.\nimport torch\n\ndef get_batch(data, block_size, batch_size, device):\n    \"\"\"\n    Return a minibatch of data. This function is not deterministic.\n    Calling this function multiple times will result in multiple different\n    return values.\n\n    Parameters:\n        `data` - a numpy array (e.g., created via a call to np.memmap)\n        `block_size` - the length of each sequence\n        `batch_size` - the number of sequences in the batch\n        `device` - the device to place the returned PyTorch tensor\n\n    Returns: A tuple of PyTorch tensors (x, t), where\n        `x` - represents the input tokens, with shape (batch_size, block_size)\n        `y` - represents the target output tokens, with shape (batch_size, block_size)\n    \"\"\"\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    t = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if 'cuda' in device:\n        # pin arrays x,t, which allows us to move them to GPU asynchronously\n        #  (non_blocking=True)\n        x, t = x.pin_memory().to(device, non_blocking=True), t.pin_memory().to(device, non_blocking=True)\n    else:\n        x, t = x.to(device), t.to(device)\n    return x, t\ndevice = 'cuda' if torch.cuda.is_available()  else 'cpu'\n# TODO: get and print a single batch from the training set\nget_batch(train_data, 10, 12, device) # SOLUTION\nGraded Task: Once again, we will be using the approach shown in the generative RNN lecture, where the model’s goal is to generate the next token given the previous context. With that in mind, explain why the target output tokens is very similar to the input tokens, just offset by 1 along the block_size dimension.\n# TODO: Your explanation goes here."
  },
  {
    "objectID": "labs/lab11.html#part-2.-model",
    "href": "labs/lab11.html#part-2.-model",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "",
    "text": "Now that we have our data set in mind, it is time to set up our GPT2 model. We will use the code provided in the nanoGPT repository, slightly modified here for succinctness. Thus, we will not re-implement the GPT2 model. Instead, let’s use the nanoGPT implementation to understand, step-by-step, what happens in a GPT model.\nWe will explore the components of the GPT2 model first in a top-down manner, to get an intuition as to how the pieces connect. Then, we will explore the same components in a bottom-up manner, so that we can fully understand the role of each component.\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport inspect\nWe begin with the GPTConfig class, which contains model architecture settings for our GPT2 model. The settings specify:\n\nblock_size: the input sequence length. Shorter sequences can be padded (with a padding token as seen in the previous lab), and longer sequences must be cut shorter. During training, we will generate batches with sequences that are exactly the block size\nvocab_size: the number of unique tokens in our vocabulary. This affects the size of the initial embedding layer.\nn_layer: the number of transformer layers.\nn_head: the number of attention heads to use in the causal self-attention layer.\nn_embd: the embedding size used throughout the model. You can think of each token position as being represented as a vector of length n_embd.\ndropout: for dropout.\nbias: A boolean to determine whether to use a bias parameter in certain layers or not.\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\nTask: Which of these settings do you think would affect the total number of trainable parameters in a GPT model? Which of them do you think have the largest impact on the number of trainable parameters? Please write down your guess before continuing, and we will check back here later.\n# TODO: Write down your thoughts here.\nWith the setting in mind, we can set up a GPT model. A GPT model will take a GPTConfig object as a parameter. Pay particular attention to the __init__() and forward() methods. These are the methods that we will study in more detail.\nThe code uses a more PyTorch features that we have not discussed, but these features are mostly cosmetic and do not provide significantly different functionality: the use of nn.ModuleDict allows us to access modules in the GPT class in a straightforward way, and nn.ModuleList allows us to create a list of modules. We have not yet defined the PyTorch neural network modules Block and LayerNorm, but we will do so soon.\nIf you see a PyTorch feature used that you don’t understand, you can always look it up in the PyTorch documentation. However, you don’t try to understand everything at one go. It is normal to read code in multiple “passes”, and focus on the big picture in the first pass.\nTask: Begin with a first pass read of the __init__() and forward() methods of the GPT module.\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t &lt;= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n\n        return logits, loss\n\n    def crop_block_size(self, block_size):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert block_size &lt;= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            if hasattr(block.attn, 'bias'):\n                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        override_args = override_args or {} # default to empty dict\n        # only dropout can be overridden see more notes below\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        config_args['bias'] = True # always True for GPT model checkpoints\n        # we can override the dropout rate, if desired\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        # start with all of the candidate parameters\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        # filter out those that do not require grad\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() &gt;= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() &lt; 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt) # per second\n        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) &lt;= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits &lt; v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\nDoing a first-pass read on both the __init__() and forward() methods, you should see that the GPT model has the following components (ignoring dropout):\n\ntransformer.wte, which is an embedding layer that maps tokens to a vector embedding.\ntransformer.wtp, is also an embedding layer, but this one maps token position indices to a vector embedding. This index is required to inject position information into the embedding—otherwise transformer computation would be invariant to the reordering of input tokens (i.e., the computation would not change if the order of the input tokens change). Since the length of a sequence is at most block_size, so there are at most block_size indices to embed.\nA sequence of Blocks — to be defined. The output from the previous block is taken as the input of the next block.\nA final LayerNorm layer after the last block. This layer is also yet to be defined, and the name suggests that this is a normalization layer (similar to batch normalization) that does not change the shape of the features (i.e., the output shape is the same as the input shape).\nlm_head, which is a linear layer that maps embeddings back to a distribution over the possible otuput tokens.\n\nTask: Compute the number of parameters in the wte embedding layer of the GPT2 model. (For these and other questions that specifically mention GPT2 model, please use the config settings above and provide an actual numbers.)\n# TODO: Perform this computation by hand.\nTask: Compute the number of parameters in the wtp embedding layer of the GPT2 model.\n# TODO: Perform this computation by hand.\nGraded Task: Explain why the linear layer lm_head has the same number of parameters as the embedding layer wte. Provide an intuitive explanation for why weight tying—i.e., using the same set of weights for both layers, just transposed—would be reasonable. The weight tying is done to reduce the total number of parameters in the GPT2 model.\n# TODO: Include your explanations here\nTask: Explain why it is that in the forward() method, the tensor tok_emb has the shape (b, t, n_embd), where b is the batch size, t is the sequence length (max block_size), and n_embd is the embedding size.\n# TODO: Include your explanations here\nTask: Notice that in the forward() method, the tensor pos_emb has the shape (t, n_embd). In other words, we embed the position only once for each batch, and then rely on PyTorch tensor broadcasting to perform the addition tok_emb + pos_emb. Why is this ok?\n# TODO: Include your explanations here\nTask: What is the shape of tok_emb + pos_emb in the forward() method in a GPT2 model? This question is not trivial because the two addend tensors are not of the same shape. Thus, the addition uses broadcasting. PyTorch broadcasting works similarly to that of Numpy’s. You can look up “PyTorch broadcasting” to find resources related to how broadcasting works.\n# TODO: Perform this computation by hand.\nGraded Task: What is the shape of x in the forward() method? This is an important shape to remember, since it is the shape of the feature map consistent in most of the transformer network.\n# TODO: Perform this computation by hand.\nTask: What is the shape of logits in the forward() method?\n# TODO: Perform this computation by hand.\nThese questions above should give you a clear idea of the main components of the transformer model, the expected input and output tensor shapes, and the shapes of intermediate tensors. With this in mind, let’s explore the two modules referenced by GPT.\nWe’ll start with the simple one. The LayerNorm layer is intended to be similar to PyTorch’s LayerNorm layer.\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\nTask: How many parameters are in a LayerNorm layer?\n# TODO: Perform this computation by hand.\nTask: Read the description of the LayerNorm layer in PyTorch at https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm. Then, explain how layer normalization differs from batch normalization.\n# TODO: Perform this computation by hand.\nLet’s move on to the Block module. Recall that here are several Block modules in a GPT model, and the output of one module is the input of the next.\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\nThis module is actually quite succinct, but it also refers to modules that are yet to be defined. It consists of:\n\nA layer normalization layer.\nA causal self attention layer (to be defined). This is the heart of the GPT model.\nAnother layer normalization layer.\nAn MLP layer (to be defined).\n\nTask: Judging by the Block.forward() method above, why must the CausalSelfAttention and the MLP layers preserve the shape of the features?\n# TODO: Include your explanation here.\nTask: How might the skip-connections in the Block.forward() method help with gradient flow? An intuitive explanation is sufficient here.\n# TODO: Include your explanation here.\nWith the GPT2 Block in mind, we will define the MLP module next.\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\nImmediately, we see that this MLP consists of two linear layers. The activation function used between these two layers is the Gaussian Error Linear Units function. You can read more about it in this paper.\nTask: Compute the number of parameters in a MLP layer in a GPT2 model.\n# TODO: Perform this computation by hand\nTask: Recall that the input of the MLP layer is a tensor with the usual dimension computed earlier. What is the shape of self.c_fc(x) in the MLP.forward() method? What about the shape of the return value in this method?\n# TODO: Perform this computation by hand\nTask: Explain why this MLP layer is also called the “pointwise feed forward” layer. (Hint: a “point” here refers to a single token or position in the input sequence)\n# TODO: Include your explanation here.\nFinally, let’s study the definition of the CausalSelfAttention layer. This is the heart of the GPT model and is also the most complex module.\nTask: Begin with a first pass read of the __init__() and forward() methods of CausalSelfAttention module. We will then trace through the case where self.flash is False, since the code provides more detailed explanation for the computation steps.\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\nTask: Compute the number of parameters in the c_attn layer in a GPT2 model.\n# TODO: Perform this computation by hand\nTask: Like the comment in the __init__() method suggests, we can think of the c_attn layer as a combination of three nn.Linear(config.n_embd, config.n_embd, bias=config.bias) modules. These three networks projects the input embedding into three parts: q (for query), k (for key), and v (for *value).\nWhat is the shape of self.c_attn(x) in the forward() method? Use this answer to show that self.c_attn(x).split(self.n_embd, dim=2) gives us the same q, k, v values had we used three separate networks.\n# TODO: Perform the computation by hand, then include your explaination.\nTask: Explain why config.n_head must be a factor of config.n_embd.\n# TODO: Your explanation goes here.\nWe will explore the manual implementation of attention in the next few questions. For this part, it helps to first consider the case where the batch size B=1, and n_head=1. For a larger batch size and number of heads, the attention computation is repeated for every sequence in the batch and every attention head. Thus, the shapes of the three important tensors are:\n\nq: (1, 1, T, n_embd)\nk: (1, 1, T, n_embd)\nv: (1, 1, T, n_embd)\n\nLike discussed in the lectures, you can think of the attention mechanism as a “soft” dictionary lookup. Instead of obtaining a single key/values for a given query, attention gives us a probability distribution over the possible keys/values. We can then use this probability distribution to obtain a weighted sum (akin to an expected value) of the lookup value. Moreover, instead of having strings, numbers, or other objects as keys/values, a key is a vector (of shape n_embd), and a value is also a vector (of shape n_embd). This is consistent with what we have seen in neural networks—everything is represented using a vector! The tensors k and v contains these keys and values, and there is one vector at every token position. The tensor q contains the queries— analogues to the item (a possible key) that we are searching for in a regular dictionary lookup. There is also one query vector for each token position: for each token position, we want to look up a corresponding (weighted sum of) values that contains information pertinent to understanding the meaning of the token in this position.\nWith that in mind, let’s go through the mathematical computations.\nGraded Task: What is the shape of (q @ k.transpose(-2, -1))? For this and the following questions, assume that q, k, v have the shape above, where we have assumed that batch size and num heads are both 1.\n# TODO: Perform this computation by hand\n# SOLUTION: (T, T)\nTask: What is the value of math.sqrt(k.size(-1))?\n# TODO: Perform this computation by hand\n# SOLUTION: n_embed\nTask: Argue that the line att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) is computing a “distance” or “similarity” metric between the query at each token position and the key at each token position.\n# TODO: Your explanation goes here\nThe following line of code references a self.bias parameter, which is defined in the last line of the __init__() method. Since block_size is quite large, we can understand what self.bias looks like by running a similar piece of code below with a smaller block_size value.\nbias = torch.tril(torch.ones(5, 5)).view(1, 1, 5, 5)\nbias\nTask: Explain what the above code returns. Explain how PyTorch broadcasting may be useful for computations involving this tensor—i.e., why is it okay that the first two dimensions of this tensor are 1, thus assuming that batch size = 1 and num heads = 1?\n# TODO: Your explanation goes here\nTask: We will use a similar technique of running a modified version of the next two lines of code in the forward() method to better understand what it does. Run the below code, and explain what the masked_fill function does.\nattn = torch.rand(1, 1, 5, 5)\nattn\nmasked = attn.masked_fill(bias[:,:,:5, :5] == 0, float('-inf'))\nmasked\nout = F.softmax(masked, dim=-1)\nout\n# Your explanation goes here\nGraded Task: This masking is in place so that query tokens cannot “look up” key/values that are at a position with a larger index. Explain why this limitation means our GPT model cannot use information in subsequent/later tokens to form an understand of what what is in the current token. (Note: this masking is the “Causal” part of Causal Self-Attention!)\n# Your explanation goes here\nTask: Your answer above explains which positions in the out tensor need to be set to zero. Explain why setting the corresponding value of pre-softmax tensor masked to -inf is necessary. Why can’t we set the value of masked to 0 in these positions?\n# Your argument goes here\nTask: Argue that out[0,0,0,0] must always be 1.\n# Your argument goes here\nTask: Now, out in our example is akin to the final value of att in the CausalSelfAttention.forward() method. Explain why the operation y = att @ v computes a weighted sum of values at each token position, where the weights are defined by att.\n# Your explanation goes here\nTask: The above explanation pertain to a single attention head. Explain why using multiple attention heads allows a token position to consider information from various other positions. Alternatively, explain why using multiple heads might help the network learn different ways in which the meaning at one token could depend on other tokens.\n# Your explanation goes here\nGraded Task: Compute the total number of parameters in a GPT2 model by computing the following. Please use actual numbers in each case, assuming the GPT2 configuration from above.\n\nThe number of parameters in a CausalSelfAttention model.\nThe number of parameters in a MLP module.\nThe number of parameters in a Block module.\nThe number of parameters in all Block modules in a GPT2 model.\nThe number of parameters in the wte embedding layer in a GPT2 model.\nThe total number of parameters in a GPT2 model.\n\nPlease perform the computation either by hand (and show your work), or with a function that clearly shows the computations.\nYou should see that approximately 30% of the GPT2 weight comes from the wte embedding layer. This is why weight tying is used in the GPT module!\n# TODO: Your work goes here\n\n# SOLUTION THAT I GET:\n# SOLUTION 1. 2360832\n# SOLUTION 2. 4722432\n# SOLUTION 3. 7086336\n# SOLUTION 4. 85036032\n# SOLUTION 5. 38633472\n# SOLUTION 6. 124457472"
  },
  {
    "objectID": "labs/lab11.html#part-3.-training",
    "href": "labs/lab11.html#part-3.-training",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "",
    "text": "We are ready to finetune our GPT2 model on the “Friends” data set! There is no graded task in this section since training this model can take some time to achieve reasonable performance.\nTo run this part of the lab, you will need to use a GPU. On Google Colab, you can select a session with a GPU by navigating to the “Runtime” menu, selecting “Change runtime type”, and then selecting the “T4 GPU” option.\nWe will set up a config object to make it easier to store and use configs.\nimport easydict\nimport math\nimport time\n\nfinetune_config_dict = {\n  'gradient_accumulation_steps': 32,\n  'block_size': 256,\n  'dropout': 0.2,\n  'bias': False,\n  'learning_rate': 3e-5,\n  'weight_decay': 0.1,\n  'beta1': 0.9,\n  'beta2': 0.99,\n  'grad_clip': 1.0,\n  'decay_lr': False,\n  'warmup_iters': 100,\n  'lr_decay_iters': 5000,\n  'min_lr': 0.0001}\nconfig = easydict.EasyDict(finetune_config_dict)\nFirst, we need to load the GPT2 weights.\n# initialize from OpenAI GPT-2 weights\noverride_args = dict(dropout=config.dropout)\nmodel = GPT.from_pretrained('gpt2', override_args)\n\n# crop down the model block size using model surgery\nif config.block_size &lt; model.config.block_size:\n    model.crop_block_size(config.block_size)\n\ndevice = 'cuda' if torch.cuda.is_available()  else 'cpu'\nmodel.to(device)\nTask: Explain why reducing the block_size do not significantly reduce the number of parameters, but does significantly reduce memory usage.\n# TODO: Include your explanation here\nThere are some additional helpers to improve training.\n# initialize a GradScaler. If enabled=False scaler is a no-op\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# learning rate decay scheduler (cosine with warmup)\ndef get_lr(config, it):\n    # 1) linear warmup for warmup_iters steps\n    if it &lt; config.warmup_iters:\n        return config.learning_rate * it / config.warmup_iters\n    # 2) if it &gt; lr_decay_iters, return min learning rate\n    if it &gt; config.lr_decay_iters:\n        return config.min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n    assert 0 &lt;= decay_ratio &lt;= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n\n# helps estimate an arbitrarily accurate loss over either split using many batches\n@torch.no_grad()\ndef estimate_loss(model, train_dataset, val_dataset, block_size):\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        dataset = train_dataset if split == 'train' else val_dataset\n        for k in range(eval_iters):\n            X, Y = get_batch(dataset, block_size, batch_size, device)\n            with ctx:\n                logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\nNow we can begin the training loop. You may need to increase max_iter to obtain good results.\niter_num = 0\nbest_val_loss = 1e9\neval_interval = 10\nlog_interval = 10\neval_iters = 40\nmax_iters = 500\nbatch_size = 1\n\n# optimizer\noptimizer = model.configure_optimizers(config.weight_decay, config.learning_rate,\n   (config.beta1, config.beta2), device)\n\n# training loop\nX, Y = get_batch(train_data, config.block_size, batch_size, device) # fetch the very first batch\nt0 = time.time()\nlocal_iter_num = 0 # number of iterations in the lifetime of this process\nraw_model = model # unwrap DDP container if needed\nrunning_mfu = -1.0\nwhile True:\n    # determine and set the learning rate for this iteration\n    lr = get_lr(config, iter_num) if config.decay_lr else config.learning_rate\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    # evaluate the loss on train/val sets and write checkpoints\n    if iter_num % eval_interval == 0:\n        losses = estimate_loss(model, train_data, val_data, config.block_size)\n        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # forward backward update, with optional gradient accumulation to simulate larger batch size\n    # and using the GradScaler if data type is float16\n    for micro_step in range(config.gradient_accumulation_steps):\n        with ctx:\n            logits, loss = model(X, Y)\n            loss = loss / config.gradient_accumulation_steps # scale the loss to account for gradient accumulation\n        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n        X, Y = get_batch(train_data, config.block_size, batch_size, device)\n        # backward pass, with gradient scaling if training in fp16\n        scaler.scale(loss).backward()\n\n    # clip the gradient\n    if config.grad_clip != 0.0:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n\n    # step the optimizer and scaler if training in fp16\n    scaler.step(optimizer)\n    scaler.update()\n\n    # flush the gradients as soon as we can, no need for this memory anymore\n    optimizer.zero_grad(set_to_none=True)\n\n    # timing and logging\n    t1 = time.time()\n    dt = t1 - t0\n    t0 = t1\n    if iter_num % log_interval == 0:\n        # get loss as float. note: this is a CPU-GPU sync point\n        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n        lossf = loss.item() * config.gradient_accumulation_steps\n        if local_iter_num &gt;= 5: # let the training loop settle a bit\n            mfu = raw_model.estimate_mfu(batch_size * config.gradient_accumulation_steps, dt)\n            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n\n    iter_num += 1\n    local_iter_num += 1\n\n    # termination conditions\n    if iter_num &gt; max_iters:\n        break\nHere is some code you can use to generate a sequence using the fine-tuned GPT2 model.\ninit_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\nout_dir = 'out' # ignored if init_from is not 'resume'\nstart = \"\\n\" # or \"&lt;|endoftext|&gt;\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\nnum_samples = 10 # number of samples to draw\nmax_new_tokens = 500 # number of tokens generated in each sample\ntemperature = 0.8 # 1.0 = no change, &lt; 1.0 = less random, &gt; 1.0 = more random, in predictions\ntop_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n\n\nenc = tiktoken.get_encoding(\"gpt2\")\nencode = lambda s: enc.encode(s, allowed_special={\"&lt;|endoftext|&gt;\"})\ndecode = lambda l: enc.decode(l)\n\nstart_ids = encode(start)\nx = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n\n# model = finetuned_model\n# run generation\nwith torch.no_grad():\n    with ctx:\n        for k in range(num_samples):\n            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n            print(decode(y[0].tolist()))\n            print('---------------')"
  },
  {
    "objectID": "labs/lab10.html",
    "href": "labs/lab10.html",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "",
    "text": "Sentiment Analysis is the problem of identifying the writer’s sentiment given a piece of text. Sentiment Analysis can be applied to movie reviews, feedback of other forms, emails, tweets, course evaluations, and much more.\nIn this lab, we will build an RNN to classify positive vs negative tweets We use the Sentiment140 data set, which contains tweets with either a positive or negative emoticon. Our goal is to determine whether which type of emoticon the tweet (with the emoticon removed) contained. The dataset was actually collected by a group of students, much like you, who are doing their first machine learning projects.\nBy the end of this lab, you will be able to:\n\nUse PyTorch to train an RNN model\nApply and analyze the components of an RNN model\nExplain how batching is done on sequence data, where the training data in a batch may have different lengths\nUse pre-trained word embeddings as part of a transfer learning strategy for text classification\nUnderstand the bias that exists in word embeddings and language models.\n\nAcknowledgements:\n\nData is sampled from http://help.sentiment140.com/for-students\n\nPlease work in groups of 1-2 during the lab.\n\n\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab10.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your output showing several positive tweets. (1 point)\nPart 2. Your explanation of the shapes of wordemb. (1 point)\nPart 2. Your explanation of the shapes of h and out. (2 points)\nPart 2. Your explanation of why computing the mean and max of hidden states across all time steps is likely more informative than using the final output state. (1 point)\nPart 3. Your demonstration of the model’s ability to “overfit” on a data set. (1 point)\nPart 3. Your output from training the model on the full data set. (1 point)\nPart 4. Your explanation of why MyGloveRNN requires fewer iteration to obtain “good” accuracy. (1 point)\nPart 4. Your comparison of MyGloveRNN and MyRNN in low data settings.. (1 point)\nPart 4. Your explanation of where the biases in embeddings come from, and whether our model will have the same sorts of baises.. (1 point)\n\n\n\n\nStart by running these two lines of code to download the data on to Google Colab.\n# Download tutorial data files.\n!wget https://www.cs.toronto.edu/~lczhang/413/sample_tweets.csv\nAs always, we start by understanding what our data looks like. Notice that the test set has been set aside for us. Both the training and test set files follow the same format. Each line in the csv file contains the tweet text, the string label “4” (positive) or “0” (negative), and some additional information about the tweet.\nimport csv\ndatafile = \"sample_tweets.csv\"\n\n# Training/Validation set\ndata = csv.reader(open(datafile))\nfor i, line in enumerate(data):\n    print(line)\n    if i &gt; 10:\n        break\nTask: How many positive and negative tweets are in this file?\n# TODO\nfrom collections import Counter # SOLUTION\nprint(Counter(x[0] for x in csv.reader(open(datafile))))\nGraded Task: We have printed several negative tweets above. Print 10 positive tweets.\n# TODO: Please make sure to include both your code and the\n# printed output\nWe will now split the dataset into training, validation, and test sets:\n# read the data; convert labels into integers\ndata = [(review, int(label=='4'))  # label 1 = positive, 0 = negative\n        for label, _, _, _, _, review in csv.reader(open(datafile))]\n\n# shuffle the data, since the file stores all negative tweets first\nimport random\nrandom.seed(42)\nrandom.shuffle(data)\n\ntrain_data = data[:50000] \nval_data = data[50000:60000] \ntest_data = data[60000:]\nIn order to be able to use neural networks to make predictions about these tweets, we need to begin by convert these tweets into sequences of numbers, each representing a words. This is akin to a one-hot encoding: each word will be converted into an a number representing the unique index of that word.\nAlthough we could do this conversion by writing our own python code, torch has a package called torchtext that has utilities useful for text classification and generation tasks. In particular, the Vocab class and build_vocab_from_iterator will be useful for us for building the mapping from words to indices.\nimport torchtext\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import Vocab, build_vocab_from_iterator\n\n# we will *tokenize* each word by using a tokenzier from \n# https://pytorch.org/text/stable/data_utils.html#get-tokenizer\n\ntokenizer = get_tokenizer(\"basic_english\")\ntrain_data_words = [tokenizer(x) for x, t in train_data]\n\n# build the vocabulary object. the parameters to this function\n# is described below\nvocab = build_vocab_from_iterator(train_data_words,\n                                  specials=['&lt;bos&gt;', '&lt;eos&gt;', '&lt;unk&gt;', '&lt;pad&gt;'],\n                                  min_freq=10)\n\n# set the index of a word not in the vocabulary\nvocab.set_default_index(2) # this is the index of the `&lt;unk&gt;` keyword\nNow, vocab is an object of class Vocab (see more here https://pytorch.org/text/stable/vocab.html ) that provides functionalities for converting words into their indices. In addition to words appearing in the training set, ther are four special tokens that we use, akin to placeholder words:\n\n&lt;bos&gt;, to indicate the beginning of a sequence.\n&lt;eos&gt;, to indicate the end of a sequence.\n&lt;unk&gt;, to indicate a word that is not in the vocabulary. This includes words that appear too infrequently to be included in the vocabulary, and any other words in the validation/test sets that are not see in training.\n&lt;pad&gt;, used for padding shorter sequences in a batch: since each tweet may have different length, the shorter tweets in each batch will be padded with the &lt;pad&gt; token so that each sequence (tweet) in a batch has the same length.\n\nThe min_freq parameter identifies the minimum number of times a word must appear in the training set in order to be included in the vocabulary.\nHere you can see the vocab object in action:\n# Print the number of words in the vocabulary\nprint(len(vocab))\n\n# Convert a tweet into a sequence of word indices.\ntweet = 'The movie Pneumonoultramicroscopicsilicovolcanoconiosis is a good movie, it is very funny'\ntokens = tokenizer(f'&lt;bos&gt; {tweet} &lt;eos&gt;')\nprint(tokens)\nindices = vocab.forward(tokens)\nprint(indices)\nTask: What is the index of the &lt;pad&gt; token?\n# TODO: write code to identify the index of the `&lt;pad&gt;` token\nvocab.forward(['&lt;pad&gt;']) # SOLUTION: 3\nNow let’s apply this transformation to the entire set of training, validation, and test data.\n\ndef convert_indices(data, vocab):\n    \"\"\"Convert data of form [(tweet, label)...] where tweet is a string\n    into an equivalent list, but where the tweets represented as a list\n    of word indices.\n    \"\"\"\n    return [(vocab.forward(tokenizer(f'&lt;bos&gt; {text} &lt;eos&gt;')), label)\n            for (text, label) in data]\n\ntrain_data_indices = convert_indices(train_data, vocab)\nval_data_indices = convert_indices(val_data, vocab)\ntest_data_indices = convert_indices(test_data, vocab)\nWe have seen that PyTorch’s DataLoader provides an easy way to form minibatches when we worked with image data. However, text and sequence data is more challenging to work with since the sequences may not be the same length.\nAlthough we can (and will!) continue to use DataLoader for our text data, we need to provide a function that merges sequences of various lengths into two PyTorch tensors correspondingg to the inputs and targets for that batch.\nTask: Following the instructions below, complete the collate_batch function, which creates the input and target tensors for a batch of data.\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_batch(batch):\n    \"\"\"\n    Returns the input and target tensors for a batch of data\n\n    Parameters:\n        `batch` - An iterable data structure of tuples (indices, label),\n                  where `indices` is a sequence of word indices, and \n                  `label` is either 1 or 0.\n\n    Returns: a tuple `(X, t)`, where \n        - `X` is a PyTorch tensor of shape (batch_size, sequence_length)\n        - `t` is a PyTorch tensor of shape (batch_size)\n    where `sequence_length` is the length of the longest sequence in the batch\n    \"\"\"\n\n    text_list = []  # collect each sample's sequence of word indices\n    label_list = [] # collect each sample's target labels\n    for (text_indices, label) in batch:\n        text_list.append(torch.tensor(text_indices))\n        # TODO: what do we need to do with `label`?\n        label_list.append(label) # SOLUTION\n\n    X = pad_sequence(text_list, padding_value=3).transpose(0, 1)\n    t = None # TODO\n    t = torch.tensor(label_list) # SOLUTION\n    return X, t\n\n\ntrain_dataloader = DataLoader(train_data_indices, batch_size=10, shuffle=True,\n                              collate_fn=collate_batch)\nWith the above code in mind, we should be able to extract batches from train_dataloader. Notice that X.shape is different in each batch. You should also see that the index 3 is used to pad shorter sequences in in a batch.\nfor i, (X, t) in enumerate(train_dataloader):\n    print(X.shape, t.shape)\n    if i &gt;= 10:\n        break\n\nprint(X)\nTask: Why does each sequence begin with the token 0, and end with the token 1 (ignoring the paddings).\n# TODO: Your explanation goes here\n\n\n\nWe will use a recurrent neural network model to classify positive vs negative sentiments. Our RNN model will have three components that are typical in a sequence classification model:\n\nAn embedding layer, which will map each word index (akin to a one-hot embedding) into a low-dimensional vector. This layer as having the same functionality as the weights \\(W^{(word)}\\) from lab 2.\nA recurrent layer, which performs the recurrent neural network computation. The input to this layer is the low-dimensional embedding vectors for each word in the sequence.\nA fully connected layer, which computes the final binary classification using features computed from the recurrent layer. In our case, we concatenate the max and mean of the hidden units across the time steps (i.e. across each word).\n\nLet’s define the model that we will use, and then explore it step by step.\nimport torch.nn as nn\n\nclass MyRNN(nn.Module):\n    def __init__(self, vocab_size, emb_size, hidden_size, num_classes):\n        super(MyRNN, self).__init__()\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.emb = nn.Embedding(vocab_size, emb_size)\n        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, X):\n        # Look up the embedding\n        wordemb = self.emb(X)\n        # Forward propagate the RNN\n        h, out = self.rnn(wordemb)\n        # combine the hidden features computed from *each* time step of\n        # the RNN. we do this by \n        features = torch.cat([torch.amax(h, dim=1),\n                              torch.mean(h, dim=1)], axis=-1)\n        # Compute the final prediction\n        z = self.fc(features)\n        return z\n\nmodel = MyRNN(len(vocab), 128, 64, 2)\nTo explore exactly what this model is doing, let’s grab one batch of data from the data loader we created. We will observe, step-by-step, what computation will be performed on the input X to obtain the final prediction. We do this by emulating the forward method of the MyRNN function.\nX, t = next(iter(train_dataloader))\n\nprint(X.shape)\nGraded Task: Run the code below to check the shape of wordemb. What shape does this tensor have? Explain what each dimension in this shape means.\nwordemb = model.emb(X)\n\nprint(wordemb.shape)\n\n# TODO: Include your explanation here\nGraded Task: Run the code below, which computes the RNN forward pass, with wordemb as input. What shape do the tensors h and out have? Explain what these tensors correspond to. (See the RNN reference https://pytorch.org/docs/stable/generated/torch.nn.RNN.html on the PyTorch documentation page.)\nh, out = model.rnn(wordemb)\n\nprint(h.shape)\nprint(out.shape)\n\n# The tensors `h` and `out` are related. To see the relation,\n# choose an index in the batch and compare the following two\n# vectors in `h` and `out`.\nindex = 2 # choose an index to iterate through the batch\nprint(h[index, -1, :])\nprint(out[0, index, :])\n\n# TODO: Include your explanation here\nGraded Task: There is a step in the MyRNN forward pass that combines the features from each time step of the RNN by computing:\n\nthe maximum value of each position in the hidden vector.\nthe mean value of each position in the hidden vector.\nconcatenating the resulting two vectors.\n\n(Note that in the demo below, we are working with a minibatch. Thus, each of out1, out2, and features below are matrices containing the vectors from each minibatch)\nThis method typically performs better than, say, taking the hidden state at the last time step (the value out from above). Explain, intuitively, why you might expect this performance to be the case for a sentiment analysis task.\nout1 = torch.amax(h, dim=1)\nout2 = torch.mean(h, dim=1)\nfeatures = torch.cat([out1, out2], axis=-1)\n\n# Compare, for a single input in the batch, the connection between\n# `h`, `out1`, `out2` and `features`:\nprint(h[index, :, :])\nprint(out1[index, :])\nprint(out2[index, :])\nprint(features[index, :])\n\n# TODO: Include your explanation here\nTask: Finally, the model uses the features tensor to compute the prediction for each element in the batch. Run the code below to complete this step.\nprint(model.fc(features))\nThere is one more thing we need to do before training the model, which is to write a function to estimate the accuracy of the model. This is done for you below.\ndef accuracy(model, dataset, max=1000):\n    \"\"\"\n    Estimate the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model`   - An object of class nn.Module\n        `dataset` - A dataset of the same type as `train_data`.\n        `max`     - The max number of samples to use to estimate \n                    model accuracy\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    correct, total = 0, 0\n    dataloader = DataLoader(dataset,\n                            batch_size=1,  # use batch size 1 to prevent padding\n                            collate_fn=collate_batch)\n    for i, (x, t) in enumerate(dataloader):\n        z = model(x)\n        y = torch.argmax(z, axis=1)\n        correct += int(torch.sum(t == y))\n        total   += 1\n        if i &gt;= max:\n            break\n    return correct / total\n\naccuracy(model, train_data_indices) # should be close to half\n\n\n\nIn this section, we will train the MyRNN model to classify tweets. As the models that we are building begin to increase in complexity, it is important to use good debugging techniques. In this section, we will introduce the technique of checking whether the model and training code is able to overfit on a small training set. This is a way to check for bugs in the implementation.\nTask: Complete the training code below\nimport torch.optim as optim \nimport matplotlib.pyplot as plt\n\ndef train_model(model,                # an instance of MLPModel\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=0.001,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=50,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               collate_fn=collate_batch,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (texts, labels) in enumerate(train_loader):\n                z = None # TODO\n                z = model(texts) # SOLUTION\n\n                loss = None # TODO\n                loss = criterion(z, labels) # SOLUTION\n\n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    ta = accuracy(model, train_data)\n                    va = accuracy(model, val_data)\n                    train_loss.append(float(loss))\n                    train_acc.append(ta)\n                    val_acc.append(va)\n                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        if plot:\n            plt.figure()\n            plt.plot(iters[:len(train_loss)], train_loss)\n            plt.title(\"Loss over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n\n            plt.figure()\n            plt.plot(iters[:len(train_acc)], train_acc)\n            plt.plot(iters[:len(val_acc)], val_acc)\n            plt.title(\"Accuracy over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n            plt.legend([\"Train\", \"Validation\"])\nGraded Task: As a way to check the model and training code, check if your model can obtain a 100% training accuracy relatively quickly (e.g. within less than a minute of training time), when training on only the first 20 element of the training data.\nmodel = MyRNN(vocab_size=len(vocab),\n              emb_size=300,\n              hidden_size=64,\n              num_classes=2)\n# TODO: Include your code and output \ntrain_model(model, train_data_indices[:20], val_data_indices[:20], # SOLUTION\n            batch_size=10, num_epochs=100, plot_every=1,    # SOLUTION\n            learning_rate=0.001)                            # SOLUTION\nTask: Will this model that you trained above have a high accuracy over the validation set? Explain why or why not.\n# TODO: Your explanation goes here\nGraded Task: Train your model on the full data set. What validation accuracy can you achieve?\n# TODO: Include your code here. Try a few hyperparameter choices until you\n# are satisfied that your model performance is reasonable (i.e. no obviously\n# poor hyperparameter choices)\nmodel = MyRNN(vocab_size=len(vocab), emb_size=300, hidden_size=64, num_classes=2) # SOLUTION\ntrain_model(model, train_data_indices, val_data_indices, batch_size=100, num_epochs=20, learning_rate=0.001) # SOLUTION\nInstead of a (vanilla) RNN model, PyTorch also makes available nn.LSTM and nn.GRU units. They can be used in place of nn.RNN without further changes to the MyRNN code.\nIn general, gated units like LSTM’s are much more frequently used than vanilla RNNs, although transformers are much more popular now as well.\n\n\n\nAs we saw in the previous lab on images, transfer learning is a useful technique in practical machine learning, especially in low-data settings: instead of training an entire neural network from scratch, we use (part of) a model that is pretrained on large amounts of similar data. We use the intermediate state of this pretrained model as features to our model—i.e. we use the pretrained models to compute features.\nJust like with images, using a pretrained model is an important strategy for working with text. Large language models is an excellent demonstration of how generalizable pretrained features can be.\nIn this part of the lab, we will use a slightly older idea of using pretrained word embeddings. In particular, instead of training our own nn.Embedding layer, we will use GloVe embeddings (2014) https://nlp.stanford.edu/projects/glove/ trained on a large data set containing all of Wikipedia and other webpages.\nNowadays, large language model (LLMs), including those with APIs provided by various organizations, can also be used to map words/sentences into embeddings. However, the basic idea of using pretrained models in low-data settings remains similar. We will also identify some bias issues with pretrained word embeddings. There is evidence that these types of bias issues continues to persist in LLMs as well.\nfrom torchtext.vocab import GloVe\n\nglove = torchtext.vocab.GloVe(name=\"6B\", dim=300)\nTask: Run the below code to print the GloVe word embedding for the word “cat”.\nprint(glove['cat'])\nUnfortunately, it is not straightforward to add the &lt;pad&gt;, &lt;unk&gt;, &lt;bos&gt; and &lt;eos&gt; tokens. So we will do without them.\nTask: Run the below code to look up GloVe word indices for the training, validation, and test sets.\ndef convert_indices_glove(data, default=len(glove)-1):\n    result = []\n    for text, label in data:\n        words = tokenizer(text) # for simplicity, we wont use &lt;bos&gt; and &lt;eos&gt;\n        indices = []\n        for w in words:\n            if w in glove.stoi:\n                indices.append(glove.stoi[w])\n            else:\n                # this is a bit of a hack, but we will repurpose *last* word\n                # (least common word) appearing in the GloVe vocabluary as our\n                # '&lt;pad&gt;' token\n                indices.append(default)\n        result.append((indices, label),)\n    return result\n\ntrain_data_glove = convert_indices_glove(train_data)\nval_data_glove = convert_indices_glove(val_data)\ntest_data_glove = convert_indices_glove(test_data)\nNow, we will modify the MyRNN to use the pretrained GloVe vectors:\nclass MyGloveRNN(nn.Module):\n    def __init__(self,  hidden_size, num_classes):\n        super(MyGloveRNN, self).__init__()\n        self.vocab_size, self.emb_size = glove.vectors.shape\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n        self.emb.requires_grad=False # do *not* update the glove embeddings\n        self.rnn = nn.RNN(self.emb_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, X):\n        # Look up the embedding\n        wordemb = self.emb(X)\n        # Forward propagate the RNN\n        h, out = self.rnn(wordemb)\n        # combine the hidden features computed from *each* time step of\n        # the RNN. we do this by \n        features = torch.cat([torch.amax(h, dim=1),\n                              torch.mean(h, dim=1)], axis=-1)\n        # Compute the final prediction\n        z = self.fc(features)\n        return z\n\n    def parameters(self):\n        # do not return the parameters of self.emb \n        # so the optimizer will not update the parameters of self.emb\n        return (p for p in super(MyGloveRNN, self).parameters() if p.requires_grad)\n\n\nmodel = MyGloveRNN(64, 2)\nTask Train this model. Use comparable hyperparameters so that you can compare your result against MyRNN.\n# TODO: Train your model here, and include the output\nmodel = MyGloveRNN(100, 2)                           # SOLUTION\ntrain_model(model, train_data_glove, val_data_glove, # SOLUTION\n              batch_size=100, # SOLUTION\n              num_epochs=200, # SOLUTION\n              plot_every=100, # SOLUTION\n              learning_rate=0.001) # SOLUTION\nGraded Task: You might notice that a very smaller number of iterations will be required to train this model to a reasonable performance (e.g. &gt;70% validation accuracy). Why might this be?\n# TODO: Include your explanation here\nGraded Task: Train both MyGloveRNN and MyRNN models using the corresponding embeddings (pretrained vs. not), but only with the first 200 data points in the training set. How do the validation accuracies compare between these two models?\n# TODO: Training code for MyGloveRNN.\n# Include outputs and training curves in your submission\nglove_model = MyGloveRNN(100, 2)                                  # SOLUTION\ntrain_model(glove_model, train_data_glove[:200], val_data_glove, # SOLUTION\n              batch_size=200, # SOLUTION\n              num_epochs=200, # SOLUTION\n              plot_every=100, # SOLUTION\n              learning_rate=0.001) # SOLUTION\n# TODO: Training code for MyRNN\n# Include outputs and training curves in your submission\nrnn_model = MyRNN(len(vocab), 300, 100, 2)                                  # SOLUTION\ntrain_model(rnn_model, train_data_indices[:200], val_data_indices, # SOLUTION\n              batch_size=200, # SOLUTION\n              num_epochs=200, # SOLUTION\n              plot_every=100, # SOLUTION\n              learning_rate=0.001) # SOLUTION\n# TODO: Compare the validation accuaries here\nMachine learning models have an air of “fairness” about them, since models make decisions without human intervention. However, models can and do learn whatever bias is present in the training data. GloVe vectors seems innocuous enough: they are just representations of words in some embedding space. Even so, we will show that the structure of the GloVe vectors encodes the everyday biases present in the texts that they are trained on.\nWe start with an example analogy to demonstrate the power of GloVe embeddings that allows us to complete analogies by applying arithmetic operations to the word vectors.\n\\[doctor - man + woman \\approx ??\\]\nTo find the answers to the above analogy, we will compute the following vector, and then find the word whose vector representation is closest to it.\nv = glove['doctor'] - glove['man'] + glove['woman']\nTask: Run the code below to find the closets word. You should see the word “nurse” fairly high up in that list.\ndef print_closest_words(vec, n=5):\n    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n    for idx, difference in lst[1:n+1]:                         # take the top n\n        print(glove.itos[idx], difference)\n\nprint_closest_words(v)\nTask: To compare, use a similar method to find the answer to this analogy: \\[doctor - woman + man \\approx ??\\]\nIn other words, we go the opposite direction in the “gender” axis to check if similarly concerning analogies exist.\nprint_closest_words(glove['doctor'] - glove['woman'] + glove['man'])\nTask: Compare the following two outputs.\nprint_closest_words(glove['programmer'] - glove['man'] + glove['woman'])\nprint_closest_words(glove['programmer'] - glove['woman'] + glove['man'])\nTask: Compare the following two outputs.\nprint_closest_words(glove['professor'] - glove['man'] + glove['woman'])\nprint_closest_words(glove['professor'] - glove['woman'] + glove['man'])\nTask: Compare the following two outputs.\nprint_closest_words(glove['engineer'] - glove['man'] + glove['woman'])\nprint_closest_words(glove['engineer'] - glove['woman'] + glove['man'])\nGraded Task: Explain where the bias in these embeddings come from. Would you expect our word embeddings (trained on tweets) to be similarly problematic? Why or why not?\n# TODO: Your explanation goes here"
  },
  {
    "objectID": "labs/lab10.html#submission",
    "href": "labs/lab10.html#submission",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "",
    "text": "If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab10.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your output showing several positive tweets. (1 point)\nPart 2. Your explanation of the shapes of wordemb. (1 point)\nPart 2. Your explanation of the shapes of h and out. (2 points)\nPart 2. Your explanation of why computing the mean and max of hidden states across all time steps is likely more informative than using the final output state. (1 point)\nPart 3. Your demonstration of the model’s ability to “overfit” on a data set. (1 point)\nPart 3. Your output from training the model on the full data set. (1 point)\nPart 4. Your explanation of why MyGloveRNN requires fewer iteration to obtain “good” accuracy. (1 point)\nPart 4. Your comparison of MyGloveRNN and MyRNN in low data settings.. (1 point)\nPart 4. Your explanation of where the biases in embeddings come from, and whether our model will have the same sorts of baises.. (1 point)"
  },
  {
    "objectID": "labs/lab10.html#part-1.-data",
    "href": "labs/lab10.html#part-1.-data",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "",
    "text": "Start by running these two lines of code to download the data on to Google Colab.\n# Download tutorial data files.\n!wget https://www.cs.toronto.edu/~lczhang/413/sample_tweets.csv\nAs always, we start by understanding what our data looks like. Notice that the test set has been set aside for us. Both the training and test set files follow the same format. Each line in the csv file contains the tweet text, the string label “4” (positive) or “0” (negative), and some additional information about the tweet.\nimport csv\ndatafile = \"sample_tweets.csv\"\n\n# Training/Validation set\ndata = csv.reader(open(datafile))\nfor i, line in enumerate(data):\n    print(line)\n    if i &gt; 10:\n        break\nTask: How many positive and negative tweets are in this file?\n# TODO\nfrom collections import Counter # SOLUTION\nprint(Counter(x[0] for x in csv.reader(open(datafile))))\nGraded Task: We have printed several negative tweets above. Print 10 positive tweets.\n# TODO: Please make sure to include both your code and the\n# printed output\nWe will now split the dataset into training, validation, and test sets:\n# read the data; convert labels into integers\ndata = [(review, int(label=='4'))  # label 1 = positive, 0 = negative\n        for label, _, _, _, _, review in csv.reader(open(datafile))]\n\n# shuffle the data, since the file stores all negative tweets first\nimport random\nrandom.seed(42)\nrandom.shuffle(data)\n\ntrain_data = data[:50000] \nval_data = data[50000:60000] \ntest_data = data[60000:]\nIn order to be able to use neural networks to make predictions about these tweets, we need to begin by convert these tweets into sequences of numbers, each representing a words. This is akin to a one-hot encoding: each word will be converted into an a number representing the unique index of that word.\nAlthough we could do this conversion by writing our own python code, torch has a package called torchtext that has utilities useful for text classification and generation tasks. In particular, the Vocab class and build_vocab_from_iterator will be useful for us for building the mapping from words to indices.\nimport torchtext\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import Vocab, build_vocab_from_iterator\n\n# we will *tokenize* each word by using a tokenzier from \n# https://pytorch.org/text/stable/data_utils.html#get-tokenizer\n\ntokenizer = get_tokenizer(\"basic_english\")\ntrain_data_words = [tokenizer(x) for x, t in train_data]\n\n# build the vocabulary object. the parameters to this function\n# is described below\nvocab = build_vocab_from_iterator(train_data_words,\n                                  specials=['&lt;bos&gt;', '&lt;eos&gt;', '&lt;unk&gt;', '&lt;pad&gt;'],\n                                  min_freq=10)\n\n# set the index of a word not in the vocabulary\nvocab.set_default_index(2) # this is the index of the `&lt;unk&gt;` keyword\nNow, vocab is an object of class Vocab (see more here https://pytorch.org/text/stable/vocab.html ) that provides functionalities for converting words into their indices. In addition to words appearing in the training set, ther are four special tokens that we use, akin to placeholder words:\n\n&lt;bos&gt;, to indicate the beginning of a sequence.\n&lt;eos&gt;, to indicate the end of a sequence.\n&lt;unk&gt;, to indicate a word that is not in the vocabulary. This includes words that appear too infrequently to be included in the vocabulary, and any other words in the validation/test sets that are not see in training.\n&lt;pad&gt;, used for padding shorter sequences in a batch: since each tweet may have different length, the shorter tweets in each batch will be padded with the &lt;pad&gt; token so that each sequence (tweet) in a batch has the same length.\n\nThe min_freq parameter identifies the minimum number of times a word must appear in the training set in order to be included in the vocabulary.\nHere you can see the vocab object in action:\n# Print the number of words in the vocabulary\nprint(len(vocab))\n\n# Convert a tweet into a sequence of word indices.\ntweet = 'The movie Pneumonoultramicroscopicsilicovolcanoconiosis is a good movie, it is very funny'\ntokens = tokenizer(f'&lt;bos&gt; {tweet} &lt;eos&gt;')\nprint(tokens)\nindices = vocab.forward(tokens)\nprint(indices)\nTask: What is the index of the &lt;pad&gt; token?\n# TODO: write code to identify the index of the `&lt;pad&gt;` token\nvocab.forward(['&lt;pad&gt;']) # SOLUTION: 3\nNow let’s apply this transformation to the entire set of training, validation, and test data.\n\ndef convert_indices(data, vocab):\n    \"\"\"Convert data of form [(tweet, label)...] where tweet is a string\n    into an equivalent list, but where the tweets represented as a list\n    of word indices.\n    \"\"\"\n    return [(vocab.forward(tokenizer(f'&lt;bos&gt; {text} &lt;eos&gt;')), label)\n            for (text, label) in data]\n\ntrain_data_indices = convert_indices(train_data, vocab)\nval_data_indices = convert_indices(val_data, vocab)\ntest_data_indices = convert_indices(test_data, vocab)\nWe have seen that PyTorch’s DataLoader provides an easy way to form minibatches when we worked with image data. However, text and sequence data is more challenging to work with since the sequences may not be the same length.\nAlthough we can (and will!) continue to use DataLoader for our text data, we need to provide a function that merges sequences of various lengths into two PyTorch tensors correspondingg to the inputs and targets for that batch.\nTask: Following the instructions below, complete the collate_batch function, which creates the input and target tensors for a batch of data.\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_batch(batch):\n    \"\"\"\n    Returns the input and target tensors for a batch of data\n\n    Parameters:\n        `batch` - An iterable data structure of tuples (indices, label),\n                  where `indices` is a sequence of word indices, and \n                  `label` is either 1 or 0.\n\n    Returns: a tuple `(X, t)`, where \n        - `X` is a PyTorch tensor of shape (batch_size, sequence_length)\n        - `t` is a PyTorch tensor of shape (batch_size)\n    where `sequence_length` is the length of the longest sequence in the batch\n    \"\"\"\n\n    text_list = []  # collect each sample's sequence of word indices\n    label_list = [] # collect each sample's target labels\n    for (text_indices, label) in batch:\n        text_list.append(torch.tensor(text_indices))\n        # TODO: what do we need to do with `label`?\n        label_list.append(label) # SOLUTION\n\n    X = pad_sequence(text_list, padding_value=3).transpose(0, 1)\n    t = None # TODO\n    t = torch.tensor(label_list) # SOLUTION\n    return X, t\n\n\ntrain_dataloader = DataLoader(train_data_indices, batch_size=10, shuffle=True,\n                              collate_fn=collate_batch)\nWith the above code in mind, we should be able to extract batches from train_dataloader. Notice that X.shape is different in each batch. You should also see that the index 3 is used to pad shorter sequences in in a batch.\nfor i, (X, t) in enumerate(train_dataloader):\n    print(X.shape, t.shape)\n    if i &gt;= 10:\n        break\n\nprint(X)\nTask: Why does each sequence begin with the token 0, and end with the token 1 (ignoring the paddings).\n# TODO: Your explanation goes here"
  },
  {
    "objectID": "labs/lab10.html#part-2.-model",
    "href": "labs/lab10.html#part-2.-model",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "",
    "text": "We will use a recurrent neural network model to classify positive vs negative sentiments. Our RNN model will have three components that are typical in a sequence classification model:\n\nAn embedding layer, which will map each word index (akin to a one-hot embedding) into a low-dimensional vector. This layer as having the same functionality as the weights \\(W^{(word)}\\) from lab 2.\nA recurrent layer, which performs the recurrent neural network computation. The input to this layer is the low-dimensional embedding vectors for each word in the sequence.\nA fully connected layer, which computes the final binary classification using features computed from the recurrent layer. In our case, we concatenate the max and mean of the hidden units across the time steps (i.e. across each word).\n\nLet’s define the model that we will use, and then explore it step by step.\nimport torch.nn as nn\n\nclass MyRNN(nn.Module):\n    def __init__(self, vocab_size, emb_size, hidden_size, num_classes):\n        super(MyRNN, self).__init__()\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.emb = nn.Embedding(vocab_size, emb_size)\n        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, X):\n        # Look up the embedding\n        wordemb = self.emb(X)\n        # Forward propagate the RNN\n        h, out = self.rnn(wordemb)\n        # combine the hidden features computed from *each* time step of\n        # the RNN. we do this by \n        features = torch.cat([torch.amax(h, dim=1),\n                              torch.mean(h, dim=1)], axis=-1)\n        # Compute the final prediction\n        z = self.fc(features)\n        return z\n\nmodel = MyRNN(len(vocab), 128, 64, 2)\nTo explore exactly what this model is doing, let’s grab one batch of data from the data loader we created. We will observe, step-by-step, what computation will be performed on the input X to obtain the final prediction. We do this by emulating the forward method of the MyRNN function.\nX, t = next(iter(train_dataloader))\n\nprint(X.shape)\nGraded Task: Run the code below to check the shape of wordemb. What shape does this tensor have? Explain what each dimension in this shape means.\nwordemb = model.emb(X)\n\nprint(wordemb.shape)\n\n# TODO: Include your explanation here\nGraded Task: Run the code below, which computes the RNN forward pass, with wordemb as input. What shape do the tensors h and out have? Explain what these tensors correspond to. (See the RNN reference https://pytorch.org/docs/stable/generated/torch.nn.RNN.html on the PyTorch documentation page.)\nh, out = model.rnn(wordemb)\n\nprint(h.shape)\nprint(out.shape)\n\n# The tensors `h` and `out` are related. To see the relation,\n# choose an index in the batch and compare the following two\n# vectors in `h` and `out`.\nindex = 2 # choose an index to iterate through the batch\nprint(h[index, -1, :])\nprint(out[0, index, :])\n\n# TODO: Include your explanation here\nGraded Task: There is a step in the MyRNN forward pass that combines the features from each time step of the RNN by computing:\n\nthe maximum value of each position in the hidden vector.\nthe mean value of each position in the hidden vector.\nconcatenating the resulting two vectors.\n\n(Note that in the demo below, we are working with a minibatch. Thus, each of out1, out2, and features below are matrices containing the vectors from each minibatch)\nThis method typically performs better than, say, taking the hidden state at the last time step (the value out from above). Explain, intuitively, why you might expect this performance to be the case for a sentiment analysis task.\nout1 = torch.amax(h, dim=1)\nout2 = torch.mean(h, dim=1)\nfeatures = torch.cat([out1, out2], axis=-1)\n\n# Compare, for a single input in the batch, the connection between\n# `h`, `out1`, `out2` and `features`:\nprint(h[index, :, :])\nprint(out1[index, :])\nprint(out2[index, :])\nprint(features[index, :])\n\n# TODO: Include your explanation here\nTask: Finally, the model uses the features tensor to compute the prediction for each element in the batch. Run the code below to complete this step.\nprint(model.fc(features))\nThere is one more thing we need to do before training the model, which is to write a function to estimate the accuracy of the model. This is done for you below.\ndef accuracy(model, dataset, max=1000):\n    \"\"\"\n    Estimate the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model`   - An object of class nn.Module\n        `dataset` - A dataset of the same type as `train_data`.\n        `max`     - The max number of samples to use to estimate \n                    model accuracy\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    correct, total = 0, 0\n    dataloader = DataLoader(dataset,\n                            batch_size=1,  # use batch size 1 to prevent padding\n                            collate_fn=collate_batch)\n    for i, (x, t) in enumerate(dataloader):\n        z = model(x)\n        y = torch.argmax(z, axis=1)\n        correct += int(torch.sum(t == y))\n        total   += 1\n        if i &gt;= max:\n            break\n    return correct / total\n\naccuracy(model, train_data_indices) # should be close to half"
  },
  {
    "objectID": "labs/lab10.html#part-3.-training",
    "href": "labs/lab10.html#part-3.-training",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "",
    "text": "In this section, we will train the MyRNN model to classify tweets. As the models that we are building begin to increase in complexity, it is important to use good debugging techniques. In this section, we will introduce the technique of checking whether the model and training code is able to overfit on a small training set. This is a way to check for bugs in the implementation.\nTask: Complete the training code below\nimport torch.optim as optim \nimport matplotlib.pyplot as plt\n\ndef train_model(model,                # an instance of MLPModel\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=0.001,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=50,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               collate_fn=collate_batch,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (texts, labels) in enumerate(train_loader):\n                z = None # TODO\n                z = model(texts) # SOLUTION\n\n                loss = None # TODO\n                loss = criterion(z, labels) # SOLUTION\n\n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    ta = accuracy(model, train_data)\n                    va = accuracy(model, val_data)\n                    train_loss.append(float(loss))\n                    train_acc.append(ta)\n                    val_acc.append(va)\n                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        if plot:\n            plt.figure()\n            plt.plot(iters[:len(train_loss)], train_loss)\n            plt.title(\"Loss over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n\n            plt.figure()\n            plt.plot(iters[:len(train_acc)], train_acc)\n            plt.plot(iters[:len(val_acc)], val_acc)\n            plt.title(\"Accuracy over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n            plt.legend([\"Train\", \"Validation\"])\nGraded Task: As a way to check the model and training code, check if your model can obtain a 100% training accuracy relatively quickly (e.g. within less than a minute of training time), when training on only the first 20 element of the training data.\nmodel = MyRNN(vocab_size=len(vocab),\n              emb_size=300,\n              hidden_size=64,\n              num_classes=2)\n# TODO: Include your code and output \ntrain_model(model, train_data_indices[:20], val_data_indices[:20], # SOLUTION\n            batch_size=10, num_epochs=100, plot_every=1,    # SOLUTION\n            learning_rate=0.001)                            # SOLUTION\nTask: Will this model that you trained above have a high accuracy over the validation set? Explain why or why not.\n# TODO: Your explanation goes here\nGraded Task: Train your model on the full data set. What validation accuracy can you achieve?\n# TODO: Include your code here. Try a few hyperparameter choices until you\n# are satisfied that your model performance is reasonable (i.e. no obviously\n# poor hyperparameter choices)\nmodel = MyRNN(vocab_size=len(vocab), emb_size=300, hidden_size=64, num_classes=2) # SOLUTION\ntrain_model(model, train_data_indices, val_data_indices, batch_size=100, num_epochs=20, learning_rate=0.001) # SOLUTION\nInstead of a (vanilla) RNN model, PyTorch also makes available nn.LSTM and nn.GRU units. They can be used in place of nn.RNN without further changes to the MyRNN code.\nIn general, gated units like LSTM’s are much more frequently used than vanilla RNNs, although transformers are much more popular now as well."
  },
  {
    "objectID": "labs/lab10.html#part-4.-pretrained-embeddings",
    "href": "labs/lab10.html#part-4.-pretrained-embeddings",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "",
    "text": "As we saw in the previous lab on images, transfer learning is a useful technique in practical machine learning, especially in low-data settings: instead of training an entire neural network from scratch, we use (part of) a model that is pretrained on large amounts of similar data. We use the intermediate state of this pretrained model as features to our model—i.e. we use the pretrained models to compute features.\nJust like with images, using a pretrained model is an important strategy for working with text. Large language models is an excellent demonstration of how generalizable pretrained features can be.\nIn this part of the lab, we will use a slightly older idea of using pretrained word embeddings. In particular, instead of training our own nn.Embedding layer, we will use GloVe embeddings (2014) https://nlp.stanford.edu/projects/glove/ trained on a large data set containing all of Wikipedia and other webpages.\nNowadays, large language model (LLMs), including those with APIs provided by various organizations, can also be used to map words/sentences into embeddings. However, the basic idea of using pretrained models in low-data settings remains similar. We will also identify some bias issues with pretrained word embeddings. There is evidence that these types of bias issues continues to persist in LLMs as well.\nfrom torchtext.vocab import GloVe\n\nglove = torchtext.vocab.GloVe(name=\"6B\", dim=300)\nTask: Run the below code to print the GloVe word embedding for the word “cat”.\nprint(glove['cat'])\nUnfortunately, it is not straightforward to add the &lt;pad&gt;, &lt;unk&gt;, &lt;bos&gt; and &lt;eos&gt; tokens. So we will do without them.\nTask: Run the below code to look up GloVe word indices for the training, validation, and test sets.\ndef convert_indices_glove(data, default=len(glove)-1):\n    result = []\n    for text, label in data:\n        words = tokenizer(text) # for simplicity, we wont use &lt;bos&gt; and &lt;eos&gt;\n        indices = []\n        for w in words:\n            if w in glove.stoi:\n                indices.append(glove.stoi[w])\n            else:\n                # this is a bit of a hack, but we will repurpose *last* word\n                # (least common word) appearing in the GloVe vocabluary as our\n                # '&lt;pad&gt;' token\n                indices.append(default)\n        result.append((indices, label),)\n    return result\n\ntrain_data_glove = convert_indices_glove(train_data)\nval_data_glove = convert_indices_glove(val_data)\ntest_data_glove = convert_indices_glove(test_data)\nNow, we will modify the MyRNN to use the pretrained GloVe vectors:\nclass MyGloveRNN(nn.Module):\n    def __init__(self,  hidden_size, num_classes):\n        super(MyGloveRNN, self).__init__()\n        self.vocab_size, self.emb_size = glove.vectors.shape\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n        self.emb.requires_grad=False # do *not* update the glove embeddings\n        self.rnn = nn.RNN(self.emb_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, X):\n        # Look up the embedding\n        wordemb = self.emb(X)\n        # Forward propagate the RNN\n        h, out = self.rnn(wordemb)\n        # combine the hidden features computed from *each* time step of\n        # the RNN. we do this by \n        features = torch.cat([torch.amax(h, dim=1),\n                              torch.mean(h, dim=1)], axis=-1)\n        # Compute the final prediction\n        z = self.fc(features)\n        return z\n\n    def parameters(self):\n        # do not return the parameters of self.emb \n        # so the optimizer will not update the parameters of self.emb\n        return (p for p in super(MyGloveRNN, self).parameters() if p.requires_grad)\n\n\nmodel = MyGloveRNN(64, 2)\nTask Train this model. Use comparable hyperparameters so that you can compare your result against MyRNN.\n# TODO: Train your model here, and include the output\nmodel = MyGloveRNN(100, 2)                           # SOLUTION\ntrain_model(model, train_data_glove, val_data_glove, # SOLUTION\n              batch_size=100, # SOLUTION\n              num_epochs=200, # SOLUTION\n              plot_every=100, # SOLUTION\n              learning_rate=0.001) # SOLUTION\nGraded Task: You might notice that a very smaller number of iterations will be required to train this model to a reasonable performance (e.g. &gt;70% validation accuracy). Why might this be?\n# TODO: Include your explanation here\nGraded Task: Train both MyGloveRNN and MyRNN models using the corresponding embeddings (pretrained vs. not), but only with the first 200 data points in the training set. How do the validation accuracies compare between these two models?\n# TODO: Training code for MyGloveRNN.\n# Include outputs and training curves in your submission\nglove_model = MyGloveRNN(100, 2)                                  # SOLUTION\ntrain_model(glove_model, train_data_glove[:200], val_data_glove, # SOLUTION\n              batch_size=200, # SOLUTION\n              num_epochs=200, # SOLUTION\n              plot_every=100, # SOLUTION\n              learning_rate=0.001) # SOLUTION\n# TODO: Training code for MyRNN\n# Include outputs and training curves in your submission\nrnn_model = MyRNN(len(vocab), 300, 100, 2)                                  # SOLUTION\ntrain_model(rnn_model, train_data_indices[:200], val_data_indices, # SOLUTION\n              batch_size=200, # SOLUTION\n              num_epochs=200, # SOLUTION\n              plot_every=100, # SOLUTION\n              learning_rate=0.001) # SOLUTION\n# TODO: Compare the validation accuaries here\nMachine learning models have an air of “fairness” about them, since models make decisions without human intervention. However, models can and do learn whatever bias is present in the training data. GloVe vectors seems innocuous enough: they are just representations of words in some embedding space. Even so, we will show that the structure of the GloVe vectors encodes the everyday biases present in the texts that they are trained on.\nWe start with an example analogy to demonstrate the power of GloVe embeddings that allows us to complete analogies by applying arithmetic operations to the word vectors.\n\\[doctor - man + woman \\approx ??\\]\nTo find the answers to the above analogy, we will compute the following vector, and then find the word whose vector representation is closest to it.\nv = glove['doctor'] - glove['man'] + glove['woman']\nTask: Run the code below to find the closets word. You should see the word “nurse” fairly high up in that list.\ndef print_closest_words(vec, n=5):\n    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n    for idx, difference in lst[1:n+1]:                         # take the top n\n        print(glove.itos[idx], difference)\n\nprint_closest_words(v)\nTask: To compare, use a similar method to find the answer to this analogy: \\[doctor - woman + man \\approx ??\\]\nIn other words, we go the opposite direction in the “gender” axis to check if similarly concerning analogies exist.\nprint_closest_words(glove['doctor'] - glove['woman'] + glove['man'])\nTask: Compare the following two outputs.\nprint_closest_words(glove['programmer'] - glove['man'] + glove['woman'])\nprint_closest_words(glove['programmer'] - glove['woman'] + glove['man'])\nTask: Compare the following two outputs.\nprint_closest_words(glove['professor'] - glove['man'] + glove['woman'])\nprint_closest_words(glove['professor'] - glove['woman'] + glove['man'])\nTask: Compare the following two outputs.\nprint_closest_words(glove['engineer'] - glove['man'] + glove['woman'])\nprint_closest_words(glove['engineer'] - glove['woman'] + glove['man'])\nGraded Task: Explain where the bias in these embeddings come from. Would you expect our word embeddings (trained on tweets) to be similarly problematic? Why or why not?\n# TODO: Your explanation goes here"
  },
  {
    "objectID": "labs/lab07.html",
    "href": "labs/lab07.html",
    "title": "CSC413 Lab 9: Transfer Learning and Descent",
    "section": "",
    "text": "Transfer learning is a technique where we use neural network weights trained to complete one task to complet a different task. In this tutorial, we will go through an example of transfer learning to detect American Sign Language (ASL) gestures letters A-I. Although we could train a CNN from scratch, you will see that using CNN weights that are pretrained on a larger dataset and more complex task provides much better results, all with less training.\nAmerican Sign Language (ASL) is a complete, complex language that employs signs made by moving the hands combined with facial expressions and postures of the body. It is the primary language of many North Americans who are deaf and is one of several communication options used by people who are deaf or hard-of-hearing.\nThe hand gestures representing English alphabets are shown below. This lab focuses on classifying a subset of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand showing one of the letters A-I, we want to detect which letter is being represented.\n\nBy the end of this lab, you will be able to:\n\nAnalyze the role of batch normalization and other model architecture choice in a neural network.\nDefine the double descent phenomenon and explain why it occurs.\nAnalyze the shape of the training curve of a convolutional neural network with respect to the double descent phenomenon.\nApply transfer learning to solve an image classification task.\nCompare transfer learning vs. training a CNN from scratch.\nIdentify and suggest corrections for model building issues by inspecting misclassified data.\n\nAcknowledgements:\n\nData is collected from a previous machine learning course APS360. Only data of students who provided consent is included.\n\nPlease work in groups of 1-2 during the lab.\n\n\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab07.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your answer to the question about the splitting of the data into train/validation/test sets. (1 point)\nPart 2. Your comparison of the CNN model with and without batch normalization. (1 point)\nPart 2. Your comparison of BatchNorm1d vs BatchNorm2d. (1 point)\nPart 2. Your analysis of the effect of varying the CNN model width. (1 point)\nPart 2. Your analysis of the effect of varying weight decay parameter. (1 point)\nPart 2. Your analysis of the training curve that illustrates double descent. (1 point)\nPart 3. Your implementation of LinearModel for transfer learning. (1 point)\nPart 3. Your comparison of transfer learning vs the CNN model. (1 point)\nPart 4. Your analysis of the confusion matrix. (1 point)\nPart 4. Your explanation for how to mitigate an issue we notice by visually inspecting misclassified images. (1 point)\n\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models, torchvision.datasets\n\n%matplotlib inline\n\n\n\nWe will begin by downloading the data onto Google Colab.\n# Download lab data file\n!wget https://www.cs.toronto.edu/~lczhang/413/asl_data.zip\n!unzip asl_data.zip\nThe file structure we use is intentional, so that we can use torchvision.datasets.ImageFolder to help load our data and create labels.\nYou can read what torchvision.datasets.ImageFolder does for us here https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\ntrain_path = \"asl_data/train/\" # edit me\nvalid_path = \"asl_data/valid/\" # edit me\ntest_path = \"asl_data/test/\"   # edit me\n\ntrain_data = torchvision.datasets.ImageFolder(train_path, transform=torchvision.transforms.ToTensor())\nvalid_data = torchvision.datasets.ImageFolder(valid_path, transform=torchvision.transforms.ToTensor())\ntest_data = torchvision.datasets.ImageFolder(test_path, transform=torchvision.transforms.ToTensor())\nAs in previous labs, we can iterate through the one training data point at a time like this:\nfor x, t in train_data:\n    print(x, t)\n    plt.imshow(x.transpose(2, 0).transpose(0, 1).numpy()) # display an image\n    break # uncomment if you'd like\nTask: What do the variables x and t contain? What is the shape of our images? What are our labels? Based on what you learned in Part (a), how were the labels generated from the folder structure?\n# Your explanation goes here\nWe saw in the earlier tutorials that PyTorch has a utility to help us creat minibatches with our data. We can use the same DataLoader helper here:\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)\n\nfor x, t in train_loader:\n    print(x, t)\n    break # uncomment if you'd like\nTask: What do the variables x and t contain? What are their shapes? What data do they contain?\n# Your explanation goes here\nTask: How many images are there in the training, validation, and test sets?\n# Your explanation goes here\nNotice that there are fewer images in the training set, compared to the validation and test sets. This is so that we can explore the effect of having a limited training set.\nGraded Task: The data set is generated by students taking pictures of their hand while making the corresponding gestures. We therefor split the training, validation, and test sets were split so that images generated by a student all belongs in a single data set. In other words, we avoid cases where some students’ images are in the training set and others end up in the test set. Why do you think this important for obtaining a representative test accuracy?\n# Your explanation goes here\n\n\n\nFor this part, we will be working with this CNN network.\nclass CNN(nn.Module):\n    def __init__(self, width=4, bn=True):\n        \"\"\"\n        A 4-layer convolutional neural network. The first layer has\n        `width` number of channels, and with each layer we half the\n        feature width/height and double the number of channels.\n\n        If `bn` is set to False, then batch normalization will not run.\n        \"\"\"\n        super(CNN, self).__init__()\n        self.width = width\n        self.bn = bn\n        # define all the conv layers\n        self.conv1 = nn.Conv2d(in_channels=3,\n                               out_channels=self.width,\n                               kernel_size=3,\n                               padding=1)\n        self.conv2 = nn.Conv2d(in_channels=self.width,\n                               out_channels=self.width*2,\n                               kernel_size=3,\n                               padding=1)\n        self.conv3 = nn.Conv2d(in_channels=self.width*2,\n                               out_channels=self.width*4,\n                               kernel_size=3,\n                               padding=1)\n        self.conv4 = nn.Conv2d(in_channels=self.width*4,\n                               out_channels=self.width*8,\n                               kernel_size=3,\n                               padding=1)\n        # define all the BN layers\n        if bn:\n            self.bn1 = nn.BatchNorm2d(self.width)\n            self.bn2 = nn.BatchNorm2d(self.width*2)\n            self.bn3 = nn.BatchNorm2d(self.width*4)\n            self.bn4 = nn.BatchNorm2d(self.width*8)\n        # pooling layer has no parameter, so one such layer\n        # can be shared across all conv layers\n        self.pool = nn.MaxPool2d(2, 2)\n        # FC layers\n        self.fc1 = nn.Linear(self.width * 8 * 14 * 14, 100)\n        self.fc2 = nn.Linear(100, 9)\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        if self.bn:\n            x = self.bn1(x)\n        x = self.pool(torch.relu(self.conv2(x)))\n        if self.bn:\n            x = self.bn2(x)\n        x = self.pool(torch.relu(self.conv3(x)))\n        if self.bn:\n            x = self.bn3(x)\n        x = self.pool(torch.relu(self.conv4(x)))\n        if self.bn:\n            x = self.bn4(x)\n        x = x.view(-1, self.width * 8 * 14 * 14)\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\nTask:\nThe training code is written for you. Train the CNN() model for at least 6 epochs, and report on the maximum validation accuracy that you can attain.\nAs your model is training, you might want to move on to the next question.\ndef get_accuracy(model, data, device=\"cpu\"):\n    loader = torch.utils.data.DataLoader(data, batch_size=256)\n    model.to(device)\n    model.eval() # annotate model for evaluation (important for batch normalization)\n    correct = 0\n    total = 0\n    for imgs, labels in loader:\n        labels = labels.to(device)\n        output = model(imgs.to(device))\n        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(labels.view_as(pred)).sum().item()\n        total += imgs.shape[0]\n    return correct / total\n\ndef train_model(model,\n                train_data,\n                valid_data,\n                batch_size=64,\n                weight_decay=0.0,\n                learning_rate=0.001,\n                num_epochs=50,\n                plot_every=20,\n                plot=True,\n                device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True)\n    model = model.to(device) # move model to GPU if applicable\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(),\n                           lr=learning_rate,\n                           weight_decay=weight_decay)\n    # for plotting\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for epoch in range(num_epochs):\n            for imgs, labels in iter(train_loader):\n                if imgs.size()[0] &lt; batch_size:\n                    continue\n                labels = labels.to(device)\n                imgs = imgs.to(device)\n                model.train()\n                out = model(imgs)\n                loss = criterion(out, labels)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    loss = float(loss)\n                    tacc = get_accuracy(model, train_data, device)\n                    vacc = get_accuracy(model, valid_data, device)\n                    print(\"Iter %d; Loss %f; Train Acc %.3f; Val Acc %.3f\" % (iter_count, loss, tacc, vacc))\n\n                    iters.append(iter_count)\n                    train_loss.append(loss)\n                    train_acc.append(tacc)\n                    val_acc.append(vacc)\n    finally:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend([\"Train\", \"Validation\"])\nTask: Run the training code below. What validation accuracy can be achieved by this CNN?\ncnn = CNN(width=4)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\n\n\n\nIn this section, we will explore the effect of various aspects of a CNN model architecture. We will pay particluar attention to architecture decisions that affect the bias and variance of the model. Finally, we explore a phenomenon called double descent.\nTo begin, let’s explore the effect of batch normalization.\nTask: Run the training code below to explore the effect of training without batch normalization.\ncnn = CNN(bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\nGraded Task: Compare the two sets of training curves above for the CNN model with and without batch normalization. What is the effect of batch normalization on the training loss and accuracy? What about the validation accuracy?\n# TODO: Include your analysis here\n# SOLUTION - training loss/acc improves a lot more quickly with BN\n# SOLUTION   but final val loss is about the same for this task\nGraded Task: We used the layer called BatchNorm2d in our CNN. What do you think is the difference between BatchNorm2d and BatchNorm1d? Why are we using BatchNorm2d in our CNN? Why would we use BatchNorm1d in an MLP? You may wish to consult the PyTorch documentation. (How can you find it?)\n# Explain your answer here\nTask: Run the training code below to explore the effect of varying the model width for this particular data set.\ncnn = CNN(width=2, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\ncnn = CNN(width=4, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\ncnn = CNN(width=16, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\nGraded Task: What is the effect of varying the model width above for this particular data set? Do you notice an effect on the training loss? What about the training/validation accuracy? The final validation accuracy? (Your answer may or may not match your expectations. Please answer based on the actual results above.)\n# TODO: Include your analysis here\nTask: Run the training code below to explore the effect of weight decay when training a large model.\ncnn = CNN(width=16, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)\ncnn = CNN(width=16, bn=True) # try with batch norm on\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)\ncnn = CNN(width=16, bn=True) # try decreasing weight decay parameter\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.0001)\nGraded Task: What is the effect of setting weight decay to the above value? Do you notice an effect on the training loss? What about the training/validation accuracy? The final validation accuracy? (Again, your answer may or may not match your expectations. Please answer based on the actual results above.)\n# TODO: Include your analysis here\nTask: Note that there is quite a bit of noise in the results that we might obtain above. That is, if you run the same code twice, you may obtain different answers. Why might that be? What are two sources of noise/randomness?\n# TODO: Include your explanation here\n# SOLUTION: SGD and initalization of parameters\nThese settings that we have been exporting are hyperparameters that should be tuned when you train a neural network. These hyperparameters interact with one another, and thus we should tune them using the grid search strategy mentioned in previous labs.\nYou are not required to perform grid search for this lab, so that we can explore a few other phenomena.\nOne interesting phenomenon is called double descent. In statistical learning theory, we expect validation error to decrease with increase model capacity, and then increase as the model overfits to the number of data points available for training. In practise, in neural networks, we often see that as model capacity increases, validation error first decreases, then increase, and then decrease again—hence the name “double descent”.\nIn fact, the increase in validation error is actually quite subtle. However, what is readily apparent is that in most cases, increasing model capacity does not result in a decrease in validation accuracy.\nOptional Task: To illustrate that validation accuracy is unlikely to decrease with increased model parameter, train the below network.\n# Uncomment to run. \n# cnn = CNN(width=40, bn=True)\n# train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=50)\nDouble descent is actually not that mysterious. It comes from the fact that when capacity is large enough there are many parameter choices that achieves 100% training accuracy, the neural network optimization procedure is effectively choosing a best parameters out of the many that can achieve this perfect training accuracy. This differs from when capacity is low, where the optimization process needs to find a set of parameter choices that best fits the training data—since no choice of parameters fits the training data perfectly. When the capacity is just large enough to be able to find parameters that fit the data, but too small for there be a range of parameter choices available to be able to select a “best” one.\nThis twitter thread written by biostatistics professor Daniela Witten also provides an intuitive explanation, using polynomial curve fitting as an example: https://twitter.com/daniela_witten/status/1292293102103748609\nDouble descent explored in depth in this paper here: https://openreview.net/pdf?id=B1g5sA4twr This paper highlights that the increase in validation/test error occurs when the training accuracy approximates 100%. Moreover, the double descent phenomena is noticable when varying model capacity (e.g. number of parameters) and when varying the number of iterations/epochs of training.\nWe will attempt to explore the latter effect—i.e. we will train a large model, use a small numer of training data points, and explore how each iteration of training impacts validation accuracy. The effect is subtle and, depending on your neural network initialization, you may not see an effect. So, a training curve is also provided for you to analyze.\nOptional Task: Run the code below to try and reproduce the “double descent” phenomena. This code will take a while to run, so you may wish to continue with the remaining questions while it runs.\n# use a subset of the training data\n# uncomment to train\n\n# train_data_subset, _ =  random_split(train_data, [50, len(train_data)-50])\n# cnn = CNN(width=20)\n# train_model(cnn,\n#             train_data_subset,\n#             valid_data,\n#             batch_size=50, # set batch_size=len(train_data_subset) to minimize training noise\n#             num_epochs=200,\n#             plot_every=1,  # plot every epoch (this is slow)\n#             learning_rate=0.0001)  # choose a low learning rate\nFor reference, here is the our training curve showing the loss and accuracy over 200 iterations:\n \nWe are not able to consistently reproduce this result (e.g., due to initialization), so it is totally reasonable for your figure to look different!\nTask: In the provided training curve, during which iterations do the validation accuracy initially increase (i.e. validation error decrease)?\n# TODO: Include your answer here\nGraded Task: In the provided training curve, during which iterations do the validation accuracy decrease slightly? Approximately what training accuracy is achieved at this piont?\n# TODO: Include your answer here\nTask: In the provided training curve, during which iterations do the validation accuracy increase for a second time (i.e. validation error descends for a second time)?\n# TODO: Include your answer here\n\n\n\nFor many image classification tasks, it is generally not a good idea to train a very large deep neural network model from scratch due to the enormous compute requirements and lack of sufficient amounts of training data.\nA better option is to try using an existing model that performs a similar task to the one you need to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed Transfer Learning. In this assignment, we will use Transfer Learning to extract features from the hand gesture images. Then, train a smaller network to use these features as input and classify the hand gestures.\nAs you have learned from the CNN lecture, convolution layers extract various features from the images which get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal role in establishing Deep Neural Nets as a go-to tool for image classification problems and we will use an ImageNet pre-trained AlexNet model to extract features in this assignment.\nHere is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch will download the pretrained weights from the internet.\nimport torchvision.models\nalexnet = torchvision.models.alexnet(pretrained=True)\n\nprint(alexnet)\nAs you can see, the alexnet model is split up into two components: alexnet.features and alexnet.classifier. The first neural network component, alexnet.features, is used to computed convolutional features, which is taken as input in alexnet.classifier.\nThe neural network alexnet.features expects an image tensor of shape Nx3x224x224 as inputs and it will output a tensor of shape Nx256x6x6 . (N = batch size).\nHere is an example code snippet showing how you can compute the AlexNet features for some images (your actual code might be different):\nimg, label = train_data[0]\nfeatures = alexnet.features(img.unsqueeze(0)).detach()\n\nprint(features.shape)\nNote that the .detach() at the end will be necessary in your code. The reason is that PyTorch automatically builds computation graphs to be able to backpropagate graidents. If we did not explicitly “detach” this tensor from the AlexNet portion of the computation graph, PyTorch might try to backpropagate gradients to the AlexNet weight and tune the AlexNet weights.\nTODO Compute the AlexNet features for each of your training, validation, and test data by completing the function compute_features. The code below creates three new arrays called train_data_fets, valid_data_fets and test_data_fets. Each of these arrays contains tuples of the form (alexnet_features, label).\ndef compute_features(data):\n    fets = []\n    for img, t in data:\n        features = None  # TODO\n        features = alexnet.features(img.unsqueeze(0)).detach() # SOLUTION\n        fets.append((features, t),)\n    return fets\n\ntrain_data_fets = compute_features(train_data)\nvalid_data_fets = compute_features(valid_data)\ntest_data_fets = compute_features(test_data)\nIn the rest of this part of the lab, we will test two models that will take as input these AlexNet features, and make a prediction for which letter the hand gesture represents. The two models are a linear model, a two-layer MLP. We will compare the performance of these two models.\nGraded Task: Complete the definition of the LinearModel class, which is a linear model (e.g., logistic regression). This model should as input these AlexNet features, and make a prediction for which letter the hand gesture represents.\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super(LinearModel, self).__init__()\n        # TODO: What layer need to be initialized?\n        self.fc = nn.Linear(256 * 6 * 6, 10) # SOLUTION\n\n    def forward(self, x):\n        x = x.view(-1, 256 * 6 * 6) # flatten the input\n        z = None # TODO: What computation needs to be performed?\n        z = self.fc(x) # SOLUTION\n        return z\n\nm_linear = LinearModel()\nm_linear(train_data_fets[0][0]) # this should produce a(n unnormalized) prediction\nTask: Train a LinearModel() for at least 6 epochs, and report on the maximum validation accuracy that you can attain. We should still be able to use the train_model function, but make sure to provide the AlexNet features as input (and not the image features).\nm_linear = LinearModel()\n# TODO: Train the linear model. Include your output in your submission\ntrain_model(m_linear, train_data_fets, valid_data_fets) # SOLUTION\nGraded Task: Compare this model with the CNN() models that we trained earlier. How does this model perform in terms of validation accuracy? What about in terms of the time it took to train this model?\n# TODO: Your observation goes here\nTask: We decide to use AlexNet features as input to our MLP, and avoided tuning AlexNet weights. However, we could have considered AlexNet to be a part of our model, and continue to tune AlexNet weights to improve our model performance. What are the advantages and disadvantages of continuing to tune AlexNet weights?\n# TODO\n\n\n\nTask: Report the test accuracy on this transfer learning model.\n# TODO\nget_accuracy(m_linear, test_data_fets) # SOLUTION\nTask: Use this code below to construct the confusion matrix for this model over the test set.\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nimport sklearn\nlabel = \"ABCDEFGHI\"\ndef plot_confusion(model, data):\n    n = 0\n    ts = []\n    ys = []\n    for x, t in data:\n        z = model(x.unsqueeze(0))\n        y = int(torch.argmax(z))\n        ts.append(t)\n        ys.append(y)\n\n    cm = confusion_matrix(ts, ys)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label)\n    plt.figure()\n    disp.plot()\n\nplot_confusion(m_linear, test_data_fets)\nGraded Task: Which class is most likely mistaken as another? Is this reasonable? (i.e. is that class particularly challenging, or very similar to another class?)\n# TODO: Include your analysis here\nTask: In order to understand where errors come from, it is crucial that we explore why and how our models fail. A first step is to visually inspect the test data points where failure occurs. That way, we can identify what we can do to prevent/fix errors before our models are deployed.\nRun the below code to display images in the test set that our model misclassifies:\nfor i, (x, t) in enumerate(test_data_fets):\n    y = int(torch.argmax(m_linear(x)))\n    if not (y == t):\n        plt.figure()\n        plt.imshow(test_data[i][0].transpose(0,1).transpose(1,2).numpy())\nTask: By visually inspecting these misclassified images, we see that there are two main reasons for misclassification. What reason for misclassification is due to a mistake in the formatting of the test set images?\n# TODO\nGraded Task: We also see a much more serious issue, where gestures made by individuals with darker skin tones may be more frequently misclasified. This result suggests that errors in the model may impact some groups more than others. What steps should we take to mitigate this issue?\n# TODO"
  },
  {
    "objectID": "labs/lab07.html#submission",
    "href": "labs/lab07.html#submission",
    "title": "CSC413 Lab 9: Transfer Learning and Descent",
    "section": "",
    "text": "If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab07.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your answer to the question about the splitting of the data into train/validation/test sets. (1 point)\nPart 2. Your comparison of the CNN model with and without batch normalization. (1 point)\nPart 2. Your comparison of BatchNorm1d vs BatchNorm2d. (1 point)\nPart 2. Your analysis of the effect of varying the CNN model width. (1 point)\nPart 2. Your analysis of the effect of varying weight decay parameter. (1 point)\nPart 2. Your analysis of the training curve that illustrates double descent. (1 point)\nPart 3. Your implementation of LinearModel for transfer learning. (1 point)\nPart 3. Your comparison of transfer learning vs the CNN model. (1 point)\nPart 4. Your analysis of the confusion matrix. (1 point)\nPart 4. Your explanation for how to mitigate an issue we notice by visually inspecting misclassified images. (1 point)\n\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models, torchvision.datasets\n\n%matplotlib inline"
  },
  {
    "objectID": "labs/lab07.html#part-1.-data",
    "href": "labs/lab07.html#part-1.-data",
    "title": "CSC413 Lab 9: Transfer Learning and Descent",
    "section": "",
    "text": "We will begin by downloading the data onto Google Colab.\n# Download lab data file\n!wget https://www.cs.toronto.edu/~lczhang/413/asl_data.zip\n!unzip asl_data.zip\nThe file structure we use is intentional, so that we can use torchvision.datasets.ImageFolder to help load our data and create labels.\nYou can read what torchvision.datasets.ImageFolder does for us here https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\ntrain_path = \"asl_data/train/\" # edit me\nvalid_path = \"asl_data/valid/\" # edit me\ntest_path = \"asl_data/test/\"   # edit me\n\ntrain_data = torchvision.datasets.ImageFolder(train_path, transform=torchvision.transforms.ToTensor())\nvalid_data = torchvision.datasets.ImageFolder(valid_path, transform=torchvision.transforms.ToTensor())\ntest_data = torchvision.datasets.ImageFolder(test_path, transform=torchvision.transforms.ToTensor())\nAs in previous labs, we can iterate through the one training data point at a time like this:\nfor x, t in train_data:\n    print(x, t)\n    plt.imshow(x.transpose(2, 0).transpose(0, 1).numpy()) # display an image\n    break # uncomment if you'd like\nTask: What do the variables x and t contain? What is the shape of our images? What are our labels? Based on what you learned in Part (a), how were the labels generated from the folder structure?\n# Your explanation goes here\nWe saw in the earlier tutorials that PyTorch has a utility to help us creat minibatches with our data. We can use the same DataLoader helper here:\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)\n\nfor x, t in train_loader:\n    print(x, t)\n    break # uncomment if you'd like\nTask: What do the variables x and t contain? What are their shapes? What data do they contain?\n# Your explanation goes here\nTask: How many images are there in the training, validation, and test sets?\n# Your explanation goes here\nNotice that there are fewer images in the training set, compared to the validation and test sets. This is so that we can explore the effect of having a limited training set.\nGraded Task: The data set is generated by students taking pictures of their hand while making the corresponding gestures. We therefor split the training, validation, and test sets were split so that images generated by a student all belongs in a single data set. In other words, we avoid cases where some students’ images are in the training set and others end up in the test set. Why do you think this important for obtaining a representative test accuracy?\n# Your explanation goes here"
  },
  {
    "objectID": "labs/lab07.html#part-2.-training-a-cnn-model",
    "href": "labs/lab07.html#part-2.-training-a-cnn-model",
    "title": "CSC413 Lab 9: Transfer Learning and Descent",
    "section": "",
    "text": "For this part, we will be working with this CNN network.\nclass CNN(nn.Module):\n    def __init__(self, width=4, bn=True):\n        \"\"\"\n        A 4-layer convolutional neural network. The first layer has\n        `width` number of channels, and with each layer we half the\n        feature width/height and double the number of channels.\n\n        If `bn` is set to False, then batch normalization will not run.\n        \"\"\"\n        super(CNN, self).__init__()\n        self.width = width\n        self.bn = bn\n        # define all the conv layers\n        self.conv1 = nn.Conv2d(in_channels=3,\n                               out_channels=self.width,\n                               kernel_size=3,\n                               padding=1)\n        self.conv2 = nn.Conv2d(in_channels=self.width,\n                               out_channels=self.width*2,\n                               kernel_size=3,\n                               padding=1)\n        self.conv3 = nn.Conv2d(in_channels=self.width*2,\n                               out_channels=self.width*4,\n                               kernel_size=3,\n                               padding=1)\n        self.conv4 = nn.Conv2d(in_channels=self.width*4,\n                               out_channels=self.width*8,\n                               kernel_size=3,\n                               padding=1)\n        # define all the BN layers\n        if bn:\n            self.bn1 = nn.BatchNorm2d(self.width)\n            self.bn2 = nn.BatchNorm2d(self.width*2)\n            self.bn3 = nn.BatchNorm2d(self.width*4)\n            self.bn4 = nn.BatchNorm2d(self.width*8)\n        # pooling layer has no parameter, so one such layer\n        # can be shared across all conv layers\n        self.pool = nn.MaxPool2d(2, 2)\n        # FC layers\n        self.fc1 = nn.Linear(self.width * 8 * 14 * 14, 100)\n        self.fc2 = nn.Linear(100, 9)\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        if self.bn:\n            x = self.bn1(x)\n        x = self.pool(torch.relu(self.conv2(x)))\n        if self.bn:\n            x = self.bn2(x)\n        x = self.pool(torch.relu(self.conv3(x)))\n        if self.bn:\n            x = self.bn3(x)\n        x = self.pool(torch.relu(self.conv4(x)))\n        if self.bn:\n            x = self.bn4(x)\n        x = x.view(-1, self.width * 8 * 14 * 14)\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\nTask:\nThe training code is written for you. Train the CNN() model for at least 6 epochs, and report on the maximum validation accuracy that you can attain.\nAs your model is training, you might want to move on to the next question.\ndef get_accuracy(model, data, device=\"cpu\"):\n    loader = torch.utils.data.DataLoader(data, batch_size=256)\n    model.to(device)\n    model.eval() # annotate model for evaluation (important for batch normalization)\n    correct = 0\n    total = 0\n    for imgs, labels in loader:\n        labels = labels.to(device)\n        output = model(imgs.to(device))\n        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(labels.view_as(pred)).sum().item()\n        total += imgs.shape[0]\n    return correct / total\n\ndef train_model(model,\n                train_data,\n                valid_data,\n                batch_size=64,\n                weight_decay=0.0,\n                learning_rate=0.001,\n                num_epochs=50,\n                plot_every=20,\n                plot=True,\n                device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True)\n    model = model.to(device) # move model to GPU if applicable\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(),\n                           lr=learning_rate,\n                           weight_decay=weight_decay)\n    # for plotting\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for epoch in range(num_epochs):\n            for imgs, labels in iter(train_loader):\n                if imgs.size()[0] &lt; batch_size:\n                    continue\n                labels = labels.to(device)\n                imgs = imgs.to(device)\n                model.train()\n                out = model(imgs)\n                loss = criterion(out, labels)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    loss = float(loss)\n                    tacc = get_accuracy(model, train_data, device)\n                    vacc = get_accuracy(model, valid_data, device)\n                    print(\"Iter %d; Loss %f; Train Acc %.3f; Val Acc %.3f\" % (iter_count, loss, tacc, vacc))\n\n                    iters.append(iter_count)\n                    train_loss.append(loss)\n                    train_acc.append(tacc)\n                    val_acc.append(vacc)\n    finally:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend([\"Train\", \"Validation\"])\nTask: Run the training code below. What validation accuracy can be achieved by this CNN?\ncnn = CNN(width=4)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)"
  },
  {
    "objectID": "labs/lab07.html#part-2.-model-architecture-biasvariance-and-double-descent",
    "href": "labs/lab07.html#part-2.-model-architecture-biasvariance-and-double-descent",
    "title": "CSC413 Lab 9: Transfer Learning and Descent",
    "section": "",
    "text": "In this section, we will explore the effect of various aspects of a CNN model architecture. We will pay particluar attention to architecture decisions that affect the bias and variance of the model. Finally, we explore a phenomenon called double descent.\nTo begin, let’s explore the effect of batch normalization.\nTask: Run the training code below to explore the effect of training without batch normalization.\ncnn = CNN(bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\nGraded Task: Compare the two sets of training curves above for the CNN model with and without batch normalization. What is the effect of batch normalization on the training loss and accuracy? What about the validation accuracy?\n# TODO: Include your analysis here\n# SOLUTION - training loss/acc improves a lot more quickly with BN\n# SOLUTION   but final val loss is about the same for this task\nGraded Task: We used the layer called BatchNorm2d in our CNN. What do you think is the difference between BatchNorm2d and BatchNorm1d? Why are we using BatchNorm2d in our CNN? Why would we use BatchNorm1d in an MLP? You may wish to consult the PyTorch documentation. (How can you find it?)\n# Explain your answer here\nTask: Run the training code below to explore the effect of varying the model width for this particular data set.\ncnn = CNN(width=2, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\ncnn = CNN(width=4, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\ncnn = CNN(width=16, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\nGraded Task: What is the effect of varying the model width above for this particular data set? Do you notice an effect on the training loss? What about the training/validation accuracy? The final validation accuracy? (Your answer may or may not match your expectations. Please answer based on the actual results above.)\n# TODO: Include your analysis here\nTask: Run the training code below to explore the effect of weight decay when training a large model.\ncnn = CNN(width=16, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)\ncnn = CNN(width=16, bn=True) # try with batch norm on\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)\ncnn = CNN(width=16, bn=True) # try decreasing weight decay parameter\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.0001)\nGraded Task: What is the effect of setting weight decay to the above value? Do you notice an effect on the training loss? What about the training/validation accuracy? The final validation accuracy? (Again, your answer may or may not match your expectations. Please answer based on the actual results above.)\n# TODO: Include your analysis here\nTask: Note that there is quite a bit of noise in the results that we might obtain above. That is, if you run the same code twice, you may obtain different answers. Why might that be? What are two sources of noise/randomness?\n# TODO: Include your explanation here\n# SOLUTION: SGD and initalization of parameters\nThese settings that we have been exporting are hyperparameters that should be tuned when you train a neural network. These hyperparameters interact with one another, and thus we should tune them using the grid search strategy mentioned in previous labs.\nYou are not required to perform grid search for this lab, so that we can explore a few other phenomena.\nOne interesting phenomenon is called double descent. In statistical learning theory, we expect validation error to decrease with increase model capacity, and then increase as the model overfits to the number of data points available for training. In practise, in neural networks, we often see that as model capacity increases, validation error first decreases, then increase, and then decrease again—hence the name “double descent”.\nIn fact, the increase in validation error is actually quite subtle. However, what is readily apparent is that in most cases, increasing model capacity does not result in a decrease in validation accuracy.\nOptional Task: To illustrate that validation accuracy is unlikely to decrease with increased model parameter, train the below network.\n# Uncomment to run. \n# cnn = CNN(width=40, bn=True)\n# train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=50)\nDouble descent is actually not that mysterious. It comes from the fact that when capacity is large enough there are many parameter choices that achieves 100% training accuracy, the neural network optimization procedure is effectively choosing a best parameters out of the many that can achieve this perfect training accuracy. This differs from when capacity is low, where the optimization process needs to find a set of parameter choices that best fits the training data—since no choice of parameters fits the training data perfectly. When the capacity is just large enough to be able to find parameters that fit the data, but too small for there be a range of parameter choices available to be able to select a “best” one.\nThis twitter thread written by biostatistics professor Daniela Witten also provides an intuitive explanation, using polynomial curve fitting as an example: https://twitter.com/daniela_witten/status/1292293102103748609\nDouble descent explored in depth in this paper here: https://openreview.net/pdf?id=B1g5sA4twr This paper highlights that the increase in validation/test error occurs when the training accuracy approximates 100%. Moreover, the double descent phenomena is noticable when varying model capacity (e.g. number of parameters) and when varying the number of iterations/epochs of training.\nWe will attempt to explore the latter effect—i.e. we will train a large model, use a small numer of training data points, and explore how each iteration of training impacts validation accuracy. The effect is subtle and, depending on your neural network initialization, you may not see an effect. So, a training curve is also provided for you to analyze.\nOptional Task: Run the code below to try and reproduce the “double descent” phenomena. This code will take a while to run, so you may wish to continue with the remaining questions while it runs.\n# use a subset of the training data\n# uncomment to train\n\n# train_data_subset, _ =  random_split(train_data, [50, len(train_data)-50])\n# cnn = CNN(width=20)\n# train_model(cnn,\n#             train_data_subset,\n#             valid_data,\n#             batch_size=50, # set batch_size=len(train_data_subset) to minimize training noise\n#             num_epochs=200,\n#             plot_every=1,  # plot every epoch (this is slow)\n#             learning_rate=0.0001)  # choose a low learning rate\nFor reference, here is the our training curve showing the loss and accuracy over 200 iterations:\n \nWe are not able to consistently reproduce this result (e.g., due to initialization), so it is totally reasonable for your figure to look different!\nTask: In the provided training curve, during which iterations do the validation accuracy initially increase (i.e. validation error decrease)?\n# TODO: Include your answer here\nGraded Task: In the provided training curve, during which iterations do the validation accuracy decrease slightly? Approximately what training accuracy is achieved at this piont?\n# TODO: Include your answer here\nTask: In the provided training curve, during which iterations do the validation accuracy increase for a second time (i.e. validation error descends for a second time)?\n# TODO: Include your answer here"
  },
  {
    "objectID": "labs/lab07.html#part-3.-transfer-learning",
    "href": "labs/lab07.html#part-3.-transfer-learning",
    "title": "CSC413 Lab 9: Transfer Learning and Descent",
    "section": "",
    "text": "For many image classification tasks, it is generally not a good idea to train a very large deep neural network model from scratch due to the enormous compute requirements and lack of sufficient amounts of training data.\nA better option is to try using an existing model that performs a similar task to the one you need to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed Transfer Learning. In this assignment, we will use Transfer Learning to extract features from the hand gesture images. Then, train a smaller network to use these features as input and classify the hand gestures.\nAs you have learned from the CNN lecture, convolution layers extract various features from the images which get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal role in establishing Deep Neural Nets as a go-to tool for image classification problems and we will use an ImageNet pre-trained AlexNet model to extract features in this assignment.\nHere is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch will download the pretrained weights from the internet.\nimport torchvision.models\nalexnet = torchvision.models.alexnet(pretrained=True)\n\nprint(alexnet)\nAs you can see, the alexnet model is split up into two components: alexnet.features and alexnet.classifier. The first neural network component, alexnet.features, is used to computed convolutional features, which is taken as input in alexnet.classifier.\nThe neural network alexnet.features expects an image tensor of shape Nx3x224x224 as inputs and it will output a tensor of shape Nx256x6x6 . (N = batch size).\nHere is an example code snippet showing how you can compute the AlexNet features for some images (your actual code might be different):\nimg, label = train_data[0]\nfeatures = alexnet.features(img.unsqueeze(0)).detach()\n\nprint(features.shape)\nNote that the .detach() at the end will be necessary in your code. The reason is that PyTorch automatically builds computation graphs to be able to backpropagate graidents. If we did not explicitly “detach” this tensor from the AlexNet portion of the computation graph, PyTorch might try to backpropagate gradients to the AlexNet weight and tune the AlexNet weights.\nTODO Compute the AlexNet features for each of your training, validation, and test data by completing the function compute_features. The code below creates three new arrays called train_data_fets, valid_data_fets and test_data_fets. Each of these arrays contains tuples of the form (alexnet_features, label).\ndef compute_features(data):\n    fets = []\n    for img, t in data:\n        features = None  # TODO\n        features = alexnet.features(img.unsqueeze(0)).detach() # SOLUTION\n        fets.append((features, t),)\n    return fets\n\ntrain_data_fets = compute_features(train_data)\nvalid_data_fets = compute_features(valid_data)\ntest_data_fets = compute_features(test_data)\nIn the rest of this part of the lab, we will test two models that will take as input these AlexNet features, and make a prediction for which letter the hand gesture represents. The two models are a linear model, a two-layer MLP. We will compare the performance of these two models.\nGraded Task: Complete the definition of the LinearModel class, which is a linear model (e.g., logistic regression). This model should as input these AlexNet features, and make a prediction for which letter the hand gesture represents.\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super(LinearModel, self).__init__()\n        # TODO: What layer need to be initialized?\n        self.fc = nn.Linear(256 * 6 * 6, 10) # SOLUTION\n\n    def forward(self, x):\n        x = x.view(-1, 256 * 6 * 6) # flatten the input\n        z = None # TODO: What computation needs to be performed?\n        z = self.fc(x) # SOLUTION\n        return z\n\nm_linear = LinearModel()\nm_linear(train_data_fets[0][0]) # this should produce a(n unnormalized) prediction\nTask: Train a LinearModel() for at least 6 epochs, and report on the maximum validation accuracy that you can attain. We should still be able to use the train_model function, but make sure to provide the AlexNet features as input (and not the image features).\nm_linear = LinearModel()\n# TODO: Train the linear model. Include your output in your submission\ntrain_model(m_linear, train_data_fets, valid_data_fets) # SOLUTION\nGraded Task: Compare this model with the CNN() models that we trained earlier. How does this model perform in terms of validation accuracy? What about in terms of the time it took to train this model?\n# TODO: Your observation goes here\nTask: We decide to use AlexNet features as input to our MLP, and avoided tuning AlexNet weights. However, we could have considered AlexNet to be a part of our model, and continue to tune AlexNet weights to improve our model performance. What are the advantages and disadvantages of continuing to tune AlexNet weights?\n# TODO"
  },
  {
    "objectID": "labs/lab07.html#part-4.-data",
    "href": "labs/lab07.html#part-4.-data",
    "title": "CSC413 Lab 9: Transfer Learning and Descent",
    "section": "",
    "text": "Task: Report the test accuracy on this transfer learning model.\n# TODO\nget_accuracy(m_linear, test_data_fets) # SOLUTION\nTask: Use this code below to construct the confusion matrix for this model over the test set.\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nimport sklearn\nlabel = \"ABCDEFGHI\"\ndef plot_confusion(model, data):\n    n = 0\n    ts = []\n    ys = []\n    for x, t in data:\n        z = model(x.unsqueeze(0))\n        y = int(torch.argmax(z))\n        ts.append(t)\n        ys.append(y)\n\n    cm = confusion_matrix(ts, ys)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label)\n    plt.figure()\n    disp.plot()\n\nplot_confusion(m_linear, test_data_fets)\nGraded Task: Which class is most likely mistaken as another? Is this reasonable? (i.e. is that class particularly challenging, or very similar to another class?)\n# TODO: Include your analysis here\nTask: In order to understand where errors come from, it is crucial that we explore why and how our models fail. A first step is to visually inspect the test data points where failure occurs. That way, we can identify what we can do to prevent/fix errors before our models are deployed.\nRun the below code to display images in the test set that our model misclassifies:\nfor i, (x, t) in enumerate(test_data_fets):\n    y = int(torch.argmax(m_linear(x)))\n    if not (y == t):\n        plt.figure()\n        plt.imshow(test_data[i][0].transpose(0,1).transpose(1,2).numpy())\nTask: By visually inspecting these misclassified images, we see that there are two main reasons for misclassification. What reason for misclassification is due to a mistake in the formatting of the test set images?\n# TODO\nGraded Task: We also see a much more serious issue, where gestures made by individuals with darker skin tones may be more frequently misclasified. This result suggests that errors in the model may impact some groups more than others. What steps should we take to mitigate this issue?\n# TODO"
  },
  {
    "objectID": "labs/lab09.html",
    "href": "labs/lab09.html",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "",
    "text": "We have seen that convolutional neural networks (CNN) are successful in many computer vision tasks, including classification, object detection and others. However, it is not immediately clear how CNNs work, and how one can explain the predictions made by CNNs. A deeper understanding of how CNNs work can also help us identify reasons why CNNs may fail to produce correct predictions for some samples.\nA line of work started to visualize and interpret computed features of convolutional neural networks. CAM and Grad-CAM are two influential and fundamental works to find which parts of the input have the most impact on the final output of the models by analyzing the model’s extracted feature maps.\nBy the end of this lab, you will be able to:\n\nExplain how visualizing the regions of an image that contribute to the CNN model’s prediction can help explain how the model works.\nImplement CAM on a convolutional neural network with GAP + a single fc layer.\nExplain the limitations of CAM, and how Grad-CAM overcomes this limitation.\nImplement Grad-CAM on a convolutional neural network.\n\nAcknowledgements: 2. We have borrowed some codes from CAM Official Repo. 3. We have borrowd texts, figures and formulas from main papers of CAM and Grad-CAM.\n\n\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab09.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your implementation of the predict function (1 point)\nPart 1. Your implementation of the get_resnet_features function (1 point)\nPart 1. Your implementation of the compute_cam function (3 point)\nPart 1. Your interpretation of the grad cam outputs (2 point)\nPart 2. Your implementation of the compute_gradcam function (3 point)\n\n\n\n\nCAM and its extension Grad-CAM takes the approach of identifying the regions of the an image that contributes most to the model’s prediction. This information can be visualized as a heat map, and provides a way to interpret a model’s prediction: did the model predict that the image is that of a “boat” because of the shape of the ears, or because of the water in the background?\nWe discussed, during lecture, that convolutional layers preserve the geometry of the image, and that these convolutional layers actually behave as feature/ object detectors of various complexity. Since the geometry of the output of a CNN layer corresponds to the geometry of input image, it is straightforward to locate the region of the image that corresponds to a particularly high activation value. This is because the computations that we use in a CNN (convolutions, max pooling, activations) are all geometry preserving (equivariant).\nHowever, fully-connected layers are typically used for classification in the final layers of a CNN. These fully-connected layers are not geometry preserving, thus information about the locations of discriminating features are lost when fully-connected layers are used for classification.\nThe idea behind CAM is to avoid using these fully-connected layers for classification, so that we can reconstruct location information in a straightforward way. Instead of fully-connected layers, we use:\n\nA global average pooling (GAP) layer. This layer will take as input the output of a CNN layer (e.g., of shape H x W x C) and perform an average operation for each channel along the entire activation height/width (producing an output vector of shape C).\nA single linear layer to map this vector (of length C) into the output space.\n\nSince both the pooling and linear layers have straightforward computation, it is possible to assign credit for a output score for a class back to specific activation values of the CNN output.\nThe framework of the Class Activation Mapping is as below (from https://github.com/zhoubolei/CAM):\n\nIn this part of the lab, we will implement CAM to produce a heatmap of the contribution to locations in the image to a predicted class. We will use the pre-trained convolutional neural network ResNet, chosen because this model’s architecture uses global average pooling (GAP). ResNet is trained on the ImageNet data set.\nfrom torchvision import models, transforms\nimport torch.nn.functional as F\nimport torch\nimport json\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Download resnet pretrained weights\nresnet = models.resnet18(pretrained=True)\n\n# CAM can only be used when the models are in \"evaluation phase\".\nresnet.eval()\n\n# Print model architecture\n# We will use the CNN activations computed after layer 4, and before GAP.\nresnet\nThe ImageNet labels are a bit challenging to read. We will download a list of human-readable labels from here:\n!wget https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\n# Load the imagenet category list\nwith open('imagenet-simple-labels.json') as f:\n    classes = json.load(f)\nTo remind ourselves of how ResNet works, let’s predict what class these images belongs to:\n \n!wget https://www.cs.toronto.edu/~lczhang/413/cat.jpg\n!wget https://www.cs.toronto.edu/~lczhang/413/boat.jpg\nfrom PIL import Image\n\ndef process_input(image_file):\n    # open the image\n    img = Image.open(image_file)\n\n    # transform the images by resizing and normalizing\n    preprocess = transforms.Compose([\n       transforms.Resize((224,224)),\n       transforms.ToTensor(),\n       transforms.Normalize(\n          mean=[0.485, 0.456, 0.406],\n          std=[0.229, 0.224, 0.225])])\n\n    return preprocess(img).unsqueeze(0)\nGraded Task: Write a function that takes a model and an image file and produces a list of the top 5 predictions with the corresponding probability score.\ndef predict(model, image_file):\n    \"\"\"\n    Return the top 5 class predictions with the corresponding probability score.\n\n    Parameters:\n        `model`      - nn.Module\n        `image_file` - file path to the image\n\n    Returns: a list of 5 (string, int, float) pairs: \n        the string is the predicted ResNet class name (see classes above),\n        the int is the predicted ResNet class id,\n        and the float is the prediction probability. The list should be ordered\n        so that the highest probabilty score appears first.\n\n    Example:\n        &gt;&gt;&gt; predict(resnet, 'cat.jpg')\n        [('prison', 743, 0.23517875373363495),\n         ('shopping cart', 791, 0.07393667101860046),\n         ('rocking chair', 765, 0.06884343922138214),\n         ('wheelbarrow', 428, 0.06603048741817474),\n         ('ring-tailed lemur', 383, 0.0434008426964283)]\n    \"\"\"\n    x = process_input(image_file)\n\n    result = None # TODO\n\n    scores = model(x) # SOLUTION\n    y = [float(s) for s in torch.softmax(scores, axis=1)[0]] # SOLUTION\n    result = sorted(zip(classes, range(1000),  y), key=lambda x: x[2], reverse=True)[:5] # SOLUTION\n    return result\nPlease include the output of the below cell in your submission.\npredict(resnet, 'cat.jpg')\nNow that we can use ResNet to make predictions, we need two additional pieces of information for CAM.\nFirst, given an image, we need to be able to compute the features/activations of the last convolutional layer. This feature map is the input to the GAP layer. Although this information is computed in a forward pass, we will need to write some code to extract this information.\nSecond, we will need the weights of the final fully-connected layer in ResNet.\nGraded Task: Complete the following function that takes an image file and produces the weights of the finally fully-connected layer in Resnet. You may find the named_children() method of resnet helpful, as it produces a sequence of (named) layers. We would like the feature map directly before the global average pooling layer.\nfor (name, model) in resnet.named_children():\n    print(name)\ndef get_resnet_features(image_file):\n    \"\"\"\n    Return the final CNN layer (layer4) feature map in resnet\n\n    Parameters:\n        `image_file` - file path to the image\n\n    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n    \"\"\"\n\n    x = process_input(image_file)\n\n    result = None # TODO\n    result = x # SOLUTION\n    for (name, model) in resnet.named_children():\n        # TODO -- update result\n        result = model.forward(result) # SOLUTION\n        if name == 'layer4':\n            break\n    return result\nfets = get_resnet_features('cat.jpg')\nprint(fets.shape) # should be [1, 512, 7, 7]\nTask: Assign the variable fc_weight to the weights of the final fully-connected layer in resnet.\nweights = None # TODO\nweights = resnet.fc.weight # SOLUTIONS\nprint(weights.shape) # should be [1000, 512]\nGraded Task: Complete the function compute_cam, which takes the CNN feature map (from the get_resnet_features function), the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the following approach.\nWe will use the notation \\({\\bf X}\\) to denote the CNN feature map (the input to the GAP), with \\(X_{i,j,c}\\) being the activation at location \\((i, j)\\) and channel \\(c\\). Here, \\({\\bf X}\\) is a tensor with shape \\(H \\times W \\times C\\), where \\(H \\times W\\) is the height and width of the feature map and \\(C\\) is the number of channels. We will use the vector \\({\\bf h}\\) to denote the output of the GAP, so that \\(h_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{i,j,c}\\). Finally, we will use \\({\\bf W}\\) to denote the finally fully connected layer weights, and \\({\\bf z}\\) to the denote the prediction score, so that \\({\\bf z} = {\\bf W}{\\bf h}\\).\nNow, we would like to relate the features \\(X_{i,j,c}\\) to the scores \\(z_k\\), so that we can compute the contribution of the features at position \\((i,j)\\) to the score \\(k\\).\nFor an output class \\(k\\), we have:\n\\[z_k  = \\sum_{c=1}^{C} w_{k,c} h_c\\]\nSubstituing \\(h_c\\) for its definition, we have:\n\\[z_k  = \\sum_{c=1}^{C} w_{k,c} \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W X_{i,j,c}\\]\nRearranging the sums, we have:\n\\[z_k  = \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W \\sum_{c=1}^C w_{k,c} X_{i,j,c}\\]\nThe inner term \\(\\sum_c w_{k,c} X_{i,j,c}\\) is exactly what we are looking for: this term indicates how much the value in location \\((i, j)\\) of the feature map \\({\\bf X}\\) attributes to class \\(k\\).\ndef compute_cam(features, label):\n    \"\"\"\n    Computes the contribution of each location in `features` towards \n    `label` using CAM.\n\n    Parameters:\n        `features`: PyTorch Tensor of shape [1, 512, 7, 7] representing\n                    final layer feature map in ResNet (e.g., from calling\n                    `get_resnet_features`)\n        `label`   : resnet label, integer between 0-999\n\n    Returns: PyTorch Tensor of shape [7, 7]\n    \"\"\"\n    features = features.squeeze(0) # remove the first dimension\n    result = None # TODO \n    weight = resnet.fc.weight[label]   # SOLUTIONS\n    result = torch.mul(weight, features.permute((1, 2, 0))).sum(dim=2)  # SOLUTIONS\n    return result\nTask: Run the below code, which superimposes the result of the compute_cam operation on the image.\ndef visualize_cam(image_file, label):\n    # open the image\n    img = Image.open(image_file)\n\n    # compute CAM features\n    fets = get_resnet_features('cat.jpg')\n    m = compute_cam(fets, label)\n\n    # normalize \"m\"\n    m = m - m.min()\n    m = m / m.max()\n    # convert \"m\" into pixel intensities\n    m = np.uint8(255 * m.detach().numpy())\n    # apply a color map\n    m = cv2.resize(m, img.size)\n    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n\n    plt.figure(figsize=(6, 6))\n    plt.title(\"%s %s\" % (image_file, classes[label]))\n    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n    plt.show()\nvisualize_cam('cat.jpg', 743)\nvisualize_cam('cat.jpg', 383)\nGraded Task Compare the above two outputs, and explain what conclusion you may be able to draw about the contribution of the pixel locations to those two classes. Why do you think the model misclassified the image?\n# TODO: your explanation goes here\n\n\n\nAlthough CAM was an important step toward understanding convolutional neural networks, the technique is only applicable to convolutional networks with GAP and a single fully-connected layer. Recall that it leveraged the following relationship between the output of the GAP layer \\({\\bf h}\\) and the score for the output class \\(z_k\\):\n\\[z_k  = \\sum_c w_{k,c} h_c\\]\nWhere \\(w_{k,c}\\) is the fully connected layer weight that describes the strength of the connection between \\(h_c\\) and \\(z_k\\). In other words, \\(w_{k,c}\\) describes the following gradient:\n\\[\\frac{\\partial z_k}{\\partial h_c}\\]\nWith this in mind, you may be able to see how CAM may be generalized so that \\({\\bf z}\\) may be a more complex function of \\({\\bf h}\\)—e.g., a MLP or even an RNN!\nGradient-weighted Class Activation Mapping (Grad-CAM) is a generalized form of CAM, and can be applied to any convolutional neural network. In Grad-CAM, we use the gradient \\(\\frac{\\partial z_k}{\\partial h_c}\\) in place of \\(w_{k,c}\\) when attributing class scores to locations \\((i, j)\\). In other words, the below term indicates how much the value in location \\((i, j)\\) of the feature map \\({\\bf X}\\) attributes to class \\(k\\).\n\\[ReLU(\\sum_c \\frac{\\partial z_k}{\\partial h_c} X_{i,j,c})\\]\nThe addition of the ReLU activation only allows positive contributions to be visualized.\nSidenote: To generalize this result even further, we can replace \\(z_k\\) with any target we would like! Grad-CAM has been used on neural networks that performs image caption generation: a model with a CNN encoder and an RNN decoder. We can use use the gradients of any target concept (say “dog” in a classification network or a sequence of words in a captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Taking a look at this video helps you to understand the power of Grad-CAM.\nLet’s explore GradCAM with the VGG network:\nvgg19 = models.vgg19(pretrained=True)\nvgg19.eval()\nvgg19\npredict(vgg19, 'cat.jpg')\nTask Just like with CAM, we will need to extract the feature map obtained from the last convolutional layer. This step is actually very straightforward with VGG since vgg19 splits the network into a features network and a classifier network.\ndef get_vgg_features(image_file):\n    \"\"\"\n    Return the output of `vgg19.features` network for the image\n\n    Parameters:\n        `image_file` - file path to the image\n\n    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n    \"\"\"\n\n    x = process_input(image_file)\n    result = None # TODO\n    result = vgg19.features(x) # SOLUTIONS\n    return result\nget_vgg_features('cat.jpg').shape\nTask: Read the forward method of the VGG model here. https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py What other steps are remaining in the forward pass?\n# TODO: Explain the remaining steps here\nGraded Task: Complete the function compute_gradcam, which takes an image file path, the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the GradCAM approach described at the beginning of Part 2.\ndef compute_gradcam(image_file, label):\n    \"\"\"\n    Computes the contribution of each location in `features` towards \n    `label` using GradCAM.\n\n    Parameters:\n        `image_file` - file path to the image\n        `label`   : resnet label, integer between 0-999\n\n    Returns: PyTorch Tensor of shape [7, 7]\n    \"\"\"\n    # obtain the image input features\n    x = process_input(image_file)\n\n    # obtain the output of the features network in the CNN\n    fets = vgg19.features(x)\n\n    # tell PyTorch to compute the gradients with respect\n    # to \"fets\"\n    fets.retain_grad()\n\n    # TODO: compute the rest of the vgg19 forward pass from `fets`\n    out = None # should be the output of the classifier network\n    out = vgg19.avgpool(fets)     # SOLUTION\n    out = torch.flatten(out, 1)  # SOLUTION\n    out = vgg19.classifier(out)  # SOLUTION\n\n    z_k = out.squeeze(0)[label] # identify the target output class\n    z_k.backward()              # backpropagation to compute gradients\n\n    features_grad = fets.grad   # identify the gradient of z_k with respect to fets\n\n    # account for the pooling operation, so that \"pooled_grad\"\n    # aligns with the notation used\n    n, c, h, w = features_grad.shape\n    features_grad = torch.reshape(features_grad, (c, h*w))\n    pooled_grad = features_grad.sum(dim=1)\n\n    # rearrange \"fets\" so that \"X\" aligns with the notation\n    # used above\n    X = fets.squeeze(0).permute((1, 2, 0))\n\n    # TODO: Compute the heatmap using the gradcam\n    m = None\n    m = torch.matmul(X, pooled_grad) # SOLUTION\n    m = F.relu(m) # apply the ReLU operation\n    return m\nTask: Run the below code, which superimposes the result of the compute_gradcam operation on the image.\ndef visualize_gradcam(image_file, label):\n    # open the image\n    img = Image.open(image_file)\n\n    # compute CAM features\n    m = compute_gradcam(image_file, label)\n\n    # normalize \"m\"\n    m = m - m.min()\n    m = m / m.max()\n    # convert \"m\" into pixel intensities\n    m = np.uint8(255 * m.detach().numpy())\n    # apply a color map\n    m = cv2.resize(m, img.size)\n    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n\n    plt.figure(figsize=(6, 6))\n    plt.title(\"%s %s\" % (image_file, classes[label]))\n    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n    plt.show()\nvisualize_cam('cat.jpg', 743)\nvisualize_cam('cat.jpg', 383)\nvisualize_cam('boat.jpg', 536)\n\n\n\nAs you might have seen in the video, Grad-CAM can be applied to text-generating models. For example, in image-captioning tasks, a text is generated describing the given image. Some methods first feed the image to a convolutional neural network to extract features, and then feed the extracted features to an RNN, to generate the text. Neuraltalk2 was one of the earliest models using this approach. Similar to the classification task, it is enough to compute the gradient of the score (what is the score in an image-captioning task?) with respect to the last convolutional layer.\nIf you are interested in how neuraltalk2 functions you can check this project. Moreover, if you are looking for more hands-on experience, this repo has implemented many image-captioning methods, and you can easily apply Grad-CAM on them (especially show and tell).\nHint: There is a file which re-implements the forward pipeline of ResNet101, where you can store the features."
  },
  {
    "objectID": "labs/lab09.html#submission",
    "href": "labs/lab09.html#submission",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "",
    "text": "If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab09.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your implementation of the predict function (1 point)\nPart 1. Your implementation of the get_resnet_features function (1 point)\nPart 1. Your implementation of the compute_cam function (3 point)\nPart 1. Your interpretation of the grad cam outputs (2 point)\nPart 2. Your implementation of the compute_gradcam function (3 point)"
  },
  {
    "objectID": "labs/lab09.html#part-1.-class-activation-maps-cam",
    "href": "labs/lab09.html#part-1.-class-activation-maps-cam",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "",
    "text": "CAM and its extension Grad-CAM takes the approach of identifying the regions of the an image that contributes most to the model’s prediction. This information can be visualized as a heat map, and provides a way to interpret a model’s prediction: did the model predict that the image is that of a “boat” because of the shape of the ears, or because of the water in the background?\nWe discussed, during lecture, that convolutional layers preserve the geometry of the image, and that these convolutional layers actually behave as feature/ object detectors of various complexity. Since the geometry of the output of a CNN layer corresponds to the geometry of input image, it is straightforward to locate the region of the image that corresponds to a particularly high activation value. This is because the computations that we use in a CNN (convolutions, max pooling, activations) are all geometry preserving (equivariant).\nHowever, fully-connected layers are typically used for classification in the final layers of a CNN. These fully-connected layers are not geometry preserving, thus information about the locations of discriminating features are lost when fully-connected layers are used for classification.\nThe idea behind CAM is to avoid using these fully-connected layers for classification, so that we can reconstruct location information in a straightforward way. Instead of fully-connected layers, we use:\n\nA global average pooling (GAP) layer. This layer will take as input the output of a CNN layer (e.g., of shape H x W x C) and perform an average operation for each channel along the entire activation height/width (producing an output vector of shape C).\nA single linear layer to map this vector (of length C) into the output space.\n\nSince both the pooling and linear layers have straightforward computation, it is possible to assign credit for a output score for a class back to specific activation values of the CNN output.\nThe framework of the Class Activation Mapping is as below (from https://github.com/zhoubolei/CAM):\n\nIn this part of the lab, we will implement CAM to produce a heatmap of the contribution to locations in the image to a predicted class. We will use the pre-trained convolutional neural network ResNet, chosen because this model’s architecture uses global average pooling (GAP). ResNet is trained on the ImageNet data set.\nfrom torchvision import models, transforms\nimport torch.nn.functional as F\nimport torch\nimport json\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Download resnet pretrained weights\nresnet = models.resnet18(pretrained=True)\n\n# CAM can only be used when the models are in \"evaluation phase\".\nresnet.eval()\n\n# Print model architecture\n# We will use the CNN activations computed after layer 4, and before GAP.\nresnet\nThe ImageNet labels are a bit challenging to read. We will download a list of human-readable labels from here:\n!wget https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\n# Load the imagenet category list\nwith open('imagenet-simple-labels.json') as f:\n    classes = json.load(f)\nTo remind ourselves of how ResNet works, let’s predict what class these images belongs to:\n \n!wget https://www.cs.toronto.edu/~lczhang/413/cat.jpg\n!wget https://www.cs.toronto.edu/~lczhang/413/boat.jpg\nfrom PIL import Image\n\ndef process_input(image_file):\n    # open the image\n    img = Image.open(image_file)\n\n    # transform the images by resizing and normalizing\n    preprocess = transforms.Compose([\n       transforms.Resize((224,224)),\n       transforms.ToTensor(),\n       transforms.Normalize(\n          mean=[0.485, 0.456, 0.406],\n          std=[0.229, 0.224, 0.225])])\n\n    return preprocess(img).unsqueeze(0)\nGraded Task: Write a function that takes a model and an image file and produces a list of the top 5 predictions with the corresponding probability score.\ndef predict(model, image_file):\n    \"\"\"\n    Return the top 5 class predictions with the corresponding probability score.\n\n    Parameters:\n        `model`      - nn.Module\n        `image_file` - file path to the image\n\n    Returns: a list of 5 (string, int, float) pairs: \n        the string is the predicted ResNet class name (see classes above),\n        the int is the predicted ResNet class id,\n        and the float is the prediction probability. The list should be ordered\n        so that the highest probabilty score appears first.\n\n    Example:\n        &gt;&gt;&gt; predict(resnet, 'cat.jpg')\n        [('prison', 743, 0.23517875373363495),\n         ('shopping cart', 791, 0.07393667101860046),\n         ('rocking chair', 765, 0.06884343922138214),\n         ('wheelbarrow', 428, 0.06603048741817474),\n         ('ring-tailed lemur', 383, 0.0434008426964283)]\n    \"\"\"\n    x = process_input(image_file)\n\n    result = None # TODO\n\n    scores = model(x) # SOLUTION\n    y = [float(s) for s in torch.softmax(scores, axis=1)[0]] # SOLUTION\n    result = sorted(zip(classes, range(1000),  y), key=lambda x: x[2], reverse=True)[:5] # SOLUTION\n    return result\nPlease include the output of the below cell in your submission.\npredict(resnet, 'cat.jpg')\nNow that we can use ResNet to make predictions, we need two additional pieces of information for CAM.\nFirst, given an image, we need to be able to compute the features/activations of the last convolutional layer. This feature map is the input to the GAP layer. Although this information is computed in a forward pass, we will need to write some code to extract this information.\nSecond, we will need the weights of the final fully-connected layer in ResNet.\nGraded Task: Complete the following function that takes an image file and produces the weights of the finally fully-connected layer in Resnet. You may find the named_children() method of resnet helpful, as it produces a sequence of (named) layers. We would like the feature map directly before the global average pooling layer.\nfor (name, model) in resnet.named_children():\n    print(name)\ndef get_resnet_features(image_file):\n    \"\"\"\n    Return the final CNN layer (layer4) feature map in resnet\n\n    Parameters:\n        `image_file` - file path to the image\n\n    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n    \"\"\"\n\n    x = process_input(image_file)\n\n    result = None # TODO\n    result = x # SOLUTION\n    for (name, model) in resnet.named_children():\n        # TODO -- update result\n        result = model.forward(result) # SOLUTION\n        if name == 'layer4':\n            break\n    return result\nfets = get_resnet_features('cat.jpg')\nprint(fets.shape) # should be [1, 512, 7, 7]\nTask: Assign the variable fc_weight to the weights of the final fully-connected layer in resnet.\nweights = None # TODO\nweights = resnet.fc.weight # SOLUTIONS\nprint(weights.shape) # should be [1000, 512]\nGraded Task: Complete the function compute_cam, which takes the CNN feature map (from the get_resnet_features function), the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the following approach.\nWe will use the notation \\({\\bf X}\\) to denote the CNN feature map (the input to the GAP), with \\(X_{i,j,c}\\) being the activation at location \\((i, j)\\) and channel \\(c\\). Here, \\({\\bf X}\\) is a tensor with shape \\(H \\times W \\times C\\), where \\(H \\times W\\) is the height and width of the feature map and \\(C\\) is the number of channels. We will use the vector \\({\\bf h}\\) to denote the output of the GAP, so that \\(h_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{i,j,c}\\). Finally, we will use \\({\\bf W}\\) to denote the finally fully connected layer weights, and \\({\\bf z}\\) to the denote the prediction score, so that \\({\\bf z} = {\\bf W}{\\bf h}\\).\nNow, we would like to relate the features \\(X_{i,j,c}\\) to the scores \\(z_k\\), so that we can compute the contribution of the features at position \\((i,j)\\) to the score \\(k\\).\nFor an output class \\(k\\), we have:\n\\[z_k  = \\sum_{c=1}^{C} w_{k,c} h_c\\]\nSubstituing \\(h_c\\) for its definition, we have:\n\\[z_k  = \\sum_{c=1}^{C} w_{k,c} \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W X_{i,j,c}\\]\nRearranging the sums, we have:\n\\[z_k  = \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W \\sum_{c=1}^C w_{k,c} X_{i,j,c}\\]\nThe inner term \\(\\sum_c w_{k,c} X_{i,j,c}\\) is exactly what we are looking for: this term indicates how much the value in location \\((i, j)\\) of the feature map \\({\\bf X}\\) attributes to class \\(k\\).\ndef compute_cam(features, label):\n    \"\"\"\n    Computes the contribution of each location in `features` towards \n    `label` using CAM.\n\n    Parameters:\n        `features`: PyTorch Tensor of shape [1, 512, 7, 7] representing\n                    final layer feature map in ResNet (e.g., from calling\n                    `get_resnet_features`)\n        `label`   : resnet label, integer between 0-999\n\n    Returns: PyTorch Tensor of shape [7, 7]\n    \"\"\"\n    features = features.squeeze(0) # remove the first dimension\n    result = None # TODO \n    weight = resnet.fc.weight[label]   # SOLUTIONS\n    result = torch.mul(weight, features.permute((1, 2, 0))).sum(dim=2)  # SOLUTIONS\n    return result\nTask: Run the below code, which superimposes the result of the compute_cam operation on the image.\ndef visualize_cam(image_file, label):\n    # open the image\n    img = Image.open(image_file)\n\n    # compute CAM features\n    fets = get_resnet_features('cat.jpg')\n    m = compute_cam(fets, label)\n\n    # normalize \"m\"\n    m = m - m.min()\n    m = m / m.max()\n    # convert \"m\" into pixel intensities\n    m = np.uint8(255 * m.detach().numpy())\n    # apply a color map\n    m = cv2.resize(m, img.size)\n    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n\n    plt.figure(figsize=(6, 6))\n    plt.title(\"%s %s\" % (image_file, classes[label]))\n    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n    plt.show()\nvisualize_cam('cat.jpg', 743)\nvisualize_cam('cat.jpg', 383)\nGraded Task Compare the above two outputs, and explain what conclusion you may be able to draw about the contribution of the pixel locations to those two classes. Why do you think the model misclassified the image?\n# TODO: your explanation goes here"
  },
  {
    "objectID": "labs/lab09.html#part-2.-grad-cam",
    "href": "labs/lab09.html#part-2.-grad-cam",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "",
    "text": "Although CAM was an important step toward understanding convolutional neural networks, the technique is only applicable to convolutional networks with GAP and a single fully-connected layer. Recall that it leveraged the following relationship between the output of the GAP layer \\({\\bf h}\\) and the score for the output class \\(z_k\\):\n\\[z_k  = \\sum_c w_{k,c} h_c\\]\nWhere \\(w_{k,c}\\) is the fully connected layer weight that describes the strength of the connection between \\(h_c\\) and \\(z_k\\). In other words, \\(w_{k,c}\\) describes the following gradient:\n\\[\\frac{\\partial z_k}{\\partial h_c}\\]\nWith this in mind, you may be able to see how CAM may be generalized so that \\({\\bf z}\\) may be a more complex function of \\({\\bf h}\\)—e.g., a MLP or even an RNN!\nGradient-weighted Class Activation Mapping (Grad-CAM) is a generalized form of CAM, and can be applied to any convolutional neural network. In Grad-CAM, we use the gradient \\(\\frac{\\partial z_k}{\\partial h_c}\\) in place of \\(w_{k,c}\\) when attributing class scores to locations \\((i, j)\\). In other words, the below term indicates how much the value in location \\((i, j)\\) of the feature map \\({\\bf X}\\) attributes to class \\(k\\).\n\\[ReLU(\\sum_c \\frac{\\partial z_k}{\\partial h_c} X_{i,j,c})\\]\nThe addition of the ReLU activation only allows positive contributions to be visualized.\nSidenote: To generalize this result even further, we can replace \\(z_k\\) with any target we would like! Grad-CAM has been used on neural networks that performs image caption generation: a model with a CNN encoder and an RNN decoder. We can use use the gradients of any target concept (say “dog” in a classification network or a sequence of words in a captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Taking a look at this video helps you to understand the power of Grad-CAM.\nLet’s explore GradCAM with the VGG network:\nvgg19 = models.vgg19(pretrained=True)\nvgg19.eval()\nvgg19\npredict(vgg19, 'cat.jpg')\nTask Just like with CAM, we will need to extract the feature map obtained from the last convolutional layer. This step is actually very straightforward with VGG since vgg19 splits the network into a features network and a classifier network.\ndef get_vgg_features(image_file):\n    \"\"\"\n    Return the output of `vgg19.features` network for the image\n\n    Parameters:\n        `image_file` - file path to the image\n\n    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n    \"\"\"\n\n    x = process_input(image_file)\n    result = None # TODO\n    result = vgg19.features(x) # SOLUTIONS\n    return result\nget_vgg_features('cat.jpg').shape\nTask: Read the forward method of the VGG model here. https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py What other steps are remaining in the forward pass?\n# TODO: Explain the remaining steps here\nGraded Task: Complete the function compute_gradcam, which takes an image file path, the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the GradCAM approach described at the beginning of Part 2.\ndef compute_gradcam(image_file, label):\n    \"\"\"\n    Computes the contribution of each location in `features` towards \n    `label` using GradCAM.\n\n    Parameters:\n        `image_file` - file path to the image\n        `label`   : resnet label, integer between 0-999\n\n    Returns: PyTorch Tensor of shape [7, 7]\n    \"\"\"\n    # obtain the image input features\n    x = process_input(image_file)\n\n    # obtain the output of the features network in the CNN\n    fets = vgg19.features(x)\n\n    # tell PyTorch to compute the gradients with respect\n    # to \"fets\"\n    fets.retain_grad()\n\n    # TODO: compute the rest of the vgg19 forward pass from `fets`\n    out = None # should be the output of the classifier network\n    out = vgg19.avgpool(fets)     # SOLUTION\n    out = torch.flatten(out, 1)  # SOLUTION\n    out = vgg19.classifier(out)  # SOLUTION\n\n    z_k = out.squeeze(0)[label] # identify the target output class\n    z_k.backward()              # backpropagation to compute gradients\n\n    features_grad = fets.grad   # identify the gradient of z_k with respect to fets\n\n    # account for the pooling operation, so that \"pooled_grad\"\n    # aligns with the notation used\n    n, c, h, w = features_grad.shape\n    features_grad = torch.reshape(features_grad, (c, h*w))\n    pooled_grad = features_grad.sum(dim=1)\n\n    # rearrange \"fets\" so that \"X\" aligns with the notation\n    # used above\n    X = fets.squeeze(0).permute((1, 2, 0))\n\n    # TODO: Compute the heatmap using the gradcam\n    m = None\n    m = torch.matmul(X, pooled_grad) # SOLUTION\n    m = F.relu(m) # apply the ReLU operation\n    return m\nTask: Run the below code, which superimposes the result of the compute_gradcam operation on the image.\ndef visualize_gradcam(image_file, label):\n    # open the image\n    img = Image.open(image_file)\n\n    # compute CAM features\n    m = compute_gradcam(image_file, label)\n\n    # normalize \"m\"\n    m = m - m.min()\n    m = m / m.max()\n    # convert \"m\" into pixel intensities\n    m = np.uint8(255 * m.detach().numpy())\n    # apply a color map\n    m = cv2.resize(m, img.size)\n    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n\n    plt.figure(figsize=(6, 6))\n    plt.title(\"%s %s\" % (image_file, classes[label]))\n    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n    plt.show()\nvisualize_cam('cat.jpg', 743)\nvisualize_cam('cat.jpg', 383)\nvisualize_cam('boat.jpg', 536)"
  },
  {
    "objectID": "labs/lab09.html#just-for-fun",
    "href": "labs/lab09.html#just-for-fun",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "",
    "text": "As you might have seen in the video, Grad-CAM can be applied to text-generating models. For example, in image-captioning tasks, a text is generated describing the given image. Some methods first feed the image to a convolutional neural network to extract features, and then feed the extracted features to an RNN, to generate the text. Neuraltalk2 was one of the earliest models using this approach. Similar to the classification task, it is enough to compute the gradient of the score (what is the score in an image-captioning task?) with respect to the last convolutional layer.\nIf you are interested in how neuraltalk2 functions you can check this project. Moreover, if you are looking for more hands-on experience, this repo has implemented many image-captioning methods, and you can easily apply Grad-CAM on them (especially show and tell).\nHint: There is a file which re-implements the forward pipeline of ResNet101, where you can store the features."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nHW 2 - Friday, Feb 18\nProject ideas - Friday, Feb 18"
  },
  {
    "objectID": "weeks/week-7.html#prepare",
    "href": "weeks/week-7.html#prepare",
    "title": "Week 7",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Tidy Modeling in R Chp 8: Feature engineering with recipes"
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 12 - MLR: Feature engineering\n🖥️ Lecture 13 - MLR: Feature engineering (cont.)"
  },
  {
    "objectID": "weeks/week-7.html#practice",
    "href": "weeks/week-7.html#practice",
    "title": "Week 7",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 5 - The Office"
  },
  {
    "objectID": "weeks/week-7.html#perform",
    "href": "weeks/week-7.html#perform",
    "title": "Week 7",
    "section": "Perform",
    "text": "Perform\n✍️ HW 2 - Multiple linear regression\n📂 Project - Topic ideas\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\n\nIf you can’t be in class for the lectures, you can watch the live stream or watch the recording later on Panopto.\nDue dates:\n\nHW 1: Fri, Jan 28, 5pm ET\nLab 1: Fri, Jan 28, 5pm ET"
  },
  {
    "objectID": "weeks/week-4.html#prepare",
    "href": "weeks/week-4.html#prepare",
    "title": "Week 4",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Sec 24.4: Mathematical model for testing the slope\n📖 Read Introduction to Modern Statistics, Sec 24.5: Mathematical model, interval for the slope\n📖 Read Introduction to Modern Statistics, Sec 24.6: Checking model conditions\n📖 Read Introduction to Modern Statistics, Sec 24.7: Chapter review"
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\n🖥️ Lab 2 - College scorecard\n🖥️ Lecture 6 - SLR: Mathematical models for inference\n🖥️ Lecture 7 - SLR: Model diagnostics"
  },
  {
    "objectID": "weeks/week-4.html#practice",
    "href": "weeks/week-4.html#practice",
    "title": "Week 4",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 3 - Checking model conditions"
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 2 - College scorecard\n✍️ HW 1 - In-person voting trends\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nWe’re back to in person classes this week! See here for class locations. And don’t forget to wear your mask! 😷\nDue dates:\n\nAE 2: Fri, Jan 21, 11:59pm ET"
  },
  {
    "objectID": "weeks/week-3.html#prepare",
    "href": "weeks/week-3.html#prepare",
    "title": "Week 3",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Sec 24.1: Case study: Sandwich store\n📖 Read Introduction to Modern Statistics, Sec 24.2: Randomization test for the slope\n📖 Read Introduction to Modern Statistics, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 4 - SLR: Prediction + model evaluation\n🖥️ Lecture 5 - SLR: Simulation-based inference"
  },
  {
    "objectID": "weeks/week-3.html#practice",
    "href": "weeks/week-3.html#practice",
    "title": "Week 3",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 2 - Bike rentals in DC"
  },
  {
    "objectID": "weeks/week-3.html#perform",
    "href": "weeks/week-3.html#perform",
    "title": "Week 3",
    "section": "Perform",
    "text": "Perform\nNone.\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\nDue date: Project proposal due Fri, Mar 18 at 5:00 pm."
  },
  {
    "objectID": "weeks/week-10.html#prepare",
    "href": "weeks/week-10.html#prepare",
    "title": "Week 10",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Chp 9: Logistic regression"
  },
  {
    "objectID": "weeks/week-10.html#participate",
    "href": "weeks/week-10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 18 - Logistic regression\n🖥️ Lecture 19 - Probabilities, odds, odds ratios"
  },
  {
    "objectID": "weeks/week-10.html#practice",
    "href": "weeks/week-10.html#practice",
    "title": "Week 10",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 9 - Odds"
  },
  {
    "objectID": "weeks/week-10.html#perform",
    "href": "weeks/week-10.html#perform",
    "title": "Week 10",
    "section": "Perform",
    "text": "Perform\n✍️ HW 3 - Logistic regression and log transformation\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nLab 6 - Friday, April 8\nProject drafts - Sunday, April 10"
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 24 - MultiLR: Prediction + inferential models\n🖥️ Lecture 25 - MultiLR: Predictive models"
  },
  {
    "objectID": "weeks/week-13.html#practice",
    "href": "weeks/week-13.html#practice",
    "title": "Week 13",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 11 - Volcanoes"
  },
  {
    "objectID": "weeks/week-13.html#perform",
    "href": "weeks/week-13.html#perform",
    "title": "Week 13",
    "section": "Perform",
    "text": "Perform\n✍️ HW 4 - Multinomial logistic regression\n💻 Lab 6 - Why Many Americans Don’t Vote\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\nDue date: Exam 1 released on Fri, Feb 35, due Mon, Feb 28 at 11:59pm"
  },
  {
    "objectID": "weeks/week-9.html#prepare",
    "href": "weeks/week-9.html#prepare",
    "title": "Week 9",
    "section": "Prepare",
    "text": "Prepare\nNo readings this week."
  },
  {
    "objectID": "weeks/week-9.html#participate",
    "href": "weeks/week-9.html#participate",
    "title": "Week 9",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 16 - MLR: Inference\n🖥️ Lecture 17 - MLR: Inference conditions + multicollinearity"
  },
  {
    "objectID": "weeks/week-9.html#practice",
    "href": "weeks/week-9.html#practice",
    "title": "Week 9",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 8 - Rail Trail"
  },
  {
    "objectID": "weeks/week-9.html#perform",
    "href": "weeks/week-9.html#perform",
    "title": "Week 9",
    "section": "Perform",
    "text": "Perform\n✅ Exam 2\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 15",
    "section": "",
    "text": "Important\n\n\n\nDue dates: Tue, Apr 19 - HW 5"
  },
  {
    "objectID": "weeks/week-15.html#participate",
    "href": "weeks/week-15.html#participate",
    "title": "Week 15",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 28 - Wrap up"
  },
  {
    "objectID": "weeks/week-15.html#practice",
    "href": "weeks/week-15.html#practice",
    "title": "Week 15",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 13 - A Tale of Two Creeks"
  },
  {
    "objectID": "weeks/week-15.html#perform",
    "href": "weeks/week-15.html#perform",
    "title": "Week 15",
    "section": "Perform",
    "text": "Perform\n✍️ HW 5 - Statistics experience\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#data-sources",
    "href": "project-tips-resources.html#data-sources",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %&gt;%\n  count(manufacturer) %&gt;%\n  mutate(manufacturer = str_to_title(manufacturer)) %&gt;%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "ex/w03/exercises03_solution.html",
    "href": "ex/w03/exercises03_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/prob-mle.md\n\n\n!include questions/prob-mle-sol.md"
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-1---maximum-likelihood-estimation-refresher",
    "href": "ex/w03/exercises03_solution.html#exercise-1---maximum-likelihood-estimation-refresher",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/prob-mle.md\n\n\n!include questions/prob-mle-sol.md"
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-2---more-gradients",
    "href": "ex/w03/exercises03_solution.html#exercise-2---more-gradients",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - More Gradients",
    "text": "Exercise 2 - More Gradients\n!include questions/backprop-batchnorm.md\n\nSolution\n!include questions/backprop-batchnorm-sol.md"
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-3---autodiff-modes",
    "href": "ex/w03/exercises03_solution.html#exercise-3---autodiff-modes",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Autodiff Modes",
    "text": "Exercise 3 - Autodiff Modes\n!include questions/autodiff-modes.md\n\nSolution\n!include questions/autodiff-modes-sol.md"
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-4---glove-embeddings",
    "href": "ex/w03/exercises03_solution.html#exercise-4---glove-embeddings",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - GloVe Embeddings",
    "text": "Exercise 4 - GloVe Embeddings\nOpen the notebook presented in class and work through it by trying some of the ideas presented therein for different word combinations."
  },
  {
    "objectID": "ex/w03/questions/autodiff-modes-sol.html",
    "href": "ex/w03/questions/autodiff-modes-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "For better readability we will write \\((g\\circ h)(x)\\) for \\(g(h(x))\\). By applying the chain rule, we obtain \\[\n\\begin{aligned}\nF'(x)\n&= (f_3 \\circ f_2 \\circ f_1)' (x) \\\\\n&= (f_2 \\circ f_1)'(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\\\\\n&= f_1'(x) \\cdot (f_2' \\circ f_1)(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\n\\end{aligned}\n\\]\nFirst, let’s start with the forward mode\nd = f1'(x)\nv = f1(x)\nd = f2'(x) * d\nv = f2(v)\nd = f3'(v) * d\nNow, for the reverse mode, we first do a “forward pass” before computing gradients:\nv1 = f1(x)\nv2 = f2(v1)\nd = f3'(v2)\nd = d*f2'(v1)\nd = d*f1'(x)\nSimply evaluating the expression in (a) is not in line with any of the modes. It also involves repeated computations because \\(f_1(x)\\) will be computed twice. Now, if we allow for caching of ingtermediate results, this doubling of compugtaiton disappears. The order written above will then be in line with forward move automatic differentiation. However, this is specific to our example and in general not true."
  },
  {
    "objectID": "ex/w03/questions/backprop-batchnorm.html",
    "href": "ex/w03/questions/backprop-batchnorm.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "You are an ML Engineer at Googlezon where you are working on an internal ML framework called TorchsorFlow. You are tasked with implementing a new layer known as BatchNormalization. The idea of this layer is as follows:\nDuring training, consider the outputs of the previous layer \\(\\fa_i=(a_i^{(1)}, \\ldots, a_i^{(N)})\\) for each element \\(i\\in \\{1, \\ldots, M\\}\\) of the input batch. Compute the mean \\(\\mu_j\\) and variance \\(\\si_j^2\\) over each input dimension \\(j\\). Use the resulting statistics to normalize the output of the previous layer. Finally, rescale the resulting vector with a learned constant \\(\\gm\\) and shift it by another learned constant \\(\\be\\).\n\nWrite down the mathematical expression for the BatchNormalization layer. What are its learnable parameters?\nCompute the gradient of the loss \\(\\mcL\\) with respect to the input of the BatchNormalization \\(\\fa_i\\) layer.\nAt test time, the batch size is usually 1. So, it is not meaningful (or even possible) to compute mean / variance. How would you implement a layer like this?"
  },
  {
    "objectID": "ex/w03/questions/backprop-batchnorm-sol.html",
    "href": "ex/w03/questions/backprop-batchnorm-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "For the purpose of batch normalization, we can consider each output neuron individually. Thus, we will simplify our notation and write \\(a_i\\), \\(y_i\\), … instead of \\(a_i^{(j)}\\), \\(y_i^{(j)}\\), … respectively.\n\nThe forward pass is given by the following equations \\[\n\\begin{aligned}\n\\mu_B\n  &:= \\frac{1}{M}\\sum_{i=1}^m a_i,\n  &\\text{(mini-batch mean)}  \\\\\n\\sigma_B^2\n  &:=\\frac{1}{M}\\sum_{i=1}^M (a_i-\\mu_B)^2,\n  &\\text{(mini-batch variance)}\\\\\n\\ha_i\n  &:= \\frac{a_i-\\mu_B}{\\sqrt{\\si_B^2+\\epsilon}},   \n&\\text{(normalize)}\\\\\n  y_i\n  &:= BN_{\\gamma, \\beta}((a_i)_{i=1}^M)\n  := \\gamma\\ha_i + \\beta.\n  &\\text{(scale and shift)}\n\\end{aligned}\n\\] The entire layer is defined as \\[\nBN(\\fa_1, \\ldots \\fa_M)=\\big(\n  BN_{\\gm^{(1)}, \\be^{(1)}}\\big((a_i^{(1)})_{i=1}^M\\big),\n  \\ldots,\n  BN_{\\gm^{(N)}, \\be^{(N)}}\\big((a_i^{(N)})_{i=1}^M\\big)\n  \\big)\n\\] where \\(\\gm^{(1)}, \\ldots, \\gm^{(N)}\\) and \\(\\be^{(1)}, \\ldots, \\be^{(N)}\\) are learnable parameters.\nThe derivatives can be expressed using the chain rule where we obtain \\(\\mu_B\\), \\(\\si_B\\), \\(\\ha_i\\), and \\(y_i\\) during the forward pass while \\(\\partial \\mcL/\\partial y_i\\) is obtained from earlier steps of the backward pass. The remaining derivatives are: \\[\n\\begin{aligned}\n\\frac{\\partial\\mcL}{\\partial \\ha_i} &= \\fr{\\partial \\mcL}{y_i}\\cdot \\gm, \\\\\n\\frac{\\partial\\mcL}{\\partial \\si_B^2}\n  &=\n  \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\\cdot(a_i-\\mu_B)\\cdot\n  \\frac{-1}{2}(\\si_B^2+\\epsilon)^{-3/2}, \\\\\n\\fr{\\partial\\mcL}{\\partial\\mu_B}\n  &=\n  \\bigg(\\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot\\frac{-1}{\\sqrt{\\sigma_B^2+\\epsilon}}\\bigg)\n  +\\fr{\\partial\\mcL}{ \\partial \\si_B^2}\n  \\cdot\\frac{\\sum_{i=1}^M  -2(a_i-\\mu_B)}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial a_i}\n  &=\n  \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot \\frac{1}{\\sqrt{\\sigma_B^2+\\epsilon}}  + \\fr{\\partial \\mcL}{\\partial \\sigma_B^2} \\cdot \\frac{2(a_i-\\mu_B)}{M} + \\fr{\\partial \\mcL}{\\partial \\mu_B}\\cdot \\frac{1}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial\\gamma}&= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i} \\cdot \\ha_i, \\\\\n\\fr{\\partial\\mcL}{\\partial\\beta} &= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i}.\n\\end{aligned}\n\\] Here \\(\\epsilon\\) is a small constant which is added in practice to the variance to avoid division by zero. It is actually not part of the derivative."
  },
  {
    "objectID": "ex/w04/exercises04-notes.html",
    "href": "ex/w04/exercises04-notes.html",
    "title": "Solution Computations",
    "section": "",
    "text": "This file is for internal use where we compute the solutions to the exercises. I also compute the solutions for seemingly easy exercises here to use the code later for automation of the process."
  },
  {
    "objectID": "ex/w04/exercises04-notes.html#cnn-size-exercise-solutions",
    "href": "ex/w04/exercises04-notes.html#cnn-size-exercise-solutions",
    "title": "Solution Computations",
    "section": "CNN Size exercise solutions",
    "text": "CNN Size exercise solutions\n\nimport torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0, bias=False)\n\nx = torch.tensor([\n    [ 1, 0, -2, 1],\n    [ 0, 1, 1, 0],\n    [ 0, 1, 0, 1],\n    [ -3, 4, 0, 0]\n])\n\nk = torch.tensor([\n    [ 2, 1],\n    [ 0, 1]\n])\n\nconv.weight.data = k.view(1, 1, 2, 2).float()\n\nprint(conv(x.view(1, 1, 4, 4).float()))\n\n\n\ntensor([[[[ 3., -1., -3.],\n          [ 2.,  3.,  3.],\n          [ 5.,  2.,  1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=0, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 3., -3.],\n          [ 5.,  1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# PyTorch uses zero-padding by default\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=1, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1., -2.,  0.],\n          [ 0.,  3.,  0.],\n          [-3.,  8.,  0.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)"
  },
  {
    "objectID": "ex/w04/exercises04-notes.html#exercise-3-mlp-sizes",
    "href": "ex/w04/exercises04-notes.html#exercise-3-mlp-sizes",
    "title": "Solution Computations",
    "section": "Exercise 3 MLP Sizes",
    "text": "Exercise 3 MLP Sizes\n\na = nn.Sequential(\n    nn.Linear(10,5),\n    nn.Linear(5,10),\n    nn.Linear(10,5)\n)\n\nfor i in a:\n    print(sum(p.numel() for p in i.parameters()))\n\n55\n60\n55"
  },
  {
    "objectID": "ex/w04/exercises04-notes.html#exercise-4-cnn-sizes",
    "href": "ex/w04/exercises04-notes.html#exercise-4-cnn-sizes",
    "title": "Solution Computations",
    "section": "Exercise 4 CNN Sizes",
    "text": "Exercise 4 CNN Sizes\n\ninp = torch.randn(1, 3, 100, 100)\narch = nn.Sequential(\n    nn.Conv2d(3, 5, kernel_size=3),\n    nn.MaxPool2d(2, stride=2),\n    nn.Conv2d(5, 10, kernel_size=3, stride=1),\n    nn.MaxPool2d(2, stride=2),\n    nn.Conv2d(10, 5, kernel_size=3, stride=1),\n    nn.Flatten(),\n    nn.Linear(2205, 20),\n    nn.Linear(20,10)\n)\n\ncur = inp\nfor num, layer in enumerate(arch):\n    cur = layer(cur)\n    print(f\"{num+1}. Activation: {cur.shape}, Params: {sum(p.numel() for p in layer.parameters())}\") \n\n1. Activation: torch.Size([1, 5, 98, 98]), Params: 140\n2. Activation: torch.Size([1, 5, 49, 49]), Params: 0\n3. Activation: torch.Size([1, 10, 47, 47]), Params: 460\n4. Activation: torch.Size([1, 10, 23, 23]), Params: 0\n5. Activation: torch.Size([1, 5, 21, 21]), Params: 455\n6. Activation: torch.Size([1, 2205]), Params: 0\n7. Activation: torch.Size([1, 20]), Params: 44120\n8. Activation: torch.Size([1, 10]), Params: 210\n\n\n\n\n\n0 5\n1 4\n2 2"
  },
  {
    "objectID": "ex/w04/questions/mlp-sizes-sol.html",
    "href": "ex/w04/questions/mlp-sizes-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The number of parameters for each neuron is the number of weights plus one for the biaas term. The number of weights corresponds to the number of inputs / activations from the previous layer. So for the first layer, we have 10 inputs and thus 11 parameters per neuron resulting in 55 parameters total per layer.\nA similar computation gives 60 and 55 as the number of parameters for the next two layers. Thus, the network has a total of 170 parameters."
  },
  {
    "objectID": "ex/w04/questions/cnn-sizes.html",
    "href": "ex/w04/questions/cnn-sizes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "You are givne a neural network with the following architecture:\nInput: 100 x 100 x 3 Image\n\nLayers:\n1. Conv(in_channels=3, out_channels=5, kernel_size=3, stride=1, padding=0)\n2. MaxPool2d(kernel_size=2, stride=2, padding=0)\n3. Conv(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=0)\n4. MaxPool2d(kernel_size=2, stride=2, padding=0)\n5. Conv(in_channels=10, out_channels=5, kernel_size=3, stride=1, padding=0)\n6. Flatten()\n7. MLP(neurons=20)\n8. MLP(neurons=10)\n\nWhat is the dimensionality of the activations after each layer.\nHow many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/questions/mlp-sizes.html",
    "href": "ex/w04/questions/mlp-sizes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "You are given an MLP with ReLU activations. It has 3 layers consisting of 5, 10, and 5 neurons respectively. The input is a vector of size 10. How many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/questions/cnn-by_hand.html",
    "href": "ex/w04/questions/cnn-by_hand.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Consider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & -2 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n-3 & 4 & 0 & 0\n\\epmat, \\qquad\n%\nK = \\bpmat\n2, & 1 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w05/exercises05-notes.html",
    "href": "ex/w05/exercises05-notes.html",
    "title": "Ex 2: Eigenvalues and Eigenvectors to matrices",
    "section": "",
    "text": "Part (a) is straightforward. One needs to simply concatenate the given eigenvalues in a matrix.\n\nimport numpy as np\n\nla1 = 2\nla2 = 3\nev1 = np.array([1, 0])\nev2 = np.array([0, -1])\n\nLa = np.diag([la1, la2])\nO = np.array([ev1, ev2])\n\nnp.matmul(np.matmul(O, La), O.T)\n\narray([[2, 0],\n       [0, 3]])\n\n\nPart (b) has a catch. The eigenvectors are not normalized. So, we need to normalize them first before concatenating them in a matrix.\n\nla1 = 2\nla2 = 3\nev1 = np.array([1, 1])\nev2 = np.array([1, -1])\n\nLa = np.diag([la1, la2])\nO = np.array([ev1, ev2])\n\nprint(np.matmul(np.matmul(O, La), np.linalg.inv(O)))\nprint(np.matmul(np.matmul(O, La), (O).T)/2) # Sanity check\n\n[[ 2.5 -0.5]\n [-0.5  2.5]]\n[[ 2.5 -0.5]\n [-0.5  2.5]]\n1.4142135623730951\n\n\n\nnp.linalg.norm(ev2)\n\n2.8284271247461903\n\n\n\nEx 3: SGD w. Momentum Implementation\nFirst, let’s define an objective function, its gradient, and a starting point for the optimizer. In our case, this is simply \\(f(x) = x^2\\) and \\(x_{init}\\)=1.\n\ndef objective(x):\n    return x**2\n\ndef obj_grad(x):\n    return 2*x\n\nNow, we implement the actual optimizer.\n\ndef sgd_with_momentum(obj, grad, x_init, learning_rate, momentum, max_iter):\n  x = x_init\n  update = 0\n  for i in range(max_iter):\n    update = momentum * update - learning_rate * grad(x)\n    x = x + update\n\n    print('&gt;%d f(%s) = %.5f' % (i, x, obj(x)))\n  return x\n\n\nsgd_with_momentum(\n    objective, obj_grad, x_init=3.0, learning_rate=0.1, momentum=0.5, \n    max_iter=20\n    )\n\n&gt;0 f(2.4) = 5.76000\n&gt;1 f(1.6199999999999999) = 2.62440\n&gt;2 f(0.9059999999999999) = 0.82084\n&gt;3 f(0.3677999999999999) = 0.13528\n&gt;4 f(0.02513999999999994) = 0.00063\n&gt;5 f(-0.15121800000000002) = 0.02287\n&gt;6 f(-0.2091534) = 0.04375\n&gt;7 f(-0.19629041999999997) = 0.03853\n&gt;8 f(-0.15060084599999995) = 0.02268\n&gt;9 f(-0.09763588979999996) = 0.00953\n&gt;10 f(-0.05162623373999997) = 0.00267\n&gt;11 f(-0.018296158961999986) = 0.00033\n&gt;12 f(0.0020281102194000047) = 0.00000\n&gt;13 f(0.011784622766219999) = 0.00014\n&gt;14 f(0.014305954486385997) = 0.00020\n&gt;15 f(0.012705429449191796) = 0.00016\n&gt;16 f(0.009364081040756336) = 0.00009\n&gt;17 f(0.005820590628387338) = 0.00003\n&gt;18 f(0.002884727296525372) = 0.00001\n&gt;19 f(0.0008398501712893144) = 0.00000\n\n\n0.0008398501712893144"
  },
  {
    "objectID": "ex/w05/exercises05.html",
    "href": "ex/w05/exercises05.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/calc-taylor.md"
  },
  {
    "objectID": "ex/w05/exercises05.html#exercise-1---taylor-series",
    "href": "ex/w05/exercises05.html#exercise-1---taylor-series",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/calc-taylor.md"
  },
  {
    "objectID": "ex/w05/exercises05.html#exercise-2---eigenvalues-eigenvectors",
    "href": "ex/w05/exercises05.html#exercise-2---eigenvalues-eigenvectors",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Eigenvalues, Eigenvectors",
    "text": "Exercise 2 - Eigenvalues, Eigenvectors\n!include questions/linalg-evs_to_mat.md"
  },
  {
    "objectID": "ex/w05/exercises05.html#exercise-3---sgd-with-momentum",
    "href": "ex/w05/exercises05.html#exercise-3---sgd-with-momentum",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - SGD with Momentum",
    "text": "Exercise 3 - SGD with Momentum\n!include questions/opt-momentum.md"
  },
  {
    "objectID": "ex/w05/questions/calc-taylor.html",
    "href": "ex/w05/questions/calc-taylor.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\)."
  },
  {
    "objectID": "ex/w05/questions/linalg-evs_to_mat-sol.html",
    "href": "ex/w05/questions/linalg-evs_to_mat-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\n\nHere, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 2 & 0 \\\\\n0 & 3\n\\epmat\n  \\]\nHere, we have to normalize the eigenvectors first. Each has length \\(\\sqrt{2}\\), so we have to divide each of them by \\(\\sqrt{2}\\), i.e. we set \\(\\fte_i:=\\fe_i/\\sqrt{2}\\). With this, we can construct an orthogonal matrix of eigenvalues as \\(\\fQ = (\\fte_1, \\fte_2)\\). The resulting matrix \\(\\fA\\) is \\[\n  \\fA = \\bpmat\n2.5 & -0.5 \\\\\n-0.5 & 2.5\n\\epmat\n  \\]"
  },
  {
    "objectID": "ex/w05/questions/opt-momentum.html",
    "href": "ex/w05/questions/opt-momentum.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Implement stochastic gradient descent with momentum and apply it to optimize some elementary functions in 1d and 2d."
  },
  {
    "objectID": "ex/w02/exercises02_solution.html",
    "href": "ex/w02/exercises02_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/linalg-evs.md\n\n\n!include questions/linalg-evs-sol.md"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-1---eigenvectors-and-eigenvalues",
    "href": "ex/w02/exercises02_solution.html#exercise-1---eigenvectors-and-eigenvalues",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/linalg-evs.md\n\n\n!include questions/linalg-evs-sol.md"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-2---variance-and-expectation",
    "href": "ex/w02/exercises02_solution.html#exercise-2---variance-and-expectation",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Variance and Expectation",
    "text": "Exercise 2 - Variance and Expectation\n!include questions/prob-evvar.md\n\nSolution\n!include questions/prob-evvar-sol.md"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-3---linear-regression",
    "href": "ex/w02/exercises02_solution.html#exercise-3---linear-regression",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Linear Regression",
    "text": "Exercise 3 - Linear Regression\n!include questions/linreg.md\n\nSolution\n!include questions/linreg-sol.md"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-4---gradients-and-computation-graphs",
    "href": "ex/w02/exercises02_solution.html#exercise-4---gradients-and-computation-graphs",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - Gradients and Computation Graphs",
    "text": "Exercise 4 - Gradients and Computation Graphs\n!include questions/nn-compgraph.md\n\nSolution\n!include questions/nn-compgraph-sol.md"
  },
  {
    "objectID": "ex/w02/questions/nn-compgraph.html",
    "href": "ex/w02/questions/nn-compgraph.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Compute the \\(\\frac{\\partial \\mathcal{L}}{\\partial w_j}\\) gradient of \\(\\mathcal{L}\\) with respect to a \\(w_j\\) in the following computation: \\[\\begin{align*}\n  \\mathcal{L}(y, t) &= - t \\log(y) - (1-t) \\log(1-y) ,\n& y &= \\sigma(z) ,\n&  z &= {\\bf w}^\\top {\\bf x} .\n  \\end{align*}\\]\nDraw the computation graph for the following neural network, showing the relevant scalar quantities. Assume that \\(\\fy, \\fh, \\fx \\in \\mathbb{R}^2\\) \\[\\begin{align*}\n  \\mathcal{L} &= \\frac{1}{2}\\sum_k (y_k - t_k)^2 ,\n& y_k &= \\sum_i w_{ki}^{(2)} h_i + b_k^{(2)} ,\n& h_i &= \\sigma(z_i) ,\n& z_i &= \\sum_j w_{ij}^{(1)} x_j + b_i^{(1)} .\n  \\end{align*}\\]"
  },
  {
    "objectID": "ex/w02/questions/linreg-sol.html",
    "href": "ex/w02/questions/linreg-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "We obtain the derivative with respect to \\(w\\) directly using the chain rule resulting in \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w}\n  = \\frac{1}{N}\\sum_i x^{(i)}((w x^{(i)} + b) - t^{(i)})\n\\] Similarly, the derivative with respect to \\(b\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial b}\n  = \\frac{1}{N}\\sum_i ((w x^{(i)} + b) - t^{(i)})\n\\]\nThe derivative with respect to \\(w_j\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w_j}\n  = \\frac{1}{N}\\sum_i x_j^{(i)}((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})\n\\]"
  },
  {
    "objectID": "ex/w02/questions/linreg.html",
    "href": "ex/w02/questions/linreg.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "In the linear regression model with one feature, we have the following model/hypothesis: \\[y = f(x) = w x + b\\] ​with parameters, \\(w\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}(w, b) = \\frac{1}{2N}\\sum_i ((w x^{(i)} + b) - t^{(i)})^2\\] What are the derivatives \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\) and \\(\\frac{\\partial \\mathcal{E}}{\\partial b}\\)?\nIn the linear regression model with many features, we have the following model/hypothesis: \\[y = f(x) = {\\bf w}^\\top {\\bf x}+ b\\] with parameters, \\({\\bf w} = [w_1, w_2, \\dots w_d]^T\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_i ((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})^2\\] What is the derivative \\(\\frac{\\partial \\mathcal{E}}{\\partial w_j}\\) for a weight \\(w_j\\)?"
  },
  {
    "objectID": "ex/w02/questions/prob-evvar.html",
    "href": "ex/w02/questions/prob-evvar.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Given a set of vectors \\(\\{\\fx_i\\}_{i=1}^N\\). Show that their empirical mean is equivalent to \\[\\hmu=\\argmin_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2.\\]\nThere are two equivalent definitons of variance of a random variable. The first one is \\(\\Var(X) := \\E[(X - \\E[X])^2]\\) and the second is \\(\\Var(X) = \\E[X^2] - \\E[X]^2\\). Show that these two definitions actually are equivalent."
  },
  {
    "objectID": "ex/w11/exercises11.html",
    "href": "ex/w11/exercises11.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/linalg-evs_to_mat.md"
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-1---eigenvalues-and-eigenvectors",
    "href": "ex/w11/exercises11.html#exercise-1---eigenvalues-and-eigenvectors",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/linalg-evs_to_mat.md"
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-2---parameter-counting",
    "href": "ex/w11/exercises11.html#exercise-2---parameter-counting",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Parameter Counting",
    "text": "Exercise 2 - Parameter Counting\n!include questions/pytorch-parameter_count.md"
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-3---convolutional-layers",
    "href": "ex/w11/exercises11.html#exercise-3---convolutional-layers",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Convolutional Layers",
    "text": "Exercise 3 - Convolutional Layers\n!include questions/cnn-by_hand.md"
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-4---scaled-dot-product-attention",
    "href": "ex/w11/exercises11.html#exercise-4---scaled-dot-product-attention",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - Scaled Dot-Product Attention",
    "text": "Exercise 4 - Scaled Dot-Product Attention\n!include questions/attn-transformers_by_hand.md"
  },
  {
    "objectID": "ex/w11/questions/pytorch-parameter_count.html",
    "href": "ex/w11/questions/pytorch-parameter_count.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Use PyTorch to load the alexnet model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w11/questions/attn-transformers_by_hand-notes.html",
    "href": "ex/w11/questions/attn-transformers_by_hand-notes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\n\n\nQ = torch.tensor([\n    [1, 3], \n    [0, 1]]).float()\n\nK = torch.tensor([\n    [1, 1], \n    [1, 2],\n    [0, 1]]).float()\n\nV = torch.tensor([\n    [1, 0, -2],\n    [2, 1, 2], \n    [0, 3, -1]]).float()\n\n\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\n\ntensor([[1.7981, 0.9986, 1.4429],\n        [1.2552, 1.2483, 0.2622]])\n\n\nLazy version\n\nF.scaled_dot_product_attention(Q, K, V)\n\ntensor([[1.7981, 0.9986, 1.4429],\n        [1.2552, 1.2483, 0.2622]])"
  },
  {
    "objectID": "ex/w11/questions/linalg-evs_to_mat.html",
    "href": "ex/w11/questions/linalg-evs_to_mat.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\)."
  },
  {
    "objectID": "ex/w11/questions/pytorch-parameter_count-notes.html",
    "href": "ex/w11/questions/pytorch-parameter_count-notes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "import torchvision\nalexnet = torchvision.models.alexnet()\n\nprint(\n    f\"Total number of parameters: {sum(p.numel() for p in alexnet.parameters())}\")\n\nprint(\"\\n\\nParameter Overview in the backbone:\")\nfor layer in alexnet.features:\n    print(f\"{layer}: {sum(p.numel() for p in layer.parameters())}\")\n\nprint(\"\\n\\nParameter Ovewview in the head:\")\nfor layer in alexnet.classifier:\n    print(layer, sum(p.numel() for p in layer.parameters()))\n\nTotal number of parameters: 61100840\n\n\nParameter Overview in the backbone:\nConv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)): 23296\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)): 307392\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 663936\nReLU(inplace=True): 0\nConv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 884992\nReLU(inplace=True): 0\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 590080\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False): 0\n\n\nParameter Ovewview in the head:\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=9216, out_features=4096, bias=True) 37752832\nReLU(inplace=True) 0\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=4096, out_features=4096, bias=True) 16781312\nReLU(inplace=True) 0\nLinear(in_features=4096, out_features=1000, bias=True) 4097000"
  },
  {
    "objectID": "ex/w11/questions/linalg-evs_to_mat-notes.html",
    "href": "ex/w11/questions/linalg-evs_to_mat-notes.html",
    "title": "Eigenvalues and Eigenvectors to matrices",
    "section": "",
    "text": "Part (a) is straightforward. One needs to simply concatenate the given eigenvalues in a matrix.\n\nimport numpy as np\n\nla1 = 1\nla2 = 2\nev1 = np.array([np.sqrt(.5), np.sqrt(.5)])\nev2 = np.array([np.sqrt(.5), -np.sqrt(.5)])\n\nLa = np.diag([la1, la2])\nO = np.array([ev1, ev2])\n\n# Check orthogonality\nprint(np.dot(ev1, ev2))\n\nnp.matmul(np.matmul(O, La), O.T)\n\n4.266421588589642e-17\n\n\narray([[ 1.5, -0.5],\n       [-0.5,  1.5]])\n\n\nPart (b) has a catch. The eigenvectors are not normalized. So, we need to normalize them first before concatenating them in a matrix.\n\nla1 = 2\nla2 = 3\nla3 = 4\nev1 = np.array([np.sqrt(1/3), np.sqrt(1/3), np.sqrt(1/3)])\nev2 = np.array([np.sqrt(.5), -np.sqrt(.5), 0])\nev3 = np.array([np.sqrt(1/6), np.sqrt(1/6), -np.sqrt(2/3)])\n\nLa = np.diag([la1, la2, la3])\nO = np.array([ev1, ev2, ev3])\n\nprint(np.matmul(O, O.T))\nprint(np.matmul(np.matmul(O, La), np.linalg.inv(O)))\n\n[[ 1.00000000e+00 -3.39032612e-18  2.15314570e-17]\n [-3.39032612e-18  1.00000000e+00  1.84419141e-17]\n [ 2.15314570e-17  1.84419141e-17  1.00000000e+00]]\n[[ 3.         -0.40824829 -0.70710678]\n [-0.40824829  2.5        -0.28867513]\n [-0.70710678 -0.28867513  3.5       ]]\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb Cell 5 line 1\n      &lt;a href='vscode-notebook-cell:/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb#W4sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt; O = np.array([ev1, ev2, ev3])\n     &lt;a href='vscode-notebook-cell:/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb#W4sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt; print(np.matmul(O, O.T))\n---&gt; &lt;a href='vscode-notebook-cell:/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb#W4sZmlsZQ%3D%3D?line=11'&gt;12&lt;/a&gt; print(np.matmul(np.matmul(O, La), np.linalg.inv(O)))*np.sqrt(.5)\n\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'float'"
  },
  {
    "objectID": "ex/w11/questions/cnn-by_hand-notes.html",
    "href": "ex/w11/questions/cnn-by_hand-notes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0, bias=False)\n\nx = torch.tensor([\n    [ 1, 0, 1, -1],\n    [ 1, 0, 1, 0],\n    [ 0, 3, 0, 1],\n    [ 1, -1, 0, 1]\n])\n\nk = torch.tensor([\n    [ 1, 2],\n    [ 0, 1]\n])\n\nconv.weight.data = k.view(1, 1, 2, 2).float()\n\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1.,  3., -1.],\n          [ 4.,  2.,  2.],\n          [ 5.,  3.,  3.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=0, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1., -1.],\n          [ 5.,  3.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# PyTorch uses zero-padding by default\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=1, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1.,  1.,  0.],\n          [ 2.,  2.,  0.],\n          [ 2., -1.,  1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)"
  },
  {
    "objectID": "ex/w10/exercises10.html",
    "href": "ex/w10/exercises10.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/tconv-sizes.md"
  },
  {
    "objectID": "ex/w10/exercises10.html#exercise-1---transposed-convolution-output-sizes",
    "href": "ex/w10/exercises10.html#exercise-1---transposed-convolution-output-sizes",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/tconv-sizes.md"
  },
  {
    "objectID": "ex/w10/exercises10.html#exercise-2---transposed-convolution-parameter-sizes",
    "href": "ex/w10/exercises10.html#exercise-2---transposed-convolution-parameter-sizes",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Transposed Convolution Parameter Sizes",
    "text": "Exercise 2 - Transposed Convolution Parameter Sizes\n!include questions/tconv-params.md"
  },
  {
    "objectID": "ex/w10/exercises10.html#exercise-3---transposed-convolution-by-hand",
    "href": "ex/w10/exercises10.html#exercise-3---transposed-convolution-by-hand",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Transposed Convolution by Hand",
    "text": "Exercise 3 - Transposed Convolution by Hand\n!include questions/tconv-by_hand.md"
  },
  {
    "objectID": "ex/w10/questions/tconv-by_hand.html",
    "href": "ex/w10/questions/tconv-by_hand.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "You are given an input matrix \\(X\\) (consisting of a single channel) and a kernel \\(K\\) as follows: \\[\nX = \\bpmat\n1 & 0 & 2 \\\\\n2 & 3 & 0 \\\\\n-1 & 0 & 3\n\\epmat, \\quad\nK = \\bpmat\n1 & 0 \\\\\n1 & 2\n\\epmat\n\\]\n\nCompute the transposed convolution by hand assuming stride 2.\nCompute the transposed convolution by hand assuming stride 1.\nUse PyTorch to verify your answers.\nImplement the transposed convolution in PyTorch without using its own implementaiton. You can assume no bias term a square kernel and no separate batch dimension. I.e. your task is to implement the following function\n\ndef conv_transpose2d(inp, weight, stride=1):\n    # inp - input of shape (C_in, H, W)\n    # weight - kernel of shape (C_in, C_out, K, K)\n    # stride - stride of the transposed convolution\n    # RETURNS\n    # output - output of shape (C_out, H_out, W_out)\n    #\n    # YOUR CODE HERE\n    return output"
  },
  {
    "objectID": "ex/w10/questions/tconv-params-notes.html",
    "href": "ex/w10/questions/tconv-params-notes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\n\ntconv = nn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3)\nprint([p.numel() for p in tconv.parameters()])\n\ntconv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=10, kernel_size=3)\nprint([p.numel() for p in tconv.parameters()])\n\ntconv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=2, kernel_size=4)\nprint([p.numel() for p in tconv.parameters()])\n\ntconv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=4, kernel_size=3)\nprint([p.numel() for p in tconv.parameters()])\n\n[54, 2]\n[270, 10]\n[96, 2]\n[108, 4]"
  },
  {
    "objectID": "ex/w10/questions/tconv-sizes-notes.html",
    "href": "ex/w10/questions/tconv-sizes-notes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\nCompute the output sizes for the exercises.\n\nx = torch.randn(1, 3, 2, 2)\ntransp_conv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=2, kernel_size=3, stride=1)\ntransp_conv(x).shape\n\ntorch.Size([1, 2, 4, 4])\n\n\n\nx = torch.randn(1, 3, 5, 5)\ntransp_conv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=4, kernel_size=2, stride=2)\ntransp_conv(x).shape\n\ntorch.Size([1, 4, 10, 10])"
  },
  {
    "objectID": "ex/w10/questions/tconv-sizes.html",
    "href": "ex/w10/questions/tconv-sizes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)"
  },
  {
    "objectID": "ex/w07/exercises07.html",
    "href": "ex/w07/exercises07.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The exercises this week involve some old material so you can check your learning and understanding."
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-1---maximum-likelihood-estimator",
    "href": "ex/w07/exercises07.html#exercise-1---maximum-likelihood-estimator",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 1 - Maximum Likelihood Estimator",
    "text": "Exercise 1 - Maximum Likelihood Estimator\n!include questions/prob-mle_exp_dist.md"
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-2---convolutional-layers",
    "href": "ex/w07/exercises07.html#exercise-2---convolutional-layers",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\n!include questions/cnn-by_hand.md"
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-3---computational-parameter-counting",
    "href": "ex/w07/exercises07.html#exercise-3---computational-parameter-counting",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Computational Parameter Counting",
    "text": "Exercise 3 - Computational Parameter Counting\n!include questions/pytorch-parameter_count.md"
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-4---influence-functions",
    "href": "ex/w07/exercises07.html#exercise-4---influence-functions",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 4 - Influence Functions",
    "text": "Exercise 4 - Influence Functions\n!include questions/opt-influence_functions.md"
  },
  {
    "objectID": "ex/w07/exercises07-notes.html",
    "href": "ex/w07/exercises07-notes.html",
    "title": "Solution Computations",
    "section": "",
    "text": "This file is for internal use where we compute the solutions to the exercises. I also compute the solutions for seemingly easy exercises here to use the code later for automation of the process."
  },
  {
    "objectID": "ex/w07/exercises07-notes.html#cnn-size",
    "href": "ex/w07/exercises07-notes.html#cnn-size",
    "title": "Solution Computations",
    "section": "CNN Size",
    "text": "CNN Size\n\nimport torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0, bias=False)\n\nx = torch.tensor([\n    [ 1, 2, -1, 1],\n    [ 1, 0, 1, 0],\n    [ 0, 1, 0, 2],\n    [ 2, 1, 0, -1]\n])\n\nk = torch.tensor([\n    [ 1, 0],\n    [ 2, 1]\n])\n\nconv.weight.data = k.view(1, 1, 2, 2).float()\n\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 3.,  3.,  1.],\n          [ 2.,  2.,  3.],\n          [ 5.,  3., -1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=0, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 3.,  1.],\n          [ 5., -1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# PyTorch uses zero-padding by default\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=1, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1.,  3.,  2.],\n          [ 0.,  2.,  4.],\n          [ 0.,  1., -1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)"
  },
  {
    "objectID": "ex/w07/exercises07-notes.html#vgg-weight-count",
    "href": "ex/w07/exercises07-notes.html#vgg-weight-count",
    "title": "Solution Computations",
    "section": "VGG weight count",
    "text": "VGG weight count\n\nimport torchvision\nvgg11 = torchvision.models.vgg.vgg11(pretrained=False)\n\nprint(\n    f\"Total number of parameters: {sum(p.numel() for p in vgg11.parameters())}\")\n\nprint(\"\\n\\nParameter Overview in the backbone:\")\nfor layer in vgg11.features:\n    print(f\"{layer}: {sum(p.numel() for p in layer.parameters())}\")\n\nprint(\"\\n\\nParameter Ovewview in the head:\")\nfor layer in vgg11.classifier:\n    print(layer, sum(p.numel() for p in layer.parameters()))\n\n/Users/igor/Conda/envs/csc413f23/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/igor/Conda/envs/csc413f23/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n\n\nTotal number of parameters: 132863336\n\n\nParameter Overview in the backbone:\nConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 1792\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 73856\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 295168\nReLU(inplace=True): 0\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 590080\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 1180160\nReLU(inplace=True): 0\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 2359808\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 2359808\nReLU(inplace=True): 0\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 2359808\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\n\n\nParameter Ovewview in the head:\nLinear(in_features=25088, out_features=4096, bias=True) 102764544\nReLU(inplace=True) 0\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=4096, out_features=4096, bias=True) 16781312\nReLU(inplace=True) 0\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=4096, out_features=1000, bias=True) 4097000"
  },
  {
    "objectID": "ex/w07/questions/cnn-by_hand-sol.html",
    "href": "ex/w07/questions/cnn-by_hand-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Here, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 &  3 &  1 \\\\\n2 &  2 &  3 \\\\\n5 &  3 & -1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & 1 \\\\\n5 & -1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 &  0 &  0 & 0 \\\\\n0 & 1 & 2 & -1 &  1 & 0 \\\\\n0 & 1 & 0 &  1 &  0 & 0 \\\\\n0 & 0 & 1 &  0 &  2 & 0 \\\\\n0 & 2 & 1 &  0 & -1 & 0 \\\\\n0 & 0 & 0 &  0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 3 & 2 \\\\\n0 & 2 & 4 \\\\\n0 & 1 & -1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w07/questions/pytorch-parameter_count.html",
    "href": "ex/w07/questions/pytorch-parameter_count.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Use PyTorch to load the vgg11 model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w07/questions/pytorch-parameter_count-sol.html",
    "href": "ex/w07/questions/pytorch-parameter_count-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First, we hvae to load the vgg11 model which is part of torchvision as has been shown in the lecture:\nimport torchvision\nvgg11 = torchvision.models.vgg.vgg11(pretrained=False)\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in vgg11.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the vgg11 model. All VGG models are ultimately instantiated by using the VGG class. Its forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in vgg11.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over vgg11.classifier instead of vgg11.features."
  },
  {
    "objectID": "ex/w07/questions/prob-mle_exp_dist.html",
    "href": "ex/w07/questions/prob-mle_exp_dist.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Assume you are given datapoints \\((x_i)_{i=1}^N\\) with \\(x_i\\in\\R\\) coming from a Exponential distribution. The probability density function of a exponential distribution is given by \\(f(x) = \\la \\exp(-\\la x)\\) with \\(x\\in\\R\\). Derive the maximum likelihood estimator of the parameter \\(\\la\\)."
  },
  {
    "objectID": "ex/w09/exercises09_solution.html",
    "href": "ex/w09/exercises09_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/attn-dot_product_attention.md\n\n\n!include questions/attn-dot_product_attention-sol.md"
  },
  {
    "objectID": "ex/w09/exercises09_solution.html#exercise-1---dot-product-attention",
    "href": "ex/w09/exercises09_solution.html#exercise-1---dot-product-attention",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/attn-dot_product_attention.md\n\n\n!include questions/attn-dot_product_attention-sol.md"
  },
  {
    "objectID": "ex/w09/exercises09_solution.html#exercise-2---attention-in-transformers",
    "href": "ex/w09/exercises09_solution.html#exercise-2---attention-in-transformers",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Attention in Transformers",
    "text": "Exercise 2 - Attention in Transformers\n!include questions/attn-transformers.md\n\nSolution\n!include questions/attn-transformers-sol.md"
  },
  {
    "objectID": "ex/w09/exercises09_solution.html#exercise-3---scaled-dot-product-attention-by-hand",
    "href": "ex/w09/exercises09_solution.html#exercise-3---scaled-dot-product-attention-by-hand",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Scaled Dot-Product Attention by Hand",
    "text": "Exercise 3 - Scaled Dot-Product Attention by Hand\n!include questions/attn-transformers_by_hand.md\n\nSolution\n!include questions/attn-transformers_by_hand-sol.md"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers_by_hand-sol.html",
    "href": "ex/w09/questions/attn-transformers_by_hand-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "The resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n0.86 & 1.58 & -0.72\\\\\n0.99 & 1.88 & -1.56\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w09/questions/attn-dot_product_attention.html",
    "href": "ex/w09/questions/attn-dot_product_attention.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\)."
  },
  {
    "objectID": "ex/w09/questions/attn-dot_product_attention-sol.html",
    "href": "ex/w09/questions/attn-dot_product_attention-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First we compute the dot products between \\(\\fs\\) and the \\(\\fh_i\\) and apply softmax resulting in: \\[\n\\fa = \\text{Softmax}\\li(\n\\bpmat\n1 & 1 & 0\\\\\n2 & 2 & 1\\\\\n3 & 1 & -1\n\\epmat^\\top\n\\cdot\n\\bpmat 1\\\\ 2\\\\ 1\\epmat\n\\ri)\n\\approx\n\\bpmat 0.88\\\\ 0.12\\\\ 0.00\\epmat\n\\] The resulting context vector is then computed as a weighted sum of the \\(\\fh_i\\): \\[\n\\fc\n  = a_1 \\fh_1 + a_2 \\fh_2 + a_3 \\fh_3\n  \\approx \\bpmat 1.00\\\\ 2.00\\\\ 2.76\\epmat\n\\]\nA simple implementation yielding the solution is as follows:\nimport torch\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c)"
  },
  {
    "objectID": "ex/w09/questions/attn-dot_product_attention-notes.html",
    "href": "ex/w09/questions/attn-dot_product_attention-notes.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn\nimport torch.nn.functional as F\n\n\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c, a)\n\ntensor([0.9992, 1.9992, 2.7586]) tensor([8.8009e-01, 1.1911e-01, 8.0254e-04])"
  },
  {
    "objectID": "ex/w08/exercises08_solution.html",
    "href": "ex/w08/exercises08_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/rnn-sentiment.md\n\n\n!include questions/rnn-sentiment-sol.md"
  },
  {
    "objectID": "ex/w08/exercises08_solution.html#exercise-1---rnn-for-sentiment-analysis",
    "href": "ex/w08/exercises08_solution.html#exercise-1---rnn-for-sentiment-analysis",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/rnn-sentiment.md\n\n\n!include questions/rnn-sentiment-sol.md"
  },
  {
    "objectID": "ex/w08/exercises08_solution.html#exercise-2---scalar-rnn",
    "href": "ex/w08/exercises08_solution.html#exercise-2---scalar-rnn",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Scalar RNN",
    "text": "Exercise 2 - Scalar RNN\n!include questions/rnn-scalar.md\n\nSolution\n!include questions/rnn-scalar-sol.md"
  },
  {
    "objectID": "ex/w08/exercises08_solution.html#exercise-3---rnn-addition",
    "href": "ex/w08/exercises08_solution.html#exercise-3---rnn-addition",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - RNN Addition",
    "text": "Exercise 3 - RNN Addition\n!include questions/rnn-addition.md\n\nSolution\n!include questions/rnn-addition-sol.md"
  },
  {
    "objectID": "ex/w08/questions/rnn-sentiment.html",
    "href": "ex/w08/questions/rnn-sentiment.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Suppose we are training a vanilla RNN like below to determine whether a sentence expresses positive or negative sentiment. This RNN will be a character-level RNN where \\(x^{(1)}, \\ldots, x^{(T)}\\) is the sequence of input characters. The RNN is given as follows: \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(U x^{(t)} + W h^{(t-1)} + b\\ri) \\\\\ny &= \\sigma\\li(V h^{(T)} + d\\ri)\n\\end{align*}\\]\n\nHow many times do we need to apply the weight matrix \\(U\\), \\(W\\), and \\(V\\)?\nWhat are the shapes of the matrices \\(U\\), \\(W\\), and \\(V\\)?\nHow many addition and multiplication operations are required to make a prediction? You can assume that no addition and multiplications are performed when applying the tanh and sigmoid activation functions."
  },
  {
    "objectID": "ex/w08/questions/rnn-scalar-sol.html",
    "href": "ex/w08/questions/rnn-scalar-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "To make the sequence length \\(T\\) explicit in the notation, we will write \\(y\\) instead of \\(y_T\\). Formally, what we have to show is \\[\n|w|&lt;1 \\implies \\lim_{T\\to\\infty} \\fr{\\partial y_T}{\\partial x^{(0)}} = 0 .\n\\] For the proof, we expand the derivative of \\(y_T\\) with respect to \\(x^{(0)}\\) using the chain rule: \\[\n\\begin{aligned}\n\\fr{\\partial y_T}{\\partial x^{(0)}}\n  & = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v \\cdot \\fr{\\partial h^{(T)}}{\\partial x^{(0)}} \\\\\n  & =\n  \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot\n      \\underbrace{\n        \\tanh'\\li(w \\cdot h^{(T-1)} + u \\cdot x^{(T-1)} + b_h\\ri)\n      }_{A_{T-1}(x^{(0)})}\n    \\cdot w \\cdot \\fr{\\partial h^{(T-1)}}{\\partial x^{(0)}} \\\\\n& = \\ldots \\\\\n& = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot \\prod_{t=2}^{T-1} A_{t}(x^{(0)})\n    \\cdot w^{T-1} \\cdot \\fr{\\partial h^{(1)}}{\\partial x^{(0)}} .\\\\\n\\end{aligned}\n\\] Using this, we can analyze the absolute value of the derivative \\(\\partial y_T/\\partial x^{(0)}\\). For \\(\\tanh\\) and \\(\\si\\), the absolute value of their respective derivatives is bounded by \\(1\\). Thus, we have \\[\n\\begin{aligned}\n\\li|\\fr{\\partial y_T}{\\partial x^{(0)}} \\ri|\n& =\n\\underbrace{\n  \\li|\\si'\\li(v \\cdot h^{(T)} + b_y\\ri) \\ri|\n}_{\\leq 1}\n\\cdot |v|\n\\cdot \\prod_{t=2}^{T-1}\n  \\underbrace{\\li| A_{t}(x^{(0)}) \\ri|}_{\\leq 1}\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n& \\leq |v|\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n\\end{aligned}\n\\] Because \\(|w|&lt;1\\), this converges to \\(0\\) as \\(T\\to\\infty\\) and thus \\(|\\partial y_T/\\partial x^{(0)}|\\) also converges to \\(0\\), i.e. the gradient vanishes.\nIt implies that in the considered setting, the input has no impact on the output."
  },
  {
    "objectID": "ex/w08/questions/rnn-addition-sol.html",
    "href": "ex/w08/questions/rnn-addition-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Since the inputs \\(\\bf{x}^{(t)}\\) are \\(2 \\times 1\\) and the hidden units \\(\\bf{h}^{(t)}\\) are \\(2 \\times 1\\), we should have:\n\n\\(\\bf{W}\\) is \\(3 \\times 3\\)\n\\(\\bf{U}\\) is \\(3 \\times 2\\)\n\\(\\bf{b}_h\\) is \\(3 \\times 1\\)\n\\(\\bf{v}\\) is \\(3 \\times 1\\)\n\nWe will follow the hint and implement the addition in our RNN such that:\n\nThe first of our hidden units \\(h_1^{(t)}\\) is 1 if and only if the sum \\(S^{(t)} \\doteq x_1^{(t)} + x_2^{(t)} + c^{(t-1)} \\geq 1\\), where by \\(c^{(t-1)}\\) we denote a carry (\\(\\bf{h_2}^{(t-1)}\\) from the previous addition). Note, these \\(S^{(t)}\\) and \\(c^{(t-1)}\\) are not variables of the model, merely our notation to help us to work out the solution.\nThe \\(h_2^{(t)}\\) is 1 iff the sum \\(S^{(t)} \\geq 2\\),\nand \\(h_3^{(t)}\\) is 1 iff the sum \\(S^{(t)}\\) is 3.\n\nNotice that the carry \\(c^{(t-1)}\\) is going to be 1 iff \\(h_2^{(t-1)}=1\\) and 0 otherwise, i.e. when the previous addition was 2 or 3. Therefore to compute \\(h_i^{(t)}\\) we need to first compute the sum \\(S^{(t)} = x_1^{(t)} + x_2^{(t)} + h_2^{(t-1)}\\) and then offset it by \\(-i+1\\) so that after applying the hard threshold function we get the desired value as specified above. This can be achieved with the following set of parameters: \\[\n\\mathbf{U}= \\begin{bmatrix}\n    1 & 1 \\\\\n    1 & 1 \\\\\n    1 & 1 \\end{bmatrix},\\quad\n\\mathbf{W}=\\begin{bmatrix}\n    0 & 1 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 1 & 0\\end{bmatrix},\\quad\n\\mathbf{b_h}= \\begin{bmatrix}\n    -0.5 \\\\\n    -1.5 \\\\\n    -2.5 \\end{bmatrix}.\n\\]\nTo compute the output \\(y^{(t)}\\) we need to check if the \\(S^{(t)}\\) is 1 or 3, that is, if either \\(h_1^{(t)} = 1\\) while all other hidden units are zero or all hidden units are 1. We can accomplish this by setting: \\(\\mathbf{v}=\\begin{bmatrix} 1, -1, 1 \\end{bmatrix}\\) and \\(b_y = -0.5\\)."
  },
  {
    "objectID": "ex/w06/exercises06.html",
    "href": "ex/w06/exercises06.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/prob-variance.md"
  },
  {
    "objectID": "ex/w06/exercises06.html#exercise-1---variance",
    "href": "ex/w06/exercises06.html#exercise-1---variance",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/prob-variance.md"
  },
  {
    "objectID": "ex/w06/exercises06.html#exercise-2---variance-bias-decomposistion",
    "href": "ex/w06/exercises06.html#exercise-2---variance-bias-decomposistion",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Variance / Bias Decomposistion",
    "text": "Exercise 2 - Variance / Bias Decomposistion\n!include questions/ml-variance_bias_decomposition.md"
  },
  {
    "objectID": "ex/w06/exercises06.html#exercise-3---ensembling",
    "href": "ex/w06/exercises06.html#exercise-3---ensembling",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Ensembling",
    "text": "Exercise 3 - Ensembling\nDownload the file exercises06-ensembling.ipynb from quercus. It contains basic Pytorch code training a classifier on MNIST. Modify that code such that it trains an ensemble of 5-10 neural networks and computes their average prediction once trained."
  },
  {
    "objectID": "ex/w06/exercises06_solution.html",
    "href": "ex/w06/exercises06_solution.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/prob-variance.md\n\n\n!include questions/prob-variance-sol.md"
  },
  {
    "objectID": "ex/w06/exercises06_solution.html#exercise-1---variance",
    "href": "ex/w06/exercises06_solution.html#exercise-1---variance",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "!include questions/prob-variance.md\n\n\n!include questions/prob-variance-sol.md"
  },
  {
    "objectID": "ex/w06/exercises06_solution.html#exercise-2---variance-bias-decomposistion",
    "href": "ex/w06/exercises06_solution.html#exercise-2---variance-bias-decomposistion",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 2 - Variance / Bias Decomposistion",
    "text": "Exercise 2 - Variance / Bias Decomposistion\n!include questions/ml-variance_bias_decomposition.md\n\nSolution\n!include questions/ml-variance_bias_decomposition-sol.md"
  },
  {
    "objectID": "ex/w06/exercises06_solution.html#exercise-3---ensembling",
    "href": "ex/w06/exercises06_solution.html#exercise-3---ensembling",
    "title": "CSC413 - Fall 2024",
    "section": "Exercise 3 - Ensembling",
    "text": "Exercise 3 - Ensembling\nDownload the file ex06-ensembling.ipynb from quercus. It contains basic Pytorch code training a classifier on MNIST. Modify that code such that it trains an ensemble of 5-10 neural networks and computes their average prediction once trained."
  },
  {
    "objectID": "ex/w06/questions/prob-variance.html",
    "href": "ex/w06/questions/prob-variance.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]"
  },
  {
    "objectID": "ex/w06/questions/ml-variance_bias_decomposition-sol.html",
    "href": "ex/w06/questions/ml-variance_bias_decomposition-sol.html",
    "title": "CSC413 - Fall 2024",
    "section": "",
    "text": "First, we reformulate (1) as \\[\\begin{align*}\n  \\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n&= \\E_{D,x,y}\\li[\\li[\\li(h_{D}(x) - \\hh(x)\\ri) + \\li(\\hh(x) - y\\ri)\\ri]^{2}\\ri] \\nonumber \\\\\n&= \\E_{x, D}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y, D} \\li[\\li(h_{D}(x) - \\hh(x)\\ri)\\li(\\hh(x) - y\\ri)\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri]\n\\end{align*}\\] Next, we note that the second term in the above equation is zero because \\[\\begin{align*}\n\\E_{D,x, y} \\li[\\li(h_{D}(x) - \\hh(x)\\ri) \\li(\\hh(x) - y\\ri)\\ri] &= \\E_{x, y} \\li[\\E_{D} \\li[ h_{D}(x) - \\hh(x)\\ri] \\li(\\hh(x) - y\\ri) \\ri] \\\\\n&= \\E_{x, y} \\li[ \\li( \\E_{D} \\li[ h_{D}(x) \\ri] - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ \\li(\\hh(x) - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ 0 \\ri] \\\\\n&= 0\\ .\n\\end{align*}\\]\nThe proof here, is similar. We start by reformulating the second term in (2) as \\[\\begin{align*}\n\\E_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri] &= \\E_{x, y} \\li[ \\li(\\hh(x) -\\bar y(x) )+(\\bar y(x) - y \\ri)^{2}\\ri]  \\\\\n  &=\\E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + \\E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y} \\li[ \\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri]\n  \\end{align*}\\] Here, the third term is zero which follows from an analogous derivation as in (a). Thus, we have \\[\\begin{align*}\n\\E_{x, y} \\li[\\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri] &= \\E_{x}\\li[\\E_{y \\mid x} \\li[\\hy(x) - y \\ri] \\li(\\hh(x) - \\hy(x) \\ri) \\ri] \\\\\n&= \\E_{x} \\li[ \\E_{y \\mid x} \\li[ \\hy(x) - y\\ri] \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\E_{y \\mid x} \\li [ y \\ri]\\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\hy(x) \\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ 0 \\ri] \\\\\n&= 0\n  \\end{align*}\\]"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Fall 2024 - CSC 2626: Imitation Learning for Robotics",
    "section": "",
    "text": "Course Overview\nIn the next few decades we are going to witness millions of people, from various backgrounds and levels of technical expertise, needing to effectively interact with robotic technologies on a daily basis. As such, people will need to modify the behavior of their robots without explicitly writing code, but by providing only a small number of kinesthetic or visual demonstrations, or even natural language commands. At the same time, robots should try to infer and predict the human’s intentions and internal objectives from past interactions, in order to provide assistance before it is explicitly asked. This graduate-level course will examine some of the most important papers in imitation learning for robot control, placing more emphasis on developments in the last 10 years. Its purpose is to familiarize students with the frontiers of this research area, to help them identify open problems, and to enable them to make a research contribution.\nThis course will broadly cover the following areas:\n\nImitating the policies of demonstrators (people, expensive algorithms, optimal controllers)\nConnections between imitation learning, optimal control, and reinforcement learning\nLearning the cost functions that best explain a set of demonstrations\nShared autonomy between humans and robots for real-time control\n\n\n\nPrerequisites\nYou need to be comfortable with: introductory machine learning concepts (such as from CSC411/CSC413/ECE521 or equivalent), linear algebra, basic multivariable calculus, intro to probability. You also need to have strong programming skills in Python. Note: if you don’t meet all the prerequisites above please contact the instructor by email. Optional, but recommended: experience with neural networks, such as from CSC321, introductory-level familiarity with reinforcement learning and control.\n\n\nCourse Delivery Details\n\nLectures: In-person, Mondays @ 1pm-4pm ET, Carr Hall 404\nAnnouncements will be posted on Quercus\nDiscussions will take place on Piazza\nZoom recordings will be posted on Quercus after lectures\nAnonymous feedback form for suggested improvements"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Instructor\n\nFlorian Shkurti is an assistant professor in computer science at the University of Toronto, where he leads the Robot Vision and Learning lab. He is a faculty member of the University of Toronto Robotics Institute, the Acceleration Consortium, and a faculty affiliate at Vector Institute. His research group develops methods that enable robots to learn to perceive, reason, plan, and act effectively and safely, particularly in dynamic environments and alongside humans. Application areas of his research include field robotics for environmental monitoring, visual navigation for autonomous vehicles, and mobile manipulation.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMondays 10:00-11:00am ET\nDH3066\n\n\n\n\nIgor Gilitschenski is an assistant professor in computer science at the University of Toronto, where he leads the Toronto Intelligent Systems Lab. His work focuses on developing novel probabilistic and learning-based techniques for robotic perception and decision-making with the ultimate goal of enabling robust interactive autonomy.\n\n\n\nOffice hours\nLocation\n\n\n\n\nTuesdays 7:00-8:00pm ET\nDH3070\n\n\n\n\n\nTeaching Assistants\n\n\n\n\n\n\n\n\n\n\n\nAidan Li\nVivian Chu\nJasper Gerigk\nNavid Hasanzadeh\nLucas Jin\nEnsieh Khazaei\n\n\n\n\n\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday 11-12pm ET\n(Online) Zoom\n\n\nThursday 2-3pm ET\n(Online) Zoom",
    "crumbs": [
      "Teaching Staff"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation 4 and Equation 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation 4 using Equation 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From Equation 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Equation 12 below is mathematically equivalent to Equation 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.1\n✔ parsnip      1.3.2     ✔ yardstick    1.3.2\n✔ recipes      1.3.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 × 9\n   .rownames body_mass_g flipper_length_mm .fitted  .resid    .hat .sigma\n   &lt;chr&gt;           &lt;int&gt;             &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 1                3750               181   3212.  538.   0.00881   394.\n 2 2                3800               186   3461.  339.   0.00622   394.\n 3 3                3250               195   3908. -658.   0.00344   393.\n 4 5                3450               193   3808. -358.   0.00385   394.\n 5 6                3650               190   3659.   -9.43 0.00469   395.\n 6 7                3625               181   3212.  413.   0.00881   394.\n 7 8                4675               195   3908.  767.   0.00344   393.\n 8 9                3475               193   3808. -333.   0.00385   394.\n 9 10               4250               190   3659.  591.   0.00469   394.\n10 11               3300               186   3461. -161.   0.00622   395.\n# ℹ 332 more rows\n# ℹ 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "Fall 2024 - CSC 413: Neural Networks and Deep Learning",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\nNotebooks\nLabs\nAssignments\nTests\nProject\n\n\n\n\n1\nSept 2-6\nLinear Model\n📖\n🖥️\n📋\n\n\n\n\n\n\n2\nSept 9-13\nMulti-Layer Perceptron, Backpropagation, Losses\n📖\n🖥️\n📋\n💻\n\n\n\n\n\n3\nSept 16-20\nAutomatic Differentiation, Distributed Representation\n📖\n🖥️\n📋\n💻\n\n\n\n\n\n4\nSept 23-27\nConvolutional Neural Networks\n📖\n🖥️\n📋\n💻\n✍️\n\n\n\n\n5\nOct 1-4\nOptimization\n📖\n🖥️\n📋\n💻\n\n\n\n\n\n6\nOct 7-11\nGeneralization\n📖\n🖥️\n📋\n💻\n🔜\n\n\n\n\n7\nOct 14-18\nInterpreting Convolutional Neural Networks\n📖\n🖥️\n📋\n\n✍️\n✅\n\n\n\n8\nOct 21-25\nRecurrent Neural Networks\n📖\n🖥️\n📋\n💻\n\n\n\n\n\n9\nOct 28-Nov 1\nFall Reading Week\n\n\n\n\n🔜\n\n\n\n\n10\nNov 4-8\nAttention and Transformers\n📖\n🖥️\n📋\n💻\n\n\n📂\n\n\n11\nNov 11-15\nAutoencoders and Transpose Convolutions\n📖\n🖥️\n📋\n\n\n✅\n\n\n\n12\nNov 18-22\nGenerative Adversarial Networks\n📖\n🖥️\n📋\n💻\n\n\n\n\n\n13\nNov 25-29\nUse of Deep Learning in the Research of Profs Shkurti and Gilitschenski\n\n🖥️\n\n💻\n\n\n\n\n\n14\nDec 2-6\nFinal project submission\n\n\n\n\n\n\n📂\n\n\n\n\n\nWeek 1\nPrepare\nSoftware Installations\nTutorials/Labs\n\nNo tutorials/labs this week but please complete the Pre-Requisite Math Problems\nSuggested Review :\n\nLinear Algebra reviewLinks to an external site. \nProbability reviewLinks to an external site. \n\n\n\n\nWeek 2\nNotes\n\nProf. Roger Grosse’s notes on backdrop\n\nExercises\n\nIn-class Exercise\nSolutions\n\nTutorials/Labs\n\nIn-person lab session: PyTorch basics with linear models lab01\n\n\n\nWeek 3\nNotes\n\nGloVe embedding demoLinks to an external site. \nVideo on autodiffLinks to an external site.\n\nExercises\n\nIn-class Exercise\nSolutions\n\nAdditional Resources\n\nProf. Roger Grosse’s notes on autodiff and word embeddings\nNotes on Backpropagation https://cs231n.github.io/optimization-2/Links to an external site.\nAutomatic Differentiation in Machine Learning: a Survey (2018) https://arxiv.org/pdf/1502.05767.pdf\n\nTutorials/Labs\n\nIn-person lab session: numerical gradients / word embeddings lab02\n\n\n\nWeek 4\n\nDemo: Colab Notebook\n\nExercises\n\nIn-class Exercise\nSolutions\n\nTutorials/Labs\n\nIn-person lab session: classification and medical MNIST lab03\n\n\n\nWeek 5\nExercises\n\nIn-class Exercise\nSolutions\n\nAdditional Resources\n\nTaylor Series https://www.youtube.com/watch?v=3d6DsjIBzJ4\n\nTutorials/Labs\n\nTutorial: how to implement SGD with momentum neural network optimization\n\n\n\nWeek 6\n\nEnsembling code skeleton\nThe definition of differential privacy\n\nExercises\n\nIn-class Exercise\nSolutions\n\nTutorials/Labs\n\nIn-person lab session: optimization and differential privacy lab05\n\n\n\nWeek 7\n\nTroubleshooting Deep Neural Networks\n\nExercises\n\nIn-class Exercise\nSolutions\n\nTutorials/Labs6 - In-person midterm #1 which covers weeks 1-5. Held during the tutorial / lab sessions\n\n\nWeek 8\nExercises\n\nIn-class Exercise\nSolutions\n\nTutorials/Labs7 - In-person lab session: transfer learning and double descent lab07\n\n\nWeek 9\nFall reading week. No lecture.\n\n\nWeek 10\nExercises\n\nIn-class Exercise\nSolutions\n\nAdditional Resources\n\nTransformer video:https://www.youtube.com/watch?v=XSSTuhyAmnI\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html\nhttp://peterbloem.nl/blog/transformers\nLast year’s A3 (a seq2seq autoencoder) http://modelai.gettysburg.edu/2021/headlines/\n\nTutorials/Labs\n\nIn-person lab session: gradcam and input gradients lab09\n\n\n\nWeek 11\n\nAutoencoder Notebook\n\nExercises\n\nIn-class Exercise\nSolutions\n\nTutorials/Labs\n\nIn-person midterm #2 which covers weeks 6-9. Held during the tutorial / lab sessions\n\n\n\nWeek 12\nExercises\n\nIn-class Exercise\nSolutions\n\nTutorials/Labs\n\nIn-person lab session: RNN text classification lab10\n\n\n\nWeek 13\nTutorials/Labs\n\nIn-person lab session: text generation with transformers lab11\n\n\n\nWeek 14\nFinal project submission",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "The project in this course is an opportunity to develop deep learning application in an area of your own choosing. It also provides the chance to complete a deep learning project that is much closer to a real-world application area, for example in medicine, finance, robotics, commerce, biology, chemistry, physics (or other sciences), social media, or other fields.\nWhile this project has some structure, you will be required to deal with the ambiguity and significant decision making that make up the life of a deep learning practitioner.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Final Project Guidelines",
    "section": "Project Proposal",
    "text": "Project Proposal\n\nLogistics\nProjects must be done in groups of 3-4. Please form groups on Markus by March 17, 10pm. Exceptions to this rule can be made only in rare cases provided there is good reason to do so. Email the instructors if this applies to you. If you do not know anyone in class feel free to post a message on Piazza. We will also set aside some time during the tutorial for students who are looking for collaborators to find each other and discuss forming a group.\nA 1-2 page project proposal is due March 21, 10pm. You will also be asked to summarize the data set that you are using for this proposal.\nEach team will submit a github repository page that describes the deep learning model built in the project. The repository should also contain the code that you wrote.\n\n\nProject Requirements\nBy default, your project must either take a sequence (of variable length) as an input, or produce a sequence as an output, or both. If you have a project proposal that does not involve sequences, please contact the instructors.\nYour model should thus involve an RNN or a Transformer component. Students who want to use methods that we have not covered in the course (e.g. diffusion models, neural ODEs) are free to do so, as long as they confirm their methodology with the instructors before they submit this project proposal. There is also flexibility for students to pursue an open research problem. If any groups want to attempt this, they need to discuss this with one of the instructors before the prject proposal deadline.\nHere are some examples of possible projects:\n\nUsing an RNN (or transformer) to classify sequences (e.g. whether a restaurant review is positive or negative)\nUsing a generative RNN to produce sequences (e.g. South Park TV scripts)\nUsing a Siamese network to determine whether two StackOverflow questions are duplicates\nPredict the next item in a sequence (e.g. Stock market)\nPredict the outcome of a patient based on some sequential factors\nPredict the dynamics of objects under contact and collision (e.g. robotics and graphics)\nGenerate molecules, or predict properties of molecules\n\nBefore choosing a project, consider whether there is data available for you. Since the project deadline is about a month away, consider tailoring your project ideas to what data is available to you.\nYou are encouraged to use transfer learning and data augmentation ideas in your project.\nYou can use deep learning packages (e.g. pytorch, huggingface). However, you should be able to explain the steps involved in the forward pass computation of your model.\n\n\nProject Proposal\nA 1-2 page project proposal is due March 21, 10pm. Please use 12-point font and standard margins. You will also be asked to summarize the data set that you are using for this proposal.\nThe proposal should:\n\nClearly describe the task that your model will perform. (2pt)\n\n2/2 for clearly describing the task using standard deep learning terminology\n1.5/2 for describing the task in a way that is understandable to the grader, but that uses non-standard terminology\n1/2 for describing the task generally (e.g. “sequence classification” without stating the exact classes)\n0/2 for a proposal that does not align with the project requirements\n\nClearly describe the model that you intend to use (2pt)\n\n2/2 for clearly describing the model using standard deep learning terminology; the grader can picture exactly how the model could be used.\n1.5/2 for describing the task in a way that is understandable to the grader, but that uses non-standard terminology\n1/2 for describing the models generally (e.g. sequence-to-sequence model, without describing which ones)\n0/2 for a model that does not align with the project requirements\n\nOutline the data set that you intend to use, and provide some statistics about the amount/type of data that is available (4pt)\n\n1 point for convincing the grader that you are able to acquire the data that you need (with the appropriate license/permission for educational use)\n1 point for convincing the grader that the type and amount of data is sufficient (e.g. via summary statistics, examples data set)\n2 points for convincing the grader that you have explored the data, and considered information about your data relevant to your model (like in A1 Q1)\n\nDiscuss any ethical implications of your model—how might the use (or misuse) of this model help or hurt people? (2pt)\n\n2/2 For a thoughtful discussion that considers the ethical implications across many groups of people (that different groups may be impacted differently).\n1/2 For a discussion that is generic, or considers the ethical implications for only one group of people.\n\nDescribe how work will be divided amongst the team members. We recommend pair-coding for parts of the project, but consider the work that it might take to load/format your data, write a first model, “overfit” to a single data point, etc… (2pt)\n\n2/2 The description provides enough detail so that if a team member is replaced, they know exactly what their responsibilities will be.\n1/2 There is clearly an attempt to describe the division of tasks, but the communication is unclear and/or only the tasks listed above are assigned.\n0/2 Only vague assertions are made (e.g. “we will divide the work equally”, “everyone will work on everything”, or “we will determine who will work on what as the project progresses).\n\nProper formatting (2pt)\n\n2/2 Proposal is 1-2 pages. The proposal is formatted so that readers can find specific information quickly (e.g. via the use of paragraphs and topic sentences)\n1/2 Proposal is slightly over the length limit. There was clearly an attempt to format the proposal, but information is still scattered in various places.\n0/2 Proposal runs extremely long. It is difficult to understand the structure of the proposal.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "project-description.html#final-project",
    "href": "project-description.html#final-project",
    "title": "Final Project Guidelines",
    "section": "Final Project",
    "text": "Final Project\n\nSubmission\nPlease submit a file called github.txt containing a link to the github repository. If your repository will be private, please email the instructors by April 7, 10pm so that TAs and instructors can be added—even if you use tokens.\n\n\nRepository Content\nThe repository should contain:\n\nThe code you used to pre-process the data, but not the data itself. It is generally a bad idea to include data in your github repository, since git is great for lots of small files, but a poor choice for sharing large files. Moreover, most groups are using data collected by other people. While you should share the source of your data, you should generally not share a copy of the data.\nThe code you used to train your model. You may opt to share model weights, or not.\nA README file with the following component:\n\n\nIntroduction that states the deep learning model that you are building\nModel:\n\nA figure/diagram of the model architecture that demonstrates understanding of the steps involved in computing the forward pass\nCount the number of parameters in the model, and a description of where the parameters come from\nExamples of how the model performs on two actual examples from the test set: one successful and one unsuccessful\n\nData:\n\nDescribe the source of your data\nProvide summary statistics of your data to help interpret your results (similar to in the proposal)\nDescribe how you transformed the data (e.g. any data augmentation techniques)\nIf appropriate to your project, describe how the train/validation/test set was split. (Note that splitting the training/validation/test set is not always straightforward!)\n\nTraining:\n\nThe training curve of your final model\nA description how you tuned hyper-parameters\n\nResults:\n\nDescribe the quantitative measure that you are using to evaluate your result\nDescribe the quantitative and qualitative results\nA justification that your implemented method performed reasonably, given the difficulty of the problem—or a hypothesis for why it doesn’t (this is extremely important)\n\nEthical Consideration:\n\nDescription of a use of the system that could give rise to ethical issues. Are there limitations of your model? Your training data?\n\nAuthors\n\nA description of how the work was split—i.e. who did what in this project.\n\n\n\n\nMarking Scheme\nHere is the marking scheme that we will use. Note that you model must be able to make reasonable predictions for your project to receive a passing project grade. In particular, without a reasonable model, you won’t be able to earn credit for Model Examples, Training Curve, Hyperparameter Tuning, Qualitative/Quantitative Results, etc.\nREADME/Writeup (70 points)\n\nIntroduction (4 points): What deep learning model are you building? We are looking for a clear and concise description that uses standard deep learning terminology. Clearly describe the type of task that you are solving, and what your input/outputs are.\nModel Figure (4 points): A figure/diagram of the model architecture that demonstrates understanding of the steps involved in computing the forward pass. We are looking to see if you understand the steps involved in the model computation (i.e. are you treating the model as a black box or do you understand what it’s doing?)\nModel Parameters(4 points): Count the number of parameters in the model, and a description of where the parameters come from. Again, we are looking to see if you understand what the model is doing, and what parameters are being tuned.\nModel Examples (4 points): Examples of how the model performs on two actual examples from the test set: one successful and one unsuccessful.\nData Source (1 point): Describe the source of your data.\nData Summary (4 points): Provide summary statistics of your data to help interpret your results, similar to in the proposal. Please review the feedback provided in the proposal for some guidance on what information is helpful for interpreting your model behaviour.\nData Transformation (3 points): Describe how you transformed the data, i.e. the steps you took to turn the data from what you downloaded, to something that a neural network can use as input. We are looking for a concise description that has just enough information for another person to replicate your process.\nData Split (2 points): If appropriate to your project, describe how the train/validation/test set was split. Note that splitting strategy is not always straightforward, so we are looking to see a split that can be justified.\nTraining Curve (4 points): The training curve of your final model. We are looking for a curve that shows both training and validation performance (if applicable). Your training curve should look reasonable for the problem that you are solving.\nHyperparamter Tuning (4 points): A description how you tuned hyper-parameters. We are looking for hyperparameter choices that makes sense.\nQuantitative Measures (2 points): A description and justification of the quantitative measure that you are using to evaluate your results. For some problems this will be straightforward. For others, please justify the measure that you chose.\nQuantitative and Qualitative Results (8 points): Describe the quantitative and qualitative results. You may choose to use a table or figure to aid in your description. We are looking for both a clear presentation, and a result that makes sense given your data summary. (As an extreme example, you should not have a result that performs worse than a model that, say, predicts the most common class.)\nJustification of Results (20 points): A justification that your implemented method performed reasonably, given the difficulty of the problem—or a hypothesis for why it doesn’t. This is extremely important. We are looking for an interpretation of the result. You may want to refer to your data summary and hyperparameter choices to make your argument.\nEthical Consideration (4 points): Description of a use of the system that could give rise to ethical issues. Are there limitations of your model? Your training data? Please review the feedback provided in the proposal for some guidance on how to think deeply about these issues.\nAuthors (2 points): A description of how the work was split—i.e. who did what in this project. If there are significant issues with the way that work is split, we may follow up with individual teams, and not award equal points to all team members.\n\nCode/Documentation (20 points) We are looking for whether TAs can generally understand what your code does, how it is organized, and the steps that needs to be taken to replicate your model and results. Your code must be in working order (otherwise the TA will not be able to replicate your results)\nAdvanced Concept (10 points). Your project involves at least one of the following:\n\nData Augmentation applied in a way that makes sense for your domain\nTransformer\nGenerative Model, Sequence-to-Sequence Architecture (e.g. that uses teacher-forcing)",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you’re having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you’ll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course forum to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. They’ll be able to help diagnose the issue."
  }
]