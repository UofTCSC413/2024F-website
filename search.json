[
  {
    "objectID": "course-grading.html",
    "href": "course-grading.html",
    "title": "Grading Scheme",
    "section": "",
    "text": "Assignment 1\n15%\n\n\nAssignment 2\n15%\n\n\nPanel Discussion\n10%\n\n\nProject Proposal\n10%\n\n\nProject Presentation\n25%\n\n\nFinal Project Report\n25%\n\n\n\n\nMarking rubric for panel discussion\nEvery week we will reserve 20-30 mins for a panel discussion based on the assigned reading for that day (4-5 papers). This discussion will include three types of roles: panel members, audience members, and a moderator. Each panel discussion will include 4 panel members, 1 moderator, and audience members. Panel members are responsible for answering questions, the audience is responsible for asking questions, and the moderator is responsible for steering the discussion and having backup questions if the audience is not asking any.\n\nPanel member evaluation\n\nAnswering questions from the moderator and the audience correctly / well (6 pts)\nEngaging with points of other panelists (1 pts)\nKeeping answers brief / allowing other people time to speak (2 pts)\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (1 pts)\n\n\n\nAudience member evaluation\n\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (10 pts)\n\n\n\nModerator evaluation\n\nSteering the discussion in terms of groups / themes of questions (2 pts)\nEnsuring there is time for every panel member to speak (4 pts)\nEngaging the audience / ensuring the audience has enough time to ask questions (3 pts)\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (1 pts)\n\n\n\n\nMarking rubric for the project proposal\n\nIntroduction (1 pts), which states the proposed problem being solved and any applications / implications.\nFigure or diagram (1 pts), showing an overview of your proposed solution, i.e. shows the overall idea in a way that is easily understandable without even reading the rest of the report.\nRelated work (1 pts) and bibliography. Highlight how your method is different from other approaches. Present other approaches in the proper light without diminishing their contributions.\nMethodology (2 pts). Describe your proposed methodology as well as any assumptions it relies on. Explain prerequisite concepts clearly and succinctly.\nEvaluation (2 pts). What experiments are you planning to do and why? What are the questions you want to answer through these experiments?\nTimeline (1 pts). What are the milestones required to complete your project and by when do you plan to complete them?\nAnticipated risks and mitigation plan (2 pts). What issues might arise with your proposed project and timeline and how will you address these issues if they occur?\n\n\n\nMarking rubric for the project presentation\n\nQuality of presentation\n\nSlide design (2 pts)\nDelivery of presentation (3 pts)\nRespecting time constraints (2 pts)\nResponse to questions (3 pts)\n\n\n\nTechnical content\n\nMotivation and definition of the problem (2 pts)\nPutting prior work into context (3 pts)\nMethodology explanation (3 pts)\nDiscussion of experiments (5 pts)\nDiscussion of limitations (2 pts)\n\n\n\n\nMarking rubric for the final project report\n\nAbstract (2 pts) that summarizes the main idea of the project and your contributions.\nIntroduction (3 pts) that states the problem being solved and any applications / implications.\nFigure or diagram (2 pts) that shows the overall idea in a way that is easily understandable.\nRelated work (2 pts) and bibliography. Highlight how your method is different from other approaches. Present other approaches in the proper light without diminishing their contributions.\nMethodology (7 pts). Describe your method in detail as well as any assumptions it relies on. Explain prerequisite concepts clearly and succinctly. Include algorithm descriptions, figures, and equations as you wish.\nEvaluation (8 pts). Include any figures or tables that illustrate your experimental results. Do not forget to include error bars if applicable. Analyze your findings, and comment on their statistical significance. In your evaluation please take into account Joelle Pineau’s ML reproducibility checklist.\nLimitations (2 pts). Describe some settings in which your approach performs poorly, and list a few ideas for how to adddress them. Describe opportunities for future work, as well as open problems.\nConclusions (1 pts). A summary of your contributions and results."
  },
  {
    "objectID": "lecs/w06/lec06.html#last-week",
    "href": "lecs/w06/lec06.html#last-week",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Last Week",
    "text": "Last Week\n\nOptimization Landscape\nGradient Descent and SGD"
  },
  {
    "objectID": "lecs/w06/lec06.html#this-week",
    "href": "lecs/w06/lec06.html#this-week",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "This week",
    "text": "This week\n\nGeneralization\nDiagnosing Issues using Learning Curves\nBias-Variance Tradeoff\nDifferential Privacy"
  },
  {
    "objectID": "lecs/w06/lec06.html#questions",
    "href": "lecs/w06/lec06.html#questions",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Questions",
    "text": "Questions\n\n\nHow do we choose between different neural network models?\n\nFor example, different number of hidden units, hidden layers, …?\n\n\n\n\n\nHow do we know how well a model will perform on new data?"
  },
  {
    "objectID": "lecs/w06/lec06.html#the-training-set",
    "href": "lecs/w06/lec06.html#the-training-set",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Training Set",
    "text": "The Training Set\nThe training set is used\n\n\nto determine the value of the parameters\n\n\n\nThe model’s prediction accuracy over the training set is called the training accuracy."
  },
  {
    "objectID": "lecs/w06/lec06.html#the-training-set-ii",
    "href": "lecs/w06/lec06.html#the-training-set-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Training Set II",
    "text": "The Training Set II\nQ: Can we use the training accuracy to estimate how well a model will perform on new data?\n\n\nNo! It is possible for a model to fit well to the training set, but fail to generalize\n\n\n\n\nWe want to know how well the model performs on new data that we didn’t already use to optimize the model"
  },
  {
    "objectID": "lecs/w06/lec06.html#poor-generalization",
    "href": "lecs/w06/lec06.html#poor-generalization",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Poor Generalization",
    "text": "Poor Generalization\n\n \n\n\nExample of poor generalization: - green: data generating process / red: fit. - Plot 1: datapoints fitted poorly. - Plot 2: datapoints fitted overly well."
  },
  {
    "objectID": "lecs/w06/lec06.html#underfitting",
    "href": "lecs/w06/lec06.html#underfitting",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Underfitting",
    "text": "Underfitting\n\nThe model is simple and doesn’t fit the data\n\n\n\nThe model does not capture discriminative features of the data"
  },
  {
    "objectID": "lecs/w06/lec06.html#overfitting",
    "href": "lecs/w06/lec06.html#overfitting",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Overfitting",
    "text": "Overfitting\n\nThe model is too complex and does not generalize\n\n\n\nThe model captures information about patterns in training set that happened by chance\n\ne.g. Ringo happens to be always wearing a red shirt in the training set\nModel learns: high red pixel content =&gt; predict Ringo"
  },
  {
    "objectID": "lecs/w06/lec06.html#the-test-set",
    "href": "lecs/w06/lec06.html#the-test-set",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Test Set",
    "text": "The Test Set\nWe set aside a test set of labeled examples.\n\nThe model’s prediction accuracy over the test set is called the test accuracy.\n\n\nThe purpose of the test set is to give us a good estimate of how well a model will perform on new data.\n\n\nQ: In general, will the test accuracy be higher or lower than the training accuracy?\n\n\nThe test accuracy will be lower because the model can “memorize” the data."
  },
  {
    "objectID": "lecs/w06/lec06.html#model-choices",
    "href": "lecs/w06/lec06.html#model-choices",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Model Choices",
    "text": "Model Choices\nBut what about decisions like:\n\n\nHow many layers?\n\n\n\n\nHow many units in each layer?\n\n\n\n\nWhat non-linear activation to use?"
  },
  {
    "objectID": "lecs/w06/lec06.html#model-choices-ii",
    "href": "lecs/w06/lec06.html#model-choices-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Model Choices II",
    "text": "Model Choices II\nQ: Why can’t we use the test set to determine which model we should deploy?\n\n\nIf we use the test set to make modeling decisions, then we will overestimate how well our model will perform on new data!\n\n\n\n\nWe are “cheating” by “looking at the test”"
  },
  {
    "objectID": "lecs/w06/lec06.html#the-validation-set",
    "href": "lecs/w06/lec06.html#the-validation-set",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Validation Set",
    "text": "The Validation Set\nWe therefore need a third set of labeled data called the validation set\nThe model’s prediction accuracy over the validation set is called the validation accuracy."
  },
  {
    "objectID": "lecs/w06/lec06.html#the-validation-set-ii",
    "href": "lecs/w06/lec06.html#the-validation-set-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Validation Set II",
    "text": "The Validation Set II\nThis dataset is used to:\n\n\nMake decisions about models that is not continuous and can’t be optimized via gradient descent\n\n\n\n\nExample: choose \\(k\\), choose which features \\(x_j\\) to use, choose \\(\\alpha\\), …\n\nThese model settings are called hyperparameters\n\n\n\n\n\nThe validation set is used to optimize hyperparameters"
  },
  {
    "objectID": "lecs/w06/lec06.html#splitting-the-dataset",
    "href": "lecs/w06/lec06.html#splitting-the-dataset",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Splitting the Dataset",
    "text": "Splitting the Dataset\nExample split:\n\n\n60% Training\n\n\n\n\n20% Validation\n\n\n\n\n20% Test\n\n\n\nThe actual split depends on the amount of data that you have.\nIf you have more data, you can get a way with a smaller % validation and set."
  },
  {
    "objectID": "lecs/w06/lec06.html#detecting-overfitting",
    "href": "lecs/w06/lec06.html#detecting-overfitting",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Detecting Overfitting",
    "text": "Detecting Overfitting\nLearning curve:\n\n\n\n\n\nx-axis: epochs or iterations\ny-axis: cost, error, or accuracy\n\n\n\nQ: In which epochs is the model overfitting? Underfitting?\n\n\nQ: Why don’t we plot the test accuracy plot?\n\n\nUnderfitting: the first ~8 epochs and overfitting after that\nTest accuracy is only supposed to be used for the evaluation of the final trained model and not for any design decisions such as number of epochs."
  },
  {
    "objectID": "lecs/w06/lec06.html#strategies-to-prevent-overfitting",
    "href": "lecs/w06/lec06.html#strategies-to-prevent-overfitting",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Strategies to Prevent Overfitting",
    "text": "Strategies to Prevent Overfitting\n\n\nData Augmentation\n\n\n\n\nReducing the number of parameters\n\n\n\n\nWeight decay\n\n\n\n\nEarly stopping\n\n\n\n\nEnsembles\n\n\n\n\nStochastic regularization (e.g. dropout)"
  },
  {
    "objectID": "lecs/w06/lec06.html#data-augmentation",
    "href": "lecs/w06/lec06.html#data-augmentation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nThe best way to improve generalization is to collect more data!\n\nBut if we already have all the data we’re willing to collect. We can augment the training data by transforming the examples.\n\n\nThis is called data augmentation."
  },
  {
    "objectID": "lecs/w06/lec06.html#data-augmentation-ii",
    "href": "lecs/w06/lec06.html#data-augmentation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Data Augmentation II",
    "text": "Data Augmentation II\nExamples (for images, but depends on task):\n\n\ntranslation / rotation\n\n\n\n\nhorizontal or vertical flip\n\n\n\n\nsmooth warping\n\n\n\n\nnoise (e.g. flip random pixels)\n\nWe should only augment the training examples, not the validation or test examples (why?)"
  },
  {
    "objectID": "lecs/w06/lec06.html#reducing-the-number-of-parameters",
    "href": "lecs/w06/lec06.html#reducing-the-number-of-parameters",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Reducing the Number of Parameters",
    "text": "Reducing the Number of Parameters\n\nNetworks with fewer trainable parameters are less likely to overfit.\n\n\n\nWe can reduce the number of layers, or the number of parameters per layer."
  },
  {
    "objectID": "lecs/w06/lec06.html#reducing-the-number-of-parameters-ii",
    "href": "lecs/w06/lec06.html#reducing-the-number-of-parameters-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Reducing the Number of Parameters II",
    "text": "Reducing the Number of Parameters II\nAdding a bottleneck layer is another way to reduce the number of parameters\n\n\n\n\nIn practise, this isn’t a great idea (as too much informaiton may get lost)."
  },
  {
    "objectID": "lecs/w06/lec06.html#weight-decay-idea",
    "href": "lecs/w06/lec06.html#weight-decay-idea",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weight Decay Idea",
    "text": "Weight Decay Idea\nIdea: Penalize large weights, by adding a term (e.g. \\(\\sum_k w_k ^ 2\\)) to the cost function\nQ: Why is it not ideal to have large (absolute value) weights?\n\nBecause large weights mean that the prediction relies a lot on the content of one feature (e.g. one pixel)"
  },
  {
    "objectID": "lecs/w06/lec06.html#small-vs-large-weights",
    "href": "lecs/w06/lec06.html#small-vs-large-weights",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Small vs Large Weights",
    "text": "Small vs Large Weights\nThe red polynomial overfits. Notice it has really large coefficients"
  },
  {
    "objectID": "lecs/w06/lec06.html#weight-decay",
    "href": "lecs/w06/lec06.html#weight-decay",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weight Decay",
    "text": "Weight Decay\n\n\n\\(L^1\\) regularization: add a term \\(\\sum_{j=1}^D |w_j|\\) to the cost function\n\nMathematically, this term encourages weights to be exactly 0\n\n\n\n\n\n\\(L^2\\) regularization: add a term \\(\\sum_{j=1}^D w_j^2\\) to the cost function\n\nMathematically, in each iteration the weight is pushed towards 0\n\n\n\n\n\nCombination of both."
  },
  {
    "objectID": "lecs/w06/lec06.html#example-weight-decay-for-regression",
    "href": "lecs/w06/lec06.html#example-weight-decay-for-regression",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: Weight Decay for Regression",
    "text": "Example: Weight Decay for Regression\nCost function:\n\\[\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_i \\left(\\left({\\bf w} {\\bf x}^{(i)} + b\\right) - t^{(i)}\\right)^2\\]\n\nCost function with weight decay:\n\\[\\mathcal{E}_{WD}({\\bf w}, b) = \\frac{1}{2N}\\sum_i \\left(\\left({\\bf w} {\\bf x}^{(i)} + b\\right) - t^{(i)}\\right)^2 + \\lambda \\sum_j w_j^2\\]"
  },
  {
    "objectID": "lecs/w06/lec06.html#weight-decay-nomanclature",
    "href": "lecs/w06/lec06.html#weight-decay-nomanclature",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weight Decay Nomanclature",
    "text": "Weight Decay Nomanclature\n\\[\\mathcal{E}_{WD}({\\bf w}, b) = \\frac{1}{2N}\\sum_i \\left(\\left({\\bf w} {\\bf x}^{(i)} + b\\right) - t^{(i)}\\right)^2 + \\lambda \\sum_j w_j^2\\]\n\n\\[\\frac{\\partial \\mathcal{E}_{WD}}{\\partial w_j} = \\frac{\\partial \\mathcal{E}}{\\partial w_j} + \\lambda 2 w_j\\]\n\n\nSo the gradient descent update rule becomes:\n\\[w_j \\leftarrow w_j - \\alpha\\left(\\frac{\\partial \\mathcal{E}}{\\partial w_j} + 2 \\lambda w_j\\right)\\]"
  },
  {
    "objectID": "lecs/w06/lec06.html#early-stopping",
    "href": "lecs/w06/lec06.html#early-stopping",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Early Stopping",
    "text": "Early Stopping\nIdea: Stop training when the validation error starts going up.\n\n\n\n\n\nIn practice, this is implemented by checkpointing (saving) the neural network weights every few iterations/epochs during training. \n\n\n\nWe choose the checkpoint with the best validation error to actually use. (And if there is a tie, use the earlier checkpoint)"
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-early-stopping-work",
    "href": "lecs/w06/lec06.html#why-does-early-stopping-work",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does Early Stopping Work?",
    "text": "Why does Early Stopping Work?\nWeights start off small, so it takes time for them to grow large.\n\nTherefore, stopping early has a similar effect to weight decay.\n\n\nIf you’re using sigmoid units, and the weights start out small, then the inputs to the activation functions take only a small range of values.\n\n\n\nThe neural network starts out approximately linear, and gradually becomes non-linear (and thus more powerful)"
  },
  {
    "objectID": "lecs/w06/lec06.html#ensembles",
    "href": "lecs/w06/lec06.html#ensembles",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Ensembles",
    "text": "Ensembles\nIf a loss function is convex (with respect to the predictions), you have a bunch of predictions for an input, and you don’t know which one is best, you are always better off averaging them!\n\n\\[\\mathcal{L}(\\lambda_1 y_1 + \\dots \\lambda_N y_N, t) \\le \\lambda_1 \\mathcal{L}(y_1, t) + \\dots \\lambda_N\\mathcal{L}(y_N, t)\\]\nfor \\(\\lambda_i \\ge 0\\) and \\(\\sum_i \\lambda_i = 1\\)\n\n\nIdea: Build multiple candidate models, and average the predictions on the test data.\n\n\nThis set of models is called an ensemble."
  },
  {
    "objectID": "lecs/w06/lec06.html#examples-of-ensembles",
    "href": "lecs/w06/lec06.html#examples-of-ensembles",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Examples of Ensembles",
    "text": "Examples of Ensembles\n\n\nTrain neural networks starting from different random initialization (might not give enough diversity)\n\n\n\n\nTrain different network on different subset of the training data (called bagging)\n\n\n\n\nTrain networks with different architectures, hyperparameters, or use other machine learning models\n\n\n\nEnsembles can improve generalization substantially.\n\n\nHowever, ensembles are expensive.\n\n\nThe first strategy is most common in practice."
  },
  {
    "objectID": "lecs/w06/lec06.html#stochastic-regularization",
    "href": "lecs/w06/lec06.html#stochastic-regularization",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Stochastic Regularization",
    "text": "Stochastic Regularization\nFor a network to overfit, its computations need to be really precise.\n\nThis suggests regularizing them by injecting noise into the computations, a strategy known as stochastic regularization.\n\n\nOne example is dropout: in each training iteration, random choose a portion of activations to set to 0."
  },
  {
    "objectID": "lecs/w06/lec06.html#stochastic-regularization-ii",
    "href": "lecs/w06/lec06.html#stochastic-regularization-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Stochastic Regularization II",
    "text": "Stochastic Regularization II\nThe probability \\(p\\) that an activation is set to 0 is a hyperparameter."
  },
  {
    "objectID": "lecs/w06/lec06.html#dropout",
    "href": "lecs/w06/lec06.html#dropout",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Dropout",
    "text": "Dropout\n\n\n\n\n\nCan be seen as training an ensemble of 2D different architectures with shared weights (where D is the number of units)\nAvoids co-adaptation\n\n\n\n\nco-adaptation refers to the phenomena where two neurons cancel each other out by learning essentially the same thing."
  },
  {
    "objectID": "lecs/w06/lec06.html#dropout-at-test-time",
    "href": "lecs/w06/lec06.html#dropout-at-test-time",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Dropout at Test Time",
    "text": "Dropout at Test Time\nDon’t do dropout at test time (why not?)\n\nMultiply the weights by \\(1-p\\) (why?)\n\n\n\n\nDropout at test time would be like using one network of an ensemble.\nSince the weights are on \\(1-p\\) fraction of the time, multiplying the weights by \\(1-p\\) matches the expected value of the activation magnitude (e.g. going into the next layer)."
  },
  {
    "objectID": "lecs/w06/lec06.html#summary-of-bag-of-tricks",
    "href": "lecs/w06/lec06.html#summary-of-bag-of-tricks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Summary of “Bag of Tricks”",
    "text": "Summary of “Bag of Tricks”\n\n\nData Augmentation\n\n\n\n\nReducing the number of parameters\n\n\n\n\nWeight decay\n\n\n\n\nEarly stopping\n\n\n\n\nEnsembles\n\n\n\n\nStochastic regularization (e.g. dropout)"
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-the-training-curve-look-like-his",
    "href": "lecs/w06/lec06.html#why-does-the-training-curve-look-like-his",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does the Training Curve look like his?",
    "text": "Why does the Training Curve look like his?\n\n\n\n\n\n\nThe learning rate is too high\nThe learning rate is too low\nThe number of iterations is too low\nThe weights are all initialized to zero\n\n\n\n\nLearning rate likely too high.\nLoss curve shows jumps and no consistent decrease"
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-the-training-curve-look-like-this",
    "href": "lecs/w06/lec06.html#why-does-the-training-curve-look-like-this",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does the Training Curve look like this?",
    "text": "Why does the Training Curve look like this?\n\n\n\n\n\n\nThe batch size is too small\nBatch normalization was used\nThe number of parameters is too large\nThe learning rate is too small\n\n\n\n\nBatch size likely too small.\nLearning curve very noisy."
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-the-training-curve-look-like-this-1",
    "href": "lecs/w06/lec06.html#why-does-the-training-curve-look-like-this-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does the Training Curve look like this?",
    "text": "Why does the Training Curve look like this?\n\n\n\n\n\nThe network probably has too few parameters\nThe network probably has too many parameters\nThe learning rate is probably too small\nIf weight decay is used, the lambda parameter is probably too large.\nEither (1) or (4) could be true\n\n\n\n\nToo few parameters as convergence stopped too early.\nToo much weight decay has similar effect."
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-the-training-curve-look-like-this-2",
    "href": "lecs/w06/lec06.html#why-does-the-training-curve-look-like-this-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does the Training Curve look like this?",
    "text": "Why does the Training Curve look like this?\n\n\n\n\n\nEvidence of overfitting\nEvidence of underfitting\nToo much momentum\nThe weights were all initialized to 0\n\n\n\n\nThis model did not fit yet and is so far underfitting the data."
  },
  {
    "objectID": "lecs/w06/lec06.html#expected-test-error-for-regression",
    "href": "lecs/w06/lec06.html#expected-test-error-for-regression",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Expected Test Error for Regression",
    "text": "Expected Test Error for Regression\n\n\nTraining set \\(D = \\{(x_1, y_1), ..., (x_n, y_n)\\}\\) drawn i.i.d. from distribution \\(P(X,Y)\\). Let’s write this as \\(D \\sim P^n\\).\n\n\n\n\nAssume for simplicity this is a regression problem with \\(y \\in \\mathbb{R}\\) and \\(L_2\\) loss.\n\n\n\n\nWhat is the expected test error for a function \\(h_D(x)=y\\) trained on the training set \\(D \\sim P^n\\), assuming a learning algorithm \\(\\mathcal{A}\\)? It is:"
  },
  {
    "objectID": "lecs/w06/lec06.html#method-rmsprop-contd",
    "href": "lecs/w06/lec06.html#method-rmsprop-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Method: RMSProp (cont’d)",
    "text": "Method: RMSProp (cont’d)\nThe following update is applied to each coordinate j independently: \\[\\begin{align*}\n\\mathbb{E}_{D \\sim P^n, (x,y) \\sim P} \\left[ (h_D(x) - y)^2 \\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w06/lec06.html#expected-test-error-for-regression-ii",
    "href": "lecs/w06/lec06.html#expected-test-error-for-regression-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Expected Test Error for Regression II",
    "text": "Expected Test Error for Regression II\n\\[\\begin{align*}\n\\mathbb{E}_{D \\sim P^n, (x,y) \\sim P} \\left[ (h_D(x) - y)^2 \\right]\n\\end{align*}\\]\n\n\nThe expectation is taken with respect to possible training sets \\(D \\sim P^n\\) and the test distribution P. Let’s write the expectation as \\(\\mathbb{E}_{D,x,y}\\) for notational simplicity.\n\n\n\n\nNote that this is the expected test error not the empirical test error that we report after training. How are they different?"
  },
  {
    "objectID": "lecs/w06/lec06.html#decomposing-the-expected-test-error",
    "href": "lecs/w06/lec06.html#decomposing-the-expected-test-error",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Decomposing the Expected Test Error",
    "text": "Decomposing the Expected Test Error\nLet’s start by adding and subtracting the same quantity \\[\\begin{multline*}\n  \\mathbb{E}_{D,x,y} \\left[ \\left(h_D(x) - y\\right)^2 \\right] \\\\\n  = \\mathbb{E}_{D,x,y} \\left[ \\left(h_D(x) - \\hat{h}(x) + \\hat{h}(x) - y\\right)^2 \\right]\n\\end{multline*}\\]\n\n\\(\\hat{h}(x) = \\mathbb{E}_{D \\sim P^n}[h_D(x)]\\) is the expected regressor over possible training sets, given the learning algorithm \\(\\mathcal{A}\\)."
  },
  {
    "objectID": "lecs/w06/lec06.html#decomposing-the-expected-test-error-1",
    "href": "lecs/w06/lec06.html#decomposing-the-expected-test-error-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Decomposing the Expected Test Error",
    "text": "Decomposing the Expected Test Error\nAfter some algebraic manipulation (proof), we can show that:\n\\[\\begin{multline*}\n  \\underbrace{\\mathbb{E}_{D,x,y} \\left[ (h_D(x) - y)^2 \\right]}_{\\text{Expected test error}}\n  = \\underbrace{\\mathbb{E}_{D,x} \\left[\\left(h_D(x) - \\hat{h}(x)\\right)^2 \\right]}_{\\text{Variance}} \\\\\n  + \\underbrace{\\mathbb{E}_{x,y} \\left[\\left(\\hat{y}(x) - y\\right)^2 \\right]}_{\\text{Noise}} +\n         \\underbrace{\\mathbb{E}_{x} \\left[\\left(\\hat{h}(x) - \\hat{y}(x)\\right)^2 \\right]}_{\\text{Bias}}\n\\end{multline*}\\]\n\n\\(\\hat{y}(x) = \\mathbb{E}_{y|x}[y]\\) is the expected label given \\(x\\). Labels might not be deterministic given x."
  },
  {
    "objectID": "lecs/w06/lec06.html#what-is-differential-privacy-dp",
    "href": "lecs/w06/lec06.html#what-is-differential-privacy-dp",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What is Differential Privacy (DP)?",
    "text": "What is Differential Privacy (DP)?\n\nDefinition: A mathematical framework for quantifying and controlling the privacy risks in data analysis.\n\n\n\nGoal: Ensures that the inclusion or exclusion of a single data point does not significantly affect the output of a model.\n\n\n\nWhy?\n\n\n\nProtects sensitive information in training data.\n\n\n\n\nPrevents models from overfitting to individual samples."
  },
  {
    "objectID": "lecs/w06/lec06.html#definition-of-differential-privacy",
    "href": "lecs/w06/lec06.html#definition-of-differential-privacy",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Definition of Differential Privacy",
    "text": "Definition of Differential Privacy\n\nIntuitive definition An algorithm is differentially private if the addition or removal of a single data point does not significantly affect the output.\n\n\n\nFormal definition: An algorithm is \\(\\varepsilon\\)-differentially private if for all datasets ( D ) and ( D’ ) differing in one element, and for all outputs ( S ):\n\n\\[\\begin{align*}\n\\Pr[Model(D) = S] \\leq \\exp(\\varepsilon) \\cdot \\Pr[Model(D') = S]\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w06/lec06.html#dp-in-deep-learning",
    "href": "lecs/w06/lec06.html#dp-in-deep-learning",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "DP in Deep Learning",
    "text": "DP in Deep Learning\nApplying DP in Neural Networks: Introduce noise to gradients during training.\n\nMechanisms (DP-SGD)\n\nClips gradients to a maximum norm.\n\n\n\n\nAdds calibrated Gaussian noise to the gradients."
  },
  {
    "objectID": "lecs/w06/lec06.html#example-healthcare-dataset",
    "href": "lecs/w06/lec06.html#example-healthcare-dataset",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example – Healthcare Dataset",
    "text": "Example – Healthcare Dataset\nScenario: Predicting Disease Risk\n\nTask: A neural network is trained to predict the risk of a disease based on patient health records.\n\n\nData: Sensitive medical information such as diagnosis history."
  },
  {
    "objectID": "lecs/w06/lec06.html#example-healthcare-dataset-ii",
    "href": "lecs/w06/lec06.html#example-healthcare-dataset-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example – Healthcare Dataset II",
    "text": "Example – Healthcare Dataset II\nWhat Happens Without DP?\n\nOutcome: The model performs well but memorizes some specific details from training samples.\n\n\n\nRisk: If an attacker queries the model with information about a specific individual, the model could reveal their presence in the training set by giving a higher probability for that individual, leaking private information."
  },
  {
    "objectID": "lecs/w06/lec06.html#example-healthcare-dataset-iii",
    "href": "lecs/w06/lec06.html#example-healthcare-dataset-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example – Healthcare Dataset III",
    "text": "Example – Healthcare Dataset III\nWith DP:\n\nOutcome: The model’s predictions are less influenced by any single training sample. The addition of noise prevents overfitting to specific patient data.\n\n\n\nBenefit: The model’s output is less likely to change noticeably even if a single patient’s record is added or removed, protecting patient privacy."
  },
  {
    "objectID": "lecs/w06/lec06.html#example-e-commerce-recommendations",
    "href": "lecs/w06/lec06.html#example-e-commerce-recommendations",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example – E-Commerce Recommendations",
    "text": "Example – E-Commerce Recommendations\nScenario: Personalized Product Recommendations\n\nTask: A deep learning model recommends products based on customer browsing history and previous purchases.\n\n\nData: Includes user shopping patterns, age, and location."
  },
  {
    "objectID": "lecs/w06/lec06.html#example-e-commerce-recommendations-ii",
    "href": "lecs/w06/lec06.html#example-e-commerce-recommendations-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example – E-Commerce Recommendations II",
    "text": "Example – E-Commerce Recommendations II\nWithout DP:\n\nOutcome: The model gives highly personalized recommendations.\n\n\n\nRisk: An attacker could infer specific details about individual users (e.g., based on specific product recommendations or similarities to other users’ behaviors), leading to a privacy breach."
  },
  {
    "objectID": "lecs/w06/lec06.html#example-e-commerce-recommendations-iii",
    "href": "lecs/w06/lec06.html#example-e-commerce-recommendations-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example – E-Commerce Recommendations III",
    "text": "Example – E-Commerce Recommendations III\nWith DP:\n\nOutcome: Slightly less personalized recommendations but better generalization to unseen users.\n\n\n\nBenefit: An attacker would have a much harder time inferring personal details from the recommendations, thanks to the noise added during model training."
  },
  {
    "objectID": "lecs/w06/lec06.html#summary",
    "href": "lecs/w06/lec06.html#summary",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "lecs/w06/lec06.html#learning-rate",
    "href": "lecs/w06/lec06.html#learning-rate",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Learning Rate",
    "text": "Learning Rate\nThe learning rate \\(\\alpha\\) is a hyperparameter we need to tune. Here are the things that can go wrong in batch mode:\n\n\n\n\n\n\n\n\n\n\\(\\alpha\\) too small:\n\\(\\alpha\\) too large:\n\\(\\alpha\\) much too large:\n\n\n\n\n\n\n\n\n\nslow progress\noscillations\ninstability"
  },
  {
    "objectID": "lecs/w06/lec06.html#stochastic-gradient-descent",
    "href": "lecs/w06/lec06.html#stochastic-gradient-descent",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nBatch gradient descent moves directly downhill. SGD takes steps in a noisy direction, but moves downhill on average.\n\n\n\n\n\n\n\n\nbatch gradient descent:\nstochastic gradient descent:"
  },
  {
    "objectID": "lecs/w06/lec06.html#sgd-learning-rate",
    "href": "lecs/w06/lec06.html#sgd-learning-rate",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "SGD Learning Rate",
    "text": "SGD Learning Rate\nIn stochastic training, the learning rate also influences the fluctuations due to the stochasticity of the gradients.\n\n\n\n\nUse a large learning rate early in training so you can get close to the optimum\nGradually decay the learning rate to reduce the fluctuations"
  },
  {
    "objectID": "lecs/w06/lec06.html#sgd-batch-size",
    "href": "lecs/w06/lec06.html#sgd-batch-size",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "SGD Batch Size",
    "text": "SGD Batch Size\nThe tradeoff between smaller vs larger batch size\n\\[\\begin{align*}\n  \\text{Var}\\left[\\frac{1}{S} \\sum_{i=1}^S \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta_j}\\right]\n  &=\n  \\frac{1}{S^2} \\text{Var} \\left[\\sum_{i=1}^S \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta_j} \\right] \\\\\n  &=\n  \\frac{1}{S} \\text{Var} \\left[\\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta_j} \\right]\n\\end{align*}\\]\nLarger batch size implies smaller variance, but at what cost?"
  },
  {
    "objectID": "lecs/w06/lec06.html#training-curve-or-learning-curve",
    "href": "lecs/w06/lec06.html#training-curve-or-learning-curve",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Training Curve (or Learning Curve)",
    "text": "Training Curve (or Learning Curve)\nTo diagnose optimization problems, it’s useful to look at learning curves: plot the training cost (or other metrics) as a function of iteration.\n\n\n\n\n\nNote: it’s very hard to tell from the training curves whether an optimizer has converged. They can reveal major problems, but they can’t guarantee convergence."
  },
  {
    "objectID": "lecs/w06/lec06.html#visualizing-optimization-algorithms",
    "href": "lecs/w06/lec06.html#visualizing-optimization-algorithms",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Visualizing Optimization Algorithms",
    "text": "Visualizing Optimization Algorithms\nYou might want to check out these links:\n\nAn overview of gradient descent algorithms: https://ruder.io/optimizing-gradient-descent\nCS231n: https://cs231n.github.io/neural-networks-3/\nWhy momentum really works: https://distill.pub/2017/momentum/"
  },
  {
    "objectID": "lecs/w06/lec06.html#questions-1",
    "href": "lecs/w06/lec06.html#questions-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Questions",
    "text": "Questions\n\nHow do we choose between different neural network models?\n\nFor example, different number of hidden units, hidden layers, …?\n\nHow do we know how well a model will perform on new data?"
  },
  {
    "objectID": "lecs/w06/lec06.html#the-training-set-1",
    "href": "lecs/w06/lec06.html#the-training-set-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Training Set",
    "text": "The Training Set\nThe training set is used\n\nto determine the value of the parameters\n\nThe model’s prediction accuracy over the training set is called the training accuracy."
  },
  {
    "objectID": "lecs/w06/lec06.html#the-training-set-contd",
    "href": "lecs/w06/lec06.html#the-training-set-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Training Set (cont’d)",
    "text": "The Training Set (cont’d)\nQ: Can we use the training accuracy to estimate how well a model will perform on new data?\n\n\nNo! It is possible for a model to fit well to the training set, but fail to generalize\nWe want to know how well the model performs on new data that we didn’t already use to optimize the model"
  },
  {
    "objectID": "lecs/w06/lec06.html#poor-generalization-1",
    "href": "lecs/w06/lec06.html#poor-generalization-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Poor Generalization",
    "text": "Poor Generalization"
  },
  {
    "objectID": "lecs/w06/lec06.html#overfitting-and-underfitting",
    "href": "lecs/w06/lec06.html#overfitting-and-underfitting",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Overfitting and Underfitting",
    "text": "Overfitting and Underfitting\nUnderfitting:\n\nThe model is simple and doesn’t fit the data\nThe model does not capture discriminative features of the data\n\nOverfitting:\n\nThe model is too complex and does not generalize\nThe model captures information about patterns in training set that happened by chance"
  },
  {
    "objectID": "lecs/w06/lec06.html#overfitting-and-underfitting-contd",
    "href": "lecs/w06/lec06.html#overfitting-and-underfitting-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Overfitting and Underfitting (cont’d)",
    "text": "Overfitting and Underfitting (cont’d)\nOverfitting:\n\nThe model is too complex and does not generalize\nThe model captures information about patterns in training set that happened by chance\n\ne.g. Ringo happens to be always wearing a red shirt in the training set\nModel learns: high red pixel content =&gt; predict Ringo"
  },
  {
    "objectID": "lecs/w06/lec06.html#the-test-set-1",
    "href": "lecs/w06/lec06.html#the-test-set-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Test Set",
    "text": "The Test Set\nWe set aside a test set of labelled examples.\nThe model’s prediction accuracy over the test set is called the test accuracy.\nThe purpose of the test set is to give us a good estimate of how well a model will perform on new data.\nQ: In general, will the test accuracy be higher or lower than the training accuracy?"
  },
  {
    "objectID": "lecs/w06/lec06.html#model-choices-1",
    "href": "lecs/w06/lec06.html#model-choices-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Model Choices",
    "text": "Model Choices\nBut what about decisions like:\n\nHow many layers?\nHow many units in each layer?\nWhat non-linear activation to use?"
  },
  {
    "objectID": "lecs/w06/lec06.html#model-choices-contd",
    "href": "lecs/w06/lec06.html#model-choices-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Model Choices (cont’d)",
    "text": "Model Choices (cont’d)\nQ: Why can’t we use the test set to determine which model we should deploy?\n\n\nIf we use the test set to make modeling decisions, then we will overestimate how well our model will perform on new data!\nWe are “cheating” by “looking at the test”"
  },
  {
    "objectID": "lecs/w06/lec06.html#the-validation-set-1",
    "href": "lecs/w06/lec06.html#the-validation-set-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Validation set",
    "text": "The Validation set\nWe therefore need a third set of labeled data called the validation set\nThe model’s prediction accuracy over the validation set is called the validation accuracy.\nThis dataset is used to:\n\nMake decisions about models that is not continuous and can’t be optimized via gradient descent\nExample: choose \\(k\\), choose which features \\(x_j\\) to use, choose \\(\\alpha\\), …"
  },
  {
    "objectID": "lecs/w06/lec06.html#the-validation-set-contd",
    "href": "lecs/w06/lec06.html#the-validation-set-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Validation set (cont’d)",
    "text": "The Validation set (cont’d)\nThe model’s prediction accuracy over the validation set is called the validation accuracy.\nThis dataset is used to:\n\nMake decisions about models that is not continuous and can’t be optimized via gradient descent\nExample: choose \\(k\\), choose which features \\(x_j\\) to use, choose \\(\\alpha\\), …\n\nThese model settings are called hyperparameters\n\nThe validation set is used to optimize hyperparameters"
  },
  {
    "objectID": "lecs/w06/lec06.html#splitting-the-data-set",
    "href": "lecs/w06/lec06.html#splitting-the-data-set",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Splitting the data set",
    "text": "Splitting the data set\nExample split:\n\n60% Training\n20% Validation\n20% Test\n\nThe actual split depends on the amount of data that you have.\nIf you have more data, you can get a way with a smaller % validation and set."
  },
  {
    "objectID": "lecs/w06/lec06.html#detecting-overfitting-1",
    "href": "lecs/w06/lec06.html#detecting-overfitting-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Detecting Overfitting",
    "text": "Detecting Overfitting\nLearning curve:\n\n\n\n\n\nx-axis: epochs or iterations\ny-axis: cost, error, or accuracy\n\n\nQ: In which epochs is the model overfitting? Underfitting?\nQ: Why don’t we plot the test accuracy plot?"
  },
  {
    "objectID": "lecs/w06/lec06.html#strategies-to-prevent-overfitting-1",
    "href": "lecs/w06/lec06.html#strategies-to-prevent-overfitting-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Strategies to Prevent Overfitting",
    "text": "Strategies to Prevent Overfitting\n\nData Augmentation\nReducing the number of parameters\nWeight decay\nEarly stopping\nEnsembles\nStochastic regularization (e.g. dropout)"
  },
  {
    "objectID": "lecs/w06/lec06.html#data-augmentation-1",
    "href": "lecs/w06/lec06.html#data-augmentation-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nThe best way to improve generalization is to collect more data!\nBut if we already have all the data we’re willing to collect. We can augment the training data by transforming the examples. This is called data augmentation."
  },
  {
    "objectID": "lecs/w06/lec06.html#data-augmentation-contd",
    "href": "lecs/w06/lec06.html#data-augmentation-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Data Augmentation (cont’d)",
    "text": "Data Augmentation (cont’d)\nExample (for images, but depends on task):\n\ntranslation\nhorizontal or vertical flip\nrotation\nsmooth warping\nnoise (e.g. flip random pixels)\n\nWe should only warp the training examples, not the validation or test examples (why?)"
  },
  {
    "objectID": "lecs/w06/lec06.html#reducing-the-number-of-parameters-1",
    "href": "lecs/w06/lec06.html#reducing-the-number-of-parameters-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Reducing the Number of Parameters",
    "text": "Reducing the Number of Parameters\nNetworks with fewer trainable parameters are less likely to overfit. We can reduce the number of layers, or the number of parameters per layer.\nAdding a bottleneck layer is another way to reduce the number of parameters"
  },
  {
    "objectID": "lecs/w06/lec06.html#reducing-the-number-of-parameters-contd",
    "href": "lecs/w06/lec06.html#reducing-the-number-of-parameters-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Reducing the Number of Parameters (cont’d)",
    "text": "Reducing the Number of Parameters (cont’d)\nAdding a bottleneck layer is another way to reduce the number of parameters\n\n\n\nIn practise, this isn’t a great idea."
  },
  {
    "objectID": "lecs/w06/lec06.html#weight-decay-idea-1",
    "href": "lecs/w06/lec06.html#weight-decay-idea-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weight Decay Idea",
    "text": "Weight Decay Idea\nIdea: Penalize large weights, by adding a term (e.g. \\(\\sum_k w_k ^ 2\\)) to the cost function\nQ: Why is it not ideal to have large (absolute value) weights?\n\nBecause large weights mean that the prediction relies a lot on the content of one feature (e.g. one pixel)"
  },
  {
    "objectID": "lecs/w06/lec06.html#small-vs-large-weights-1",
    "href": "lecs/w06/lec06.html#small-vs-large-weights-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Small vs Large Weights",
    "text": "Small vs Large Weights\nThe red polynomial overfits. Notice it has really large coefficients"
  },
  {
    "objectID": "lecs/w06/lec06.html#weight-decay-1",
    "href": "lecs/w06/lec06.html#weight-decay-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weight Decay",
    "text": "Weight Decay\n\n\\(L^1\\) regularization: add a term \\(\\sum_{j=1}^D |w_j|\\) to the cost function\n\nMathematically, this term encourages weights to be exactly 0\n\n\\(L^2\\) regularization: add a term \\(\\sum_{j=1}^D w_j^2\\) to the cost function\n\nMathematically, in each iteration the weight is pushed towards 0\n\nCombination of \\(L^1\\) and \\(L^2\\) regularization: add a term \\(\\sum_{j=1}^D |w_j| + w_j^2\\) to the cost function"
  },
  {
    "objectID": "lecs/w06/lec06.html#example-weight-decay-for-regression-1",
    "href": "lecs/w06/lec06.html#example-weight-decay-for-regression-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: Weight Decay for Regression",
    "text": "Example: Weight Decay for Regression\nCost function:\n\\[\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_i \\left(\\left({\\bf w} {\\bf x}^{(i)} + b\\right) - t^{(i)}\\right)^2\\]\nCost function with weight decay:\n\\[\\mathcal{E}_{WD}({\\bf w}, b) = \\frac{1}{2N}\\sum_i \\left(\\left({\\bf w} {\\bf x}^{(i)} + b\\right) - t^{(i)}\\right)^2 + \\lambda \\sum_j w_j^2\\]"
  },
  {
    "objectID": "lecs/w06/lec06.html#weight-decay-nomanclature-1",
    "href": "lecs/w06/lec06.html#weight-decay-nomanclature-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weight Decay Nomanclature",
    "text": "Weight Decay Nomanclature\n\\[\\mathcal{E}_{WD}({\\bf w}, b) = \\frac{1}{2N}\\sum_i \\left(\\left({\\bf w} {\\bf x}^{(i)} + b\\right) - t^{(i)}\\right)^2 + \\lambda \\sum_j w_j^2\\]\n\\[\\frac{\\partial \\mathcal{E}_{WD}}{\\partial w_j} = \\frac{\\partial \\mathcal{E}}{\\partial w_j} + \\lambda 2 w_j\\]\nSo the gradient descent update rule becomes:\n\\[w_j \\leftarrow w_j - \\alpha\\left(\\frac{\\partial \\mathcal{E}}{\\partial w_j} + 2 \\lambda w_j\\right)\\]"
  },
  {
    "objectID": "lecs/w06/lec06.html#early-stopping-1",
    "href": "lecs/w06/lec06.html#early-stopping-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Early Stopping",
    "text": "Early Stopping\nIdea: Stop training when the validation error starts going up.\n\n\n\n\nIn practice, this is implemented by checkpointing (saving) the neural network weights every few iterations/epochs during training. \n\n\n\nWe choose the checkpoint with the best validation error to actually use. (And if there is a tie, use the earlier checkpoint)"
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-early-stopping-work-1",
    "href": "lecs/w06/lec06.html#why-does-early-stopping-work-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does early stopping work?",
    "text": "Why does early stopping work?\nWeights start off small, so it takes time for them to grow large.\nTherefore, stopping early has a similar effect to weight decay.\nIf you’re using sigmoid units, and the weights start out small, then the inputs to the activation functions take only a small range of values.\n\nThe neural network starts out approximately linear, and gradually becomes non-linear (and thus more powerful)"
  },
  {
    "objectID": "lecs/w06/lec06.html#ensembles-1",
    "href": "lecs/w06/lec06.html#ensembles-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Ensembles",
    "text": "Ensembles\nIf a loss function is convex (with respect to the predictions), you have a bunch of predictions for an input, and you don’t know which one is best, you are always better off averaging them!\n\\[\\mathcal{L}(\\lambda_1 y_1 + \\dots \\lambda_N y_N, t) \\le \\lambda_1 \\mathcal{L}(y_1, t) + \\dots \\lambda_N\\mathcal{L}(y_N, t)\\]\nfor \\(\\lambda_i \\ge 0\\) and \\(\\sum_i \\lambda_i = 1\\)\nIdea: Build multiple candidate models, and average the predictions on the test data.\nThis set of models is called an ensemble."
  },
  {
    "objectID": "lecs/w06/lec06.html#examples-of-ensembles-1",
    "href": "lecs/w06/lec06.html#examples-of-ensembles-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Examples of Ensembles",
    "text": "Examples of Ensembles\n\nTrain neural networks starting from different random initialization (might not give enough diversity)\nTrain different network on different subset of the training data (called bagging)\nTrain networks with different architectures, hyperparameters, or use other machine learning models\n\nEnsembles can improve generalization substantially.\nHowever, ensembles are expensive."
  },
  {
    "objectID": "lecs/w06/lec06.html#stochastic-regularization-1",
    "href": "lecs/w06/lec06.html#stochastic-regularization-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Stochastic Regularization",
    "text": "Stochastic Regularization\nFor a network to overfit, its computations need to be really precise. This suggests regularizing them by injecting noise into the computations, a strategy known as stochastic regularization.\nOne example is dropout: in each training iteration, random choose a portion of activations to set to 0."
  },
  {
    "objectID": "lecs/w06/lec06.html#stochastic-regularization-contd",
    "href": "lecs/w06/lec06.html#stochastic-regularization-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Stochastic Regularization (cont’d)",
    "text": "Stochastic Regularization (cont’d)\nThe probability \\(p\\) that an activation is set to 0 is a hyperparameter."
  },
  {
    "objectID": "lecs/w06/lec06.html#dropout-1",
    "href": "lecs/w06/lec06.html#dropout-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Dropout",
    "text": "Dropout\n\n\n\n\nDropout can be seen as training an ensemble of 2D different architectures with shared weights (where D is the number of units)"
  },
  {
    "objectID": "lecs/w06/lec06.html#dropout-at-test-time-1",
    "href": "lecs/w06/lec06.html#dropout-at-test-time-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Dropout at Test Time",
    "text": "Dropout at Test Time\nDon’t do dropout at test time (why not?)\nMultiply the weights by \\(1-p\\) (why?)\n\nSince the weights are on \\(1-p\\) fraction of the time, multiplying the weights by \\(1-p\\) matches the expected value of the activation magnitude (e.g. going into the next layer)."
  },
  {
    "objectID": "lecs/w06/lec06.html#summary-of-bag-of-tricks-1",
    "href": "lecs/w06/lec06.html#summary-of-bag-of-tricks-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Summary of “Bag of Tricks”",
    "text": "Summary of “Bag of Tricks”\n\nData Augmentation\nReducing the number of parameters\nWeight decay\nEarly stopping\nEnsembles\nStochastic regularization (e.g. dropout)"
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-the-training-curve-to-look-like-this",
    "href": "lecs/w06/lec06.html#why-does-the-training-curve-to-look-like-this",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does the training curve to look like this?",
    "text": "Why does the training curve to look like this?\n\n\n\n\n\n\nThe learning rate is too high\nThe learning rate is too low\nThe number of iterations is too low\nThe weights are all initialized to zero"
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-the-training-curve-to-look-like-this-1",
    "href": "lecs/w06/lec06.html#why-does-the-training-curve-to-look-like-this-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does the training curve to look like this?",
    "text": "Why does the training curve to look like this?\n\n\n\n\n\n\nThe batch size is too small\nBatch normalization was used\nThe number of parameters is too large\nThe learning rate is too small"
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-the-training-curve-to-look-like-this-2",
    "href": "lecs/w06/lec06.html#why-does-the-training-curve-to-look-like-this-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does the training curve to look like this?",
    "text": "Why does the training curve to look like this?\n\n\n\n\n\nThe network probably has too few parameters\nThe network probably has too many parameters\nThe learning rate is probably too small\nIf weight decay is used, the lambda parameter is probably too large.\nEither (1) or (4) could be true"
  },
  {
    "objectID": "lecs/w06/lec06.html#why-does-the-training-curve-to-look-like-this-3",
    "href": "lecs/w06/lec06.html#why-does-the-training-curve-to-look-like-this-3",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why does the training curve to look like this?",
    "text": "Why does the training curve to look like this?\n\n\n\n\n\nEvidence of overfitting\nEvidence of underfitting\nToo much momentum\nThe weights were all initialized to 0"
  },
  {
    "objectID": "lecs/w06/lec06.html#expected-test-error-for-regression-1",
    "href": "lecs/w06/lec06.html#expected-test-error-for-regression-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Expected Test Error for Regression",
    "text": "Expected Test Error for Regression\n\nTraining set \\(D = \\{(x_1, y_1), ..., (x_n, y_n)\\}\\) drawn i.i.d. from distribution \\(P(X,Y)\\). Let’s write this as \\(D \\sim P^n\\).\nAssume for simplicity this is a regression problem with \\(y \\in \\mathbb{R}\\) and \\(L_2\\) loss.\nWhat is the expected test error for a function \\(h_D(x)=y\\) trained on the training set \\(D \\sim P^n\\), assuming a learning algorithm \\(\\mathcal{A}\\)? It is:\n\n\\[\\begin{align*}\n\\mathbb{E}_{D \\sim P^n, (x,y) \\sim P} \\left[ (h_D(x) - y)^2 \\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w06/lec06.html#expected-test-error-for-regression-contd",
    "href": "lecs/w06/lec06.html#expected-test-error-for-regression-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Expected Test Error for Regression (cont’d)",
    "text": "Expected Test Error for Regression (cont’d)\n\\[\\begin{align*}\n\\mathbb{E}_{D \\sim P^n, (x,y) \\sim P} \\left[ (h_D(x) - y)^2 \\right]\n\\end{align*}\\]\n\nThe expectation is taken with respect to possible training sets \\(D \\sim P^n\\) and the test distribution P. Let’s write the expectation as \\(\\mathbb{E}_{D,x,y}\\) for notational simplicity.\nNote that this is the expected test error not the empirical test error that we report after training. How are they different?"
  },
  {
    "objectID": "lecs/w06/lec06.html#decomposing-the-expected-test-error-2",
    "href": "lecs/w06/lec06.html#decomposing-the-expected-test-error-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Decomposing the Expected Test Error",
    "text": "Decomposing the Expected Test Error\nLet’s start by adding and subtracting the same quantity \\[\\begin{align*}\n  \\mathbb{E}_{D,x,y} \\left[ \\left(h_D(x) - y\\right)^2 \\right] = \\mathbb{E}_{D,x,y} \\left[ \\left(h_D(x) - \\hat{h}(x) + \\hat{h}(x) - y\\right)^2 \\right]\n\\end{align*}\\]\n\\(\\hat{h}(x) = \\mathbb{E}_{D \\sim P^n}[h_D(x)]\\) is the expected regressor over possible training sets, given the learning algorithm \\(\\mathcal{A}\\).\n\\(\\hat{y}(x) = \\mathbb{E}_{y|x}[y]\\) is the expected label given \\(x\\). Labels might not be deterministic given x."
  },
  {
    "objectID": "lecs/w06/lec06.html#decomposing-the-expected-test-error-3",
    "href": "lecs/w06/lec06.html#decomposing-the-expected-test-error-3",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Decomposing the Expected Test Error",
    "text": "Decomposing the Expected Test Error\nAfter some algebraic manipulation (proof), we can show that:\n\\[\\begin{align*}\n  \\underbrace{\\mathbb{E}_{D,x,y} \\left[ (h_D(x) - y)^2 \\right]}_{\\text{Expected test error}} =\\;& \\underbrace{\\mathbb{E}_{D,x} \\left[\\left(h_D(x) - \\hat{h}(x)\\right)^2 \\right]}_{\\text{Variance}} + \\\\\n    & \\underbrace{\\mathbb{E}_{x,y} \\left[\\left(\\hat{y}(x) - y\\right)^2 \\right]}_{\\text{Noise}} + \\\\\n        & \\underbrace{\\mathbb{E}_{x} \\left[\\left(\\hat{h}(x) - \\hat{y}(x)\\right)^2 \\right]}_{\\text{Bias}}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w06/lec06.html#bias-variance",
    "href": "lecs/w06/lec06.html#bias-variance",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Bias, Variance, …",
    "text": "Bias, Variance, …\n\nVariance: Captures how much your regressor \\(h_D\\) changes if you train on a different training set. How “over-specialized” is your regressor \\(h_D\\) to a particular training set \\(D\\)? I.e. how much does it overfit? If we have the best possible model for our training data, how far off are we from the average regressor \\(\\hat{h}\\)?\nBias: What is the inherent error that you obtain from your regressor \\(h_D\\) even with infinite training data? This is due to your model being “biased” to a particular kind of solution (e.g. linear model). In other words, bias is inherent to your model/architecture."
  },
  {
    "objectID": "lecs/w06/lec06.html#and-noise",
    "href": "lecs/w06/lec06.html#and-noise",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "… and Noise",
    "text": "… and Noise\n\nNoise: How big is the data-intrinsic noise? This error measures ambiguity due to your data distribution and feature representation. You can never beat this, it is an aspect of the physical data generation process, over which you have no control. You cannot improve this with more training data. It is sometimes called “aleatoric uncertainty”."
  },
  {
    "objectID": "lecs/w06/lec06.html#the-bias-variance-tradeoff",
    "href": "lecs/w06/lec06.html#the-bias-variance-tradeoff",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\n\n\n\n\n\nIf you use a high-capacity model, you will get low bias, but the variance over different training sets will be high.\n\n\n\n\n\nIf you use a low-capacity model, you will get high bias, but the variance over different training sets will be low.\nThere is a sweet spot that trades off between the two."
  },
  {
    "objectID": "lecs/w08/lec08.html#last-week",
    "href": "lecs/w08/lec08.html#last-week",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Last Week",
    "text": "Last Week\n\nCNN Feature Visualization and Interpretation\nTransfer Learning\nAdversarial Examples\nInfluence Functions"
  },
  {
    "objectID": "lecs/w08/lec08.html#this-week",
    "href": "lecs/w08/lec08.html#this-week",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "This week",
    "text": "This week\n\nRecurrent Neural Networks\nSentiment Analysis with Recurrent Neural Networks\nGradient Explosion and Vanishing\nText Generation with RNN\nSequence-to-Sequence Modelling"
  },
  {
    "objectID": "lecs/w08/lec08.html#goal-and-overview",
    "href": "lecs/w08/lec08.html#goal-and-overview",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Goal and Overview",
    "text": "Goal and Overview\nSometimes we’re interested in making predictions about data in the form of sequences. Some examples are:\n\n\nGiven the price of a stock in the last week, predict whether stock price will go up\n\n\n\n\nGiven a sentence (sequence of chars/words) predict its sentiment\n\n\n\n\nGiven a sentence in English, translate it to French\n\n\n\nThis last example is a sequence-to-sequence prediction task, because both inputs and outputs are sequences."
  },
  {
    "objectID": "lecs/w08/lec08.html#language-model",
    "href": "lecs/w08/lec08.html#language-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Language Model",
    "text": "Language Model\nWe have already seen neural language models that make the Markov Assumption:\n\\[p(w_i | w_1, \\ldots, w_{i-1}) = p(w_i | w_{i-3}, w_{i-2}, w_{i-1})\\]\n\nThis means the model is memoryless, so it can only use information from its immediate context.\n\n\n\n\n\n\n\nIn this image, the context length is 1."
  },
  {
    "objectID": "lecs/w08/lec08.html#recurrent-neural-network",
    "href": "lecs/w08/lec08.html#recurrent-neural-network",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Recurrent Neural Network",
    "text": "Recurrent Neural Network\nBut sometimes long-distance context can be important.\n\nIf we add connections between the hidden units, it becomes a recurrent neural network (RNN).\n\n\nHaving a memory lets an RNN use longer-term dependencies:"
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-diagram",
    "href": "lecs/w08/lec08.html#rnn-diagram",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN Diagram",
    "text": "RNN Diagram\nWe can think of an RNN as a dynamical system with one set of hidden units which feed into themselves. The network’s graph would then have self-loops.\n\nWe can unroll the RNN’s graph by explicitly representing the units at all time steps. The weights and biases are shared between all time steps"
  },
  {
    "objectID": "lecs/w08/lec08.html#simple-rnns",
    "href": "lecs/w08/lec08.html#simple-rnns",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Simple RNNs",
    "text": "Simple RNNs\nLet’s go through a few examples of very simple RNNs to understand how RNNs compute predictions."
  },
  {
    "objectID": "lecs/w08/lec08.html#simple-rnn-example-sum",
    "href": "lecs/w08/lec08.html#simple-rnn-example-sum",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Simple RNN Example: Sum",
    "text": "Simple RNN Example: Sum\nThis simple RNN takes a sequence of numbers as input (scalars), and sums its inputs."
  },
  {
    "objectID": "lecs/w08/lec08.html#simple-rnn-example-2-comparison",
    "href": "lecs/w08/lec08.html#simple-rnn-example-2-comparison",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Simple RNN Example 2: Comparison",
    "text": "Simple RNN Example 2: Comparison\nThis RNN takes a sequence of pairs of numbers as input, and determines if the total values of the first or second input are larger:"
  },
  {
    "objectID": "lecs/w08/lec08.html#simple-rnn-example-3-parity",
    "href": "lecs/w08/lec08.html#simple-rnn-example-3-parity",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Simple RNN Example 3: Parity",
    "text": "Simple RNN Example 3: Parity\nAssume we have a sequence of binary inputs. We’ll consider how to determine the parity, i.e. whether the number of 1’s is even or odd. We can compute parity incrementally by keeping track of the parity of the input so far:\n\n\n\n\n\n\n\n\n\nParity bits:\n\n Input:\n\n\n\n\n0 1 1 0 1 1\n\\(\\longrightarrow\\)\n0 1 0 1 1 0 1 0 1 1\n\n\n\n\n\nEach parity bit is the XOR of the input and the previous parity bit. Parity is a classic example of a problem that’s hard to solve with a shallow feed-forward net, but easy to solve with an RNN."
  },
  {
    "objectID": "lecs/w08/lec08.html#parity-approach",
    "href": "lecs/w08/lec08.html#parity-approach",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Parity Approach",
    "text": "Parity Approach\nLet’s find weights and biases for the RNN on the right so that it computes the parity. All hidden and output units are binary threshold units (\\(h(x) = 1\\) if \\(x &gt; 0\\) and \\(h(x) = 0\\) otherise).\n\nStrategy\n\nThe output unit tracks the current parity, which is the XOR of the current input and previous output.\n\n\n\n\nThe hidden units help us compute the XOR."
  },
  {
    "objectID": "lecs/w08/lec08.html#parity-approach-ii",
    "href": "lecs/w08/lec08.html#parity-approach-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Parity Approach II",
    "text": "Parity Approach II"
  },
  {
    "objectID": "lecs/w08/lec08.html#unrolling-parity-rnn",
    "href": "lecs/w08/lec08.html#unrolling-parity-rnn",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Unrolling Parity RNN",
    "text": "Unrolling Parity RNN"
  },
  {
    "objectID": "lecs/w08/lec08.html#parity-computation",
    "href": "lecs/w08/lec08.html#parity-computation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Parity Computation",
    "text": "Parity Computation\nThe output unit should compute the XOR of the current input and previous output:\n\n\n\n\n\\(y^{(t-1)}\\)\n\\(x^{(t)}\\)\n\\(y^{(t)}\\)\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\n\n\n\n\\(y^{(t)}=0\\) meants the number of 1’s in the past input has been even.\n\\(y^{(t)}=1\\) meants the number of 1’s in the past input has been odd."
  },
  {
    "objectID": "lecs/w08/lec08.html#computing-parity",
    "href": "lecs/w08/lec08.html#computing-parity",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing Parity",
    "text": "Computing Parity\nLet’s use hidden units to help us compute XOR.\n\nHave one unit compute AND, and the other one compute OR.\n\n\n\nThen we can pick weights and biases just like we did for multilayer perceptrons."
  },
  {
    "objectID": "lecs/w08/lec08.html#computing-parity-ii",
    "href": "lecs/w08/lec08.html#computing-parity-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing Parity II",
    "text": "Computing Parity II\n\n\n\n\n\n\n\\(y^{(t-1)}\\)\n\\(x^{(t)}\\)\n\\(h_1^{(t)}\\)\n\\(h_2^{(t)}\\)\n\\(y^{(t)}\\)\n\n\n\n\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n1\n1\n\n\n1\n0\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n\n\n\n\n\n\n\n\n\n\n\n\\(h_1\\) computes and by subtracting -1.5 from the sum of \\(y\\) and the current input \\(x\\) and then applying the binary threshold activation function.\n\\(h_2\\) computes or by subtracting -0.5 from the sum of \\(y\\) and the current input \\(x\\) and then applying the binary threshold activation function."
  },
  {
    "objectID": "lecs/w08/lec08.html#back-propagation-through-time",
    "href": "lecs/w08/lec08.html#back-propagation-through-time",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Back Propagation Through Time",
    "text": "Back Propagation Through Time\nAs you can guess, we don’t usually set RNN weights by hand. Instead, we learn them using backprop.\n\nIn particular, we do backprop on the unrolled network. This is known as backprop through time."
  },
  {
    "objectID": "lecs/w08/lec08.html#unrolled-bptt",
    "href": "lecs/w08/lec08.html#unrolled-bptt",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Unrolled BPTT",
    "text": "Unrolled BPTT\nHere’s the unrolled computation graph. Notice the weight sharing."
  },
  {
    "objectID": "lecs/w08/lec08.html#what-can-rnns-compute",
    "href": "lecs/w08/lec08.html#what-can-rnns-compute",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What can RNNs compute?",
    "text": "What can RNNs compute?\nIn 2014, Google researchers built an encoder-decoder RNN that learns to execute simple Python programs, one character at a time! https://arxiv.org/abs/1410.4615"
  },
  {
    "objectID": "lecs/w08/lec08.html#what-can-rnns-compute-1",
    "href": "lecs/w08/lec08.html#what-can-rnns-compute-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What can RNNs compute?",
    "text": "What can RNNs compute?\nRNNs are good at learning complex syntactic structures: generate Algebraic Geometry LaTex source files that almost compiles:\n\n\n\nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness/"
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-for-language-modelling",
    "href": "lecs/w08/lec08.html#rnn-for-language-modelling",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN for language modelling",
    "text": "RNN for language modelling\nUsually, the sequence of inputs \\(x_t\\) will be vectors. The hidden states \\(h_t\\) are also vectors.\n\nFor example, we might use a sequence of one-hot vectors \\({\\bf x}_t\\) of words (or characters) to represent a sentence. (What else can we use?)\n\n\nHow would we use a RNN to determine (say) the sentiment conveyed by the sentence?\n\n\nAs usual, start with the forward pass…"
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-initial-hidden-state",
    "href": "lecs/w08/lec08.html#rnn-initial-hidden-state",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN: Initial Hidden State",
    "text": "RNN: Initial Hidden State\n\n\n\nStart with an initial hidden state with a blank slate (can be a vector of all zeros, or a parameter that we train)"
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-update-hidden-state",
    "href": "lecs/w08/lec08.html#rnn-update-hidden-state",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN: Update Hidden State",
    "text": "RNN: Update Hidden State\n\n\n\nCompute the first hidden state based on the initial hidden state, and the input (the one-hot vector \\({\\bf x}_1\\) of the first word)."
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-continue-updating-hidden-state",
    "href": "lecs/w08/lec08.html#rnn-continue-updating-hidden-state",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN: Continue Updating Hidden State",
    "text": "RNN: Continue Updating Hidden State\n\n\n\nUpdate the hidden state based on the subsequent inputs. Note that we are using the same weights to perform the update each time."
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-last-hidden-state",
    "href": "lecs/w08/lec08.html#rnn-last-hidden-state",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN: Last Hidden State",
    "text": "RNN: Last Hidden State\n\n\n\nContinue updating the hidden state until we run out of words in our sentence."
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-compute-prediction",
    "href": "lecs/w08/lec08.html#rnn-compute-prediction",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN: Compute Prediction",
    "text": "RNN: Compute Prediction\n\n\n\nUse the last hidden state as input to a prediction network, usually a MLP.\n\nAlternative: take the max-pool and average-pool over all computed hidden states."
  },
  {
    "objectID": "lecs/w08/lec08.html#sequence-classification",
    "href": "lecs/w08/lec08.html#sequence-classification",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Sequence Classification",
    "text": "Sequence Classification\nLab: let’s build this model!"
  },
  {
    "objectID": "lecs/w08/lec08.html#sentiment140-data",
    "href": "lecs/w08/lec08.html#sentiment140-data",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Sentiment140 Data",
    "text": "Sentiment140 Data\nDataset of tweets with either a positive or negative emoticon, but with the emoticon removed.\nInput: Tweet (sequence of words/characters)\nTarget: Positive or negative emoticon?\nExample:\n\nNegative: “Just going to cry myself to sleep after watching Marley and Me”\n\n\n\nPositive: “WOOOOO! Xbox is back”"
  },
  {
    "objectID": "lecs/w08/lec08.html#approach",
    "href": "lecs/w08/lec08.html#approach",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Approach",
    "text": "Approach\n\n\n\n\n\n\n\nUse GloVe embeddings to represent words as input \\({\\bf x}^{(t)}\\) (note: we could have chosen to work at the character level)\n\n\n\n\nUse a recurrent neural network to get a combined embedding of the entire tweet\n\n\n\n\nUse a fully-connected layer to make predictions (happy vs sad)"
  },
  {
    "objectID": "lecs/w08/lec08.html#key-considerations",
    "href": "lecs/w08/lec08.html#key-considerations",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Key Considerations",
    "text": "Key Considerations\n\nWe’ll be using the PyTorch nn.RNN module, which can be unintuitive\n\n\n\nBatching difficulties: each tweet is a different length, so how can we batch?\n\nOne way is to pad shorter sequences with a special “padding” token at the end of the sequence\nHowever, we want to minimize this padding due to computational complexity"
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-gradients",
    "href": "lecs/w08/lec08.html#rnn-gradients",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN Gradients",
    "text": "RNN Gradients\nRecall the unrolled computation graph for a small RNN:"
  },
  {
    "objectID": "lecs/w08/lec08.html#backprop-through-time",
    "href": "lecs/w08/lec08.html#backprop-through-time",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backprop Through Time",
    "text": "Backprop Through Time\n\n\nActivations: \\[\\begin{align*}\n    \\overline{\\mathcal{L}} &= 1 \\\\\n    \\overline{y^{(t)}} &= \\overline{\\mathcal{L}} \\, \\frac{\\partial \\mathcal{L}}{\\partial y^{(t)}} \\\\\n    \\overline{r^{(t)}} &= \\overline{y^{(t)}} \\, \\phi^\\prime(r^{(t)}) \\\\\n    {\\color{magenta} \\overline{h^{(t)}}} & {\\color{magenta}\\, = \\overline{r^{(t)}} \\, v + \\overline{z^{(t+1)}} \\, w} \\\\\n    \\overline{z^{(t)}} &= \\overline{h^{(t)}} \\, \\phi^\\prime(z^{(t)})\n\\end{align*}\\]\n\nParameters: \\[\\begin{align*}\n    {\\color{magenta} \\overline{u}} &{\\color{magenta} \\, = \\sum_t \\overline{z^{(t)}} \\, x^{(t)}} \\\\\n    {\\color{magenta} \\overline{v}} &{\\color{magenta} \\,= \\sum_t \\overline{r^{(t)}} \\, h^{(t)}} \\\\\n    {\\color{magenta} \\overline{w}} &{\\color{magenta} \\,= \\sum_t \\overline{z^{(t+1)}} \\, h^{(t)}}\n\\end{align*}\\]\n\nKey idea: multivariate chain rule!"
  },
  {
    "objectID": "lecs/w08/lec08.html#gradient-explosion-and-vanishing-1",
    "href": "lecs/w08/lec08.html#gradient-explosion-and-vanishing-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Explosion and Vanishing",
    "text": "Gradient Explosion and Vanishing\nThe longer your sequence, the longer gap the time step between when we see potentially important information and when we need it:\n\n\n\n\nThe derivatives need to travel this entire pathway."
  },
  {
    "objectID": "lecs/w08/lec08.html#why-gradients-explode-or-vanish",
    "href": "lecs/w08/lec08.html#why-gradients-explode-or-vanish",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Gradients Explode or Vanish",
    "text": "Why Gradients Explode or Vanish\nConsider a univariate version of the RNN:\n\n\n\n\n\nBackpropagation updates: \\[\\begin{align*}\n    \\overline{h^{(t)}} &= \\overline{z^{(t+1)}} \\, w \\\\\n    \\overline{z^{(t)}} &= \\overline{h^{(t)}} \\, \\phi^\\prime(z^{(t)})\n\\end{align*}\\]\n\nApplying this recursively: \\[\\begin{align*}\n    \\overline{h^{(1)}} = w^{T-1} \\phi^\\prime(z^{(2)}) \\cdots \\phi^\\prime(z^{(T)}) \\overline{h^{(T)}}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w08/lec08.html#why-gradients-explode-or-vanish-ii",
    "href": "lecs/w08/lec08.html#why-gradients-explode-or-vanish-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Gradients Explode or Vanish II",
    "text": "Why Gradients Explode or Vanish II\nWith linear activations: \\[\\frac{\\partial h^{(T)}}{\\partial h^{(1)}} = w^{T-1}\\]\n\\[\\textbf{Exploding:} \\qquad w=1.1, T=50 \\Rightarrow \\frac{\\partial h^{(T)}}{\\partial h^{(1)}} = 117.4\\]\n\\[\\textbf{Vanishing:} \\qquad w=0.9, T=50 \\Rightarrow \\frac{\\partial h^{(T)}}{\\partial h^{(1)}} = 0.00515\\]"
  },
  {
    "objectID": "lecs/w08/lec08.html#multivariate-hidden-states",
    "href": "lecs/w08/lec08.html#multivariate-hidden-states",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multivariate Hidden States",
    "text": "Multivariate Hidden States\nMore generally, in the multivariate case, the Jacobians multiply:\n\\[\\frac{\\partial {\\bf h}^{(T)}}{\\partial {\\bf h}^{(1)}} = \\prod_{t = 1}^{T - 1} \\frac{\\partial {\\bf h}^{(t + 1)}}{\\partial {\\bf h}^{(t)}}\\]\n\nMatrices can “explode” or “vanish” just like scalar values, though it’s slightly harder to make precise."
  },
  {
    "objectID": "lecs/w08/lec08.html#repeated-application-of-functions",
    "href": "lecs/w08/lec08.html#repeated-application-of-functions",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Repeated Application of Functions",
    "text": "Repeated Application of Functions\nAnother way to look at why gradients explode or vanish is that we are applying a function over and over again.\n\nEach hidden layer computes some function of previous hidden layer and the current input: \\({\\bf h}^{(t)} = f({\\bf h}^{(t-1)}, {\\bf x}^{(t)})\\)\n\n\nThis function gets repeatedly applied:\n\\[\\begin{align*}\n{\\bf h}^{(4)} &= f({\\bf h}^{(3)}, {\\bf x}^{(4)}) \\\\\n              &= f(f({\\bf h}^{(2)}, {\\bf x}^{(3)}), {\\bf x}^{(4)}) \\\\\n              &= f(f(f({\\bf h}^{(1)}, {\\bf x}^{(2)}), {\\bf x}^{(3)}), {\\bf x}^{(4)})\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w08/lec08.html#iterated-functions-intuition",
    "href": "lecs/w08/lec08.html#iterated-functions-intuition",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Iterated Functions (Intuition)",
    "text": "Iterated Functions (Intuition)\nWe get complicated behaviour from iterated functions. Consider \\(f(x) = 3.5x(1-x)\\)\n\n\n\n\\(y = f(x)\\)\n\n\n\\(y = f(f(x))\\)\n\n\n\\(y = f^{\\circ 3}(x)\\)\n\n\n\\(y = f^{\\circ 6}(x)\\)\n\n\n\n\nNote that the function values gravitate towards fixed points, and that the derivatives becomes either very large or very small."
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-with-tanh-activation",
    "href": "lecs/w08/lec08.html#rnn-with-tanh-activation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN with tanh activation",
    "text": "RNN with tanh activation\nMore concretely, consider an RNN with a tanh activation function:"
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-with-tanh-activation-ii",
    "href": "lecs/w08/lec08.html#rnn-with-tanh-activation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN with tanh activation II",
    "text": "RNN with tanh activation II\nThe function computed by the network:"
  },
  {
    "objectID": "lecs/w08/lec08.html#cliffs",
    "href": "lecs/w08/lec08.html#cliffs",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cliffs",
    "text": "Cliffs\nRepeatedly applying a function adds a new type possible loss landscape: cliffs, where the gradient of the loss with respect to a parameter is either close to 0, or very large."
  },
  {
    "objectID": "lecs/w08/lec08.html#cliffs-ii",
    "href": "lecs/w08/lec08.html#cliffs-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cliffs II",
    "text": "Cliffs II\nGenerally, the gradient will explode on some inputs and vanish on others. In expectation, the cost may be fairly smooth."
  },
  {
    "objectID": "lecs/w08/lec08.html#gradient-clipping",
    "href": "lecs/w08/lec08.html#gradient-clipping",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Clipping",
    "text": "Gradient Clipping\nOne solution is to “clip” the gradient so that it has a norm of at most \\(\\eta\\). Otherwise, update the gradient \\({\\bf g}\\) with \\({\\bf g} \\leftarrow \\eta\\frac{{\\bf g}}{||{\\bf g}||}\\)\n\nThe gradients are biased, but at least they don’t blow up:\n\n\n\n\n\nGradient clipping solves the exploding gradient problem, but not the vanishing gradient problem."
  },
  {
    "objectID": "lecs/w08/lec08.html#learning-long-term-dependencies",
    "href": "lecs/w08/lec08.html#learning-long-term-dependencies",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Learning Long-Term Dependencies",
    "text": "Learning Long-Term Dependencies\nIdea: Initialization\nHidden units are a kind of memory. Their default behaviour should be to keep their previous value.\n\nIf the function \\({\\bf h}^{(t)} = f({\\bf h}^{(t-1)}, {\\bf x}^{(t)})\\) is close to the identity, then the gradient computations \\(\\displaystyle \\frac{\\partial {\\bf h}^{(t)}}{\\partial {\\bf h}^{(t-1)}}\\) are stable.\n\n\nThis initialization allows learning much longer-term dependencies than “vanilla” RNNs"
  },
  {
    "objectID": "lecs/w08/lec08.html#long-term-short-term-memory",
    "href": "lecs/w08/lec08.html#long-term-short-term-memory",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Long-Term Short Term Memory",
    "text": "Long-Term Short Term Memory\nChange the architecture of the recurrent neural network by replacing each single unit in an RNN by a “memory block”:"
  },
  {
    "objectID": "lecs/w08/lec08.html#lstm",
    "href": "lecs/w08/lec08.html#lstm",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "LSTM",
    "text": "LSTM"
  },
  {
    "objectID": "lecs/w08/lec08.html#lstm-math",
    "href": "lecs/w08/lec08.html#lstm-math",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "LSTM Math",
    "text": "LSTM Math\nIn each step, we have a vector of memory cells \\({\\bf c}\\), a vector of hidden units \\({\\bf h}\\) and vectors of input, output, and forget gates \\({\\bf i}\\), \\({\\bf o}\\), and \\({\\bf f}\\).\n\nThere’s a full set of connections from all the inputs and hiddens to the inputs and all of the gates:\n\\[\n\\begin{pmatrix}\n\\bf{i}_t \\\\\n\\bf{f}_t \\\\\n\\bf{o}_t \\\\\n\\bf{g}_t \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\sigma \\\\\n\\sigma \\\\\n\\sigma \\\\\n\\tanh\n\\end{pmatrix} {\\bf W}\n\\begin{pmatrix}\n{\\bf x}_t \\\\\n{\\bf h}_{t - 1}\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "lecs/w08/lec08.html#lstm-math-ii",
    "href": "lecs/w08/lec08.html#lstm-math-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "LSTM Math II",
    "text": "LSTM Math II\n\\[\\begin{align*}\n{\\bf c_t } &= {\\bf f_t } \\circ {\\bf c_{t-1} } + {\\bf i_t } \\circ {\\bf g_t } \\\\\n{\\bf h_t } &= {\\bf o_t } \\circ \\tanh({\\bf c_t })\n\\end{align*}\\]\n\nExercise: show that if \\({\\bf f}_{t+1} = 1\\), \\({\\bf i}_{t+1} = 0\\), and \\({\\bf o}_{t} = 0\\), then the gradient of the memory cell gets passed through unmodified, i.e., \\(\\bar{{\\bf c}_t} = \\bar{{\\bf c}_{t+1}}\\)."
  },
  {
    "objectID": "lecs/w08/lec08.html#key-takeaways",
    "href": "lecs/w08/lec08.html#key-takeaways",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nYou should be able to understand…\n\nwhy learning long-term dependencies is hard for vanilla RNNs\n\n\n\nwhy gradients vanish/explode in a vanilla RNN\n\n\n\n\nwhat cliffs are and how repeated application of a function generates cliffs\n\n\n\n\nwhat gradient clipping is and when it is useful\n\n\n\n\nthe mathematics behind why gating works"
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-hidden-states",
    "href": "lecs/w08/lec08.html#rnn-hidden-states",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN Hidden States",
    "text": "RNN Hidden States\nRNN For Prediction:\n\nProcess tokens one at a time\n\n\n\nHidden state is a representation of all the tokens read thus far\n\n\n\nRNN For Generation:\n\nGenerate tokens one at a time\n\n\n\n\nHidden state is a representation of all the tokens to be generated"
  },
  {
    "objectID": "lecs/w08/lec08.html#rnn-hidden-state-updates",
    "href": "lecs/w08/lec08.html#rnn-hidden-state-updates",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN Hidden State Updates",
    "text": "RNN Hidden State Updates\nRNN for Prediction:\n\nUpdate hidden state with new input (token)\n\n\n\nGet prediction (e.g. distribution over possible labels)\n\n\n\nRNN for Generation:\n\nGet prediction distribution of next token\n\n\n\n\nGenerate a token from the distribution\n\n\n\n\nUpdate the hidden state with new token"
  },
  {
    "objectID": "lecs/w08/lec08.html#text-generation-diagram",
    "href": "lecs/w08/lec08.html#text-generation-diagram",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation Diagram",
    "text": "Text Generation Diagram\n\n\n\n\nGet prediction distribution of next token\n\n\n\nGenerate a token from the distribution\n\n\n\n\nUpdate the hidden state with new token"
  },
  {
    "objectID": "lecs/w08/lec08.html#test-time-behaviour-of-generative-rnn",
    "href": "lecs/w08/lec08.html#test-time-behaviour-of-generative-rnn",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Test Time Behaviour of Generative RNN",
    "text": "Test Time Behaviour of Generative RNN\nUnlike other models we discussed so far, the training time behaviour of Generative RNNs will be different from the test time behaviour\n\nTest time behaviour at each time step:\n\nObtain a distribution over possible next tokens\n\n\n\n\nSample a token from that distribution\n\n\n\n\nUpdate the hidden state based on the sample token"
  },
  {
    "objectID": "lecs/w08/lec08.html#training-time-behaviour-of-generative-rnn",
    "href": "lecs/w08/lec08.html#training-time-behaviour-of-generative-rnn",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Training Time Behaviour of Generative RNN",
    "text": "Training Time Behaviour of Generative RNN\nDuring training, we try to get the RNN to generate one particular sequence in the training set. At each time step:\n\nObtain a distribution over possible next tokens\n\n\n\nCompare this with the actual next token\n\n\n\nQ1: What kind of a problem is this? (regression or classification?)\n\n\nQ2: What loss function should we use during training?\n\n\nA1: The problem is classification. The different tokens are the respective classes.\nA2: The loss is a cross entropy loss as usual in classification"
  },
  {
    "objectID": "lecs/w08/lec08.html#text-generation-first-step",
    "href": "lecs/w08/lec08.html#text-generation-first-step",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation: First Step",
    "text": "Text Generation: First Step\n\n\n\n\nStart with an initial hidden state\n\n\n\nUpdate the hidden state with a “&lt;BOS&gt;” (beginning of string) token to initiate the hidden state\n\n\n\n\nGet the distribution over the first character\n\n\n\n\nCompute the cross-entropy loss against the ground truth (R)\n\n\n\nEach token is its own classification problem. This is the first one."
  },
  {
    "objectID": "lecs/w08/lec08.html#text-generation-with-teacher-forcing",
    "href": "lecs/w08/lec08.html#text-generation-with-teacher-forcing",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation with Teacher Forcing",
    "text": "Text Generation with Teacher Forcing\n\n\n\n\nUpdate the hidden state with the ground truth token (R) regardless of the prediction from the previous step\n\nThis technique is called teaching forcing\n\n\n\n\nGet the distribution over the second character\n\n\n\n\nCompute the cross-entropy loss against the ground truth (I)"
  },
  {
    "objectID": "lecs/w08/lec08.html#text-generation-later-steps",
    "href": "lecs/w08/lec08.html#text-generation-later-steps",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation: Later Steps",
    "text": "Text Generation: Later Steps\n\n\n\nContinue until we get to the “&lt;EOS&gt;” (end of string) token"
  },
  {
    "objectID": "lecs/w08/lec08.html#some-remaining-challenges",
    "href": "lecs/w08/lec08.html#some-remaining-challenges",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Some Remaining Challenges",
    "text": "Some Remaining Challenges\n\nVocabularies can be very large once you include people, places, etc.\n\n\n\nIt’s computationally difficult to predict distributions over millions of words.\n\n\n\n\nHow do we deal with words we haven’t seen before?\n\n\n\n\nIn some languages, it’s hard to define what should be considered a word."
  },
  {
    "objectID": "lecs/w08/lec08.html#character-vs-word-level",
    "href": "lecs/w08/lec08.html#character-vs-word-level",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Character vs Word-level",
    "text": "Character vs Word-level\nAnother approach is to model text one character at a time\n\nThis solves the problem of what to do about previously unseen words.\n\n\nNote that long-term memory is essential at the character level!"
  },
  {
    "objectID": "lecs/w08/lec08.html#neural-machine-translation",
    "href": "lecs/w08/lec08.html#neural-machine-translation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Neural Machine Translation",
    "text": "Neural Machine Translation\nSay we want to translate, e.g. English to French sentences.\n\nWe have pairs of translated sentences to train on.\n\n\nHere, both the inputs and outputs are sequences!\nWhat can we do?"
  },
  {
    "objectID": "lecs/w08/lec08.html#sequence-to-sequence-architecture-1",
    "href": "lecs/w08/lec08.html#sequence-to-sequence-architecture-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Sequence-to-sequence architecture",
    "text": "Sequence-to-sequence architecture\n\nThe network first reads and memorizes the sentences.\n\nWhen it sees the “end token”, it starts outputting the translation.\n\n\nThe “encoder” and “decoder” are two different networks with different weights."
  },
  {
    "objectID": "lecs/w08/lec08.html#summary",
    "href": "lecs/w08/lec08.html#summary",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Summary",
    "text": "Summary\n\nRecurrent Neural Networks can be used for learning sequence data\nTraining RNNs may suffer from gradient explosion and vanishing\nImportant Applications of RNNs are text generation and sequence to sequence modelling"
  },
  {
    "objectID": "lecs/w09/lec09.html#announcements",
    "href": "lecs/w09/lec09.html#announcements",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Announcements",
    "text": "Announcements\n\nThe project plan is due early next week (March 21)"
  },
  {
    "objectID": "lecs/w09/lec09.html#in-what-situation-would-you-use-an-rnn",
    "href": "lecs/w09/lec09.html#in-what-situation-would-you-use-an-rnn",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "In what Situation would you use an RNN?",
    "text": "In what Situation would you use an RNN?\n\nWhen we want to make predictions about a sequence\nWhen we want to make predictions about an image\nWhen we want to generate a sequence\nWhen we want to generate an image"
  },
  {
    "objectID": "lecs/w09/lec09.html#using-glove-embeddings-of-words-as-input-features-to-an-rnn-is-an-example-of",
    "href": "lecs/w09/lec09.html#using-glove-embeddings-of-words-as-input-features-to-an-rnn-is-an-example-of",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Using GloVe Embeddings of Words as Input Features to an RNN is an Example of…",
    "text": "Using GloVe Embeddings of Words as Input Features to an RNN is an Example of…\n\nLSTM\nGRU\nMLP\nTransfer Learning"
  },
  {
    "objectID": "lecs/w09/lec09.html#what-is-a-disadvantage-of-using-a-vanilla-rnn-without-gating",
    "href": "lecs/w09/lec09.html#what-is-a-disadvantage-of-using-a-vanilla-rnn-without-gating",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What is a Disadvantage of using a Vanilla RNN (without gating)?",
    "text": "What is a Disadvantage of using a Vanilla RNN (without gating)?\n\nIt is challenging to learn long-term dependencies\nThe gradient might vanish\nThe gradient might explode\nAll of the above\nNone of the above"
  },
  {
    "objectID": "lecs/w09/lec09.html#what-is-a-cliff",
    "href": "lecs/w09/lec09.html#what-is-a-cliff",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What is a Cliff?",
    "text": "What is a Cliff?\nA part of the loss landscape where the gradient of the loss with respect to a parameter…\n\n… is large\n… is close to 0\n… is close to 0 in most regions, and very large in some places"
  },
  {
    "objectID": "lecs/w09/lec09.html#when-deep-learning-practitioners-talk-about-rnns-they-are-usually-referring-to",
    "href": "lecs/w09/lec09.html#when-deep-learning-practitioners-talk-about-rnns-they-are-usually-referring-to",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "When Deep Learning Practitioners talk about RNNs, they are usually referring to…",
    "text": "When Deep Learning Practitioners talk about RNNs, they are usually referring to…\n\nvanilla RNN (without gating)\nRNN with LSTM units, or sometimes GRU units"
  },
  {
    "objectID": "lecs/w09/lec09.html#text-generation-with-rnn",
    "href": "lecs/w09/lec09.html#text-generation-with-rnn",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation with RNN",
    "text": "Text Generation with RNN\nRNN For Prediction:\n\nProcess tokens one at a time\n\n\n\nHidden state is a representation of all the tokens read thus far"
  },
  {
    "objectID": "lecs/w09/lec09.html#text-generation-with-rnn-ii",
    "href": "lecs/w09/lec09.html#text-generation-with-rnn-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation with RNN II",
    "text": "Text Generation with RNN II\nRNN For Generation:\n\nGenerate tokens one at a time\n\n\n\nHidden state is a representation of all the tokens to be generated"
  },
  {
    "objectID": "lecs/w09/lec09.html#rnn-hidden-state-updates",
    "href": "lecs/w09/lec09.html#rnn-hidden-state-updates",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN Hidden State Updates",
    "text": "RNN Hidden State Updates\nRNN For Prediction:\n\nUpdate hidden state with new input (token)\n\n\n\nGet prediction (e.g. distribution over possible labels)\n\n\n\nRNN For Generation:\n\nGet prediction distribution of next token\n\n\n\n\nGenerate a token from the distribution\n\n\n\n\nUpdate the hidden state with new token"
  },
  {
    "objectID": "lecs/w09/lec09.html#text-generation-diagram",
    "href": "lecs/w09/lec09.html#text-generation-diagram",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation Diagram",
    "text": "Text Generation Diagram\n\n\n\n\nGet prediction distribution of next token\n\n\n\nGenerate a token from the distribution\n\n\n\n\nUpdate the hidden state with new token:"
  },
  {
    "objectID": "lecs/w09/lec09.html#test-time-behaviour-of-generative-rnn",
    "href": "lecs/w09/lec09.html#test-time-behaviour-of-generative-rnn",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Test Time Behaviour of Generative RNN",
    "text": "Test Time Behaviour of Generative RNN\nUnlike other models we discussed so far, the training time behaviour of Generative RNNs will be different from the test time behaviour\nTest time behaviour:\n\nAt each time step:\n\nObtain a distribution over possible next tokens\nSample a token from that distribution\nUpdate the hidden state based on the sample token"
  },
  {
    "objectID": "lecs/w09/lec09.html#training-time-behaviour-of-generative-rnn",
    "href": "lecs/w09/lec09.html#training-time-behaviour-of-generative-rnn",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Training Time Behaviour of Generative RNN",
    "text": "Training Time Behaviour of Generative RNN\nDuring training, we try to get the RNN to generate one particular sequence in the training set:\n\nAt each time step:\n\nObtain a distribution over possible next tokens\nCompare this with the actual next token\n\n\nQ1: What kind of a problem is this? (regression or classification?)\nQ2: What loss function should we use during training?"
  },
  {
    "objectID": "lecs/w09/lec09.html#text-generation-step-1",
    "href": "lecs/w09/lec09.html#text-generation-step-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation: Step 1",
    "text": "Text Generation: Step 1\n\n\n\nFirst classification problem:\n\nStart with an initial hidden state\n\n\n\nUpdate the hidden state with a “&lt;BOS&gt;” (beginning of string) token, so that the hidden state becomes meaningful (not just zeros)"
  },
  {
    "objectID": "lecs/w09/lec09.html#text-generation-step-1-ii",
    "href": "lecs/w09/lec09.html#text-generation-step-1-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation: Step 1 II",
    "text": "Text Generation: Step 1 II\n\n\n\nFirst classification problem:\n\nGet the distribution over the first character\n\n\n\nCompute the cross-entropy loss against the ground truth (R)"
  },
  {
    "objectID": "lecs/w09/lec09.html#text-generation-with-teacher-forcing",
    "href": "lecs/w09/lec09.html#text-generation-with-teacher-forcing",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation with Teacher Forcing",
    "text": "Text Generation with Teacher Forcing\n\n\n\nSecond classification problem:\n\nUpdate the hidden state with the ground truth token (R) regardless of the prediction from the previous step\n\nThis technique is called teacher forcing"
  },
  {
    "objectID": "lecs/w09/lec09.html#text-generation-with-teacher-forcing-ii",
    "href": "lecs/w09/lec09.html#text-generation-with-teacher-forcing-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation with Teacher Forcing II",
    "text": "Text Generation with Teacher Forcing II\n\n\n\nSecond classification problem:\n\nGet the distribution over the second character\n\n\n\nCompute the cross-entropy loss against the ground truth (I)"
  },
  {
    "objectID": "lecs/w09/lec09.html#text-generation-later-steps",
    "href": "lecs/w09/lec09.html#text-generation-later-steps",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Text Generation: Later Steps",
    "text": "Text Generation: Later Steps\n\nContinue until we get to the “&lt;EOS&gt;” (end of string) token"
  },
  {
    "objectID": "lecs/w09/lec09.html#some-remaining-challenges",
    "href": "lecs/w09/lec09.html#some-remaining-challenges",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Some Remaining Challenges",
    "text": "Some Remaining Challenges\n\nVocabularies can be very large once you include people, places, etc.\n\n\n\nIt’s computationally difficult to predict distributions over millions of words.\n\n\n\n\nHow do we deal with words we haven’t seen before?\n\n\n\n\nIn some languages (e.g. German), it’s hard to define what should be considered a word."
  },
  {
    "objectID": "lecs/w09/lec09.html#character-vs-word-level",
    "href": "lecs/w09/lec09.html#character-vs-word-level",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Character vs Word-level",
    "text": "Character vs Word-level\nAnother approach is to model text one character at a time\nThis solves the problem of what to do about previously unseen words.\nNote that long-term memory is essential at the character level!"
  },
  {
    "objectID": "lecs/w09/lec09.html#recurrent-neural-networks",
    "href": "lecs/w09/lec09.html#recurrent-neural-networks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\nIn lecture 8, we showed a discriminative RNN that makes a prediction based on a sequence (sequence as an input).\nIn the week 11 tutorial, we will build a generator RNN to generate sequences (sequence as an output)"
  },
  {
    "objectID": "lecs/w09/lec09.html#sequence-to-sequence-tasks",
    "href": "lecs/w09/lec09.html#sequence-to-sequence-tasks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Sequence-to-sequence Tasks",
    "text": "Sequence-to-sequence Tasks\nAnother common example of a sequence-to-sequence task (seq2seq) is machine translation.\n\nThe network first reads and memorizes the sentences. When it sees the “end token”, it starts outputting the translation. The “encoder” and “decoder” are two different networks with different weights."
  },
  {
    "objectID": "lecs/w09/lec09.html#how-seq2seq-works",
    "href": "lecs/w09/lec09.html#how-seq2seq-works",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "How Seq2Seq Works",
    "text": "How Seq2Seq Works\nThe encoder network reads an input sentence and stores all the information in its hidden units.\nThe decoder network then generates the output sentence one word at a time."
  },
  {
    "objectID": "lecs/w09/lec09.html#how-seq2seq-works-ii",
    "href": "lecs/w09/lec09.html#how-seq2seq-works-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "How Seq2Seq Works II",
    "text": "How Seq2Seq Works II\n\nBut some sentences can be really long. Can we really store all the information in a vector of hidden units?\nHuman translators refer back to the input."
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-based-machine-translation",
    "href": "lecs/w09/lec09.html#attention-based-machine-translation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention-Based Machine Translation",
    "text": "Attention-Based Machine Translation\nWe’ll look at the translation model from the classic paper:\n\nBahdanau et al., Neural machine translation by jointly learning to align and translate. ICLR, 2015.\n\nBasic idea: each output word comes from one input word, or a handful of input words. Maybe we can learn to attend to only the relevant ones as we produce the output.\nWe’ll use the opportunity to look at architectural changes we can make to RNN models to make it even more performant."
  },
  {
    "objectID": "lecs/w09/lec09.html#encoder-decoder-architectures",
    "href": "lecs/w09/lec09.html#encoder-decoder-architectures",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Encoder & Decoder Architectures",
    "text": "Encoder & Decoder Architectures\nThe encoder computes an annotation (hidden state) of each word in the input.\n\nThe encoder is a bidirectional RNN\n\n\nThe decoder network is also an RNN, and makes predictions one word at a time.\n\nThe decoder uses an attention mechanism (RNN with attention)"
  },
  {
    "objectID": "lecs/w09/lec09.html#encoder-bidirectional-rnn",
    "href": "lecs/w09/lec09.html#encoder-bidirectional-rnn",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Encoder: Bidirectional RNN",
    "text": "Encoder: Bidirectional RNN\nThe encoder is a bidirectional RNN. We have two RNNs: one that runs forward and one that runs backwards. These RNNs can be LSTMs or GRUs.\nThe annotation of a word is the concatenation of the forward and backward hidden vectors."
  },
  {
    "objectID": "lecs/w09/lec09.html#decoder-rnn-with-attention",
    "href": "lecs/w09/lec09.html#decoder-rnn-with-attention",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Decoder: RNN with Attention",
    "text": "Decoder: RNN with Attention\nThe decoder network is also an RNN, and makes predictions one word at a time.\n\n\n\n\n\n\nThe difference is that it also derives a context vector \\({\\bf c}^{(t)}\\) at each time step, computed by attending to the inputs"
  },
  {
    "objectID": "lecs/w09/lec09.html#intuition-behind-attending-to-the-input",
    "href": "lecs/w09/lec09.html#intuition-behind-attending-to-the-input",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Intuition Behind “Attending to the Input”",
    "text": "Intuition Behind “Attending to the Input”\n\n“My language model tells me the next word should be an adjective. Find me an adjective in the input”\n\nWe would like to refer back to one (or a few) of the input words to help with the translation task (e.g. find the adjective)\nIf you were programming a translator, you might…"
  },
  {
    "objectID": "lecs/w09/lec09.html#intuition-behind-aattending-to-the-input-ii",
    "href": "lecs/w09/lec09.html#intuition-behind-aattending-to-the-input-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Intuition Behind Aattending to the Input” II",
    "text": "Intuition Behind Aattending to the Input” II\nIf you were programming a translator, you might\n\nfind an input word that is most likely an adjective (the attention function)\nlook up the input word that is the adjective (the weighted average)\ntranslate the input word, e.g. using the input word, and a dictionary (the projection MLP)\n\nAn attentional decoder is like a continuous form of these last three steps."
  },
  {
    "objectID": "lecs/w09/lec09.html#the-math-behind-attending-to-the-input",
    "href": "lecs/w09/lec09.html#the-math-behind-attending-to-the-input",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Math Behind “Attending to the Input”",
    "text": "The Math Behind “Attending to the Input”\n\n“My language model tells me the next word should be an adjective. Find me an adjective in the input”\n\nThe context vector is computed as a weighted average of the encoder’s annotations:\n\\[{\\bf c}^{(i)} = \\sum_j \\alpha_{ij} {\\bf h}^{(j)}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#the-math-behind-attending-to-the-input-ii",
    "href": "lecs/w09/lec09.html#the-math-behind-attending-to-the-input-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Math Behind “Attending to the Input” II",
    "text": "The Math Behind “Attending to the Input” II\nThe attention weights are computed as a softmax, where the input depends on the annotation \\({\\bf h}^{(j)}\\) and the decoder states \\({\\bf s}^{(t)}\\):\n\\[\ne_{ij} = a({\\bf s}^{(i-1)}, {\\bf h}^{(j)}), \\qquad \\alpha_{ij} = \\frac{ \\exp(e_{ij}) }{\\sum_{j^\\prime} exp(e_{ij^\\prime})}\n\\]\nThe attention function depends on the annotation vector, rather than the position in the sentence. It is a form of content-based addressing."
  },
  {
    "objectID": "lecs/w09/lec09.html#example-how-to-obtain-a-context-vector",
    "href": "lecs/w09/lec09.html#example-how-to-obtain-a-context-vector",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: How to Obtain a Context Vector?",
    "text": "Example: How to Obtain a Context Vector?\n\\[\\begin{align*}\n{\\bf h}^{(1)} &= \\begin{bmatrix}1 & 3 & 9\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(2)} &= \\begin{bmatrix}0 & 0 & 1\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(3)} &= \\begin{bmatrix}5 & -1 & 2\\end{bmatrix}^\\top \\\\\n\\\\\n{\\bf c}^{(t)} &= \\begin{bmatrix}? & ? & ?\\end{bmatrix}^\\top \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#example-average-pooling-is-context-independent",
    "href": "lecs/w09/lec09.html#example-average-pooling-is-context-independent",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: Average Pooling is Context Independent",
    "text": "Example: Average Pooling is Context Independent\n\\[\\begin{align*}\n{\\bf h}^{(1)} &= \\begin{bmatrix}1 & 3 & 9\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(2)} &= \\begin{bmatrix}0 & 0 & 1\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(3)} &= \\begin{bmatrix}5 & -1 & 2\\end{bmatrix}^\\top \\\\\n\\\\\n{\\bf c}^{(t)} &= \\text{average}({\\bf h}^{(1)} , {\\bf h}^{(2)}, {\\bf h}^{(3)})\\\\\n&= \\begin{bmatrix}2 & 0.6 & 1\\end{bmatrix}^\\top \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#example-attention",
    "href": "lecs/w09/lec09.html#example-attention",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: Attention",
    "text": "Example: Attention\n\\[\\begin{align*}\n{\\bf h}^{(1)} &= \\begin{bmatrix}1 & 3 & 9\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(2)} &= \\begin{bmatrix}0 & 0 & 1\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(3)} &= \\begin{bmatrix}5 & -1 & 2\\end{bmatrix}^\\top \\\\\n\\\\\n{\\bf s}^{(t-1)} &= \\begin{bmatrix}0 & 1 & 1\\end{bmatrix}^\\top \\\\\n\\alpha_t &= \\text{softmax}\\left(\\begin{bmatrix}\nf({\\bf s}^{(t-1)}, {\\bf h}^{(1)}) \\\\\nf({\\bf s}^{(t-1)}, {\\bf h}^{(2)}) \\\\\nf({\\bf s}^{(t-1)}, {\\bf h}^{(3)}) \\\\\n\\end{bmatrix}\\right) = \\begin{bmatrix}\\alpha_{t1} \\\\ \\alpha_{t2} \\\\ \\alpha_{t3}\\end{bmatrix}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#example-dot-product-attention",
    "href": "lecs/w09/lec09.html#example-dot-product-attention",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: Dot-Product Attention",
    "text": "Example: Dot-Product Attention\n\\[\\begin{align*}\n{\\bf h}^{(1)} &= \\begin{bmatrix}1 & 3 & 9\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(2)} &= \\begin{bmatrix}0 & 0 & 1\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(3)} &= \\begin{bmatrix}5 & -1 & 2\\end{bmatrix}^\\top \\\\\n\\\\\n{\\bf s}^{(t-1)} &= \\begin{bmatrix}0 & 1 & 1\\end{bmatrix}^\\top \\\\\n\\alpha_t &= \\text{softmax}\\left(\\begin{bmatrix}\n               {\\bf s}^{(t-1)} \\cdot {\\bf h}^{(1)} \\\\\n               {\\bf s}^{(t-1)} \\cdot {\\bf h}^{(2)} \\\\\n               {\\bf s}^{(t-1)} \\cdot {\\bf h}^{(3)} \\\\\n               \\end{bmatrix}\\right) = \\begin{bmatrix}\\alpha_{t1} \\\\ \\alpha_{t2} \\\\ \\alpha_{t3}\\end{bmatrix}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#example-dot-product-attention-ii",
    "href": "lecs/w09/lec09.html#example-dot-product-attention-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: Dot-Product Attention II",
    "text": "Example: Dot-Product Attention II\n\\[\\begin{align*}\n{\\bf h}^{(1)} &= \\begin{bmatrix}1 & 3 & 9\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(2)} &= \\begin{bmatrix}0 & 0 & 1\\end{bmatrix}^\\top \\qquad \\quad {\\bf s}^{(t-1)} = \\begin{bmatrix}0 & 1 & 1\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(3)} &= \\begin{bmatrix}5 & -1 & 2\\end{bmatrix}^\\top \\\\\n\\\\\n\\alpha_t &= \\text{softmax}\\left(\\begin{bmatrix}1 & 0 & 5 \\\\ 3 & 0 & -1 \\\\ 0 & 1 &2 \\end{bmatrix}^\\top\n                    \\begin{bmatrix}0 \\\\ 1 \\\\ 1\\end{bmatrix}\\right) \\\\\n{\\bf c}^{(t)} &= \\alpha_{t1} {\\bf h}^{(1)} + \\alpha_{t2} {\\bf h}^{(2)} + \\alpha_{t2} {\\bf h}^{(3)}\\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-demo-video",
    "href": "lecs/w09/lec09.html#attention-demo-video",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention Demo Video",
    "text": "Attention Demo Video\nhttps://play.library.utoronto.ca/watch/9ed8b3c497f82b510e9ecf441c5eef4f"
  },
  {
    "objectID": "lecs/w09/lec09.html#visualization-of-attention",
    "href": "lecs/w09/lec09.html#visualization-of-attention",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Visualization of Attention",
    "text": "Visualization of Attention\nVisualization of the attention map (the \\(\\alpha_{ij}\\)s at each time step)\n\n\n\nNothing forces the model to go (roughly) linearly through the input sentences, but somehow it learns to do it!"
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-performance",
    "href": "lecs/w09/lec09.html#attention-performance",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention Performance",
    "text": "Attention Performance\nThe attention-based translation model does much better than the encoder/decoder model on long sentences."
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-based-caption-generation",
    "href": "lecs/w09/lec09.html#attention-based-caption-generation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention-Based Caption Generation",
    "text": "Attention-Based Caption Generation\nCaption Generation Task:\n\nInput: Image\n\n\n\nOutput: Caption (sequence of words or characters)"
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-based-caption-generation-ii",
    "href": "lecs/w09/lec09.html#attention-based-caption-generation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention-Based Caption Generation II",
    "text": "Attention-Based Caption Generation II\nAttention can also be used to understand images.\n\nWe humans can’t process a whole visual scene at once.\n\n\n\nThe fovea of the eye gives us high-acuity vision in only a tiny region of our field of view.\n\n\n\n\nInstead, we must integrate information from a series of glimpses."
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-based-caption-generation-iii",
    "href": "lecs/w09/lec09.html#attention-based-caption-generation-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention-Based Caption Generation III",
    "text": "Attention-Based Caption Generation III\nThe next few slides are based on this paper from the UofT machine learning group:\n\nXu et al. Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention. ICML, 2015."
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-for-caption-generation",
    "href": "lecs/w09/lec09.html#attention-for-caption-generation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention for Caption Generation",
    "text": "Attention for Caption Generation\nThe caption generation task: take an image as input, and produce a sentence describing the image.\n\nEncoder: a classification conv net like VGG. This computes a bunch of feature maps over the image."
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-for-caption-generation-ii",
    "href": "lecs/w09/lec09.html#attention-for-caption-generation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention for Caption Generation II",
    "text": "Attention for Caption Generation II\n\nDecoder: an attention-based RNN, analogous to the decoder in the translation model\n\nIn each time step, the decoder computes an attention map over the entire image, effectively deciding which regions to focus on.\nIt receives a context vector, which is the weighted average of the convnet features."
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-for-caption-generation-iii",
    "href": "lecs/w09/lec09.html#attention-for-caption-generation-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention for Caption Generation III",
    "text": "Attention for Caption Generation III\nSimilar math as before: difference is that \\(j\\) is a pixel location\n\\[\\begin{align*}\ne_{ij} &= a({\\bf s}^{(i-1)}, {\\bf h}^{(j)}) \\\\\n\\alpha_{ij} &= \\frac{ \\exp(e_{ij}) }{\\sum_{j^\\prime} exp(e_{ij^\\prime})}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#what-attention-tells-us",
    "href": "lecs/w09/lec09.html#what-attention-tells-us",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What Attention tells us",
    "text": "What Attention tells us\nThis lets us understand where the network is looking as it generates a sentence."
  },
  {
    "objectID": "lecs/w09/lec09.html#what-attention-tells-us-about-mistakes",
    "href": "lecs/w09/lec09.html#what-attention-tells-us-about-mistakes",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What Attention tells us about mistakes",
    "text": "What Attention tells us about mistakes\nThis can also help us understand the network’s mistakes."
  },
  {
    "objectID": "lecs/w09/lec09.html#multi-layer-rnns",
    "href": "lecs/w09/lec09.html#multi-layer-rnns",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multi-Layer RNNs",
    "text": "Multi-Layer RNNs\nFinally, to get more capacity/performance out of RNNs, you can stack multiple RNN’s together!\nThe hidden state of your first RNN becomes the input to your second layer RNN."
  },
  {
    "objectID": "lecs/w09/lec09.html#rnn-disadvantage",
    "href": "lecs/w09/lec09.html#rnn-disadvantage",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "RNN Disadvantage",
    "text": "RNN Disadvantage\nOne disadvantage of RNNS (and especially multi-layer RNNs) is that they require a long time to train, and are more difficult to parallelize. (Need the previous hidden state \\(h^{(t)}\\) to be able to compute \\(h^{(t+1)}\\))"
  },
  {
    "objectID": "lecs/w09/lec09.html#chatgpt",
    "href": "lecs/w09/lec09.html#chatgpt",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ChatGPT",
    "text": "ChatGPT\n\n\n\n\nLogo of OpenAI’s ChatGPT"
  },
  {
    "objectID": "lecs/w09/lec09.html#chatgpt-ii",
    "href": "lecs/w09/lec09.html#chatgpt-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ChatGPT II",
    "text": "ChatGPT II\nWhat is ChatGPT? We’ll let it speak for itself:\n\nI am ChatGPT, a large language model developed by OpenAI. I use machine learning algorithms to generate responses to questions and statements posed to me by users. I am designed to understand and generate natural language responses in a variety of domains and topics, from general knowledge to specific technical fields. My purpose is to assist users in generating accurate and informative responses to their queries and to provide helpful insights and suggestions."
  },
  {
    "objectID": "lecs/w09/lec09.html#chatgpt-iii",
    "href": "lecs/w09/lec09.html#chatgpt-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ChatGPT III",
    "text": "ChatGPT III\nChatGPT is based on OpenAI’s GPT-3, which itself is based on the transformer architecture."
  },
  {
    "objectID": "lecs/w09/lec09.html#transformer-1",
    "href": "lecs/w09/lec09.html#transformer-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Transformer",
    "text": "Transformer\nIdea: Do away with recurrent networks altogether; instead exclusively use attention to obtain the history at the hidden layers\n\n\n\n\nVaswani, Ashish, et al. ”Attention is all you need.” Advances in Neural Information Processing Systems. 2017."
  },
  {
    "objectID": "lecs/w09/lec09.html#better-language-models",
    "href": "lecs/w09/lec09.html#better-language-models",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Better Language Models",
    "text": "Better Language Models\nhttps://openai.com/blog/better-language-models/\n\nInput: Human-Written Prompt (small paragraph)\n\n\n\nOutput: Article about the topic (many paragraphs)"
  },
  {
    "objectID": "lecs/w09/lec09.html#transformer-tutorial",
    "href": "lecs/w09/lec09.html#transformer-tutorial",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Transformer Tutorial",
    "text": "Transformer Tutorial\nwww.youtube.com/watch?v=XSSTuhyAmnI&ab_channel=AriSeff"
  },
  {
    "objectID": "lecs/w09/lec09.html#vaswani-ashish-et-al.-attention-is-all-you-need.",
    "href": "lecs/w09/lec09.html#vaswani-ashish-et-al.-attention-is-all-you-need.",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Vaswani, Ashish, et al. Attention is all you need.",
    "text": "Vaswani, Ashish, et al. Attention is all you need.\n\n\n\n\n\n\ntypical encoder-decoder blocks of a transformer\n\n\n\n\nTransformer has a encoder-decoder architecture similar to the previous sequence-to-sequence RNN models, except all the recurrent connections are replaced by the attention modules."
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-mapping",
    "href": "lecs/w09/lec09.html#attention-mapping",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention Mapping",
    "text": "Attention Mapping\nIn general, attention mapping can be described as a function of a query and a set of key-value pairs. Transformer uses a “scaled dot-product attention” to obtain the context vector:\n\\[\\begin{align*}\n{\\bf c}^{(t)} = \\text{attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_K}}\\right) V\n\\end{align*}\\]\nThis is very similar to the attetion mechanism we saw eariler, but we scale the pre-softmax values (the logits) down by the square root of the key dimension \\(d_K\\)."
  },
  {
    "objectID": "lecs/w09/lec09.html#attention-mapping-in-the-decoder",
    "href": "lecs/w09/lec09.html#attention-mapping-in-the-decoder",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attention Mapping in the Decoder",
    "text": "Attention Mapping in the Decoder\nWhen training the decoder (e.g. to generate a sequence), we desired output so that have to be careful to mask out the desired output so that we preserve the autoregressive property."
  },
  {
    "objectID": "lecs/w09/lec09.html#recall-dot-product-attention",
    "href": "lecs/w09/lec09.html#recall-dot-product-attention",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Recall: Dot-Product Attention",
    "text": "Recall: Dot-Product Attention\n\\[\\begin{align*}\n{\\bf h}^{(1)} &= \\begin{bmatrix}1 & 3 & 9\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(2)} &= \\begin{bmatrix}0 & 0 & 1\\end{bmatrix}^\\top \\qquad \\quad {\\bf s}^{(t-1)} = \\begin{bmatrix}0 & 1 & 1\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(3)} &= \\begin{bmatrix}5 & -1 & 2\\end{bmatrix}^\\top \\\\\n\\\\\n\\alpha_t &= \\text{softmax}\\left(\\begin{bmatrix}1 & 0 & 5 \\\\ 3 & 0 & -1 \\\\ 0 & 1 &2 \\end{bmatrix}^\\top\n                    \\begin{bmatrix}0 \\\\ 1 \\\\ 1\\end{bmatrix}\\right) \\\\\n{\\bf c}^{(t)} &= \\alpha_{t1} {\\bf h}^{(1)} + \\alpha_{t2} {\\bf h}^{(2)} + \\alpha_{t2} {\\bf h}^{(3)}\\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#scaled-dot-product-attention",
    "href": "lecs/w09/lec09.html#scaled-dot-product-attention",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Scaled Dot-Product Attention",
    "text": "Scaled Dot-Product Attention\n\\[\\begin{align*}\n{\\bf h}^{(1)} &= \\begin{bmatrix}1 & 3 & 9\\end{bmatrix}^\\top \\qquad \\quad {\\bf h}^{(2)} = \\begin{bmatrix}0 & 0 & 1\\end{bmatrix}^\\top \\\\\n{\\bf h}^{(3)} &= \\begin{bmatrix}5 & -1 & 2\\end{bmatrix}^\\top \\qquad \\quad {\\bf s}^{(t-1)} = \\begin{bmatrix}0 & 1 & 1\\end{bmatrix}^\\top \\\\\n\\alpha_t &= \\text{softmax}\\left(\\frac{1}{\\sqrt{3}} \\begin{bmatrix}1 & 0 & 5 \\\\ 3 & 0 & -1 \\\\ 0 & 1 &2 \\end{bmatrix}^\\top\n                    \\begin{bmatrix}0 \\\\ 1 \\\\ 1\\end{bmatrix}\\right) \\\\\n{\\bf c}^{(t)} &= \\alpha_{t1} {\\bf h}^{(1)} + \\alpha_{t2} {\\bf h}^{(2)} + \\alpha_{t2} {\\bf h}^{(3)}\\\\\n\\end{align*}\\] Q: Which values represent the Q, K, and V?"
  },
  {
    "objectID": "lecs/w09/lec09.html#attending-to-the-input-encoder",
    "href": "lecs/w09/lec09.html#attending-to-the-input-encoder",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attending to the Input (Encoder)",
    "text": "Attending to the Input (Encoder)\nTransformer models attend to both the encoder annotations and its previous hidden layers.\n\n\n\n\nhow the encoder contributes \\(K\\) and \\(V\\)"
  },
  {
    "objectID": "lecs/w09/lec09.html#attending-to-the-input-encoder-ii",
    "href": "lecs/w09/lec09.html#attending-to-the-input-encoder-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Attending to the Input (Encoder) II",
    "text": "Attending to the Input (Encoder) II\n\n\n\n\nhow the encoder contributes \\(K\\) and \\(V\\)\n\n\n\nWhen attending to the encoder annotations, the model computes the key-value pairs using linearly transformed the encoder outputs."
  },
  {
    "objectID": "lecs/w09/lec09.html#self-attention",
    "href": "lecs/w09/lec09.html#self-attention",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Self-attention",
    "text": "Self-attention\nTransformer models also use “self-attention” on its previous hidden layers. When applying attention to the previous hidden layers, the causal structure is preserved:\n\n\n\n\nfeeding to deeper layers and future hidden states"
  },
  {
    "objectID": "lecs/w09/lec09.html#multi-headed-attention",
    "href": "lecs/w09/lec09.html#multi-headed-attention",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multi-headed Attention",
    "text": "Multi-headed Attention\nThe Scaled Dot-Product Attention attends to one or few entries in the input key-value pairs.\nBut humans can attend to many things simultaneously\nIdea: apply scaled dot-product attention multiple times on the linearly transformed inputs:\n\\[\\begin{align*}\n\\mathbf c_i &= \\text{attention}\\left(QW_i^Q, KW_i^K, VW_i^V\\right) \\\\\n\\text{MultiHead}(Q, K, V) &= \\text{concat}({\\bf c_1}, \\dots, {\\mathbf c_h})W^O\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#input-sequence-order",
    "href": "lecs/w09/lec09.html#input-sequence-order",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Input Sequence Order",
    "text": "Input Sequence Order\n\\[\\begin{align*}\n\\mathbf c_i &= \\text{attention}\\left(QW_i^Q, KW_i^K, VW_i^V\\right) \\\\\n\\text{MultiHead}(Q, K, V) &= \\text{concat}({\\bf c_1}, \\dots, {\\mathbf c_h})W^O\n\\end{align*}\\]\nUnlike RNNs and CNN encoders, the attention encoder output do not depend on the order of the inputs. Can you see why?\nHowever, the order of the sequence convey important information for the machine translation task, language modeling, and other tasks."
  },
  {
    "objectID": "lecs/w09/lec09.html#positional-encoding",
    "href": "lecs/w09/lec09.html#positional-encoding",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Positional Encoding",
    "text": "Positional Encoding\nIdea: Add positional information of each input token in the sequence into the input embedding vectors.\n\\[\\begin{align*}\nPE_{\\text{pos}, 2i} &= \\sin\\left(\\text{pos}/10000^{2i/d_{emb}}\\right) \\\\\nPE_{\\text{pos}, 2i+1} &= \\cos\\left(\\text{pos}/10000^{2i/d_{emb}}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#positional-encoding-ii",
    "href": "lecs/w09/lec09.html#positional-encoding-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Positional Encoding II",
    "text": "Positional Encoding II\nThe final input embeddings are the concatenation of the learnable embeddings and the positional encoding.\n\n\n\n\nsinusoidal position encodings visualized"
  },
  {
    "objectID": "lecs/w09/lec09.html#transformer-machine-translation",
    "href": "lecs/w09/lec09.html#transformer-machine-translation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Transformer Machine Translation",
    "text": "Transformer Machine Translation\n\n\n\n\n\n\ntypical encoder-decoder blocks of a transformer\n\n\n\n\n\nEncoder-Decoder architecture like RNN, but use attention modules instead of recurrent modules.\nUse N stacked self-attention layers.\nSkip-connections help preserve the positional and identity information from the input sequences."
  },
  {
    "objectID": "lecs/w09/lec09.html#visualizing-attention",
    "href": "lecs/w09/lec09.html#visualizing-attention",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Visualizing Attention",
    "text": "Visualizing Attention\nSelf-attention layer learns that “it” could refer to different entities in different contexts.\n\n\n\nvisualized attention on a sentence\n\n\nSee ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
  },
  {
    "objectID": "lecs/w09/lec09.html#backprop",
    "href": "lecs/w09/lec09.html#backprop",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backprop",
    "text": "Backprop\nDuring backprop, in the standard encoder-decoder RNN, the maximum path length across time is the number of time steps.\n\n\n\n\nbackprop paths in RNNs vs transformers"
  },
  {
    "objectID": "lecs/w09/lec09.html#backprop-ii",
    "href": "lecs/w09/lec09.html#backprop-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backprop II",
    "text": "Backprop II\n\n\n\n\nbackprop paths in RNNs vs transformers\n\n\n\nAttention-based RNNs have a constant path length between the encoder inputs and the decoder hidden states. (Learning becomes easier. Why?)"
  },
  {
    "objectID": "lecs/w09/lec09.html#transformer-language-pre-training",
    "href": "lecs/w09/lec09.html#transformer-language-pre-training",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Transformer Language Pre-Training",
    "text": "Transformer Language Pre-Training\nSimilar to pre-training computer vision models on ImageNet, we can pre-train a language model for NLP tasks.\n\n\n\n\n\n\nRadford, Alec, et al. ”Improving Language Understanding by Generative Pre-Training.” 2018\n\n\n\n\nThe pre-trained model is then fine-tuned on textual entailment, question answering, semantic similarity assessment, and document classification"
  },
  {
    "objectID": "lecs/w09/lec09.html#gpt-3-generating-an-article",
    "href": "lecs/w09/lec09.html#gpt-3-generating-an-article",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GPT-3: Generating an Article",
    "text": "GPT-3: Generating an Article\nTitle: United Methodists Agree to Historic Split\nSubtitle: Those who oppose gay marriage will form their own denomination"
  },
  {
    "objectID": "lecs/w09/lec09.html#gpt-3-generating-an-article-ii",
    "href": "lecs/w09/lec09.html#gpt-3-generating-an-article-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GPT-3: Generating an Article II",
    "text": "GPT-3: Generating an Article II\nArticle: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be “theologically and socially conservative,” according to The Washington Post. The majority of delegates attending the church’s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will “discipline” clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination."
  },
  {
    "objectID": "lecs/w09/lec09.html#gpt-3-generating-an-article-iii",
    "href": "lecs/w09/lec09.html#gpt-3-generating-an-article-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GPT-3: Generating an Article III",
    "text": "GPT-3: Generating an Article III\nThe Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the “largest Protestant denomination in the U.S.,” but that it has been shrinking in recent decades. The new split will be the second in the church’s history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church."
  },
  {
    "objectID": "lecs/w09/lec09.html#gpt-3-generating-an-article-iv",
    "href": "lecs/w09/lec09.html#gpt-3-generating-an-article-iv",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GPT-3: Generating an Article IV",
    "text": "GPT-3: Generating an Article IV\nThe Post notes that the proposed split “comes at a critical time for the church, which has been losing members for years,” which has been “pushed toward the brink of a schism over the role of LGBTQ people in the church.” Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban the\nBrown et al. 2020, Language Models are Few-Shot Learners"
  },
  {
    "objectID": "lecs/w09/lec09.html#gpt-3-correcting-english-grammar",
    "href": "lecs/w09/lec09.html#gpt-3-correcting-english-grammar",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GPT-3: Correcting English Grammar",
    "text": "GPT-3: Correcting English Grammar\nPoor English input: The patient was died.\nGood English output: The patient died.\nPoor English input: We think that Leslie likes ourselves.\nGood English output: We think that Leslie likes us.\nPoor English input: Janet broke Bill on the finger.\nGood English output: Janet broke Bill’s finger.\nPoor English input: Mary arranged for, in St. Louis, John to rent a house cheap.\nGood English output: Mary arranged for John to rent a house in St. Louis."
  },
  {
    "objectID": "lecs/w09/lec09.html#gpt-3-correcting-english-grammar-ii",
    "href": "lecs/w09/lec09.html#gpt-3-correcting-english-grammar-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GPT-3: Correcting English Grammar II",
    "text": "GPT-3: Correcting English Grammar II\nPoor English input: Today I have went to the store to to buys some many bottle of water.\nGood English output: Today I went to the store to buy some bottles of water.\nPoor English input: I have tried to hit ball with bat, but my swing is has miss.\nGood English output: I tried to hit the ball with the bat, but my swing missed.\nBrown et al. 2020, Language Models are Few-Shot Learners"
  },
  {
    "objectID": "lecs/w09/lec09.html#large-language-models-llms",
    "href": "lecs/w09/lec09.html#large-language-models-llms",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Large Language Models (LLMs)",
    "text": "Large Language Models (LLMs)\nMany Transformer-based models have been developed since 2017.\n\nGenerative Pre-Trained Transformer (GPT-1/2/3)\n\nDecoder-only 12-layer/12-headed Transformer.\nInspired GPT-Neo, GPT-J, GPT-NeoX, ChatGPT.\nGPT-4 coming out soon."
  },
  {
    "objectID": "lecs/w09/lec09.html#large-language-models-llms-ii",
    "href": "lecs/w09/lec09.html#large-language-models-llms-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Large Language Models (LLMs) II",
    "text": "Large Language Models (LLMs) II\n\nBidirectional Encoder Representations from Transformers (BERT)\n\nEncoder-only 12-layer/12-headed Transformer.\nInspired ALBERT, RoBERTa, SBERT, DeBERTa, etc."
  },
  {
    "objectID": "lecs/w09/lec09.html#large-language-models-llms-iii",
    "href": "lecs/w09/lec09.html#large-language-models-llms-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Large Language Models (LLMs) III",
    "text": "Large Language Models (LLMs) III\n\nAnd many, many more: en.wikipedia.org/wiki/Large_language_model\n\nMany benchmarks have been developed such as GLUE and SQuAD.\nBig players in the LLM space include Google (Brain, DeepMind), Meta (formerly Facebook, FAIR), Microsoft, Amazon, EleutherAI, OpenAI, Cohere, Hugging Face."
  },
  {
    "objectID": "lecs/w09/Image_generation_notebook.html",
    "href": "lecs/w09/Image_generation_notebook.html",
    "title": "Vision Transformers",
    "section": "",
    "text": "The Vision Transformer (ViT) brings the power of transformers, which have excelled in text generation tasks like T5, to the field of image analysis. In text-based transformers like T5, the model processes sequences of words or tokens, learning contextual relationships to generate coherent text. Vision Transformers work similarly, but instead of text tokens, they use image patches as input tokens.\nIn ViT, an image is divided into small, fixed-size patches, each of which is flattened into a sequence, similar to how words are arranged in text transformers. These patches are then embedded and fed into the transformer model. Using self-attention, the ViT learns to focus on different parts of the image simultaneously, capturing both local and global visual patterns. The model can then use these patterns to classify images, detect objects, or perform other vision tasks.\nDuring training, ViTs are typically pre-trained on large image datasets to learn general features, then fine-tuned on specific tasks for better performance. This two-step training process is similar to language models and helps the model adapt to new visual tasks while retaining strong foundational image representations."
  },
  {
    "objectID": "lecs/w09/Image_generation_notebook.html#setting-up-environment",
    "href": "lecs/w09/Image_generation_notebook.html#setting-up-environment",
    "title": "Vision Transformers",
    "section": "Setting-up environment",
    "text": "Setting-up environment\nWe first install HuggingFace Transformers.\n\n!pip install -q git+https://github.com/huggingface/transformers.git\n\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done"
  },
  {
    "objectID": "lecs/w09/Image_generation_notebook.html#unconditional-image-generation",
    "href": "lecs/w09/Image_generation_notebook.html#unconditional-image-generation",
    "title": "Vision Transformers",
    "section": "Unconditional image generation",
    "text": "Unconditional image generation\nIn this section, we aim to generate a few images using ImageGPT by providing the model with some random initial inputs.\nWe start by initializing the feature extractor and the model, then move the model to the GPU for faster processing.\n\nfrom transformers import ImageGPTFeatureExtractor, ImageGPTForCausalImageModeling\nimport numpy as np\nimport torch\n\nfeature_extractor = ImageGPTFeatureExtractor.from_pretrained('openai/imagegpt-medium')\nmodel = ImageGPTForCausalImageModeling.from_pretrained('openai/imagegpt-medium')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device);\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/models/imagegpt/feature_extraction_imagegpt.py:28: FutureWarning: The class ImageGPTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ImageGPTImageProcessor instead.\n  warnings.warn(\n\n\nHere we only feed the start of sequence (SOS) special token to the model, and let it generate 32x32 = 1024 pixel values using the generate() method. Each pixel value is one of 512 possible color clusters.\n\n# unconditional generation of 8 images\nbatch_size = 8\ncontext = torch.full((batch_size, 1), model.config.vocab_size - 1) #initialize with SOS token (with ID 512)\ncontext = torch.tensor(context).to(device)\noutput = model.generate(input_ids=context, max_length=model.config.n_positions + 1, temperature=1.0, do_sample=True, top_k=40)\n\nUserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  context = torch.tensor(context).to(device)\nThis is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n\n\n\n#visualize samples with Image-GPT color palette.\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclusters = feature_extractor.clusters\nn_px = feature_extractor.size\n\nsamples = output[:,1:].cpu().detach().numpy()\nsamples_img = [np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [32, 32, 3]).astype(np.uint8) for s in samples] # convert color cluster tokens back to pixels\nf, axes = plt.subplots(1, batch_size, dpi=300)\n\nfor img, ax in zip(samples_img, axes):\n    ax.axis('off')\n    ax.imshow(img)"
  },
  {
    "objectID": "lecs/w10/transpose.html",
    "href": "lecs/w10/transpose.html",
    "title": "Transpose Convolution",
    "section": "",
    "text": "A prediction problem where we label the content of each pixel is known as a pixel-wise prediction problem\n\n\n\nhttp://deeplearning.net/tutorial/fcn_2D_segm.html\n\n\nQ: How do we generate pixel-wise predictions?\n\n\n\nWe need to be able to up-sample features, i.e. to obtain high-resolution features from low-resolution features\n\nOpposite of max-pooling OR\nOpposite of a strided convolution\n\nWe need an inverse convolution – a.k.a a deconvolution or transpose convolution.\n\n\n\n\n\n\n\n\n\n\n\n&gt;&gt;&gt; x = torch.randn(2, 8, 64, 64)\n&gt;&gt;&gt; conv = nn.Conv2d(in_channels=8,\n...                  out_channels=8,\n...                  kernel_size=5)\n&gt;&gt;&gt; y = conv(x)\n&gt;&gt;&gt; y.shape\n. . .\n&gt;&gt;&gt; convt = nn.ConvTranspose2d(in_channels=8,\n...                            out_channels=8,\n...                            kernel_size=5)\n&gt;&gt;&gt; x = convt(y)\n&gt;&gt;&gt; x.shape\n. . .\nshould get the same shape back!\n\n\n\n&gt;&gt;&gt; x = torch.randn(2, 8, 64, 64)\n&gt;&gt;&gt; conv = nn.Conv2d(in_channels=8,\n...                  out_channels=8,\n...                  kernel_size=5,\n...                  padding=2)\n&gt;&gt;&gt; y = conv(x)\n&gt;&gt;&gt; y.shape\n. . .\n&gt;&gt;&gt; convt = nn.ConvTranspose2d(in_channels=8,\n...                            out_channels=8,\n...                            kernel_size=5,\n...                            padding=2)\n&gt;&gt;&gt; x = convt(y)\n&gt;&gt;&gt; x.shape\n. . .\nshould get the same shape back!\n\n\n\n&gt;&gt;&gt; x = torch.randn(2, 8, 64, 64)\n&gt;&gt;&gt; conv = nn.Conv2d(in_channels=8,\n...                  out_channels=8,\n...                  kernel_size=5,\n...                  stride=2)\n&gt;&gt;&gt; y = conv(x)\n&gt;&gt;&gt; y.shape\n. . .\n&gt;&gt;&gt; convt = nn.ConvTranspose2d(in_channels=8,\n...                            out_channels=8,\n...                            kernel_size=5,\n...                            stride=2)\n&gt;&gt;&gt; x = convt(y)\n&gt;&gt;&gt; x.shape\n. . .\n… almost the same shape …\n\n\n\n\n\n\nhttps://www.mdpi.com/2072-4292/9/6/522/htm\n\n\nMore at https://github.com/vdumoulin/conv_arithmetic\n\n\n\nnn.ConvTranspose2d(in_channels=8,\n                   out_channels=8,\n                   kernel_size=5,\n                   stride=2,\n                   output_padding=1) # +1 to output\n                                     # width/height\n\n\n\nTo demonstrate ConvTranspose2d, we will build a network that:\n\nFinds a lower dimensional representation of the image\nThen reconstructs the image from the low-dimensional representation\n\n\n\n\nhttps://hackernoon.com/how-to-autoencode-your-pok%C3%A9mon-6b0f5c7b7d97\n\n\n\n\n\n\nEncoder:\n\nInput = image\nOutput = low-dimensional embedding\n\nDecoder:\n\nInput = low-dimensional embedding\nOutput = image\n\n\n\n\n\nDimension reduction:\n\nfind a low dimensional representation of the image\n\nImage Generation:\n\ngenerate new images not in the training set\n\n\nAutoencoders are not used for supervised learning. The task is not to predict something about the image!\nAutoencoders are considered a generative model.\n\n\n\n\nLoss function:\n\nHow close were the reconstructed image from the original?\nMean Sqaure Error Loss: look at the mean square error across all the pixels.\n\nOptimizer:\n\nJust like before!\nIntroduce a new optimizer: Adam\nCommonly used for other network architectures too\n\nTraining loop:\n\nJust like before!\n\n\n\n\n\nThe dimensionality reduction means that there will be structure in the embedding space.\nIf the dimensionality of the embedding space is not too large, similar images should map to similar locations."
  },
  {
    "objectID": "lecs/w10/transpose.html#pixel-wise-prediction",
    "href": "lecs/w10/transpose.html#pixel-wise-prediction",
    "title": "Transpose Convolution",
    "section": "",
    "text": "A prediction problem where we label the content of each pixel is known as a pixel-wise prediction problem\n\n\n\nhttp://deeplearning.net/tutorial/fcn_2D_segm.html\n\n\nQ: How do we generate pixel-wise predictions?"
  },
  {
    "objectID": "lecs/w10/transpose.html#what-we-need",
    "href": "lecs/w10/transpose.html#what-we-need",
    "title": "Transpose Convolution",
    "section": "",
    "text": "We need to be able to up-sample features, i.e. to obtain high-resolution features from low-resolution features\n\nOpposite of max-pooling OR\nOpposite of a strided convolution\n\nWe need an inverse convolution – a.k.a a deconvolution or transpose convolution."
  },
  {
    "objectID": "lecs/w10/transpose.html#inverse-convolution",
    "href": "lecs/w10/transpose.html#inverse-convolution",
    "title": "Transpose Convolution",
    "section": "",
    "text": "&gt;&gt;&gt; x = torch.randn(2, 8, 64, 64)\n&gt;&gt;&gt; conv = nn.Conv2d(in_channels=8,\n...                  out_channels=8,\n...                  kernel_size=5)\n&gt;&gt;&gt; y = conv(x)\n&gt;&gt;&gt; y.shape\n. . .\n&gt;&gt;&gt; convt = nn.ConvTranspose2d(in_channels=8,\n...                            out_channels=8,\n...                            kernel_size=5)\n&gt;&gt;&gt; x = convt(y)\n&gt;&gt;&gt; x.shape\n. . .\nshould get the same shape back!"
  },
  {
    "objectID": "lecs/w10/transpose.html#inverse-convolution-padding",
    "href": "lecs/w10/transpose.html#inverse-convolution-padding",
    "title": "Transpose Convolution",
    "section": "",
    "text": "&gt;&gt;&gt; x = torch.randn(2, 8, 64, 64)\n&gt;&gt;&gt; conv = nn.Conv2d(in_channels=8,\n...                  out_channels=8,\n...                  kernel_size=5,\n...                  padding=2)\n&gt;&gt;&gt; y = conv(x)\n&gt;&gt;&gt; y.shape\n. . .\n&gt;&gt;&gt; convt = nn.ConvTranspose2d(in_channels=8,\n...                            out_channels=8,\n...                            kernel_size=5,\n...                            padding=2)\n&gt;&gt;&gt; x = convt(y)\n&gt;&gt;&gt; x.shape\n. . .\nshould get the same shape back!"
  },
  {
    "objectID": "lecs/w10/transpose.html#inverse-convolution-stride",
    "href": "lecs/w10/transpose.html#inverse-convolution-stride",
    "title": "Transpose Convolution",
    "section": "",
    "text": "&gt;&gt;&gt; x = torch.randn(2, 8, 64, 64)\n&gt;&gt;&gt; conv = nn.Conv2d(in_channels=8,\n...                  out_channels=8,\n...                  kernel_size=5,\n...                  stride=2)\n&gt;&gt;&gt; y = conv(x)\n&gt;&gt;&gt; y.shape\n. . .\n&gt;&gt;&gt; convt = nn.ConvTranspose2d(in_channels=8,\n...                            out_channels=8,\n...                            kernel_size=5,\n...                            stride=2)\n&gt;&gt;&gt; x = convt(y)\n&gt;&gt;&gt; x.shape\n. . .\n… almost the same shape …"
  },
  {
    "objectID": "lecs/w10/transpose.html#transpose-convolution-layer",
    "href": "lecs/w10/transpose.html#transpose-convolution-layer",
    "title": "Transpose Convolution",
    "section": "",
    "text": "https://www.mdpi.com/2072-4292/9/6/522/htm\n\n\nMore at https://github.com/vdumoulin/conv_arithmetic"
  },
  {
    "objectID": "lecs/w10/transpose.html#output-padding",
    "href": "lecs/w10/transpose.html#output-padding",
    "title": "Transpose Convolution",
    "section": "",
    "text": "nn.ConvTranspose2d(in_channels=8,\n                   out_channels=8,\n                   kernel_size=5,\n                   stride=2,\n                   output_padding=1) # +1 to output\n                                     # width/height"
  },
  {
    "objectID": "lecs/w10/transpose.html#autoencoder",
    "href": "lecs/w10/transpose.html#autoencoder",
    "title": "Transpose Convolution",
    "section": "",
    "text": "To demonstrate ConvTranspose2d, we will build a network that:\n\nFinds a lower dimensional representation of the image\nThen reconstructs the image from the low-dimensional representation\n\n\n\n\nhttps://hackernoon.com/how-to-autoencode-your-pok%C3%A9mon-6b0f5c7b7d97"
  },
  {
    "objectID": "lecs/w10/transpose.html#the-components-of-an-autoencoder",
    "href": "lecs/w10/transpose.html#the-components-of-an-autoencoder",
    "title": "Transpose Convolution",
    "section": "",
    "text": "Encoder:\n\nInput = image\nOutput = low-dimensional embedding\n\nDecoder:\n\nInput = low-dimensional embedding\nOutput = image"
  },
  {
    "objectID": "lecs/w10/transpose.html#why-autoencoders",
    "href": "lecs/w10/transpose.html#why-autoencoders",
    "title": "Transpose Convolution",
    "section": "",
    "text": "Dimension reduction:\n\nfind a low dimensional representation of the image\n\nImage Generation:\n\ngenerate new images not in the training set\n\n\nAutoencoders are not used for supervised learning. The task is not to predict something about the image!\nAutoencoders are considered a generative model."
  },
  {
    "objectID": "lecs/w10/transpose.html#how-to-train-autoencoders",
    "href": "lecs/w10/transpose.html#how-to-train-autoencoders",
    "title": "Transpose Convolution",
    "section": "",
    "text": "Loss function:\n\nHow close were the reconstructed image from the original?\nMean Sqaure Error Loss: look at the mean square error across all the pixels.\n\nOptimizer:\n\nJust like before!\nIntroduce a new optimizer: Adam\nCommonly used for other network architectures too\n\nTraining loop:\n\nJust like before!"
  },
  {
    "objectID": "lecs/w10/transpose.html#structure-in-the-embedding-space",
    "href": "lecs/w10/transpose.html#structure-in-the-embedding-space",
    "title": "Transpose Convolution",
    "section": "",
    "text": "The dimensionality reduction means that there will be structure in the embedding space.\nIf the dimensionality of the embedding space is not too large, similar images should map to similar locations."
  },
  {
    "objectID": "lecs/w11/lec11.html#todays-agenda",
    "href": "lecs/w11/lec11.html#todays-agenda",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nGANs"
  },
  {
    "objectID": "lecs/w11/lec11.html#generative-model",
    "href": "lecs/w11/lec11.html#generative-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Generative Model",
    "text": "Generative Model\n\nA generative model learns the structure of a set of input data, and can be used to generate new data\n\n\n\nExamples:\n\nRNN for text generation\nAutoencoder\nVAE"
  },
  {
    "objectID": "lecs/w11/lec11.html#blurriness-of-autoencoder-images",
    "href": "lecs/w11/lec11.html#blurriness-of-autoencoder-images",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Blurriness of Autoencoder Images",
    "text": "Blurriness of Autoencoder Images\n\nBlurry images, blurry backgrounds\n\n\n\nWhy? Because the loss function used to train an autoencoder is the mean square error loss (MSELoss)\n\n\n\n\nTo minimize the MSE loss, autoencoders predict the “average” pixel\n\nCan we use a better loss function?"
  },
  {
    "objectID": "lecs/w11/lec11.html#generative-adversarial-network",
    "href": "lecs/w11/lec11.html#generative-adversarial-network",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Generative Adversarial Network",
    "text": "Generative Adversarial Network\n\n\n\n\n\n\n\nGenerator network: try to fool the discriminator by generating real-looking images\nDiscriminator network: try to distinguish between real and fake images\n\n\nThe loss function of the generator (the model we care about) is defined by the discriminator!"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-generator",
    "href": "lecs/w11/lec11.html#gan-generator",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN Generator",
    "text": "GAN Generator\n\n\nGenerator Input: a random noise vector (Q: Why do we need to input noise?)\n\n\n\nGenerator Output: a generated image"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-architecture",
    "href": "lecs/w11/lec11.html#gan-architecture",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN Architecture",
    "text": "GAN Architecture\n\n\n\n\n\n\n\nDiscriminator Input: an image\nDiscriminator Output: a binary label (real vs fake)"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-loss-function-notation",
    "href": "lecs/w11/lec11.html#gan-loss-function-notation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN Loss Function Notation",
    "text": "GAN Loss Function Notation\nDiscriminator:\n\n\\(D\\) – the discriminator neural network\n\n\n\n\\(\\theta\\) – the trainable parameters of the discriminator (we’ll write \\(D_\\theta\\) if we want to make the dependency clear)\n\n\n\n\n\\(x\\) – an image (either real or fake)\n\n\n\n\n\\(D(x)\\) or \\(D_\\theta(x)\\) – the discriminator’s determination of whether the image is real (1 = real, 0 = fake)"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-loss-function-notation-ii",
    "href": "lecs/w11/lec11.html#gan-loss-function-notation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN Loss Function Notation II",
    "text": "GAN Loss Function Notation II\nGenerator:\n\n\\(G\\) – the generator neural netowrk\n\n\n\n\\(\\phi\\) – the trainable parameters of the generator (we’ll write \\(G_\\phi\\) if we want to make the dependency clear)\n\n\n\n\n\\(z\\) – a random noise vector\n\n\n\n\n\\(G(z)\\) or \\(G_\\phi(z)\\) – a generated image\n\n\n\nQ: What does \\(D(G(z))\\) mean?"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-optimizing-the-generator",
    "href": "lecs/w11/lec11.html#gan-optimizing-the-generator",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN: Optimizing the Generator",
    "text": "GAN: Optimizing the Generator\nLet’s assume the discriminator is fixed. Tune generator weights to:\n\nmaximize the probability that…\n\ndiscriminator labels a generated image as real\nQ: What loss function should we use?\n\n\n\nWe wish to tune \\(\\phi\\) to increase \\(D_\\theta(G_\\phi(z))\\)\n\n\n\\[\n\\min_\\phi \\left(\\mathbb{E}_{z \\sim  \\mathcal{N}(0,I)}\\left[\\log \\left(1 - D_\\theta(G_\\phi(z)) \\right) \\right]\\right)\n\\]"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-optimizing-the-discriminator",
    "href": "lecs/w11/lec11.html#gan-optimizing-the-discriminator",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN: Optimizing the Discriminator",
    "text": "GAN: Optimizing the Discriminator\nLet’s assume the generator is fixed. Tune discriminator weights to:\n\nmaximize the probability that the\n\ndiscriminator labels a real image as real\ndiscriminator labels a generated image as fake\nQ: What loss function should we use?"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-optimizing-the-discriminator-ii",
    "href": "lecs/w11/lec11.html#gan-optimizing-the-discriminator-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN: Optimizing the Discriminator II",
    "text": "GAN: Optimizing the Discriminator II\nWe wish to tune \\(\\theta\\) to:\n\ndecrease \\(D_\\theta(G_\\phi(z))\\), where \\(z \\sim \\mathcal{N}(0, I)\\) (the data distribution)\n\n\n\nincrease \\(D_\\theta(x)\\), where \\(x \\sim \\mathcal{D}\\) (the data distribution)\n\n\n\n\\[\n\\max_\\theta \\mathbb{E}_{x \\sim \\mathcal{D}}\\left[\\log D_\\theta(x)\\right] + \\mathbb{E}_{z}\\left[\\log \\left( 1 - D_\\theta(G_\\phi(z)) \\right) \\right]\n\\]"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-optimization-problem",
    "href": "lecs/w11/lec11.html#gan-optimization-problem",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN Optimization Problem",
    "text": "GAN Optimization Problem\nIf we optimize both the generator and the discriminator then:\n\\[\n\\min_\\phi \\left(\\max_\\theta \\left(\\mathbb{E}_{x \\sim \\mathcal{D}}\\left[\\log D_\\theta(x)\\right] + \\mathbb{E}_{z}\\left[\\log \\left( 1 - D_\\theta(G_\\phi(z)) \\right) \\right]\\right)\\right)\n\\]\nThis is called the minimax optimization problem since the generator and discriminator are playing a zero-sum game against each other"
  },
  {
    "objectID": "lecs/w11/lec11.html#training",
    "href": "lecs/w11/lec11.html#training",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Training",
    "text": "Training\nAlternate between:\n\nTraining the discriminator\n\n\n\nTraining the generator"
  },
  {
    "objectID": "lecs/w11/lec11.html#updating-the-discriminator",
    "href": "lecs/w11/lec11.html#updating-the-discriminator",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Updating the Discriminator",
    "text": "Updating the Discriminator"
  },
  {
    "objectID": "lecs/w11/lec11.html#updating-the-generator",
    "href": "lecs/w11/lec11.html#updating-the-generator",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Updating the Generator",
    "text": "Updating the Generator"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-alternating-training-visualized",
    "href": "lecs/w11/lec11.html#gan-alternating-training-visualized",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN Alternating Training Visualized",
    "text": "GAN Alternating Training Visualized\nBlack dots is the data distribution \\(\\mathcal{D}\\), green line is the generator distribution \\(G(z)\\), and blue dotted line is the discriminator:"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-alternating-training-visualized-1",
    "href": "lecs/w11/lec11.html#gan-alternating-training-visualized-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN Alternating Training Visualized",
    "text": "GAN Alternating Training Visualized\n\nThe distributions \\(G(z)\\) and \\(\\mathcal{D}\\) are quite different\nThe discriminator is updated to be able to better distinguish real vs fake\nThe generator is updated to be better match \\(\\mathcal{D}\\)\nIf training is successful, \\(G(z)\\) is indistinguisable from \\(\\mathcal{D}\\)"
  },
  {
    "objectID": "lecs/w11/lec11.html#a-nice-visualization-of-gan-training",
    "href": "lecs/w11/lec11.html#a-nice-visualization-of-gan-training",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "A nice visualization of GAN training",
    "text": "A nice visualization of GAN training\nhttps://poloclub.github.io/ganlab/"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-training-caveats",
    "href": "lecs/w11/lec11.html#gan-training-caveats",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN Training Caveats",
    "text": "GAN Training Caveats\n\nCan work very well and produces crisp, high-res images, but difficult to train!\n\n\n\nDifficult to numerically see whether there is progress\n\nPlotting the “training curve” (discriminator/generator loss) doesn’t help much\n\n\n\n\n\nTakes a long time to train (a long time before we see progress)"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-interpolation-in-z",
    "href": "lecs/w11/lec11.html#gan-interpolation-in-z",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN: Interpolation in \\(z\\)",
    "text": "GAN: Interpolation in \\(z\\)\n\n\n\nRadford et al. (2016) https://arxiv.org/pdf/1511.06434.pdf"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-vector-arithmetic-in-z",
    "href": "lecs/w11/lec11.html#gan-vector-arithmetic-in-z",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN: Vector Arithmetic in \\(z\\)",
    "text": "GAN: Vector Arithmetic in \\(z\\)\n\n\n\nRadford et al. (2016) https://arxiv.org/pdf/1511.06434.pdf"
  },
  {
    "objectID": "lecs/w11/lec11.html#gan-samples-2019",
    "href": "lecs/w11/lec11.html#gan-samples-2019",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GAN Samples (2019)",
    "text": "GAN Samples (2019)\nImageNet object categories (by BigGAN, a much larger model, with a bunch more engineering tricks)\n\n\n\nBrock et al., 2019. Large scale GAN training for high fidelity natural image synthesis"
  },
  {
    "objectID": "lecs/w11/lec11.html#mode-collapse",
    "href": "lecs/w11/lec11.html#mode-collapse",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Mode Collapse",
    "text": "Mode Collapse\nWe don’t actually know how well a GAN is modelling the distribution. One prominent issue is mode collapse\n\nThe word “mode” here means “peak” or ” high-value local optimum”\n\n\n\nGAN model learns to generate one type of input data (e.g. only digit 1)\n\n\n\n\nGenerating anything else leads to detection by discriminator\n\n\n\n\nGenerator gets stuck in that local optima"
  },
  {
    "objectID": "lecs/w11/lec11.html#balance-between-generator-and-discriminator",
    "href": "lecs/w11/lec11.html#balance-between-generator-and-discriminator",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Balance between Generator and Discriminator",
    "text": "Balance between Generator and Discriminator\nIf the discriminator is too good, then the generator will not learn due to saturation:\n\nRemember that we are using the discriminator like a “loss function” for the generator\n\n\n\nIf the discriminator is too good, small changes in the generator weights won’t change the discriminator output\n\n\n\n\nIf small changes in generator weights make no difference, then we can’t incrementally improve the generator"
  },
  {
    "objectID": "lecs/w11/lec11.html#wasserstein-gan-wgan",
    "href": "lecs/w11/lec11.html#wasserstein-gan-wgan",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Wasserstein GAN (WGAN)",
    "text": "Wasserstein GAN (WGAN)\nIdea: Use a different loss function.\nArjovsky et al. (2017) Wasserstein GAN. https://arxiv.org/abs/1701.07875\n\nUse the Wasserstein distance between the generator distribution and the data distribution\n\n\n\nReduces mode collapse, better measurement of progress"
  },
  {
    "objectID": "lecs/w11/lec11.html#style-transfer-with-cycle-gan",
    "href": "lecs/w11/lec11.html#style-transfer-with-cycle-gan",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Style Transfer with Cycle GAN",
    "text": "Style Transfer with Cycle GAN\nStyle transfer problem: change the style of an image while preserving the content.\n\n\n\nData: Two unrelated collections of images, one for each style"
  },
  {
    "objectID": "lecs/w11/lec11.html#cycle-gan-idea",
    "href": "lecs/w11/lec11.html#cycle-gan-idea",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cycle GAN Idea",
    "text": "Cycle GAN Idea\n\nIf we had paired data (same content in both styles), this would be a supervised learning problem. But this is hard to find."
  },
  {
    "objectID": "lecs/w11/lec11.html#cycle-gan-idea-1",
    "href": "lecs/w11/lec11.html#cycle-gan-idea-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cycle GAN Idea",
    "text": "Cycle GAN Idea\n\nThe CycleGAN architecture learns to do it from unpaired data.\n\nTrain two different generator nets to go from style 1 to style 2, and vice versa.\nMake sure the generated samples of style 2 are indistinguishable from real images by a discriminator net.\nMake sure the generators are cycle-consistent: mapping from style 1 to style 2 and back again should give you almost the original image."
  },
  {
    "objectID": "lecs/w11/lec11.html#cycle-gan-architecture",
    "href": "lecs/w11/lec11.html#cycle-gan-architecture",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cycle GAN Architecture",
    "text": "Cycle GAN Architecture"
  },
  {
    "objectID": "lecs/w11/lec11.html#cycle-gan-aerial-photos-and-maps",
    "href": "lecs/w11/lec11.html#cycle-gan-aerial-photos-and-maps",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cycle GAN: Aerial photos and maps",
    "text": "Cycle GAN: Aerial photos and maps"
  },
  {
    "objectID": "lecs/w11/lec11.html#cycle-gan-road-scenes-and-semantic-segmentation",
    "href": "lecs/w11/lec11.html#cycle-gan-road-scenes-and-semantic-segmentation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cycle GAN: Road scenes and semantic segmentation",
    "text": "Cycle GAN: Road scenes and semantic segmentation"
  },
  {
    "objectID": "lecs/w11/lec11.html#ethical-and-security-issues-with-gans",
    "href": "lecs/w11/lec11.html#ethical-and-security-issues-with-gans",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Ethical and security issues with GANs",
    "text": "Ethical and security issues with GANs\nhttps://thispersondoesnotexist.com"
  },
  {
    "objectID": "lecs/w11/lec11.html#deepfakes-for-voice",
    "href": "lecs/w11/lec11.html#deepfakes-for-voice",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Deepfakes for voice",
    "text": "Deepfakes for voice"
  },
  {
    "objectID": "lecs/w05/lec05.html#lecture-plan",
    "href": "lecs/w05/lec05.html#lecture-plan",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Lecture Plan",
    "text": "Lecture Plan\n\nOptimization"
  },
  {
    "objectID": "lecs/w05/lec05.html#optimization-problem",
    "href": "lecs/w05/lec05.html#optimization-problem",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Optimization Problem",
    "text": "Optimization Problem\nLet’s group all the parameters (weights and biases) of a network into a single vector \\({\\bf \\theta}\\)\nWe wish to find the minima of a function \\(f({\\theta}): \\mathbb{R}^D \\rightarrow \\mathbb{R}\\).\nWe already discussed gradient descent, but …\n\nWhat property does \\(f\\) need to have for gradient descent to work well?\nAre there techniques that can work better than (vanilla) gradient descent?"
  },
  {
    "objectID": "lecs/w05/lec05.html#optimization-problem-contd",
    "href": "lecs/w05/lec05.html#optimization-problem-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Optimization Problem (cont’d)",
    "text": "Optimization Problem (cont’d)\nWe wish to find the minima of a function \\(f({\\theta}): \\mathbb{R}^D \\rightarrow \\mathbb{R}\\).\nWe already discussed gradient descent, but …\n\nWhat property does \\(f\\) need to have for gradient descent to work well?\nAre there techniques that can work better than (vanilla) gradient descent?\nAre there cases where gradient descent (and related) optimization methods fail?\nHow can deep learning practitioners diagnose and fix optimization issues?"
  },
  {
    "objectID": "lecs/w05/lec05.html#visualizing-optimization-problems",
    "href": "lecs/w05/lec05.html#visualizing-optimization-problems",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Visualizing Optimization Problems",
    "text": "Visualizing Optimization Problems\nVisualizing optimization problems in high dimensions is challenging. Intuitions that we get from 1D and 2D optimization problems can be helpful.\nIn 1D and 2D, we can visualize \\(f\\) by drawing plots, e.g. surface plots and contour plots\n\n\n\nQ: Sketch a contour plot that represents the same function as the figure above."
  },
  {
    "objectID": "lecs/w05/lec05.html#visualizing-optimization-problems-contd",
    "href": "lecs/w05/lec05.html#visualizing-optimization-problems-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Visualizing Optimization Problems (cont’d)",
    "text": "Visualizing Optimization Problems (cont’d)\n\n\n\nQ: Sketch a contour plot that represents the same function as the figure above."
  },
  {
    "objectID": "lecs/w05/lec05.html#review-contour-plots",
    "href": "lecs/w05/lec05.html#review-contour-plots",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Review: Contour Plots",
    "text": "Review: Contour Plots\n\nQ: Where are the 4 local minima in this contour plot?"
  },
  {
    "objectID": "lecs/w05/lec05.html#minimization-intuition-in-1d",
    "href": "lecs/w05/lec05.html#minimization-intuition-in-1d",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Minimization Intuition in 1D",
    "text": "Minimization Intuition in 1D\nSuppose we have a function \\(f({\\theta}): \\mathbb{R}^1 \\rightarrow \\mathbb{R}\\) that we wish to minimize. How do we go about doing this?\n\n\\(\\theta \\rightarrow \\theta - \\alpha f'(\\theta)\\)\n\n\nGradient descent and other techniques all are founded on approximating \\(f\\) with its Taylor series expansion:\n\\[\\begin{align*}\nf(\\theta) \\approx f(\\theta_0) + f'(\\theta_0)^T(\\theta - \\theta_0) + \\frac{1}{2} (\\theta - \\theta_0)^2 f''(\\theta_0) + \\ldots\n\\end{align*}\\]\nUnderstanding this use of the Taylor series approximation helps us understand more involved optimization techniques that use higher-order derivatives."
  },
  {
    "objectID": "lecs/w05/lec05.html#taylor-series-expansion-in-high-dimensions",
    "href": "lecs/w05/lec05.html#taylor-series-expansion-in-high-dimensions",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Taylor Series Expansion in High Dimensions",
    "text": "Taylor Series Expansion in High Dimensions\nSuppose we have a function \\(f({\\theta}): \\mathbb{R}^D \\rightarrow \\mathbb{R}\\) that we wish to minimize.\nAgain, we can explore approximating \\(f\\) with its Taylor series expansion:\n\\[\\begin{align*}\nf(\\theta) \\approx f(\\theta_0) + \\nabla f(\\theta_0)^T(\\theta - \\theta_0) + \\frac{1}{2} (\\theta - \\theta_0)^T H(\\theta_0) (\\theta - \\theta_0) + \\ldots\n\\end{align*}\\]\nGradient descent uses first-order information, but other optimization algorithms use some information about the Hessian."
  },
  {
    "objectID": "lecs/w05/lec05.html#recap-gradient",
    "href": "lecs/w05/lec05.html#recap-gradient",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Recap: Gradient",
    "text": "Recap: Gradient\nThe gradient of a function \\(f({\\theta}): \\mathbb{R}^D \\rightarrow \\mathbb{R}\\) is the vector of partial derivatives:\n\\[\\begin{align*}\n\\nabla_{\\theta} f = \\frac{\\partial f}{\\partial {\\theta}} = \\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\theta_1}  \\\\\n\\frac{\\partial f}{\\partial \\theta_2}  \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial \\theta_D}  \\\\\n\\end{bmatrix}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#recap-hessian",
    "href": "lecs/w05/lec05.html#recap-hessian",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Recap: Hessian",
    "text": "Recap: Hessian\nThe Hessian Matrix, denoted \\({\\bf H}\\) or \\(\\nabla^2 f\\) is the matrix of second derivatives\n\\[\\begin{align*}\nH = \\nabla^2 f = \\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial \\theta_1^2}  & \\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_2} & \\dots  &\n     & \\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_D} \\\\\n\\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_2}  & \\frac{\\partial^2 f}{\\partial \\theta_2^2} & \\dots  &\n     & \\frac{\\partial^2 f}{\\partial \\theta_2 \\partial \\theta_D} \\\\\n\\vdots\n\\frac{\\partial^2 f}{\\partial \\theta_n \\partial \\theta_2}  & \\frac{\\partial^2 f}{\\partial \\theta_n \\partial \\theta_2} & \\dots  &\n     & \\frac{\\partial^2 f}{\\partial \\theta_D^2} \\\\\n\\end{bmatrix}\n\\end{align*}\\]\nThe Hessian is symmetric because \\(\\frac{\\partial^2 f}{\\partial \\theta_i \\partial \\theta_j} = \\frac{\\partial^2 f}{\\partial \\theta_j \\partial \\theta_i}\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#multivariate-taylor-series",
    "href": "lecs/w05/lec05.html#multivariate-taylor-series",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multivariate Taylor Series",
    "text": "Multivariate Taylor Series\nRecall the second-order Taylor series expansion of \\(f\\):\n\\[\\begin{align*}\nf(\\theta) \\approx f(\\theta_0) + \\nabla f(\\theta_0)^T(\\theta - \\theta_0) + \\frac{1}{2} (\\theta - \\theta_0)^T H(\\theta_0) (\\theta - \\theta_0)\n\\end{align*}\\]\nA critical point of \\(f\\) is a point where the gradient is zero, so that\n\\[\\begin{align*}\nf(\\theta) \\approx f(\\theta_0) + \\frac{1}{2} (\\theta - \\theta_0)^T H(\\theta_0) (\\theta - \\theta_0)\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#multivariate-taylor-series-contd",
    "href": "lecs/w05/lec05.html#multivariate-taylor-series-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multivariate Taylor Series (cont’d)",
    "text": "Multivariate Taylor Series (cont’d)\nA critical point of \\(f\\) is a point where the gradient is zero, so that\n\\[\\begin{align*}\nf(\\theta) \\approx f(\\theta_0) + \\frac{1}{2} (\\theta - \\theta_0)^T H(\\theta_0) (\\theta - \\theta_0)\n\\end{align*}\\]\nHow do we know if the critical point is a maximum, minimum, or something else?\n\n\nMinimum: The Hessian is positive definite.\nMaximum: The Hessian is negative definite."
  },
  {
    "objectID": "lecs/w05/lec05.html#spectral-decomposition-of-h",
    "href": "lecs/w05/lec05.html#spectral-decomposition-of-h",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Spectral Decomposition of \\(H\\)",
    "text": "Spectral Decomposition of \\(H\\)\n\\[\\begin{align*}\nf(\\theta) \\approx f(\\theta_0) + f'(\\theta_0)^T(\\theta - \\theta_0) + \\frac{1}{2} (\\theta - \\theta_0)^2 f''(\\theta_0) + \\ldots\n\\end{align*}\\]\nWe won’t go into details in this course, but…\n\nA lot of important features of the optimization landscape can be characterized by the eigenvalues of the Hessian \\(H\\).\nRecall that a symmetric matrix (such as \\(H\\)) has only real eigenvalues, and there is an orthogonal basis of eigenvectors."
  },
  {
    "objectID": "lecs/w05/lec05.html#spectral-decomposition-of-h-contd",
    "href": "lecs/w05/lec05.html#spectral-decomposition-of-h-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Spectral Decomposition of \\(H\\) (cont’d)",
    "text": "Spectral Decomposition of \\(H\\) (cont’d)\n\nA lot of important features of the optimization landscape can be characterized by the eigenvalues of the Hessian \\(H\\).\nRecall that a symmetric matrix (such as \\(H\\)) has only real eigenvalues, and there is an orthogonal basis of eigenvectors.\nThis can be expressed in terms of the spectral decomposition: \\(H = Q\\Lambda Q^T\\), where \\(Q\\) is an orthogonal matrix (whose columns are the eigenvectors) and \\(\\Lambda\\) is a diagonal matrix (whose diagonal entries are the eigenvalues)."
  },
  {
    "objectID": "lecs/w05/lec05.html#first--vs-second-order-information",
    "href": "lecs/w05/lec05.html#first--vs-second-order-information",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "First- vs Second-Order Information",
    "text": "First- vs Second-Order Information\nIn Gradient Descent, we approximate \\(f\\) with its first-order Taylor approximation. In other words, we are using first-order information in our optimization procedure.\nAn area of research known as second-order optimization develops algorithms which explicitly use curvature information (the Hessian \\(H\\)), but these are complicated and difficult to scale to large neural nets and large datasets.\nBut before we get there…"
  },
  {
    "objectID": "lecs/w05/lec05.html#features-of-the-optimization-landscape",
    "href": "lecs/w05/lec05.html#features-of-the-optimization-landscape",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Features of the Optimization Landscape",
    "text": "Features of the Optimization Landscape"
  },
  {
    "objectID": "lecs/w05/lec05.html#feature-1-convexity-of-linear-models",
    "href": "lecs/w05/lec05.html#feature-1-convexity-of-linear-models",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature 1: Convexity of Linear Models",
    "text": "Feature 1: Convexity of Linear Models\nLinear regression and logistic regressions are convex problems—i.e. its loss function is convex.\nA function \\(f\\) is convex if for any \\(a \\in (0, 1)\\)\n\\[f(ax + (1 - a) y) &lt;  af(x) + (1-a)f(y)\\]\n\nThe cost function only has one minima.\nThere are no local minima that is not global minima.\nIntuitively: the cost function is “bowl-shaped”."
  },
  {
    "objectID": "lecs/w05/lec05.html#q-are-these-loss-surfaces-convex",
    "href": "lecs/w05/lec05.html#q-are-these-loss-surfaces-convex",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Q: Are these loss surfaces convex?",
    "text": "Q: Are these loss surfaces convex?"
  },
  {
    "objectID": "lecs/w05/lec05.html#convexity-in-1d",
    "href": "lecs/w05/lec05.html#convexity-in-1d",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convexity in 1D",
    "text": "Convexity in 1D\nHow do we know if a function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) is convex?\n\nWhen \\(f''(x)\\) is positive everywhere!\nLikewise, analyzing the Hessian matrix \\(H\\) tells us whether a function \\(f: \\mathbb{R}^D \\rightarrow \\mathbb{R}\\) is convex. (Hint: \\(H\\) needs to have only positive eigenvalues)"
  },
  {
    "objectID": "lecs/w05/lec05.html#neural-networks-are-not-convex",
    "href": "lecs/w05/lec05.html#neural-networks-are-not-convex",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Neural Networks are Not Convex",
    "text": "Neural Networks are Not Convex\nIn general, neural networks are not convex.\nOne way to see this is that neural networks have weight space symmetry:"
  },
  {
    "objectID": "lecs/w05/lec05.html#neural-networks-are-not-convex-contd",
    "href": "lecs/w05/lec05.html#neural-networks-are-not-convex-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Neural Networks are Not Convex (cont’d)",
    "text": "Neural Networks are Not Convex (cont’d)\n\nSuppose you are at a local minima \\({\\bf \\theta}\\).\nYou can swap two hidden units, and therefore swap the corresponding weights/biases, and get \\({\\bf \\theta}^\\prime\\),\nthen \\({\\bf \\theta}^\\prime\\) must also be a local minima!\n\nVideo: Convexity of MLP"
  },
  {
    "objectID": "lecs/w05/lec05.html#feature-2-local-minima-in-neural-networks",
    "href": "lecs/w05/lec05.html#feature-2-local-minima-in-neural-networks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature 2: Local Minima in Neural Networks",
    "text": "Feature 2: Local Minima in Neural Networks\nEven though any multilayer neural net can have local optima, we usually don’t worry too much about them.\nIt’s possible to construct arbitrarily bad local minima even for ordinary classification MLPs. It’s poorly understood why these don’t arise in practice.\nOver the past 5 years or so, CS theorists have made lots of progress proving gradient descent converges to global minima for some non-convex problems, including some specific neural net architectures."
  },
  {
    "objectID": "lecs/w05/lec05.html#feature-3-saddle-points",
    "href": "lecs/w05/lec05.html#feature-3-saddle-points",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature 3: Saddle Points",
    "text": "Feature 3: Saddle Points\n\n\n\nA saddle point has \\(\\nabla_\\theta \\mathcal{E} = {\\bf 0}\\), even though we are not at a minimum.\nMinima with respect to some directions, maxima with respect to others. In other words, \\(H\\) has some positive and some negative eigenvalues."
  },
  {
    "objectID": "lecs/w05/lec05.html#feature-3-saddle-points-contd",
    "href": "lecs/w05/lec05.html#feature-3-saddle-points-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature 3: Saddle Points (cont’d)",
    "text": "Feature 3: Saddle Points (cont’d)\nA saddle point has \\(\\nabla_\\theta \\mathcal{E} = {\\bf 0}\\), even though we are not at a minimum.\nMinima with respect to some directions, maxima with respect to others. In other words, \\(H\\) has some positive and some negative eigenvalues.\nWhen would saddle points be a problem?\n\nIf we’re exactly on the saddle point, then we’re stuck.\nIf we’re slightly to the side, then we can get unstuck."
  },
  {
    "objectID": "lecs/w05/lec05.html#initialization",
    "href": "lecs/w05/lec05.html#initialization",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Initialization",
    "text": "Initialization\n\nIf we initialize all weights/biases to the same value, (e.g. 0)\n…then all the hidden states in the same layer will have the same value, (e.g. \\({\\bf h}\\) will be a vector containing the same value repeated)\n…then all of the error signals for weights in the same layer are the same. (e.g. each row of \\(\\overline{W^{(2)}}\\) will be identical)"
  },
  {
    "objectID": "lecs/w05/lec05.html#initialization-contd",
    "href": "lecs/w05/lec05.html#initialization-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Initialization (cont’d)",
    "text": "Initialization (cont’d)\n\n…then all of the error signals for weights in the same layer are the same. (e.g. each row of \\(\\overline{W^{(2)}}\\) will be identical)\n\n\\[\\begin{align*}\n\\overline{{\\bf y}} &= \\overline{\\mathcal{L}}({\\bf y} - {\\bf t}) \\\\\n\\overline{W^{(2)}} &= \\overline{{\\bf y}}{\\bf h}^T \\\\\n\\overline{{\\bf h}} &= {W^{(2)}}^T \\overline{{\\bf y}} \\\\\n\\overline{{\\bf z}} &= \\overline{{\\bf h}} \\circ \\sigma^\\prime({\\bf z}) \\\\\n\\overline{W^{(1)}} &= \\overline{{\\bf z}} {\\bf x}^T\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#random-initialization",
    "href": "lecs/w05/lec05.html#random-initialization",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Random Initialization",
    "text": "Random Initialization\nSolution: don’t initialize all your weights to zero!\nInstead, break the symmetry by using small random values.\nWe initialize the weights by sampling from a random normal distribution with:\n\nMean = 0\nVariance = \\(\\frac{2}{{\\rm fan\\_in}}\\) where fan_in is the number of input neurons that feed into this feature. (He et al. 2015)"
  },
  {
    "objectID": "lecs/w05/lec05.html#feature-4-plateaux",
    "href": "lecs/w05/lec05.html#feature-4-plateaux",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature 4: Plateaux",
    "text": "Feature 4: Plateaux\nA flat region in the cost is called a plateau. (Plural: plateaux)\n\nCan you think of examples?\n\nlogistic activation with least squares\n0-1 loss\nReLU activation (potentially)"
  },
  {
    "objectID": "lecs/w05/lec05.html#plateaux-and-saturated-units",
    "href": "lecs/w05/lec05.html#plateaux-and-saturated-units",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Plateaux and Saturated Units",
    "text": "Plateaux and Saturated Units\nAn important example of a plateau is a saturated unit. This is when activations always end up in the flat region of its activation function. Recall the backprop equation for the weight derivative:\n\nIf \\(\\phi^{\\prime}(z)\\) is always close to zero, then the weights will get stuck.\nIf there is a ReLU unit whose input \\(z_i\\) is always negative, the weight derivatives will be exactly 0. We call this neuron a dead unit."
  },
  {
    "objectID": "lecs/w05/lec05.html#ravines",
    "href": "lecs/w05/lec05.html#ravines",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Ravines",
    "text": "Ravines\nLots of sloshing around the walls, only a small derivative along the slope of the ravine’s floor."
  },
  {
    "objectID": "lecs/w05/lec05.html#ravines-2d-intuition",
    "href": "lecs/w05/lec05.html#ravines-2d-intuition",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Ravines (2D Intuition)",
    "text": "Ravines (2D Intuition)\n\n\nGradient component \\(\\frac{\\partial \\mathcal{E}}{\\partial w_1}\\) is large\nGradient component \\(\\frac{\\partial \\mathcal{E}}{\\partial w_2}\\) is small"
  },
  {
    "objectID": "lecs/w05/lec05.html#ravines-example",
    "href": "lecs/w05/lec05.html#ravines-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Ravines Example",
    "text": "Ravines Example\nSuppose we have the following dataset for linear regression.\n\nWhich weight, \\(w_1\\) or \\(w_2\\), will receive a larger gradient descent update?\nWhich one do you want to receive a larger update?\nNote: the figure vastly understates the narrowness of the ravine!"
  },
  {
    "objectID": "lecs/w05/lec05.html#ravines-another-examples",
    "href": "lecs/w05/lec05.html#ravines-another-examples",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Ravines: another examples",
    "text": "Ravines: another examples\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(t\\)\n\n\n\n\n1003.2\n1005.1\n3.3\n\n\n1001.1\n1008.2\n4.8\n\n\n998.3\n1003.4\n2.9"
  },
  {
    "objectID": "lecs/w05/lec05.html#avoiding-ravines",
    "href": "lecs/w05/lec05.html#avoiding-ravines",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Avoiding Ravines",
    "text": "Avoiding Ravines\nTo help avoid these problems, it’s a good idea to center or normalize your inputs to zero mean and unit variance, especially when they’re in arbitrary units (feet, seconds, etc.).\nHidden units may have non-centered activations, and this is harder to deal with.\nA recent method called batch normalization explicitly centers each hidden activation. It often speeds up training by 1.5-2x."
  },
  {
    "objectID": "lecs/w05/lec05.html#method-batch-normalization",
    "href": "lecs/w05/lec05.html#method-batch-normalization",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Method: Batch Normalization",
    "text": "Method: Batch Normalization\nIdea: Normalize the activations per batch during training, so that the activations have zero mean and unit variance.\nWhat about during test time (i.e. during model evaluation)?\n\nKeep track of the activation mean \\(\\mu\\) and variance \\(\\sigma\\) during training.\nUse that \\(\\mu\\) and \\(\\sigma\\) at test time: \\(z^\\prime = \\frac{z - \\mu}{\\sigma}\\)."
  },
  {
    "objectID": "lecs/w05/lec05.html#batch-normalization-video",
    "href": "lecs/w05/lec05.html#batch-normalization-video",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Batch Normalization Video",
    "text": "Batch Normalization Video\nhttps://play.library.utoronto.ca/watch/3e2b87ac8e5730f404893ce9270b4b75"
  },
  {
    "objectID": "lecs/w05/lec05.html#method-momentum",
    "href": "lecs/w05/lec05.html#method-momentum",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Method: Momentum",
    "text": "Method: Momentum\nMomentum is a simple and highly effective method to deal with narrow ravines. Imagine a hockey puck on a frictionless surface (representing the cost function). It will accumulate momentum in the downhill direction:\n\\[\\begin{align*}\n{\\bf p} &\\gets \\mu {\\bf p} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial \\theta} \\\\\n\\theta &\\gets \\theta + {\\bf p}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#method-momentum-contd",
    "href": "lecs/w05/lec05.html#method-momentum-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Method: Momentum (cont’d)",
    "text": "Method: Momentum (cont’d)\n\n\\(\\alpha\\) is the learning rate, just like in gradient descent.\n\\(\\mu\\) is a damping parameter. It should be slightly less than 1 (e.g. 0.9 or 0.99).\n\nIf \\(\\mu = 1\\), conservation of energy implies it will never settle down."
  },
  {
    "objectID": "lecs/w05/lec05.html#why-momentum-works",
    "href": "lecs/w05/lec05.html#why-momentum-works",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Momentum Works",
    "text": "Why Momentum Works\n\nIn the high curvature directions, the gradients cancel each other out, so momentum dampens the oscillations.\nIn the low curvature directions, the gradients point in the same direction, allowing the parameters to pick up speed."
  },
  {
    "objectID": "lecs/w05/lec05.html#why-momentum-works-contd",
    "href": "lecs/w05/lec05.html#why-momentum-works-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Momentum Works (cont’d)",
    "text": "Why Momentum Works (cont’d)\n\n\n\n\nIf the gradient is constant (i.e. the cost surface is a plane), the parameters will reach a terminal velocity of \\(-\\frac{\\alpha}{1 - \\mu} \\cdot \\frac{\\partial \\mathcal{E}}{\\partial \\theta}\\) This suggests if you increase \\(\\mu\\), you should lower \\(\\alpha\\) to compensate.\nMomentum sometimes helps a lot, and almost never hurts."
  },
  {
    "objectID": "lecs/w05/lec05.html#gradient-descent-with-momentum",
    "href": "lecs/w05/lec05.html#gradient-descent-with-momentum",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\nQ: Which trajectory has the highest/lowest momentum setting?"
  },
  {
    "objectID": "lecs/w05/lec05.html#second-order-information",
    "href": "lecs/w05/lec05.html#second-order-information",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Second-Order Information",
    "text": "Second-Order Information\nAn area of research known as second-order optimization develops algorithms which explicitly use curvature information (the Hessian \\(H\\)), but these are complicated and difficult to scale to large neural nets and large datasets.\nBut can we use just a bit of second-order information?"
  },
  {
    "objectID": "lecs/w05/lec05.html#method-rmsprop",
    "href": "lecs/w05/lec05.html#method-rmsprop",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Method: RMSProp",
    "text": "Method: RMSProp\nSGD takes large steps in directions of high curvature and small steps in directions of low curvature.\nRMSprop is a variant of SGD which rescales each coordinate of the gradient to have norm 1 on average. It does this by keeping an exponential moving average \\(s_j\\) of the squared gradients."
  },
  {
    "objectID": "lecs/w05/lec05.html#method-rmsprop-contd",
    "href": "lecs/w05/lec05.html#method-rmsprop-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Method: RMSProp (cont’d)",
    "text": "Method: RMSProp (cont’d)\nThe following update is applied to each coordinate j independently: \\[\\begin{align*}\ns_j &\\leftarrow  (1-\\gamma)s_j + \\gamma \\left(\\frac{\\partial \\mathcal{J}}{\\partial \\theta_j}\\right)^2 \\\\\n\\theta_j &\\leftarrow \\theta_j - \\frac{\\alpha}{\\sqrt{s_j + \\epsilon}} \\frac{\\partial \\mathcal{J}}{\\partial \\theta_j}\n\\end{align*}\\]\nIf the eigenvectors of the Hessian are axis-aligned (dubious assumption) then RMSprop can correct for the curvature. In practice, it typically works slightly better than SGD."
  },
  {
    "objectID": "lecs/w05/lec05.html#method-adam",
    "href": "lecs/w05/lec05.html#method-adam",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Method: Adam",
    "text": "Method: Adam\nAdam = RMSprop + momentum\nAdam is the most commonly used optimizer for neural network"
  },
  {
    "objectID": "lecs/w05/lec05.html#learning-rate",
    "href": "lecs/w05/lec05.html#learning-rate",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Learning Rate",
    "text": "Learning Rate\nThe learning rate \\(\\alpha\\) is a hyperparameter we need to tune. Here are the things that can go wrong in batch mode:\n\n  \n\n\n\n\\(\\alpha\\) too small\n\\(\\alpha\\) too large\n\\(\\alpha\\) much too large\n\n\n\n\nslow progress\noscillations\ninstability"
  },
  {
    "objectID": "lecs/w05/lec05.html#stochastic-gradient-descent",
    "href": "lecs/w05/lec05.html#stochastic-gradient-descent",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nBatch gradient descent moves directly downhill. SGD takes steps in a noisy direction, but moves downhill on average.\n\n\n\n\n\n\n\n\nbatch gradient descent\nstochastic gradient descent"
  },
  {
    "objectID": "lecs/w05/lec05.html#sgd-learning-rate",
    "href": "lecs/w05/lec05.html#sgd-learning-rate",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "SGD Learning Rate",
    "text": "SGD Learning Rate\nIn stochastic training, the learning rate also influences the fluctuations due to the stochasticity of the gradients.\n\n\n\n\nUse a large learning rate early in training so you can get close to the optimum\nGradually decay the learning rate to reduce the fluctuations"
  },
  {
    "objectID": "lecs/w05/lec05.html#sgd-batch-size",
    "href": "lecs/w05/lec05.html#sgd-batch-size",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "SGD Batch Size",
    "text": "SGD Batch Size\nThe tradeoff between smaller vs larger batch size\n\\[\\begin{align*}\nVar\\left[\\frac{1}{S} \\sum_{i=1}^S \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta_j}\\right]\n&=\n\\frac{1}{S^2} Var \\left[\\sum_{i=1}^S \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta_j} \\right] \\\\\n&=\n\\frac{1}{S} Var \\left[\\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta_j} \\right]\n\\end{align*}\\]\nLarger batch size implies smaller variance, but at what cost?"
  },
  {
    "objectID": "lecs/w05/lec05.html#training-curve-or-learning-curve",
    "href": "lecs/w05/lec05.html#training-curve-or-learning-curve",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Training Curve (or Learning Curve)",
    "text": "Training Curve (or Learning Curve)\nTo diagnose optimization problems, it’s useful to look at learning curves: plot the training cost (or other metrics) as a function of iteration."
  },
  {
    "objectID": "lecs/w05/lec05.html#training-curve-or-learning-curve-contd",
    "href": "lecs/w05/lec05.html#training-curve-or-learning-curve-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Training Curve (or Learning Curve) (cont’d)",
    "text": "Training Curve (or Learning Curve) (cont’d)\n\nNote: use a fixed subset of the training data to monitor the training error. Evaluating on a different batch (e.g. the current one) in each iteration adds a lot of noise to the curve!\nNote: it’s very hard to tell from the training curves whether an optimizer has converged. They can reveal major problems, but they can’t guarantee convergence."
  },
  {
    "objectID": "lecs/w05/lec05.html#visualizing-optimization-algorithms",
    "href": "lecs/w05/lec05.html#visualizing-optimization-algorithms",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Visualizing Optimization Algorithms",
    "text": "Visualizing Optimization Algorithms\nYou might want to check out these links.\nAn overview of gradient descent algorithms:\n\nhttps://www.ruder.io/optimizing-gradient-descent/\n\nCS231n:\n\nhttps://cs231n.github.io/neural-networks-3/\n\nWhy momentum really works:\n\nhttps://distill.pub/2017/momentum/"
  },
  {
    "objectID": "lecs/w03/lec03.html#lecture-plan",
    "href": "lecs/w03/lec03.html#lecture-plan",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Lecture Plan",
    "text": "Lecture Plan\nLast week:\n\nFrom linear models to multilayer perceptrons\n\n\n\nBackpropagation to compute gradients efficiently"
  },
  {
    "objectID": "lecs/w03/lec03.html#lecture-plan-ii",
    "href": "lecs/w03/lec03.html#lecture-plan-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Lecture Plan II",
    "text": "Lecture Plan II\nThis week:\n\nFirst hour:\n\nautomatic differentiation\n\n\n\n\nSecond hour:\n\ndistributed representations\nGloVe embeddings\n\n\nBoth will be helpful for Assignment 1"
  },
  {
    "objectID": "lecs/w03/lec03.html#derivatives-in-machine-learning",
    "href": "lecs/w03/lec03.html#derivatives-in-machine-learning",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Derivatives in Machine Learning",
    "text": "Derivatives in Machine Learning\nThe machine learning approach requires the minimization of some cost/loss function, which is often done using some variation of gradient descent. \\[\\theta \\leftarrow \\theta - \\alpha\\frac{\\partial \\mathcal{E}}{\\partial \\theta}\\]"
  },
  {
    "objectID": "lecs/w03/lec03.html#derivatives-in-machine-learning-1",
    "href": "lecs/w03/lec03.html#derivatives-in-machine-learning-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Derivatives in Machine Learning",
    "text": "Derivatives in Machine Learning\n\\[\\theta \\leftarrow \\theta - \\alpha\\frac{\\partial \\mathcal{E}}{\\partial \\theta}\\]\nApproaches to computing derivatives:\n\nManually working out derivatives\n\n\n\nNumeric differentiation (using finite difference approximations)\n\n\n\n\nSymbolic differentiation (using expression manipulation)"
  },
  {
    "objectID": "lecs/w03/lec03.html#derivatives-in-machine-learning-2",
    "href": "lecs/w03/lec03.html#derivatives-in-machine-learning-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Derivatives in Machine Learning",
    "text": "Derivatives in Machine Learning\n\\[\\theta \\leftarrow \\theta - \\alpha\\frac{\\partial \\mathcal{E}}{\\partial \\theta}\\]\nApproaches to computing derivatives:\n\nNumeric differentiation (using finite difference approximations)\nSymbolic differentiation (using expression manipulation)\n\n\n\nAutomatic differentiation or algorithmic differentiation"
  },
  {
    "objectID": "lecs/w03/lec03.html#terminology",
    "href": "lecs/w03/lec03.html#terminology",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Terminology",
    "text": "Terminology\n\nAutomatic differentiation: convert the program into a sequence of primitive operations, which have specified routines for computing derivatives.Then, we can compute gradients in a mechanical way via the chain rule.\n\nAlso used in computational fluid dynamics, atmospheric sciences, etc.\n\n\n\n\nBackpropagation: special case of autodiff where the program is a neural network forward pass.\n\n\n\n\nAutograd, JAX, PyTorch, TensorFlow are examples of particular implementations of autodiff, i.e. different libraries"
  },
  {
    "objectID": "lecs/w03/lec03.html#backpropagation",
    "href": "lecs/w03/lec03.html#backpropagation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation",
    "text": "Backpropagation\nSteps:\n\nConvert the computation into a sequence of primitive operations\n\nPrimitive operations have easily computed derivatives\n\n\n\n\nBuild the computation graph\n\n\n\n\nPerform a forward pass: compute the values of each node\n\n\n\n\nPerform the backward pass: compute the derivative of the loss with respect to each node"
  },
  {
    "objectID": "lecs/w03/lec03.html#autodiff-more-generally",
    "href": "lecs/w03/lec03.html#autodiff-more-generally",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Autodiff, more generally",
    "text": "Autodiff, more generally\nWe will discuss how an automatic differentiation library could be implemented\n\nbuild the computation graph\n\n\n\nvector-Jacobian products (VJP) for primitive ops\n\n\n\n\nperform the backward pass\n\nYou will probably never have to implement autodiff yourself but it is good to know its inner workings!"
  },
  {
    "objectID": "lecs/w03/lec03.html#autodiff-more-generally-1",
    "href": "lecs/w03/lec03.html#autodiff-more-generally-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Autodiff, more generally",
    "text": "Autodiff, more generally\nKey Insight: For any new deep learning model that we can come up with, if each step of our computation is differentiable, then we can train that model using gradient descent."
  },
  {
    "objectID": "lecs/w03/lec03.html#scalar-example",
    "href": "lecs/w03/lec03.html#scalar-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Scalar Example",
    "text": "Scalar Example\ndef f(x):\n    h = 1.5\n    for i in range(3):\n        h = x * 1.5 + h\n    return x * h\nNotation: \\(x\\) is the input, \\(y=f(x)\\) is the output, we want to compute \\(\\frac{dy}{dx}\\)"
  },
  {
    "objectID": "lecs/w03/lec03.html#scalar-example-1",
    "href": "lecs/w03/lec03.html#scalar-example-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Scalar Example",
    "text": "Scalar Example\nAutomatic Differentiation Steps:\n\nconvert the computation into a sequence of primitive operations\n\nwe need to be able to compute derivatives for these primitive operations\n\n\n\n\nbuild the computation graph\n\n\n\n\nperform forward pass\n\n\n\n\nperform backward pass"
  },
  {
    "objectID": "lecs/w03/lec03.html#scalar-example-primitive-ops",
    "href": "lecs/w03/lec03.html#scalar-example-primitive-ops",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Scalar Example: Primitive Ops",
    "text": "Scalar Example: Primitive Ops\ndef f(x):\n    h = 1.5\n    for i in range(3):\n        h = x * 1.5 + h\n    return x * h\nOperations:\n\nh0 = 1.5\nz1 = x * 1.5\nh1 = z1 + h0\nz2 = x * 1.5\nh2 = z2 + h1\nz3 = x * 1.5\nh3 = z3 + h2\ny  = x * h3"
  },
  {
    "objectID": "lecs/w03/lec03.html#scalar-example-computation-graph",
    "href": "lecs/w03/lec03.html#scalar-example-computation-graph",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Scalar Example: Computation Graph",
    "text": "Scalar Example: Computation Graph\nExercise: Draw the computation graph:\nh0 = 1.5\nz1 = x * 1.5\nh1 = z1 + h0\nz2 = x * 1.5\nh2 = z2 + h1\nz3 = x * 1.5\nh3 = z3 + h2\ny  = x * h3\nBased on the computation graph, we can compute \\(\\frac{dy}{dx}\\) via a forward and a backward pass."
  },
  {
    "objectID": "lecs/w03/lec03.html#vector-inputs-and-outputs",
    "href": "lecs/w03/lec03.html#vector-inputs-and-outputs",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Vector Inputs and Outputs",
    "text": "Vector Inputs and Outputs\nMore generally, input/output to a computation may be vectors\ndef f(a, w): # a and w are both vectors with size 10\n    h = a\n    for i in range(3):\n        h = np.dot(w, h) + h\n    z = w * h # element wise multiplication\n    return z\nSo we have \\(\\bf{y} = f(\\bf{x})\\) (in this example, \\(\\bf{x}\\) consists of values in both a and w)\nQ: In our running example, what are the dimensions of \\({\\bf x}\\) and \\({\\bf y}\\)?"
  },
  {
    "objectID": "lecs/w03/lec03.html#the-jacobian-matrix",
    "href": "lecs/w03/lec03.html#the-jacobian-matrix",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Jacobian Matrix",
    "text": "The Jacobian Matrix\nWe wish to compute the gradients \\(\\frac{\\partial y_k}{\\partial x_i}\\) for each \\(k\\) and \\(i\\), at some \\(\\bf{x}\\).\nIn other words, we would like the to work with the Jacobian matrix\n\\[\\begin{align*}\nJ_f({\\bf x}) &= \\begin{bmatrix}\n\\frac{\\partial y_1}{\\partial x_1}({\\bf x}) & \\ldots & \\frac{\\partial y_1}{\\partial x_n}({\\bf x}) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial y_m}{\\partial x_1}({\\bf x}) & \\ldots & \\frac{\\partial y_m}{\\partial x_n}({\\bf x})\n\\end{bmatrix}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w03/lec03.html#the-jacobian-matrix-1",
    "href": "lecs/w03/lec03.html#the-jacobian-matrix-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Jacobian Matrix",
    "text": "The Jacobian Matrix\n\\[\\begin{align*}\nJ_f({\\bf x}) &= \\begin{bmatrix}\n\\frac{\\partial y_1}{\\partial x_1}({\\bf x}) & \\ldots & \\frac{\\partial y_1}{\\partial x_n}({\\bf x}) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial y_m}{\\partial x_1}({\\bf x}) & \\ldots & \\frac{\\partial y_m}{\\partial x_n}({\\bf x})\n\\end{bmatrix}\n\\end{align*}\\]\nNote that we usually want to avoid explicitly constructing the entries of this Jacobian one by one.\nWhy? Computing all the partial derivatives one by one is expensive, even with backprop."
  },
  {
    "objectID": "lecs/w03/lec03.html#decomposing-into-primitive-operations",
    "href": "lecs/w03/lec03.html#decomposing-into-primitive-operations",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Decomposing Into Primitive Operations",
    "text": "Decomposing Into Primitive Operations\nSuppose \\(f = f_2 \\circ f_1\\), so we have the computations \\({\\bf y} = f_2 \\circ f_1(\\bf{x})\\), or in other words: \\[\\begin{align*}\n{\\bf z} &= f_1(\\bf{x}) \\\\\n{\\bf y} &= f_2(\\bf{z})\n\\end{align*}\\]\nIf \\(f_1\\) and \\(f_2\\) are primitive operations with simple Jacobians, we can apply the Jacobian chain rule:\n\\[J_{f_2 \\circ f_1}({\\bf x}) = J_{f_2}({\\bf z})J_{f_1}({\\bf x})\\]"
  },
  {
    "objectID": "lecs/w03/lec03.html#autodiff-more-generally-2",
    "href": "lecs/w03/lec03.html#autodiff-more-generally-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Autodiff, more generally",
    "text": "Autodiff, more generally\nThis video explains the different ways to automatically compute derivatives:\nhttps://www.youtube.com/watch?v=wG_nF1awSSY\n\nmanual\n\n\n\nfinite differences\n\n\n\n\nsymbolic differentiation\n\n\n\n\nautodiff (forward-mode and reverse-mode differentiation)\n\nhow to avoid computing Jacobians one by one"
  },
  {
    "objectID": "lecs/w03/lec03.html#avoiding-jacobian-products",
    "href": "lecs/w03/lec03.html#avoiding-jacobian-products",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Avoiding Jacobian Products",
    "text": "Avoiding Jacobian Products\nIn practice, computing entries of Jacobians one by one is expensive and we try to avoid it:\n\nIf the dimension of \\({\\bf y} = f({\\bf x})\\) is small, use reverse-mode automatic differentiation\n\n\n\nIf the dimension of \\({\\bf x}\\) is small, use forward-mode automatic differentiation\n\nQ: Which of these two cases apply to deep learning most often?"
  },
  {
    "objectID": "lecs/w03/lec03.html#reverse-mode-automatic-differentiation",
    "href": "lecs/w03/lec03.html#reverse-mode-automatic-differentiation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Reverse-Mode Automatic Differentiation",
    "text": "Reverse-Mode Automatic Differentiation\nSuppose \\({\\bf y}\\) is a scalar, and represents the loss \\(\\mathcal{L}\\) that we wish to minimize. \\[\\begin{align*}\n{\\bf z} &= f_1(\\bf{x}) \\\\\n\\mathcal{L} &= f_2(\\bf{z}) = {\\bf y} \\in \\mathbb{R}\n\\end{align*}\\]\nThen we have:\n\n\\(\\overline{z} = \\frac{\\partial \\mathcal{L}}{\\partial z} = J_{f_2}(\\bf{z})^\\top\\)\n\n\n\nSince \\(\\overline{x_j} = \\sum_i \\overline{z_i} \\frac{\\partial z_i}{\\partial x_j}\\)"
  },
  {
    "objectID": "lecs/w03/lec03.html#reverse-mode-automatic-differentiation-1",
    "href": "lecs/w03/lec03.html#reverse-mode-automatic-differentiation-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Reverse-Mode Automatic Differentiation",
    "text": "Reverse-Mode Automatic Differentiation\n\n… we have \\(\\overline{\\bf x}^\\top = \\overline{\\bf z}^\\top J_{f_1}(\\bf{x})\\)\n\n\n\n… which is a vector-Jacobian product\n\n\n\nSummary: For each primitive operation, we don’t need to be able to compute entire Jacobian matrix. We need to be able to compute the vector-Jacobian product."
  },
  {
    "objectID": "lecs/w03/lec03.html#vector-jacobian-products",
    "href": "lecs/w03/lec03.html#vector-jacobian-products",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Vector Jacobian Products",
    "text": "Vector Jacobian Products\nFor each primitive operation, we must specify the VJPs for each of its arguments\nThe VJP function should takes in the output gradient (i.e. \\({\\bar y}\\)), the answer (\\(y\\)), and the arguments (\\(x\\)), and returns the input gradient (\\({\\bar x}\\)). Here are some examples from https://github.com/mattjj/autodidact/blob/master/autograd/numpy/numpy_vjps.py\ndefvjp(anp.negative, lambda g, ans, x: -g)\ndefvjp(anp.exp,    lambda g, ans, x: ans * g)\ndefvjp(anp.log,    lambda g, ans, x: g / x)\n\ndefvjp(anp.add,         lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, g))\ndefvjp(anp.multiply,    lambda g, ans, x, y : unbroadcast(x, y * g),\n                        lambda g, ans, x, y : unbroadcast(y, x * g))\ndefvjp(anp.subtract,    lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, -g))"
  },
  {
    "objectID": "lecs/w03/lec03.html#backprop-as-message-passing",
    "href": "lecs/w03/lec03.html#backprop-as-message-passing",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backprop as Message Passing",
    "text": "Backprop as Message Passing\n\n\nEach node in the computation graph receives messages from its children, which it aggregates to compute its error signal\n\n\n\nMessages then get passed to its parents\n\n\n\n\nEach message is a VJP"
  },
  {
    "objectID": "lecs/w03/lec03.html#backprop-as-message-passing-1",
    "href": "lecs/w03/lec03.html#backprop-as-message-passing-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backprop as Message Passing",
    "text": "Backprop as Message Passing\nThis design provides modularity! Each node needs to know how to compute its outgoing messages, i.e. the VJPs corresponding to each of its parents (arguments to the function)."
  },
  {
    "objectID": "lecs/w03/lec03.html#differentiable-programming",
    "href": "lecs/w03/lec03.html#differentiable-programming",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Differentiable Programming",
    "text": "Differentiable Programming\nRecall the key insight from earlier: For any new deep learning model that we can come up with, if each step of our computation is differentiable, then we can train that model using gradient descent.\nExample: Learning to learning by gradient descent by gradient descent https://arxiv.org/pdf/1606.04474.pdf\nWith AD, any program that has differentiable components can be optimized via gradient descent"
  },
  {
    "objectID": "lecs/w03/lec03.html#feature-mapping",
    "href": "lecs/w03/lec03.html#feature-mapping",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature Mapping",
    "text": "Feature Mapping\n\nLearning good representations is an important goal in machine learning\n\nThese representations are also called feature mappings, or embeddings\nThe representations we learn are often reusable for other tasks\nFinding good representations is an unsupervised learning problem!"
  },
  {
    "objectID": "lecs/w03/lec03.html#feature-mapping-1",
    "href": "lecs/w03/lec03.html#feature-mapping-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature Mapping",
    "text": "Feature Mapping\n\nAssignment 1:\n\nLearn vector representations of words: unsupervised learning\nSometimes also referred to as self-supervised learning"
  },
  {
    "objectID": "lecs/w03/lec03.html#language-modeling",
    "href": "lecs/w03/lec03.html#language-modeling",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Language Modeling",
    "text": "Language Modeling\nA language model…\n\nModels the probability distribution of natural language text.\n\n\n\nDetermine the probability \\(p({\\bf s})\\) that a sequence of words (or a sentence) \\({\\bf s}\\) occurs in text.\n\nA language model gives us a way to compute \\(p({\\bf s})\\)"
  },
  {
    "objectID": "lecs/w03/lec03.html#why-language-models-pbf-s",
    "href": "lecs/w03/lec03.html#why-language-models-pbf-s",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Language Models \\(p({\\bf s})\\)?",
    "text": "Why Language Models \\(p({\\bf s})\\)?\n\nDetermine authorship:\n\nbuild a language model \\(p({\\bf s})\\) of Shakespeare\ndetermine whether a script is written by Shakespeare\n\n\n\n\nGenerate a machine learning paper (given a corpus of machine learning papers)"
  },
  {
    "objectID": "lecs/w03/lec03.html#why-language-models-pbf-s-1",
    "href": "lecs/w03/lec03.html#why-language-models-pbf-s-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why language models \\(p({\\bf s})\\)?",
    "text": "Why language models \\(p({\\bf s})\\)?\n\nUse as a prior for a speech recognition system \\(p({\\bf s} | {\\bf a})\\), where \\({\\bf a}\\) represents the observed speech signal.\n\nAn observation model, or likelihood, represented as \\(p({\\bf a} | {\\bf s})\\), which tells us how likely the sentence \\({\\bf s}\\) is to lead to the acoustic signal \\({\\bf a}\\).\nA prior, represented as \\(p({\\bf s})\\) which tells us how likely a given sentence \\({\\bf s}\\) is. For example, “recognize speech” is more likely than “wreck a nice beach”"
  },
  {
    "objectID": "lecs/w03/lec03.html#why-language-models-pbf-s-2",
    "href": "lecs/w03/lec03.html#why-language-models-pbf-s-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why language models \\(p({\\bf s})\\)?",
    "text": "Why language models \\(p({\\bf s})\\)?\n\nUse as a prior for a speech recognition system \\(p({\\bf s} | {\\bf a})\\), where \\({\\bf a}\\) represents the observed speech signal.\n\nUse Bayes rule to infer a posterior distribution over sentences given the speech signal: \\[p({\\bf s} | {\\bf a}) = \\frac{p({\\bf s}) p({\\bf a} | {\\bf s})}{\\displaystyle \\sum_{{\\bf s^\\prime}} p({\\bf s^\\prime})p({\\bf a} | {\\bf s^\\prime})}\\]"
  },
  {
    "objectID": "lecs/w03/lec03.html#training-a-language-model",
    "href": "lecs/w03/lec03.html#training-a-language-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Training a Language Model",
    "text": "Training a Language Model\nAssume we have a corpus of sentences \\({\\bf s}^{(1)}, \\ldots, {\\bf s}^{(N)}\\)\nThe maximum likelihood criterion says we want our model to maximize the probability that our model assigns to the observed sentences. We assume the sentences are independent, so that their probabilities multiply."
  },
  {
    "objectID": "lecs/w03/lec03.html#training-a-language-model-1",
    "href": "lecs/w03/lec03.html#training-a-language-model-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Training a Language Model",
    "text": "Training a Language Model\nIn maximum likelihood training, we want to maximize \\[\\prod_{i=1}^N p\\left({\\bf s}^{(i)}\\right)\\]\n\nOr minimize: \\[-\\sum_{i=1}^N \\log p\\left({\\bf s}^{(i)}\\right)\\]\nSince \\(p({\\bf s})\\) is usually small, \\(-\\log p({\\bf s})\\) is reasonably sized, and positive."
  },
  {
    "objectID": "lecs/w03/lec03.html#probability-of-a-sentence",
    "href": "lecs/w03/lec03.html#probability-of-a-sentence",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Probability of a sentence",
    "text": "Probability of a sentence\nA sentence is a sequence of words \\(w_1, w_2, \\ldots, w_T\\), so \\[\\begin{align*}\np({\\bf s}) &= p(w_1, w_2, \\ldots, w_T) \\\\\n           &= p(w_1)p(w_2 | w_1) \\ldots p(w_T | w_1, w_2, \\ldots, w_{T-1})\n\\end{align*}\\] We can make a simplifying Markov assumption that the distribution over the next word depends on the preceding few words."
  },
  {
    "objectID": "lecs/w03/lec03.html#probability-of-a-sentence-1",
    "href": "lecs/w03/lec03.html#probability-of-a-sentence-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Probability of a sentence",
    "text": "Probability of a sentence\nIn assignment 1, we use a context length of 3 and model: \\[\\begin{align*}\np(w_t | w_1, w_2, \\ldots, w_{t-1}) = p(w_t | w_{t-3}, w_{t-2}, w_{t-1})\n\\end{align*}\\]\nThis is a self-supervised learning problem!"
  },
  {
    "objectID": "lecs/w03/lec03.html#n-gram-language-model",
    "href": "lecs/w03/lec03.html#n-gram-language-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "N-Gram Language Model",
    "text": "N-Gram Language Model\nA simple way of modeling \\(p(w_t | w_{t-2}, w_{t-1})\\) is by constructing a table of conditional probabilities:\nWhere the probabilities come from the empirical distribution: \\[p(w_3 = {\\rm cat} | w_1 = {\\rm the}, w_2 = {\\rm fat}) = \\frac{{\\rm count}({\\rm the\\ fat\\ cat})}{{\\rm count}({\\rm the\\ fat})}\\]\nThe phrases we’re counting are called n-grams (where n is the length), so this is an n-gram language model. (Note: the above example is considered a 3-gram model, not a 2-gram model!)"
  },
  {
    "objectID": "lecs/w03/lec03.html#example-shakespeare-n-gram-language-model",
    "href": "lecs/w03/lec03.html#example-shakespeare-n-gram-language-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: Shakespeare N-Gram Language Model",
    "text": "Example: Shakespeare N-Gram Language Model\n\n\n\n\nFrom https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf"
  },
  {
    "objectID": "lecs/w03/lec03.html#problems-with-n-gram-language-model",
    "href": "lecs/w03/lec03.html#problems-with-n-gram-language-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Problems with N-Gram Language Model",
    "text": "Problems with N-Gram Language Model\n\nThe number of entries in the conditional probability table is exponential in the context length.\n\n\n\nData sparsity: most n-grams never appear in the corpus, even if they are possible."
  },
  {
    "objectID": "lecs/w03/lec03.html#problems-with-n-gram-language-model-1",
    "href": "lecs/w03/lec03.html#problems-with-n-gram-language-model-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Problems with N-Gram Language Model",
    "text": "Problems with N-Gram Language Model\nWays to deal with data sparsity:\n\nUse a short context (but this means the model is less powerful),\n\n\n\nSmooth the probabilities, e.g. by adding imaginary counts,\n\n\n\n\nMake predictions using an ensemble of n-gram models with different \\(n\\)s."
  },
  {
    "objectID": "lecs/w03/lec03.html#local-vs-distributed-representations",
    "href": "lecs/w03/lec03.html#local-vs-distributed-representations",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Local vs Distributed Representations",
    "text": "Local vs Distributed Representations\nConditional probability tables are a kind of local representation: all the information about a particular word is stored in one place: a column of the table.\nBut different words are related, so we ought to be able to share information between them."
  },
  {
    "objectID": "lecs/w03/lec03.html#local-vs-distributed-representations-1",
    "href": "lecs/w03/lec03.html#local-vs-distributed-representations-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Local vs Distributed Representations",
    "text": "Local vs Distributed Representations"
  },
  {
    "objectID": "lecs/w03/lec03.html#distributed-representations-word-attributes",
    "href": "lecs/w03/lec03.html#distributed-representations-word-attributes",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Distributed Representations: Word Attributes",
    "text": "Distributed Representations: Word Attributes\nIdea:\n\nuse the word attributes to predict the next word.\nlearn the word attributes using an MLP with backpropagation"
  },
  {
    "objectID": "lecs/w03/lec03.html#sharing-information",
    "href": "lecs/w03/lec03.html#sharing-information",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Sharing Information",
    "text": "Sharing Information\nDistributed representations allows us to share information between related words. E.g., suppose we’ve seen the sentence\n\nThe cat got squashed in the garden on Friday.\n\nThis should help us predict the words in the sentence\n\nThe dog got flattened in the yard on (???)\n\nAn n-gram model can’t generalize this way, but a distributed representation might let us do so."
  },
  {
    "objectID": "lecs/w03/lec03.html#neural-language-model",
    "href": "lecs/w03/lec03.html#neural-language-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Neural Language Model",
    "text": "Neural Language Model"
  },
  {
    "objectID": "lecs/w03/lec03.html#word-representations",
    "href": "lecs/w03/lec03.html#word-representations",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Word Representations",
    "text": "Word Representations\nSince we are using one-hot encodings for the words, the weight matrix of the word embedding layer acts like a lookup table."
  },
  {
    "objectID": "lecs/w03/lec03.html#word-representations-1",
    "href": "lecs/w03/lec03.html#word-representations-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Word Representations",
    "text": "Word Representations\nTerminology:\n\n“Embedding” emphasizes that it’s a location in a high-dimensional space; words that are closer together are more semantically similar.\n“Feature vector” emphasizes that it’s a vector that can be used for making predictions, just like other feature mappings we’ve looked at (e.g. polynomials)."
  },
  {
    "objectID": "lecs/w03/lec03.html#what-do-word-embeddings-look-like",
    "href": "lecs/w03/lec03.html#what-do-word-embeddings-look-like",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What do Word Embeddings look like?",
    "text": "What do Word Embeddings look like?\nIt’s hard to visualize an \\(n\\)-dimensional space, but there are algorithms for mapping the embeddings to two dimensions.\n\n\n\n\n\n\nIn assignment 1, we use algorithm called tSNE, which tries to make distances in the 2-D embedding match the original high-dimensional distances as closely as possible."
  },
  {
    "objectID": "lecs/w03/lec03.html#a-note-about-these-visualizations",
    "href": "lecs/w03/lec03.html#a-note-about-these-visualizations",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "A note about these visualizations",
    "text": "A note about these visualizations\n\nThinking about high-dimensional embeddings\n\nMost vectors are nearly orthogonal (i.e. dot product is close to 0)\nMost points are far away from each other\n“In a 30-dimensional grocery store, anchovies can be next to fish and next to pizza toppings” - Geoff Hinton"
  },
  {
    "objectID": "lecs/w03/lec03.html#a-note-about-these-visualizations-1",
    "href": "lecs/w03/lec03.html#a-note-about-these-visualizations-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "A note about these visualizations",
    "text": "A note about these visualizations\n\nThe 2D embeddings might be fairly misleading, since they can’t preserve the distance relationship from a higher-dimensional embedding. (Unrelated words might be close together in 2D but far apart in 3D)"
  },
  {
    "objectID": "lecs/w03/lec03.html#glove",
    "href": "lecs/w03/lec03.html#glove",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GloVe",
    "text": "GloVe\n\nFitting language models is really hard\n\nIt’s really important to make good predictions about relative probabilities of rare words\nComputing the predictive distribution requires a large softmax"
  },
  {
    "objectID": "lecs/w03/lec03.html#glove-1",
    "href": "lecs/w03/lec03.html#glove-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GloVe",
    "text": "GloVe\n\nMaybe this is overkill if all you want is word representations\n\n\n\nGlobal Vector (GloVe) embeddings are a simpler and faster approach based on a matrix factorization similar to principal component analysis (PCA)\n\nIdea: First fit the distributed word representations using GloVe, then plug these embeddings into a neural net that does some other task (e.g. translation)"
  },
  {
    "objectID": "lecs/w03/lec03.html#the-distributional-hypothesis",
    "href": "lecs/w03/lec03.html#the-distributional-hypothesis",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Distributional Hypothesis",
    "text": "The Distributional Hypothesis\nDistributional Hypothesis: Words with similar distributions have similar meaning\nConsider a co-occurrence matrix \\(X\\), which counts the number of times the words appear nearby (say, less than 5 positions apart)\nThis is a \\(V \\times V\\) matrix, where \\(V\\) is the vocabulary size."
  },
  {
    "objectID": "lecs/w03/lec03.html#co-occurrence-matrix",
    "href": "lecs/w03/lec03.html#co-occurrence-matrix",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Co-occurrence Matrix",
    "text": "Co-occurrence Matrix\nSentence:\n\nThe cat got squashed in the garden on Friday. The dog got flattened in the yard on Thursday."
  },
  {
    "objectID": "lecs/w03/lec03.html#co-occurrence-matrix-1",
    "href": "lecs/w03/lec03.html#co-occurrence-matrix-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Co-occurrence Matrix",
    "text": "Co-occurrence Matrix\nPart of the co-occurrence matrix:\n\n\n\n\nthe\ncat\ndog\ngot\nsquashed\n\n\n\n\n\n\nthe\n0\n1\n0\n1\n0\n\n\n\n\ncat\n1\n0\n0\n1\n1\n\n\n\n\ndog\n1\n0\n0\n1\n0\n\n\n\n\ngot\n2\n1\n1\n0\n1\n\n\n\n\nsquashed\n0\n1\n1\n1\n0"
  },
  {
    "objectID": "lecs/w03/lec03.html#glove-embedding-training",
    "href": "lecs/w03/lec03.html#glove-embedding-training",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GloVe Embedding Training",
    "text": "GloVe Embedding Training\nKey insight: The co-occurrence matrix of words contain information about the semantic information (meaning) of words\n\nIn particular, the ratio of co-occurrences encodes semantic information!"
  },
  {
    "objectID": "lecs/w03/lec03.html#intuition-pump",
    "href": "lecs/w03/lec03.html#intuition-pump",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Intuition Pump",
    "text": "Intuition Pump\nSuppose we fit a rank-K approximation\n\\[{\\bf X} \\approx {\\bf R}{\\bf \\hat{R}}^\\top\\]\nWhere \\({\\bf R}\\) and \\({\\bf \\hat{R}}\\) are \\(V \\times K\\) matrices\n\nEach row \\({\\bf r}_i\\) of \\({\\bf R}\\) is the K-dim representation of a word.\nEach entry of \\({\\bf X}\\) is approximated as \\(x_{ij} \\approx {\\bf r}_i^T {\\bf \\hat{r}}_j\\) \nMinimizing the squared Frobenius norm of the \\(\\left|\\left|{\\bf X} - {\\bf R}{\\bf \\hat{R}}^\\top\\right|\\right|_F^2\\) is basically PCA\n\n\n\nThere are some other tricks to make the optimization work"
  },
  {
    "objectID": "lecs/w03/lec03.html#glove-embeddings-1",
    "href": "lecs/w03/lec03.html#glove-embeddings-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GloVe Embeddings",
    "text": "GloVe Embeddings\nPre-trained models are available for download:\nhttps://nlp.stanford.edu/projects/glove/\nPractitioners often use these embeddings to do other language modeling tasks."
  },
  {
    "objectID": "lecs/w03/lec03.html#glove-embedding-demo",
    "href": "lecs/w03/lec03.html#glove-embedding-demo",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "GloVe Embedding Demo",
    "text": "GloVe Embedding Demo\nDemo on Google Colab\nhttps://colab.research.google.com/drive/1aNbE6HcawVF67RV0hWi4qK33Um7cKykr?usp=sharing"
  },
  {
    "objectID": "lecs/w03/lec03.html#key-idea-from-the-demo",
    "href": "lecs/w03/lec03.html#key-idea-from-the-demo",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Key idea from the Demo",
    "text": "Key idea from the Demo\n\nDistances are somewhat meaningful, and are based on word co-occurrences\n\nthe words “black” and “white” will have similar embeddings because they co-occur with similar other words.\n“cat” and “dog” is more similar to each other than “cat” and “kitten” because the latter two words occur in different contexts!"
  },
  {
    "objectID": "lecs/w03/lec03.html#key-idea-from-the-demo-1",
    "href": "lecs/w03/lec03.html#key-idea-from-the-demo-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Key idea from the Demo",
    "text": "Key idea from the Demo\n\nWord Analogies: Directions in the embedding space can be meaningful\n\n“king” - “man” + “woman” \\(\\approx\\) “queen”\n\n\n\n\nBias in Word Embeddings (and Neural Networks in General)\n\nneural networks pick up pattern in the data\nthese patterns can be biased and discriminatory"
  },
  {
    "objectID": "lecs/w03/lec03.html#bias-and-fairness",
    "href": "lecs/w03/lec03.html#bias-and-fairness",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Bias and Fairness",
    "text": "Bias and Fairness\nWord embeddings are inherently biased because there is bias in the training data.\nNeural networks learn patterns in the training data, so if the training data contains human biases, then so will the trained model! This effect was seen in:\n\ncriminal sentencing\n\n\n\npredictive policing\n\n\n\n\nresume filtering"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Mine Çetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STA 210” in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "href": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you’ll upload your PDF and them mark the page(s) where each question can be found. It’s OK if a question spans multiple pages, just mark them all. It’s also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fall 2024 - UTM - CSC 413: Neural Networks and Deep Learning",
    "section": "",
    "text": "Course Overview\nIt is very hard to hand-design programs to solve many real-world problems, e.g. distinguishing images of cats v.s. dogs. Machine learning algorithms allow computers to learn from example data, and produce a program that does the job. Neural networks are a class of machine learning algorithms originally inspired by the brain, but which have recently have seen a lot of success at practical applications. They’re at the heart of production systems at companies like Google and Facebook for image processing, speech-to-text, and language understanding.\nThis course gives an overview of both the foundational ideas and the recent advances in neural net algorithms. Roughly the first 2/3 of the course focuses on supervised learning - training the network to produce a specified behavior when one has lots of labeled examples of that behavior. The last 1/3 focuses on unsupervised learning.\n\n\nPrerequisites\nPrerequisite: CSC311H5 or CSC411H5\nExclusion: CSC321H5 or CSC321H1 or CSC413H1 or CSC421H1 (SCI)\nDistribution Requirement: SCI\nStudents who lack a pre/co-requisite can be removed at any time unless they have received an explicit waiver from the department. The waiver form can be downloaded from here.\n\n\nCourse Delivery Details\n\n\n\n\n\n\n\n\n\n\nLectures\nProf\nDay\nTime\nLocation\n\n\n\n\nLEC0101\nIgor Gilitschenski\nTuesday\n5:00 pm - 7:00 pm\nMN3190\n\n\nLEC0102\nFlorian Shkurti\nWednesday\n11:00 am - 1:00 pm\nMN3190\n\n\n\n\n\n\nTutorials/Labs\nDay\nTime\nLocation\n\n\n\n\nPRA0101\nFriday\n10:00 am - 11:00 am\nDH2020\n\n\nPRA0102\nFriday\n11:00 am - 12:00 pm\nDH2020\n\n\nPRA0103\nFriday\n12:00 pm - 1:00 pm\nDH2020\n\n\n\n\nAnnouncements will be posted on Quercus\nDiscussions will take place on Piazza\nZoom recordings will be posted on Quercus after lectures",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, you’ll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance you’ll use for the course."
  },
  {
    "objectID": "ex/w06/questions/prob-variance-sol.html",
    "href": "ex/w06/questions/prob-variance-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First, we use the definition of variance and rewrite the left hand side as \\[\n\\Var(aX+bY) = \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2.\n\\] Next, we expand the squares for each of the terms on the right hand side: \\[\\begin{align*}\n\\E\\li[ (aX+bY)^2 \\ri]\n  &= \\E\\li[ a^2X^2 + 2abXY + b^2Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ XY ] + b^2\\E\\li[ Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri] ,\\\\\n\\E[aX+bY]^2\n  &= \\li(a\\E[X] + b\\E[Y]\\ri)^2 \\\\\n  &= a^2\\E[X]^2 + 2ab\\E[X]\\,\\E[Y] + b^2\\E[Y]^2.\n\\end{align*}\\] Subtracting the two terms, we get \\[\\begin{align*}\n& \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri]\n    - a^2\\E[X]^2 - 2ab\\E[X]\\,\\E[Y] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] - a^2\\E[X]^2\n   + b^2\\E\\li[ Y^2 \\ri] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2(\\E\\li[ X^2 \\ri] - \\E[X]^2)\n   + b^2\\li(\\E\\li[ Y^2 \\ri] - \\E[Y]^2\\ri) \\\\\n& \\quad = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w06/questions/ml-variance_bias_decomposition.html",
    "href": "ex/w06/questions/ml-variance_bias_decomposition.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Let \\(D=\\lbrace (x_i,y_i) | i=1 \\ldots n\\rbrace\\) be a dataset obtained from the true underlying data distribution \\(P\\), i.e. \\(D\\sim P^n\\). And let \\(h_D(\\cdot)\\) be a classifier trained on \\(D\\). Show the variance bias decomposition \\[\n\\underbrace{\\mathbb{E}_{D,x,y} \\li[ (h_D(x) - y)^2 \\ri]}_{\\text{Expected test error}}  \n  = \\underbrace{\\mathbb{E}_{D,x} \\li[ (h_D(x) - \\hat{h}(x))^2 \\ri]}_{\\text{Variance}} +\n  \\underbrace{\\mathbb{E}_{x,y} \\li[ (\\hat{y}(x) - y)^2 \\ri]}_{\\text{Noise}}  +  \\underbrace{\\mathbb{E}_{x} \\li[ (\\hat{h}(x) - \\hat{y}(x))^2 \\ri]}_{\\text{Bias}^2}\n\\] where \\(\\hat{h}(x) = \\mathbb{E}_{D \\sim P^n}[h_D(x)]\\) is the expected regressor over possible training sets, given the learning algorithm \\(\\mathcal{A}\\) and \\(\\hat{y}(x) = \\mathbb{E}_{y|x}[y]\\) is the expected label given \\(x\\). As mentioned in the lecture, labels might not be deterministic given x. To carry out the proof, proceed in the following steps:\n\nShow that the following identity holds \\[\\begin{align}\n\\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n= \\E_{D, x}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri].\n   \\end{align}\\]\nNext, show \\[\\begin{align}\nE_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri]\n=E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri]\n\\end{align}\\] which completes the proof by substituting (2) into (1)."
  },
  {
    "objectID": "ex/w06/exercises06-ensembling.html",
    "href": "ex/w06/exercises06-ensembling.html",
    "title": "Ensembling",
    "section": "",
    "text": "Tre training code below (based on a tutorial from here) trains a simple MNIS classifier.\nYour task is to adjust it to use an ensemble of 5 models instead of a single one.\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom torchvision import datasets, transforms\nfrom torch import nn, optim\ntransform = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5,), (0.5,)),\n                              ])\nSetup the dataset and dataloaders.\ntrainset = datasets.MNIST(\n    'datasets', download=True, train=True, transform=transform)\nvalset = datasets.MNIST(\n    'datasets', download=True, train=False, transform=transform)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=64, shuffle=True)\nvalloader = torch.utils.data.DataLoader(\n    valset, batch_size=64, shuffle=True)\nDefine the model.\ninput_size = 784\nhidden_sizes = [128, 64]\noutput_size = 10\n\nmodel = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[1], output_size),\n                      nn.LogSoftmax(dim=1))\ncriterion = nn.NLLLoss()\nimages, labels = next(iter(trainloader))\nimages = images.view(images.shape[0], -1)\n\nlogps = model(images) #log probabilities\nloss = criterion(logps, labels) #calculate the NLL loss\nRun the main training loop.\noptimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\ntime0 = time()\nepochs = 15\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in trainloader:\n        # Flatten MNIST images into a 784 long vector\n        images = images.view(images.shape[0], -1)\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    else:\n        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\nprint(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\ncorrect_count, all_count = 0, 0\nfor images,labels in valloader:\n  for i in range(len(labels)):\n    img = images[i].view(1, 784)\n    with torch.no_grad():\n        logps = model(img)\n\n    \n    ps = torch.exp(logps)\n    probab = list(ps.numpy()[0])\n    pred_label = probab.index(max(probab))\n    true_label = labels.numpy()[i]\n    if(true_label == pred_label):\n      correct_count += 1\n    all_count += 1\n\nprint(\"Number Of Images Tested =\", all_count)\nprint(\"\\nModel Accuracy =\", (correct_count/all_count))"
  },
  {
    "objectID": "ex/w06/exercises06-ensembling.html#version-with-ensembles",
    "href": "ex/w06/exercises06-ensembling.html#version-with-ensembles",
    "title": "Ensembling",
    "section": "Version with Ensembles",
    "text": "Version with Ensembles\nTODO: Add your code below."
  },
  {
    "objectID": "ex/w08/questions/rnn-addition.html",
    "href": "ex/w08/questions/rnn-addition.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "In this problem, you will implement a recurrent neural network which implements binary addition. The inputs are given as binary sequences, starting with the significant binary digit. (It is easier to start from the least significant bit, just like how you did addition in grade school.) The sequences will be padded with at least one zero as the most significant digit, so that the output length is the same as the input length. For example, the problem \\(100111 + 110010\\), whose target output value is \\(1011001\\), will be represented as follows: \\[\\begin{align*}\n\\bf{x}^{(1)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(2)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(3)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(4)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(5)} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(6)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(7)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{align*}\\]\nWith the target output: \\[\\begin{align*}\ny^{(1)} = 1,\ny^{(2)} = 0,\ny^{(3)} = 0,\ny^{(4)} = 1,\ny^{(5)} = 1,\ny^{(6)} = 0,\ny^{(7)} = 1,\n\\end{align*}\\]\nThere are two input units corresponding to the two inputs, and one output unit. Therefore, the pattern of inputs and outputs for this example would be:\n\nDesign, by hand, the weights and biases for an RNN which has two input units, three hidden units, and one output unit, which implements binary addition as discussed above. All of the units use the hard threshold activation function (\\(f(x) = 1\\) if \\(x &gt; 0\\) and \\(0\\) otherwise). In particular, specify weight matrices \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{W}\\), bias vector \\(\\mathbf{b}_{\\mathbf{h}}\\), and scalar bias \\(b_y\\) for the following architecture: \\[\\begin{align*}\nh^{(t)} &= f(\\bf{W}h^{(t-1)} + \\bf{U}\\bf{x}^{(t)} + \\bf{b_h}) \\\\\ny^{(t)} &= f(\\bf{v}^T h^{(t)} + b_y)\n\\end{align*}\\]\n\nWhat are the shapes of \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\)?\nCome up with values for \\(\\mathbf{U}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\). Justify your answer. Hint: When performing binary addition, in addition to adding up two digits in a column, we need to track whether there is a digit from the previous column. We will choose one of the three units in \\(\\bf{h}^{(t)}\\), say \\(\\bf{h}_2^{(t)}\\), to represent this carry digit. You may also find it helpful to set \\(\\bf{h}_1\\) to activate if the sum of the 3 digits is at least 1, \\(\\bf{h}_2\\) to activate if the sum is at least 2, and \\(\\bf{h}_3\\) to activate if the sum is at least 3.\nCome up with the values of \\(\\bf{v}\\) and \\(b_y\\). Justify your answer."
  },
  {
    "objectID": "ex/w08/questions/rnn-sentiment-sol.html",
    "href": "ex/w08/questions/rnn-sentiment-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "We will need to compute \\(h^{(t)}\\) for \\(t = 1, \\ldots, T\\). Each of this computation requires applying the weight matrices \\(W\\) and \\(T\\) once. The matrix \\(V\\) is only applied once at the end. Therefore, we need to apply \\(W\\) and \\(U\\) \\(T\\) times each and \\(V\\) once.\nThe shape of \\(U\\) is \\(d_h \\times d_x\\), the shape of \\(W\\) is \\(d_h \\times d_h\\), and the shape of \\(V\\) is \\(d_y \\times d_h\\), where \\(d_h\\) is the dimensionality of the \\(h^{(i)}\\) (i.e. \\(h^{(i)}\\in\\R^{d_h}\\)), \\(d_x\\) is the dimensionality of the inputs \\(x^{(i)}\\), and \\(d_y\\) is the dimensionality of the ouput \\(y\\).\nFor each of the \\(T\\) steps, we need to perform two matrix-vector multiplications (one for \\(Ux^{(i)}\\) and one for \\(Uh^{(i)}\\)) and two vector additions. To compute the output, we need one additional matrix-vector multiplication and one vector addition."
  },
  {
    "objectID": "ex/w08/questions/rnn-scalar.html",
    "href": "ex/w08/questions/rnn-scalar.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Suppose we have the following vanilla RNN network, where the inputs and hidden units are scalars. \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(w \\cdot h^{(t-1)} + u \\cdot x^{(t-1)} + b_h\\ri) \\\\\ny &= \\sigma\\li(v \\cdot h^{(T)} + b_y\\ri)\n\\end{align*}\\]\n\nShow that if \\(|w| &lt; 1\\), and the number of time steps \\(T\\) is large, then the gradient \\(\\frac{\\partial y}{\\partial x^{(0)}}\\) vanishes.\nWhy is the result from Part (a) troubling?"
  },
  {
    "objectID": "ex/w08/exercises08.html",
    "href": "ex/w08/exercises08.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "!include questions/rnn-sentiment.md"
  },
  {
    "objectID": "ex/w08/exercises08.html#exercise-1---rnn-for-sentiment-analysis",
    "href": "ex/w08/exercises08.html#exercise-1---rnn-for-sentiment-analysis",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "!include questions/rnn-sentiment.md"
  },
  {
    "objectID": "ex/w08/exercises08.html#exercise-2---scalar-rnn",
    "href": "ex/w08/exercises08.html#exercise-2---scalar-rnn",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Scalar RNN",
    "text": "Exercise 2 - Scalar RNN\n!include questions/rnn-scalar.md"
  },
  {
    "objectID": "ex/w08/exercises08.html#exercise-3---rnn-addition",
    "href": "ex/w08/exercises08.html#exercise-3---rnn-addition",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - RNN Addition",
    "text": "Exercise 3 - RNN Addition\n!include questions/rnn-addition.md"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers-sol.html",
    "href": "ex/w09/questions/attn-transformers-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The softmax function is applied row-wise and the shape of the result is \\(n_q\\times n_k\\). One way to see this is by looking at the shape of the dot product \\(QK^\\top\\) which is \\(n_q\\times n_k\\). Each row represents the pre-softmax scores of all keys and a given query. Because we need to normalize our attention weights per query, the normalization happens along the rows.\nThe value of \\(d\\) is \\(d_k\\). It is needed to scale the dot product so that the gradient of the softmax function does not vanish.\nTo obtain the computational complexity, let’s look at all the operations individually:\n\n\\(QK^\\top\\) requires \\(n_q n_k d_k\\) multiplications and \\(n_qn_k(d_k-1)\\) additions.\nDividing by \\(\\sqrt{d_k}\\) needs to be carried out \\(n_q n_k\\) times.\nApplying the softmax function can be implemented in \\(n_q n_k\\) divisions and \\(n_q(n_k-1)\\) additions.\nThe final matrix multiplication requires \\(n_qd_vn_k\\) multiplications and \\(n_q d_v (n_k-1)\\) additions.\n\nThe masking matrix is a triangular matrix with \\(-\\infty\\) on its top right half. This results in softmax weights being \\(0\\) for all key-query combinations to which \\(-\\infty\\) is added."
  },
  {
    "objectID": "ex/w09/questions/attn-transformers-notes.html",
    "href": "ex/w09/questions/attn-transformers-notes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\n\n\nQ = torch.tensor([[1, 2], [3, 1]]).float()\n\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\n\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\n\n\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])\n\n\nLazy version\n\nF.scaled_dot_product_attention(Q, K, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers_by_hand.html",
    "href": "ex/w09/questions/attn-transformers_by_hand.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Consider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 2\\\\\n3 & 1\n\\epmat,\\quad\nK = \\bpmat\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 2 & -2\\\\\n1 & 1 & 2 \\\\\n0 & 1 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w09/questions/attn-transformers_by_hand-notes.html",
    "href": "ex/w09/questions/attn-transformers_by_hand-notes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\n\n\nQ = torch.tensor([[1, 2], [3, 1]]).float()\n\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\n\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\n\n\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])\n\n\nLazy version\n\nF.scaled_dot_product_attention(Q, K, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers.html",
    "href": "ex/w09/questions/attn-transformers.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Transformers use a scaled dot product attention mechanism given by \\[\nC\n= \\text{attention}(Q, K, V)\n= \\text{softmax}\\left(\\fr{QK^\\top}{\\sqrt{d}}\\right) V,\n\\] where \\(Q\\in\\R^{n_q\\times d_k}\\), \\(K\\in\\R^{n_k\\times d_k}\\), \\(V\\in\\R^{n_k\\times d_v}\\).\n\nIs the softmax function here applied row-wise or column-wise? What is the shape of the result?\nWhat is the value of \\(d\\)? Why is it needed?\nWhat is the computational complexity of this attention mechanism? How many additions and multiplications are required? Assume the canonical matrix multiplcation and not counting \\(\\exp(x)\\) towards computational cost.\nIn the masked variant of the module, a masking matrix is added before the softmax function is applied. What are its values and its shape? For simplicity, assume \\(n_q=n_k\\)."
  },
  {
    "objectID": "ex/w09/exercises09.html",
    "href": "ex/w09/exercises09.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\)."
  },
  {
    "objectID": "ex/w09/exercises09.html#exercise-1---dot-product-attention",
    "href": "ex/w09/exercises09.html#exercise-1---dot-product-attention",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\)."
  },
  {
    "objectID": "ex/w09/exercises09.html#exercise-2---attention-in-transformers",
    "href": "ex/w09/exercises09.html#exercise-2---attention-in-transformers",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Attention in Transformers",
    "text": "Exercise 2 - Attention in Transformers\nTransformers use a scaled dot product attention mechanism given by \\[\nC\n= \\text{attention}(Q, K, V)\n= \\text{softmax}\\left(\\fr{QK^\\top}{\\sqrt{d}}\\right) V,\n\\] where \\(Q\\in\\R^{n_q\\times d_k}\\), \\(K\\in\\R^{n_k\\times d_k}\\), \\(V\\in\\R^{n_k\\times d_v}\\).\n\nIs the softmax function here applied row-wise or column-wise? What is the shape of the result?\nWhat is the value of \\(d\\)? Why is it needed?\nWhat is the computational complexity of this attention mechanism? How many additions and multiplications are required? Assume the canonical matrix multiplcation and not counting \\(\\exp(x)\\) towards computational cost.\nIn the masked variant of the module, a masking matrix is added before the softmax function is applied. What are its values and its shape? For simplicity, assume \\(n_q=n_k\\)."
  },
  {
    "objectID": "ex/w09/exercises09.html#exercise-3---scaled-dot-product-attention-by-hand",
    "href": "ex/w09/exercises09.html#exercise-3---scaled-dot-product-attention-by-hand",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Scaled Dot-Product Attention by Hand",
    "text": "Exercise 3 - Scaled Dot-Product Attention by Hand\nConsider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 2\\\\\n3 & 1\n\\epmat,\\quad\nK = \\bpmat\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 2 & -2\\\\\n1 & 1 & 2 \\\\\n0 & 1 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w07/questions/cnn-by_hand.html",
    "href": "ex/w07/questions/cnn-by_hand.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Consider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 2 & -1 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 2 \\\\\n2 & 1 & 0 & -1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 0 \\\\\n2, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w07/questions/opt-influence_functions-sol.html",
    "href": "ex/w07/questions/opt-influence_functions-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First, let’s recall the definitions of \\(\\te\\) and \\(\\hte(\\epsilon)\\): \\[\\begin{align*}\n\\hat{\\theta}\n&= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] \\\\\n\\hte({\\epsilon})\n  &= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] + \\epsilon L(x,y; \\theta)\n\\end{align*}\\] The first order taylor series expansion around \\(\\epsilon = 0\\) is given by \\[\n\\hte(0) + \\epsilon \\fr{d\\hte(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0}\n\\] From the definitions above, it can directly be seen that \\(\\hte(0) = \\hte\\) which completes the proof."
  },
  {
    "objectID": "ex/w07/questions/prob-mle_exp_dist-sol.html",
    "href": "ex/w07/questions/prob-mle_exp_dist-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters).\nAs mentioned in our previous exercise, we usually work with the log-likelihood in practice. In this particular case, the log likelihood is given by \\[\\begin{aligned}\nl(\\la | x_1, \\ldots, x_N)\n  & := \\sum_{i=1}^N \\ln f(\\fx_i | \\la) \\\\\n  & = \\sum_{i=1}^N \\ln\\li(\\la \\exp(-\\la x_i) \\ri) \\\\\n  & = \\sum_{i=1}^N \\ln(\\la) + \\ln\\li( \\exp(-\\la x_i) \\ri) \\\\\n  & =  N \\ln(\\la) - \\sum_{i=1}^N \\la x_i .\n\\end{aligned}\\] The derivative with respect to \\(\\la\\) is \\[\n\\fr{\\partial l(\\la | x_1, \\ldots, x_N)}{\\partial \\la}\n  = \\fr{N}{\\la} - \\sum_{i=1}^N x_i .\n\\] The MLE is obtained by setting it to 0 and solving for \\(\\la\\) as \\[\n\\hla_{MLE} = N \\li( \\sum_{i=1}^N x_i\\ri)^{-1}.\n\\]"
  },
  {
    "objectID": "ex/w07/questions/opt-influence_functions.html",
    "href": "ex/w07/questions/opt-influence_functions.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Let \\(\\hte\\) and \\(\\hte(\\ve)\\) be as defined in class. Show that the first order Taylor expansion of \\(\\hte(\\ve)\\) around \\(\\ve=0\\) is given by the equation given in class, i.e. by \\[\\begin{align*}\n\\hat{\\theta}({\\epsilon}) \\approx \\hat{\\theta} + \\epsilon\\frac{d\\hat{\\theta}(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0} .\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html",
    "href": "ex/w07/exercises07_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The exercises this week involve some old material so you can check your learning and understanding."
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-1---maximum-likelihood-estimator",
    "href": "ex/w07/exercises07_solution.html#exercise-1---maximum-likelihood-estimator",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 1 - Maximum Likelihood Estimator",
    "text": "Exercise 1 - Maximum Likelihood Estimator\nAssume you are given datapoints \\((x_i)_{i=1}^N\\) with \\(x_i\\in\\R\\) coming from a Exponential distribution. The probability density function of a exponential distribution is given by \\(f(x) = \\la \\exp(-\\la x)\\) with \\(x\\in\\R\\). Derive the maximum likelihood estimator of the parameter \\(\\la\\).\n\nSolution\nFirst, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters).\nAs mentioned in our previous exercise, we usually work with the log-likelihood in practice. In this particular case, the log likelihood is given by \\[\\begin{aligned}\nl(\\la | x_1, \\ldots, x_N)\n  & := \\sum_{i=1}^N \\ln f(\\fx_i | \\la) \\\\\n  & = \\sum_{i=1}^N \\ln\\li(\\la \\exp(-\\la x_i) \\ri) \\\\\n  & = \\sum_{i=1}^N \\ln(\\la) + \\ln\\li( \\exp(-\\la x_i) \\ri) \\\\\n  & =  N \\ln(\\la) - \\sum_{i=1}^N \\la x_i .\n\\end{aligned}\\] The derivative with respect to \\(\\la\\) is \\[\n\\fr{\\partial l(\\la | x_1, \\ldots, x_N)}{\\partial \\la}\n  = \\fr{N}{\\la} - \\sum_{i=1}^N x_i .\n\\] The MLE is obtained by setting it to 0 and solving for \\(\\la\\) as \\[\n\\hla_{MLE} = N \\li( \\sum_{i=1}^N x_i\\ri)^{-1}.\n\\]"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-2---convolutional-layers",
    "href": "ex/w07/exercises07_solution.html#exercise-2---convolutional-layers",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 2 & -1 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 2 \\\\\n2 & 1 & 0 & -1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 0 \\\\\n2, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?\n\n\nSolution\n\nHere, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 &  3 &  1 \\\\\n2 &  2 &  3 \\\\\n5 &  3 & -1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & 1 \\\\\n5 & -1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 &  0 &  0 & 0 \\\\\n0 & 1 & 2 & -1 &  1 & 0 \\\\\n0 & 1 & 0 &  1 &  0 & 0 \\\\\n0 & 0 & 1 &  0 &  2 & 0 \\\\\n0 & 2 & 1 &  0 & -1 & 0 \\\\\n0 & 0 & 0 &  0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 3 & 2 \\\\\n0 & 2 & 4 \\\\\n0 & 1 & -1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-3---computational-parameter-counting",
    "href": "ex/w07/exercises07_solution.html#exercise-3---computational-parameter-counting",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Computational Parameter Counting",
    "text": "Exercise 3 - Computational Parameter Counting\nUse PyTorch to load the vgg11 model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model.\n\nSolution\nFirst, we hvae to load the vgg11 model which is part of torchvision as has been shown in the lecture:\nimport torchvision\nvgg11 = torchvision.models.vgg.vgg11(pretrained=False)\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in vgg11.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the vgg11 model. All VGG models are ultimately instantiated by using the VGG class. Its forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in vgg11.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over vgg11.classifier instead of vgg11.features."
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-4---influence-functions",
    "href": "ex/w07/exercises07_solution.html#exercise-4---influence-functions",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - Influence Functions",
    "text": "Exercise 4 - Influence Functions\nLet \\(\\hte\\) and \\(\\hte(\\ve)\\) be as defined in class. Show that the first order Taylor expansion of \\(\\hte(\\ve)\\) around \\(\\ve=0\\) is given by the equation given in class, i.e. by \\[\\begin{align*}\n\\hat{\\theta}({\\epsilon}) \\approx \\hat{\\theta} + \\epsilon\\frac{d\\hat{\\theta}(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0} .\n\\end{align*}\\]\n\nSolution\nFirst, let’s recall the definitions of \\(\\te\\) and \\(\\hte(\\epsilon)\\): \\[\\begin{align*}\n\\hat{\\theta}\n&= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] \\\\\n\\hte({\\epsilon})\n  &= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] + \\epsilon L(x,y; \\theta)\n\\end{align*}\\] The first order taylor series expansion around \\(\\epsilon = 0\\) is given by \\[\n\\hte(0) + \\epsilon \\fr{d\\hte(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0}\n\\] From the definitions above, it can directly be seen that \\(\\hte(0) = \\hte\\) which completes the proof."
  },
  {
    "objectID": "ex/w10/questions/tconv-params.html",
    "href": "ex/w10/questions/tconv-params.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "What is the number of learnable parameters for each of the following transposed convolution layers defined in PyTorch. Try to calculate those by hand first and use pytorch later to verify your results.\n\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=1)\nnn.ConvTranspose2d(in_channels=3, out_channels=10, kernel_size=3, stride=2)\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=4, stride=5)\nnn.ConvTranspose2d(in_channels=3, out_channels=4, kernel_size=3, stride=23)"
  },
  {
    "objectID": "ex/w10/questions/tconv-sizes-sol.html",
    "href": "ex/w10/questions/tconv-sizes-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Output tensor size: \\(2\\times 4\\times 4\\)\nOutput tensor size: \\(4\\times 10\\times 10\\)\nThe first dimension is always the number of input channels, i.e. \\(c_{out}\\). The output for the first value of the input tensor is of size \\(3\\times 3\\). For all remaining values, the stride determines the additional values of the ouput. Stride 2 means, that we need two values for each additional value. Thus, the output height \\(h_{out}\\) and width are \\(w_{out}\\): \\[\n\\begin{aligned}\nh_{out} &= 3 + (h-1)\\cdot 2 = 2h+1\\\\\nw_{out} &= 3 + (w-1)\\cdot 2 = 2w+1\\\\\n\\end{aligned}\n\\] Thus, the resulting size is \\(2\\times (2h+1) \\times (2w+1)\\).\nThis generalization requires a similar consideration and we can basically reuse the equations from above: \\[\n\\begin{aligned}\nh_{out} &= h_k + (h-1)\\cdot s\\\\\nw_{out} &= w_k + (w-1)\\cdot s\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "ex/w10/questions/tconv-by_hand-sol.html",
    "href": "ex/w10/questions/tconv-by_hand-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The resulting output tensor is of shape \\(6 \\times 6\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  0 &  0 &  2 &  0\\\\\n1 &  2 &  0 &  0 &  2 &  4\\\\\n2 &  0 &  3 &  0 &  0 &  0\\\\\n2 &  4 &  3 &  6 &  0 &  0\\\\\n-1 &  0 &  0 &  0 &  3 &  0\\\\\n-1 & -2 &  0 &  0 &  3 &  6\n\\epmat\n\\]\nThe resulting output tensor is of shape \\(4 \\times 4\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  2 &  0 \\\\\n3 &  5 &  2 &  4 \\\\\n1 &  7 &  9 &  0 \\\\\n-1 & -2 &  3 &  6\n\\epmat\n\\]\nWe can verify our answers with PyTorch with the following commands:\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=2)\n\nnn.functional.conv_transpose2d(X.unsqueeze(0), K)\nThe call to .unsqueeze(0) can be left out if X has a batch dimension and is already a 4D tensor.\nA potential implementaiton (that isn’t optimized in any way) of transposed convolutions looks like this:\n\n\ndef conv_transpose2d(input, weight, stride=1):\n  # input - input of shape (C_in, H, W)\n  # weight - kernel of shape (C_in, C_out, K, K)\n  # stride - stride of the transposed convolutio\n  # RETURNS\n  # output - output of shape (C_out, H_out, W_out)\n  (c_in, h_in, w_in) = X.size()\n  (c2_in, c_out, k, k2) = K.size()\n\n  assert c_in == c2_in, \"Number of input channels must match\"\n  assert k == k2, \"Kernel must be square\"\n\n  h_out = (h_in - 1) * stride + k\n  w_out = (w_in - 1) * stride + k\n  output = torch.zeros((c_out, h_out, w_out))\n\n  for c_cur_in in range(c_in):\n    for c_cur_out in range(c_out):\n      for h in range(0, h_in):\n        for w in range(0, w_in):\n          output[c_cur_out, h*stride:h*stride+k, w*stride:w*stride+k] \\\n              += weight[c_cur_in, c_cur_out,:,:] * input[c_cur_in,h,w]\n\n  return output"
  },
  {
    "objectID": "ex/w10/questions/tconv-params-sol.html",
    "href": "ex/w10/questions/tconv-params-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The number of parameters is the product of input channels, output channels and kernel size. For each output channel, there is one bias parameter (if bias=True which is default in PyTorch). The stride does not matter for the number of paramters.\n\n\\(3\\cdot 2 \\cdot 9 + 2=56\\)\n\\(3\\cdot 10 \\cdot 9 + 10=280\\)\n\\(3\\cdot 2 \\cdot 16 + 2=98\\)\n\\(3\\cdot 4 \\cdot 9 + 4=112\\)"
  },
  {
    "objectID": "ex/w10/questions/tcons-by_hand-notes.html",
    "href": "ex/w10/questions/tcons-by_hand-notes.html",
    "title": "Solutions to (a), (b), and (c)",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nX = torch.tensor(\n    [[1, 0, 2], \n     [2, 3, 0], \n     [-1, 0, 3]]).unsqueeze(0)\n\nK = torch.tensor(\n    [[1, 0],\n     [1, 2]]).unsqueeze(0).unsqueeze(0)\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=2)\n\ntensor([[[[ 1,  0,  0,  0,  2,  0],\n          [ 1,  2,  0,  0,  2,  4],\n          [ 2,  0,  3,  0,  0,  0],\n          [ 2,  4,  3,  6,  0,  0],\n          [-1,  0,  0,  0,  3,  0],\n          [-1, -2,  0,  0,  3,  6]]]])\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=1)\n\ntensor([[[[ 1,  0,  2,  0],\n          [ 3,  5,  2,  4],\n          [ 1,  7,  9,  0],\n          [-1, -2,  3,  6]]]])"
  },
  {
    "objectID": "ex/w10/questions/tcons-by_hand-notes.html#solution-to-d",
    "href": "ex/w10/questions/tcons-by_hand-notes.html#solution-to-d",
    "title": "Solutions to (a), (b), and (c)",
    "section": "Solution to (d)",
    "text": "Solution to (d)\nBelow are some computations for verification.\n\ndef conv_transpose2d(input, weight, stride=1):\n  # input - input of shape (C_in, H, W)\n  # weight - kernel of shape (C_in, C_out, K, K)\n  # stride - stride of the transposed convolutio\n  # RETURNS\n  # output - output of shape (C_out, H_out, W_out)\n  (c_in, h_in, w_in) = X.size()\n  (c2_in, c_out, k, k2) = K.size()\n\n  assert c_in == c2_in, \"Number of input channels must match\"\n  assert k == k2, \"Kernel must be square\"\n\n  h_out = (h_in - 1) * stride + k\n  w_out = (w_in - 1) * stride + k\n  output = torch.zeros((c_out, h_out, w_out))\n\n  for c_cur_in in range(c_in):\n    for c_cur_out in range(c_out):\n      for h in range(0, h_in):\n        for w in range(0, w_in):\n          output[c_cur_out, h*stride:h*stride+k, w*stride:w*stride+k] \\\n              += weight[c_cur_in, c_cur_out,:,:] * input[c_cur_in,h,w]\n\n  return output\n\n\nconv_transpose2d(X, K, stride=2)\n\ntensor([[[ 1.,  0.,  0.,  0.,  2.,  0.],\n         [ 1.,  2.,  0.,  0.,  2.,  4.],\n         [ 2.,  0.,  3.,  0.,  0.,  0.],\n         [ 2.,  4.,  3.,  6.,  0.,  0.],\n         [-1.,  0.,  0.,  0.,  3.,  0.],\n         [-1., -2.,  0.,  0.,  3.,  6.]]])\n\n\n\nconv_transpose2d(X, K, stride=1)\n\ntensor([[[ 1.,  0.,  2.,  0.],\n         [ 3.,  5.,  2.,  4.],\n         [ 1.,  7.,  9.,  0.],\n         [-1., -2.,  3.,  6.]]])\n\n\n\nX = torch.randn((2,3,3))\nK = torch.randn((2,1,2,2))\n\n\nconv_transpose2d(X, K, stride=1)\n\ntensor([[[ 2.5196, -3.0373, -1.7501,  1.1531],\n         [-0.2329, -1.2616, -1.2943,  0.5111],\n         [-1.0364, -0.8051, -2.0395, -1.4085],\n         [ 1.9276,  0.1688, -3.1863, -1.4459]]])\n\n\n\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=1)\n\ntensor([[[[ 2.5196, -3.0373, -1.7501,  1.1531],\n          [-0.2329, -1.2616, -1.2943,  0.5111],\n          [-1.0364, -0.8051, -2.0395, -1.4085],\n          [ 1.9276,  0.1688, -3.1863, -1.4459]]]])"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html",
    "href": "ex/w10/exercises10_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)\n\n\n\n\nOutput tensor size: \\(2\\times 4\\times 4\\)\nOutput tensor size: \\(4\\times 10\\times 10\\)\nThe first dimension is always the number of input channels, i.e. \\(c_{out}\\). The output for the first value of the input tensor is of size \\(3\\times 3\\). For all remaining values, the stride determines the additional values of the ouput. Stride 2 means, that we need two values for each additional value. Thus, the output height \\(h_{out}\\) and width are \\(w_{out}\\): \\[\n\\begin{aligned}\nh_{out} &= 3 + (h-1)\\cdot 2 = 2h+1\\\\\nw_{out} &= 3 + (w-1)\\cdot 2 = 2w+1\\\\\n\\end{aligned}\n\\] Thus, the resulting size is \\(2\\times (2h+1) \\times (2w+1)\\).\nThis generalization requires a similar consideration and we can basically reuse the equations from above: \\[\n\\begin{aligned}\nh_{out} &= h_k + (h-1)\\cdot s\\\\\nw_{out} &= w_k + (w-1)\\cdot s\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html#exercise-1---transposed-convolution-output-sizes",
    "href": "ex/w10/exercises10_solution.html#exercise-1---transposed-convolution-output-sizes",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)\n\n\n\n\nOutput tensor size: \\(2\\times 4\\times 4\\)\nOutput tensor size: \\(4\\times 10\\times 10\\)\nThe first dimension is always the number of input channels, i.e. \\(c_{out}\\). The output for the first value of the input tensor is of size \\(3\\times 3\\). For all remaining values, the stride determines the additional values of the ouput. Stride 2 means, that we need two values for each additional value. Thus, the output height \\(h_{out}\\) and width are \\(w_{out}\\): \\[\n\\begin{aligned}\nh_{out} &= 3 + (h-1)\\cdot 2 = 2h+1\\\\\nw_{out} &= 3 + (w-1)\\cdot 2 = 2w+1\\\\\n\\end{aligned}\n\\] Thus, the resulting size is \\(2\\times (2h+1) \\times (2w+1)\\).\nThis generalization requires a similar consideration and we can basically reuse the equations from above: \\[\n\\begin{aligned}\nh_{out} &= h_k + (h-1)\\cdot s\\\\\nw_{out} &= w_k + (w-1)\\cdot s\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html#exercise-2---transposed-convolution-parameter-sizes",
    "href": "ex/w10/exercises10_solution.html#exercise-2---transposed-convolution-parameter-sizes",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Transposed Convolution Parameter Sizes",
    "text": "Exercise 2 - Transposed Convolution Parameter Sizes\nWhat is the number of learnable parameters for each of the following transposed convolution layers defined in PyTorch. Try to calculate those by hand first and use pytorch later to verify your results.\n\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=1)\nnn.ConvTranspose2d(in_channels=3, out_channels=10, kernel_size=3, stride=2)\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=4, stride=5)\nnn.ConvTranspose2d(in_channels=3, out_channels=4, kernel_size=3, stride=23)\n\n\nSolution\nThe number of parameters is the product of input channels, output channels and kernel size. For each output channel, there is one bias parameter (if bias=True which is default in PyTorch). The stride does not matter for the number of paramters.\n\n\\(3\\cdot 2 \\cdot 9 + 2=56\\)\n\\(3\\cdot 10 \\cdot 9 + 10=280\\)\n\\(3\\cdot 2 \\cdot 16 + 2=98\\)\n\\(3\\cdot 4 \\cdot 9 + 4=112\\)"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html#exercise-3---transposed-convolution-by-hand",
    "href": "ex/w10/exercises10_solution.html#exercise-3---transposed-convolution-by-hand",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Transposed Convolution by Hand",
    "text": "Exercise 3 - Transposed Convolution by Hand\nYou are given an input matrix \\(X\\) (consisting of a single channel) and a kernel \\(K\\) as follows: \\[\nX = \\bpmat\n1 & 0 & 2 \\\\\n2 & 3 & 0 \\\\\n-1 & 0 & 3\n\\epmat, \\quad\nK = \\bpmat\n1 & 0 \\\\\n1 & 2\n\\epmat\n\\]\n\nCompute the transposed convolution by hand assuming stride 2.\nCompute the transposed convolution by hand assuming stride 1.\nUse PyTorch to verify your answers.\nImplement the transposed convolution in PyTorch without using its own implementaiton. You can assume no bias term a square kernel and no separate batch dimension. I.e. your task is to implement the following function\n\ndef conv_transpose2d(inp, weight, stride=1):\n    # inp - input of shape (C_in, H, W)\n    # weight - kernel of shape (C_in, C_out, K, K)\n    # stride - stride of the transposed convolution\n    # RETURNS\n    # output - output of shape (C_out, H_out, W_out)\n    #\n    # YOUR CODE HERE\n    return output\n\nSolution\n\nThe resulting output tensor is of shape \\(6 \\times 6\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  0 &  0 &  2 &  0\\\\\n1 &  2 &  0 &  0 &  2 &  4\\\\\n2 &  0 &  3 &  0 &  0 &  0\\\\\n2 &  4 &  3 &  6 &  0 &  0\\\\\n-1 &  0 &  0 &  0 &  3 &  0\\\\\n-1 & -2 &  0 &  0 &  3 &  6\n\\epmat\n\\]\nThe resulting output tensor is of shape \\(4 \\times 4\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  2 &  0 \\\\\n3 &  5 &  2 &  4 \\\\\n1 &  7 &  9 &  0 \\\\\n-1 & -2 &  3 &  6\n\\epmat\n\\]\nWe can verify our answers with PyTorch with the following commands:\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=2)\n\nnn.functional.conv_transpose2d(X.unsqueeze(0), K)\nThe call to .unsqueeze(0) can be left out if X has a batch dimension and is already a 4D tensor.\nA potential implementaiton (that isn’t optimized in any way) of transposed convolutions looks like this:\n\n\ndef conv_transpose2d(input, weight, stride=1):\n  # input - input of shape (C_in, H, W)\n  # weight - kernel of shape (C_in, C_out, K, K)\n  # stride - stride of the transposed convolutio\n  # RETURNS\n  # output - output of shape (C_out, H_out, W_out)\n  (c_in, h_in, w_in) = X.size()\n  (c2_in, c_out, k, k2) = K.size()\n\n  assert c_in == c2_in, \"Number of input channels must match\"\n  assert k == k2, \"Kernel must be square\"\n\n  h_out = (h_in - 1) * stride + k\n  w_out = (w_in - 1) * stride + k\n  output = torch.zeros((c_out, h_out, w_out))\n\n  for c_cur_in in range(c_in):\n    for c_cur_out in range(c_out):\n      for h in range(0, h_in):\n        for w in range(0, w_in):\n          output[c_cur_out, h*stride:h*stride+k, w*stride:w*stride+k] \\\n              += weight[c_cur_in, c_cur_out,:,:] * input[c_cur_in,h,w]\n\n  return output"
  },
  {
    "objectID": "ex/w11/questions/cnn-by_hand.html",
    "href": "ex/w11/questions/cnn-by_hand.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Consider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & 1 & -1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 3 & 0 & 1 \\\\\n1 & -1 & 0 & 1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 2 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w11/questions/pytorch-parameter_count-sol.html",
    "href": "ex/w11/questions/pytorch-parameter_count-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First, we hvae to load the alexnet model which is part of torchvision:\nimport torchvision\nalexnet = torchvision.models.alexnet()\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in alexnet.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the alexnet model. The structure is similar to vgg and the forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in alexnet.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over alexnet.classifier instead of alexnet.features."
  },
  {
    "objectID": "ex/w11/questions/linalg-evs_to_mat-sol.html",
    "href": "ex/w11/questions/linalg-evs_to_mat-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\nIn our case, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 1.5 & -0.5 \\\\\n    -0.5 & 1.5\n    \\epmat\n  \\]"
  },
  {
    "objectID": "ex/w11/questions/attn-transformers_by_hand.html",
    "href": "ex/w11/questions/attn-transformers_by_hand.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Consider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 3\\\\\n0 & 1\n\\epmat,\\quad\nK = \\bpmat\n1 & 1\\\\\n1 & 2\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 0 & -2\\\\\n2 & 1 & 2 \\\\\n0 & 3 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w11/questions/attn-transformers_by_hand-sol.html",
    "href": "ex/w11/questions/attn-transformers_by_hand-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n1.80 & 1.00 & 1.44\\\\\n1.26 & 1.25 & 0.26\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w11/questions/cnn-by_hand-sol.html",
    "href": "ex/w11/questions/cnn-by_hand-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Here, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 &  3 &  -1 \\\\\n4 &  2 &  2 \\\\\n5 &  3 & 3\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 & -1 \\\\\n5 & 3\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 &  0 & 0 &  0 & 0 \\\\\n0 & 1 &  0 & 1 & -1 & 0 \\\\\n0 & 1 &  0 & 1 &  0 & 0 \\\\\n0 & 0 &  3 & 0 &  1 & 0 \\\\\n0 & 1 & -1 & 0 &  1 & 0 \\\\\n0 & 0 &  0 & 0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 1 & 0 \\\\\n2 & 2 & 0 \\\\\n2 & -1 & 1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html",
    "href": "ex/w11/exercises11_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\).\n\n\nFirst, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\nIn our case, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 1.5 & -0.5 \\\\\n    -0.5 & 1.5\n    \\epmat\n  \\]"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-1---eigenvalues-and-eigenvectors",
    "href": "ex/w11/exercises11_solution.html#exercise-1---eigenvalues-and-eigenvectors",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\).\n\n\nFirst, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\nIn our case, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 1.5 & -0.5 \\\\\n    -0.5 & 1.5\n    \\epmat\n  \\]"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-2---parameter-counting",
    "href": "ex/w11/exercises11_solution.html#exercise-2---parameter-counting",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Parameter Counting",
    "text": "Exercise 2 - Parameter Counting\nUse PyTorch to load the alexnet model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model.\n\nSolution\nFirst, we hvae to load the alexnet model which is part of torchvision:\nimport torchvision\nalexnet = torchvision.models.alexnet()\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in alexnet.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the alexnet model. The structure is similar to vgg and the forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in alexnet.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over alexnet.classifier instead of alexnet.features."
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-3---convolutional-layers",
    "href": "ex/w11/exercises11_solution.html#exercise-3---convolutional-layers",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Convolutional Layers",
    "text": "Exercise 3 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & 1 & -1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 3 & 0 & 1 \\\\\n1 & -1 & 0 & 1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 2 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?\n\n\nSolution\n\nHere, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 &  3 &  -1 \\\\\n4 &  2 &  2 \\\\\n5 &  3 & 3\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 & -1 \\\\\n5 & 3\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 &  0 & 0 &  0 & 0 \\\\\n0 & 1 &  0 & 1 & -1 & 0 \\\\\n0 & 1 &  0 & 1 &  0 & 0 \\\\\n0 & 0 &  3 & 0 &  1 & 0 \\\\\n0 & 1 & -1 & 0 &  1 & 0 \\\\\n0 & 0 &  0 & 0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 1 & 0 \\\\\n2 & 2 & 0 \\\\\n2 & -1 & 1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-4---scaled-dot-product-attention",
    "href": "ex/w11/exercises11_solution.html#exercise-4---scaled-dot-product-attention",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - Scaled Dot-Product Attention",
    "text": "Exercise 4 - Scaled Dot-Product Attention\nConsider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 3\\\\\n0 & 1\n\\epmat,\\quad\nK = \\bpmat\n1 & 1\\\\\n1 & 2\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 0 & -2\\\\\n2 & 1 & 2 \\\\\n0 & 3 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention.\n\nSolution\nThe resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n1.80 & 1.00 & 1.44\\\\\n1.26 & 1.25 & 0.26\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w02/questions/linalg-evs.html",
    "href": "ex/w02/questions/linalg-evs.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue."
  },
  {
    "objectID": "ex/w02/questions/linalg-evs-sol.html",
    "href": "ex/w02/questions/linalg-evs-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Because \\(\\la_1=\\la_2\\), we will write \\(\\la\\) for simplicity. The result is obtained by applying the definition of eigenvalues and distributivity via \\[\\begin{equation*}\n\\fA\\fu\n  = \\fA (\\al \\fu_1 + \\be \\fu_2)\n  =   \\al \\fA \\fu_1 + \\be \\fA \\fu_2\n  =   \\al \\la \\fu_1 + \\be \\la \\fu_2\n  =   \\la (\\al\\fu_1 + \\be \\fu_2)\n  =   \\la \\fu .\n\\end{equation*}\\]"
  },
  {
    "objectID": "ex/w02/questions/nn-compgraph-sol.html",
    "href": "ex/w02/questions/nn-compgraph-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Applying the chain rule, we have\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial w_j} \\]\nLooking at each term individually yields \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial y}\n  &= \\frac{\\partial}{\\partial y} [-t \\log(y) - (1 - t) \\log(1 - y)]\n  = - \\frac{t}{y} + \\frac{1 - t}{1 - y}\\\\\n\\frac{\\partial y}{\\partial z}\n  &= \\frac{\\partial \\sigma(z)}{\\partial z}\n  = \\sigma(z) (1 - \\sigma(z))\n  = y (1 - y)\\\\\n\\frac{\\partial z}{\\partial w_j}\n  &= \\frac{\\partial}{\\partial w_j} (w^\\top x) = x_j\n\\end{aligned}\n\\]\nBringing it all together yields: \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial w_j}\n  &= \\left( - \\frac{t}{y} + \\frac{1 - t}{1 - y} \\right) \\cdot y (1 - y) \\cdot x_j \\\\\n  &= (-t + ty + 1 - t - y + ty) x_j \\\\\n  &= (y - t) x_j\n\\end{aligned}\n\\]\nThe computation graph is given in the figure below.\n\n\n\n\nComputation graph for exercise 4 (b)"
  },
  {
    "objectID": "ex/w02/questions/prob-evvar-sol.html",
    "href": "ex/w02/questions/prob-evvar-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The key idea here is to compute the gradient of the objective function and solve for \\(\\fmu\\). The gradient is obtained by applying the chain rule resulting in \\[0=\\nabla_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2 = -2 \\sum_i (\\fx_i - \\fmu) .\\] Now, we solve this for \\(\\fmu\\) to obtain \\[\\fmu = \\frac{1}{N} \\sum_i \\fx_i .\\]\nHere, we simply need to apply some algebraic manipulations to show that the two definitions are equivalent. We start with the first definition and expand the square: \\[\\begin{align*}\n\\E[(X - \\E[X])^2]\n  &= \\E\\li[X^2 - 2X\\E[X]+\\E[X]^2\\ri] \\\\\n  &= \\E\\li[X^2\\ri] - \\E[2X\\E[X]]+\\E\\li[\\E[X]^2\\ri]\\\\\n  &= \\E\\li[X^2\\ri] - 2\\E[X]\\E[X]+\\E[X]^2\\\\\n  &= \\E\\li[X^2\\ri] - \\E[X]^2\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w02/exercises02.html",
    "href": "ex/w02/exercises02.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue."
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-1---eigenvectors-and-eigenvalues",
    "href": "ex/w02/exercises02.html#exercise-1---eigenvectors-and-eigenvalues",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue."
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-2---variance-and-expectation",
    "href": "ex/w02/exercises02.html#exercise-2---variance-and-expectation",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Variance and Expectation",
    "text": "Exercise 2 - Variance and Expectation\n\nGiven a set of vectors \\(\\{\\fx_i\\}_{i=1}^N\\). Show that their empirical mean is equivalent to \\[\\hmu=\\argmin_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2.\\]\nThere are two equivalent definitons of variance of a random variable. The first one is \\(\\Var(X) := \\E[(X - \\E[X])^2]\\) and the second is \\(\\Var(X) = \\E[X^2] - \\E[X]^2\\). Show that these two definitions actually are equivalent."
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-3---linear-regression",
    "href": "ex/w02/exercises02.html#exercise-3---linear-regression",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Linear Regression",
    "text": "Exercise 3 - Linear Regression\n\n\nIn the linear regression model with one feature, we have the following model/hypothesis: \\[y = f(x) = w x + b\\] ​with parameters, \\(w\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}(w, b) = \\frac{1}{2N}\\sum_i ((w x^{(i)} + b) - t^{(i)})^2\\] What are the derivatives \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\) and \\(\\frac{\\partial \\mathcal{E}}{\\partial b}\\)?\nIn the linear regression model with many features, we have the following model/hypothesis: \\[y = f(x) = {\\bf w}^\\top {\\bf x}+ b\\] with parameters, \\({\\bf w} = [w_1, w_2, \\dots w_d]^T\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_i ((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})^2\\] What is the derivative \\(\\frac{\\partial \\mathcal{E}}{\\partial w_j}\\) for a weight \\(w_j\\)?"
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-4---gradients-and-computation-graphs",
    "href": "ex/w02/exercises02.html#exercise-4---gradients-and-computation-graphs",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - Gradients and Computation Graphs",
    "text": "Exercise 4 - Gradients and Computation Graphs\n\n\nCompute the \\(\\frac{\\partial \\mathcal{L}}{\\partial w_j}\\) gradient of \\(\\mathcal{L}\\) with respect to a \\(w_j\\) in the following computation: \\[\\begin{align*}\n  \\mathcal{L}(y, t) &= - t \\log(y) - (1-t) \\log(1-y) ,\n& y &= \\sigma(z) ,\n&  z &= {\\bf w}^\\top {\\bf x} .\n  \\end{align*}\\]\nDraw the computation graph for the following neural network, showing the relevant scalar quantities. Assume that \\(\\fy, \\fh, \\fx \\in \\mathbb{R}^2\\) \\[\\begin{align*}\n  \\mathcal{L} &= \\frac{1}{2}\\sum_k (y_k - t_k)^2 ,\n& y_k &= \\sum_i w_{ki}^{(2)} h_i + b_k^{(2)} ,\n& h_i &= \\sigma(z_i) ,\n& z_i &= \\sum_j w_{ij}^{(1)} x_j + b_i^{(1)} .\n  \\end{align*}\\]"
  },
  {
    "objectID": "ex/w05/questions/opt-momentum-sol.html",
    "href": "ex/w05/questions/opt-momentum-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "An example implementation could look like this. First we deinfe the objective function and its gradient:\ndef objective(x):\n    return x**2\n\ndef obj_grad(x):\n    return 2*x\nThen, we implement the momentum optimizer itself:\ndef sgd_with_momentum(obj, grad, x_init, learning_rate, momentum, max_iter):\n  x = x_init\n  update = 0\n  for i in range(max_iter):\n    update = momentum * update - learning_rate * grad(x)\n    x = x + update\n\n    print('&gt;%d f(%s) = %.5f' % (i, x, obj(x)))\n  return x\nIt can now be invoked directly via:\nsgd_with_momentum(\n    objective, obj_grad, x_init=3.0, learning_rate=0.1, momentum=0.5, \n    max_iter=20\n    )"
  },
  {
    "objectID": "ex/w05/questions/linalg-evs_to_mat.html",
    "href": "ex/w05/questions/linalg-evs_to_mat.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given the sets of eigevalues and eigenvectors. Compute the corresponding matrix.\n\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,0)^\\top\\), \\(\\fv_2 = (0,1)^\\top\\).\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,1)^\\top\\), \\(\\fv_2 = (1,-1)^\\top\\)."
  },
  {
    "objectID": "ex/w05/questions/calc-taylor-sol.html",
    "href": "ex/w05/questions/calc-taylor-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "There are different ways to write the second order taylor series expansion a at a point \\(\\fa\\) for multivariate functions \\(f\\). We will use the following form\n\\[T(x) = f(\\fa) + \\nabla f(\\fa)^\\top (\\fx-\\fa) + \\frac{1}{2}(\\fx-\\fa)^\\top \\fH (\\fx-\\fa)\\]\nwhere \\(\\nabla f(\\fa)\\) is the gradient of \\(f\\) at \\(\\fa\\) and \\(\\fH\\) is the Hessian of \\(f\\) at \\(\\fa\\). As a reminder, the Hessian is the matrix of second order partial derivatives. So, all we need to do for all of the exercises is evaluate \\(f(\\fa)\\) and compute its gradient and Hessian.\n\n\\(T(x) = 5 + 15 (x-1) + 15 (x-1)^2\\)\nFirst, we simplify \\(f(x,y)=x^2 (y^3+1)\\) and compute \\(f(3,2)=81\\) and then we compute the gradient and Hessian of \\(f\\) resulting in \\[\n\\nabla f(x,y) = \\bpmat 2x (y^3-1) \\\\ x^2(3y^2-1) \\epmat,\\quad\n\\fH = \\bpmat 2(y^3-1) & 6 xy^2 \\\\ 6xy^2 & 6x^2y \\epmat.\n\\]\nThis one is similar to (b) but we have to be careful with the logarithm. First, we have \\(f(\\fx_0)=0\\) because \\(\\log(1)=0\\). Then, we compute the gradient and Hessian of \\(f\\) resulting in \\[\n    \\nabla f(\\fx) = \\bpmat\n   3x_1^2 x_2 \\log(x_2) \\\\ x_1^3 (1+\\log(x_2))\n   \\epmat,\\quad\n    \\fH = \\bpmat\n   6x_1 x_2 \\log(x_2)   & 3x_1^2 (1+\\log(x_2)) \\\\\n   3x_1^2 (1+\\log(x_2)) & x_1^3 x_2^{-1}\n    \\epmat\n    \\]\nEvaluating the trigonometric functions here is simpler than it seems because they are evaluated at \\(\\pi\\) and \\(-\\pi\\). We get \\(f(\\fx_0) = \\sin(-\\pi) + \\cos(\\pi) = 0 - 1 = -1\\) and \\[\n\\nabla f(\\fx) = \\bpmat \\cos(x_1) \\\\ -\\sin(x_2) \\epmat, \\quad\n\\fH = \\bpmat -\\sin(x_1) & 0 \\\\ 0 & -\\cos(x_2) \\epmat.\n\\]"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html",
    "href": "ex/w05/exercises05_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\).\n\n\n\nThere are different ways to write the second order taylor series expansion a at a point \\(\\fa\\) for multivariate functions \\(f\\). We will use the following form\n\\[T(x) = f(\\fa) + \\nabla f(\\fa)^\\top (\\fx-\\fa) + \\frac{1}{2}(\\fx-\\fa)^\\top \\fH (\\fx-\\fa)\\]\nwhere \\(\\nabla f(\\fa)\\) is the gradient of \\(f\\) at \\(\\fa\\) and \\(\\fH\\) is the Hessian of \\(f\\) at \\(\\fa\\). As a reminder, the Hessian is the matrix of second order partial derivatives. So, all we need to do for all of the exercises is evaluate \\(f(\\fa)\\) and compute its gradient and Hessian.\n\n\\(T(x) = 5 + 15 (x-1) + 15 (x-1)^2\\)\nFirst, we simplify \\(f(x,y)=x^2 (y^3+1)\\) and compute \\(f(3,2)=81\\) and then we compute the gradient and Hessian of \\(f\\) resulting in \\[\n\\nabla f(x,y) = \\bpmat 2x (y^3-1) \\\\ x^2(3y^2-1) \\epmat,\\quad\n\\fH = \\bpmat 2(y^3-1) & 6 xy^2 \\\\ 6xy^2 & 6x^2y \\epmat.\n\\]\nThis one is similar to (b) but we have to be careful with the logarithm. First, we have \\(f(\\fx_0)=0\\) because \\(\\log(1)=0\\). Then, we compute the gradient and Hessian of \\(f\\) resulting in \\[\n    \\nabla f(\\fx) = \\bpmat\n   3x_1^2 x_2 \\log(x_2) \\\\ x_1^3 (1+\\log(x_2))\n   \\epmat,\\quad\n    \\fH = \\bpmat\n   6x_1 x_2 \\log(x_2)   & 3x_1^2 (1+\\log(x_2)) \\\\\n   3x_1^2 (1+\\log(x_2)) & x_1^3 x_2^{-1}\n    \\epmat\n    \\]\nEvaluating the trigonometric functions here is simpler than it seems because they are evaluated at \\(\\pi\\) and \\(-\\pi\\). We get \\(f(\\fx_0) = \\sin(-\\pi) + \\cos(\\pi) = 0 - 1 = -1\\) and \\[\n\\nabla f(\\fx) = \\bpmat \\cos(x_1) \\\\ -\\sin(x_2) \\epmat, \\quad\n\\fH = \\bpmat -\\sin(x_1) & 0 \\\\ 0 & -\\cos(x_2) \\epmat.\n\\]"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html#exercise-1---taylor-series",
    "href": "ex/w05/exercises05_solution.html#exercise-1---taylor-series",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\).\n\n\n\nThere are different ways to write the second order taylor series expansion a at a point \\(\\fa\\) for multivariate functions \\(f\\). We will use the following form\n\\[T(x) = f(\\fa) + \\nabla f(\\fa)^\\top (\\fx-\\fa) + \\frac{1}{2}(\\fx-\\fa)^\\top \\fH (\\fx-\\fa)\\]\nwhere \\(\\nabla f(\\fa)\\) is the gradient of \\(f\\) at \\(\\fa\\) and \\(\\fH\\) is the Hessian of \\(f\\) at \\(\\fa\\). As a reminder, the Hessian is the matrix of second order partial derivatives. So, all we need to do for all of the exercises is evaluate \\(f(\\fa)\\) and compute its gradient and Hessian.\n\n\\(T(x) = 5 + 15 (x-1) + 15 (x-1)^2\\)\nFirst, we simplify \\(f(x,y)=x^2 (y^3+1)\\) and compute \\(f(3,2)=81\\) and then we compute the gradient and Hessian of \\(f\\) resulting in \\[\n\\nabla f(x,y) = \\bpmat 2x (y^3-1) \\\\ x^2(3y^2-1) \\epmat,\\quad\n\\fH = \\bpmat 2(y^3-1) & 6 xy^2 \\\\ 6xy^2 & 6x^2y \\epmat.\n\\]\nThis one is similar to (b) but we have to be careful with the logarithm. First, we have \\(f(\\fx_0)=0\\) because \\(\\log(1)=0\\). Then, we compute the gradient and Hessian of \\(f\\) resulting in \\[\n    \\nabla f(\\fx) = \\bpmat\n   3x_1^2 x_2 \\log(x_2) \\\\ x_1^3 (1+\\log(x_2))\n   \\epmat,\\quad\n    \\fH = \\bpmat\n   6x_1 x_2 \\log(x_2)   & 3x_1^2 (1+\\log(x_2)) \\\\\n   3x_1^2 (1+\\log(x_2)) & x_1^3 x_2^{-1}\n    \\epmat\n    \\]\nEvaluating the trigonometric functions here is simpler than it seems because they are evaluated at \\(\\pi\\) and \\(-\\pi\\). We get \\(f(\\fx_0) = \\sin(-\\pi) + \\cos(\\pi) = 0 - 1 = -1\\) and \\[\n\\nabla f(\\fx) = \\bpmat \\cos(x_1) \\\\ -\\sin(x_2) \\epmat, \\quad\n\\fH = \\bpmat -\\sin(x_1) & 0 \\\\ 0 & -\\cos(x_2) \\epmat.\n\\]"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html#exercise-2---eigenvalues-eigenvectors",
    "href": "ex/w05/exercises05_solution.html#exercise-2---eigenvalues-eigenvectors",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Eigenvalues, Eigenvectors",
    "text": "Exercise 2 - Eigenvalues, Eigenvectors\nYou are given the sets of eigevalues and eigenvectors. Compute the corresponding matrix.\n\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,0)^\\top\\), \\(\\fv_2 = (0,1)^\\top\\).\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,1)^\\top\\), \\(\\fv_2 = (1,-1)^\\top\\).\n\n\nSolution\nFirst, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\n\nHere, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 2 & 0 \\\\\n0 & 3\n\\epmat\n  \\]\nHere, we have to normalize the eigenvectors first. Each has length \\(\\sqrt{2}\\), so we have to divide each of them by \\(\\sqrt{2}\\), i.e. we set \\(\\fte_i:=\\fe_i/\\sqrt{2}\\). With this, we can construct an orthogonal matrix of eigenvalues as \\(\\fQ = (\\fte_1, \\fte_2)\\). The resulting matrix \\(\\fA\\) is \\[\n  \\fA = \\bpmat\n2.5 & -0.5 \\\\\n-0.5 & 2.5\n\\epmat\n  \\]"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html#exercise-3---sgd-with-momentum",
    "href": "ex/w05/exercises05_solution.html#exercise-3---sgd-with-momentum",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - SGD with Momentum",
    "text": "Exercise 3 - SGD with Momentum\nImplement stochastic gradient descent with momentum and apply it to optimize some elementary functions in 1d and 2d.\n\nSolution\nAn example implementation could look like this. First we deinfe the objective function and its gradient:\ndef objective(x):\n    return x**2\n\ndef obj_grad(x):\n    return 2*x\nThen, we implement the momentum optimizer itself:\ndef sgd_with_momentum(obj, grad, x_init, learning_rate, momentum, max_iter):\n  x = x_init\n  update = 0\n  for i in range(max_iter):\n    update = momentum * update - learning_rate * grad(x)\n    x = x + update\n\n    print('&gt;%d f(%s) = %.5f' % (i, x, obj(x)))\n  return x\nIt can now be invoked directly via:\nsgd_with_momentum(\n    objective, obj_grad, x_init=3.0, learning_rate=0.1, momentum=0.5, \n    max_iter=20\n    )"
  },
  {
    "objectID": "ex/w04/questions/cnn-sizes-sol.html",
    "href": "ex/w04/questions/cnn-sizes-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The outputs dimensions after each layer are:\n\n1. 98 x 98 x 5 \n2. 49 x 49 x 5\n3. 47 x 47 x 10\n4. 23 x 23 x 10\n5. 21 x 21 x 5 \n6. 2205\n7. 20\n8. 10\n\nThe number of parameters for each layer is:\n\n1. 140\n2. 0\n3. 460\n4. 0\n5. 455\n6. 0\n7. 44120\n8. 210"
  },
  {
    "objectID": "ex/w04/questions/nlp-co_occurrence-sol.html",
    "href": "ex/w04/questions/nlp-co_occurrence-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The solution is not unique because we can change the order of the reference entries. In our case we will simply follow the intuitive ordering resulting in the following co-occurence matrix:\n\n\n\nReference\nA\nbird\nin\nthe\nhand\nis\nworth\ntwo\nbush\n\n\n\n\nA\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nbird\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nin\n0\n0\n0\n2\n0\n0\n0\n0\n0\n\n\nthe\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nhand\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nis\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nworth\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\ntwo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nbush\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "ex/w04/questions/nlp-co_occurrence.html",
    "href": "ex/w04/questions/nlp-co_occurrence.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?"
  },
  {
    "objectID": "ex/w04/questions/cnn-by_hand-sol.html",
    "href": "ex/w04/questions/cnn-by_hand-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Here, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -1 & -3 \\\\\n2 & 3 &  3 \\\\\n5 & 2 &  1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -3 \\\\\n5 &  1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -2 & 1 & 0\\\\\n0 & 0 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 1 & 0\\\\\n0 & -3 & 4 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & -2 & 0 \\\\\n0 & 3  &  0 \\\\\n-3 & 8 & 0\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w04/exercises04.html",
    "href": "ex/w04/exercises04.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-1---co-occurrence-matrix",
    "href": "ex/w04/exercises04.html#exercise-1---co-occurrence-matrix",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-2---convolutional-layers",
    "href": "ex/w04/exercises04.html#exercise-2---convolutional-layers",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & -2 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n-3 & 4 & 0 & 0\n\\epmat, \\qquad\n%\nK = \\bpmat\n2, & 1 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-3---sizes-in-mlps-refresher",
    "href": "ex/w04/exercises04.html#exercise-3---sizes-in-mlps-refresher",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Sizes in MLPs Refresher",
    "text": "Exercise 3 - Sizes in MLPs Refresher\nYou are given an MLP with ReLU activations. It has 3 layers consisting of 5, 10, and 5 neurons respectively. The input is a vector of size 10. How many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-4---sizes-in-cnns",
    "href": "ex/w04/exercises04.html#exercise-4---sizes-in-cnns",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - Sizes in CNNs",
    "text": "Exercise 4 - Sizes in CNNs\nYou are givne a neural network with the following architecture:\nInput: 100 x 100 x 3 Image\n\nLayers:\n1. Conv(in_channels=3, out_channels=5, kernel_size=3, stride=1, padding=0)\n2. MaxPool2d(kernel_size=2, stride=2, padding=0)\n3. Conv(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=0)\n4. MaxPool2d(kernel_size=2, stride=2, padding=0)\n5. Conv(in_channels=10, out_channels=5, kernel_size=3, stride=1, padding=0)\n6. Flatten()\n7. MLP(neurons=20)\n8. MLP(neurons=10)\n\nWhat is the dimensionality of the activations after each layer.\nHow many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html",
    "href": "ex/w04/exercises04_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?\n\n\nThe solution is not unique because we can change the order of the reference entries. In our case we will simply follow the intuitive ordering resulting in the following co-occurence matrix:\n\n\n\nReference\nA\nbird\nin\nthe\nhand\nis\nworth\ntwo\nbush\n\n\n\n\nA\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nbird\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nin\n0\n0\n0\n2\n0\n0\n0\n0\n0\n\n\nthe\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nhand\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nis\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nworth\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\ntwo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nbush\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-1---co-occurrence-matrix",
    "href": "ex/w04/exercises04_solution.html#exercise-1---co-occurrence-matrix",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?\n\n\nThe solution is not unique because we can change the order of the reference entries. In our case we will simply follow the intuitive ordering resulting in the following co-occurence matrix:\n\n\n\nReference\nA\nbird\nin\nthe\nhand\nis\nworth\ntwo\nbush\n\n\n\n\nA\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nbird\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nin\n0\n0\n0\n2\n0\n0\n0\n0\n0\n\n\nthe\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nhand\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nis\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nworth\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\ntwo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nbush\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-2---convolutional-layers",
    "href": "ex/w04/exercises04_solution.html#exercise-2---convolutional-layers",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & -2 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n-3 & 4 & 0 & 0\n\\epmat, \\qquad\n%\nK = \\bpmat\n2, & 1 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?\n\n\nSolution\n\nHere, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -1 & -3 \\\\\n2 & 3 &  3 \\\\\n5 & 2 &  1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -3 \\\\\n5 &  1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -2 & 1 & 0\\\\\n0 & 0 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 1 & 0\\\\\n0 & -3 & 4 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & -2 & 0 \\\\\n0 & 3  &  0 \\\\\n-3 & 8 & 0\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-3---sizes-in-mlps-refresher",
    "href": "ex/w04/exercises04_solution.html#exercise-3---sizes-in-mlps-refresher",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Sizes in MLPs Refresher",
    "text": "Exercise 3 - Sizes in MLPs Refresher\nYou are given an MLP with ReLU activations. It has 3 layers consisting of 5, 10, and 5 neurons respectively. The input is a vector of size 10. How many parameters does this network have?\n\nSolution\nThe number of parameters for each neuron is the number of weights plus one for the biaas term. The number of weights corresponds to the number of inputs / activations from the previous layer. So for the first layer, we have 10 inputs and thus 11 parameters per neuron resulting in 55 parameters total per layer.\nA similar computation gives 60 and 55 as the number of parameters for the next two layers. Thus, the network has a total of 170 parameters."
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-4---sizes-in-cnns",
    "href": "ex/w04/exercises04_solution.html#exercise-4---sizes-in-cnns",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - Sizes in CNNs",
    "text": "Exercise 4 - Sizes in CNNs\nYou are givne a neural network with the following architecture:\nInput: 100 x 100 x 3 Image\n\nLayers:\n1. Conv(in_channels=3, out_channels=5, kernel_size=3, stride=1, padding=0)\n2. MaxPool2d(kernel_size=2, stride=2, padding=0)\n3. Conv(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=0)\n4. MaxPool2d(kernel_size=2, stride=2, padding=0)\n5. Conv(in_channels=10, out_channels=5, kernel_size=3, stride=1, padding=0)\n6. Flatten()\n7. MLP(neurons=20)\n8. MLP(neurons=10)\n\nWhat is the dimensionality of the activations after each layer.\nHow many parameters does this network have?\n\n\nSolution\n\nThe outputs dimensions after each layer are:\n\n1. 98 x 98 x 5 \n2. 49 x 49 x 5\n3. 47 x 47 x 10\n4. 23 x 23 x 10\n5. 21 x 21 x 5 \n6. 2205\n7. 20\n8. 10\n\nThe number of parameters for each layer is:\n\n1. 140\n2. 0\n3. 460\n4. 0\n5. 455\n6. 0\n7. 44120\n8. 210"
  },
  {
    "objectID": "ex/w03/questions/autodiff-modes.html",
    "href": "ex/w03/questions/autodiff-modes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Consider the function \\(F(x) = f_3(f_2(f_1(x))\\) and assume you also know the derivatives \\(f_i'\\) for all \\(f_i\\).\n\nApply the chain rule to express \\(F'(x)\\) in terms of \\(f_i'\\)s and \\(f_i\\).\nWrite down the pseudocode for computing \\(F'(x)\\) using the forward mode and the reverse mode respectively. Assume all functions to be scalar functions of a scalar variable, i.e. \\(f_i: \\R \\rightarrow \\R\\).\nIf you simply ask your interpreter / compiler to evaluate the expression in (a), will the computation be in forward mode, reverse mode, or neither of the modes? Why? You can assume that your interpreter / compiler does not do any caching or optimization and simply evaluates the expression from left to right. Does anything change if you assume that your interpreter caches results that have been computed before?"
  },
  {
    "objectID": "ex/w03/questions/prob-mle.html",
    "href": "ex/w03/questions/prob-mle.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean."
  },
  {
    "objectID": "ex/w03/questions/prob-mle-sol.html",
    "href": "ex/w03/questions/prob-mle-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters). The product on the right hand side is also known as the likelihood function. In practice, we usually work with the log-likelihood function instead. Because the logarithm is monotonously increasing, the resulting estimators are the same, i.e. \\[\n\\hte_{\\mathrm{MLE}}\n= \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n= \\argmax_{\\te \\in \\Te} \\sum_{i=1}^N \\log \\li( f(\\fx_i | \\te)\\ri).\n\\]\nSecond, let’s remember that the probability density function of a multivariate Gaussian distribution is given by \\[\nf(\\fx_i | \\mu, \\Si)\n= \\frac{1}{(2 \\pi)^{n/2} |\\Si|^{1/2}}\n   \\exp \\li(\n     - \\frac{1}{2}\n     (\\fx_i - \\mu)^\\top\n     \\Si^{-1}\n     (\\fx_i - \\mu)\n   \\ri)\n\\] with parameters \\(\\te=(\\mu, \\Si)\\), where \\(\\mu\\in\\R^n\\) is the mean vector and \\(\\Si\\in\\R^{n\\times n}\\) is the covariance matrix. Moreover, the notation \\(|\\Si|\\) denotes the determinant of \\(\\Si\\).\nOur goal is to maximize the log-likelihood function which in this case is \\[\\begin{aligned}\nl(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n  & := \\sum_{i=1}^N \\log f(\\fx_i | \\mu, \\Si) \\\\\n  & = \\sum_{i=1}^N \\li(\n    - \\frac{n}{2} \\log (2 \\pi)\n    - \\frac{1}{2} \\log |\\Si|  \n    - \\frac{1}{2}  (\\fx_i - \\mu)^\\top \\Si^{-1} (\\fx_i - \\mu)\n  \\ri).\n\\end{aligned}\\] For obtaining the MLE of \\(\\mu\\), we can simply take the gradient of \\(l\\) with respect to \\(\\mu\\) and set it to zero resulting in: \\[\n\\nabla_\\mu l(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n= \\sum_{i=1}^N  \\Si^{-1} ( \\fx_i - \\mu )\n= 0\n\\] Since \\(\\Si\\) is a covariance matrix, it is positive definite. Thus, we can multiply both sides of the equation by \\(\\Si\\) and obtain \\[\n\\begin{aligned}\n0 & = N \\mu - \\sum_{i=1}^N  \\fx_i,\n\\\\\n\\Rightarrow \\hmu_{\\mathrm{MLE}} &=  \\frac{1}{N} \\sum_{i=1}^N \\fx_i .\n\\end{aligned}\n\\] Conveniently, \\(\\Si\\) disappears and thus we do not have to worry about it."
  },
  {
    "objectID": "ex/w03/exercises03.html",
    "href": "ex/w03/exercises03.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean."
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-1---maximum-likelihood-estimation-refresher",
    "href": "ex/w03/exercises03.html#exercise-1---maximum-likelihood-estimation-refresher",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean."
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-2---more-gradients",
    "href": "ex/w03/exercises03.html#exercise-2---more-gradients",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - More Gradients",
    "text": "Exercise 2 - More Gradients\nYou are an ML Engineer at Googlezon where you are working on an internal ML framework called TorchsorFlow. You are tasked with implementing a new layer known as BatchNormalization. The idea of this layer is as follows:\nDuring training, consider the outputs of the previous layer \\(\\fa_i=(a_i^{(1)}, \\ldots, a_i^{(N)})\\) for each element \\(i\\in \\{1, \\ldots, M\\}\\) of the input batch. Compute the mean \\(\\mu_j\\) and variance \\(\\si_j^2\\) over each input dimension \\(j\\). Use the resulting statistics to normalize the output of the previous layer. Finally, rescale the resulting vector with a learned constant \\(\\gm\\) and shift it by another learned constant \\(\\be\\).\n\nWrite down the mathematical expression for the BatchNormalization layer. What are its learnable parameters?\nCompute the gradient of the loss \\(\\mcL\\) with respect to the input of the BatchNormalization \\(\\fa_i\\) layer.\nAt test time, the batch size is usually 1. So, it is not meaningful (or even possible) to compute mean / variance. How would you implement a layer like this?"
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-3---autodiff-modes",
    "href": "ex/w03/exercises03.html#exercise-3---autodiff-modes",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Autodiff Modes",
    "text": "Exercise 3 - Autodiff Modes\nConsider the function \\(F(x) = f_3(f_2(f_1(x))\\) and assume you also know the derivatives \\(f_i'\\) for all \\(f_i\\).\n\nApply the chain rule to express \\(F'(x)\\) in terms of \\(f_i'\\)s and \\(f_i\\).\nWrite down the pseudocode for computing \\(F'(x)\\) using the forward mode and the reverse mode respectively. Assume all functions to be scalar functions of a scalar variable, i.e. \\(f_i: \\R \\rightarrow \\R\\).\nIf you simply ask your interpreter / compiler to evaluate the expression in (a), will the computation be in forward mode, reverse mode, or neither of the modes? Why? You can assume that your interpreter / compiler does not do any caching or optimization and simply evaluates the expression from left to right. Does anything change if you assume that your interpreter caches results that have been computed before?"
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-4---glove-embeddings",
    "href": "ex/w03/exercises03.html#exercise-4---glove-embeddings",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - GloVe Embeddings",
    "text": "Exercise 4 - GloVe Embeddings\nOpen the notebook presented in class and work through it by trying some of the ideas presented therein for different word combinations."
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 15",
    "section": "",
    "text": "Important\n\n\n\nDue dates: Tue, Apr 19 - HW 5"
  },
  {
    "objectID": "weeks/week-15.html#participate",
    "href": "weeks/week-15.html#participate",
    "title": "Week 15",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 28 - Wrap up"
  },
  {
    "objectID": "weeks/week-15.html#practice",
    "href": "weeks/week-15.html#practice",
    "title": "Week 15",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 13 - A Tale of Two Creeks"
  },
  {
    "objectID": "weeks/week-15.html#perform",
    "href": "weeks/week-15.html#perform",
    "title": "Week 15",
    "section": "Perform",
    "text": "Perform\n✍️ HW 5 - Statistics experience\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Fall reading week. No lecture.\nMath assignment 2 due.\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "No preparation for this week",
    "crumbs": [
      "Weekly materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-13.html#prepare",
    "href": "weeks/week-13.html#prepare",
    "title": "Week 13",
    "section": "",
    "text": "No preparation for this week",
    "crumbs": [
      "Weekly materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\nSlides & Recordings\nIf you are interested in both groups’ research, it is worth attending or watching both lectures this week.",
    "crumbs": [
      "Weekly materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-13.html#practice",
    "href": "weeks/week-13.html#practice",
    "title": "Week 13",
    "section": "Practice",
    "text": "Practice\nNo exercise for this week",
    "crumbs": [
      "Weekly materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-13.html#perform",
    "href": "weeks/week-13.html#perform",
    "title": "Week 13",
    "section": "Perform",
    "text": "Perform\nIn-person lab session: text generation with transformers lab11\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Week 10 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#prepare",
    "href": "weeks/week-10.html#prepare",
    "title": "Week 10",
    "section": "",
    "text": "Week 10 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#participate",
    "href": "weeks/week-10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\nSlides & Recordings\nTransformer video: https://www.youtube.com/watch?v=XSSTuhyAmnI\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html\nhttp://peterbloem.nl/blog/transformers\nLast year’s A3 (a seq2seq autoencoder) http://modelai.gettysburg.edu/2021/headlines/",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#practice",
    "href": "weeks/week-10.html#practice",
    "title": "Week 10",
    "section": "Practice",
    "text": "Practice\nIn-class Exercise & Solutions",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#perform",
    "href": "weeks/week-10.html#perform",
    "title": "Week 10",
    "section": "Perform",
    "text": "Perform\nIn-person lab session: gradcam and input gradients lab09\nFinal Project - Deadline to form project groups on Markus, and submit a project proposal\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Week 3 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#prepare",
    "href": "weeks/week-3.html#prepare",
    "title": "Week 3",
    "section": "",
    "text": "Week 3 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\nGloVe embedding demo \nVideo on autodiff\nSlides & Recordings",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#practice",
    "href": "weeks/week-3.html#practice",
    "title": "Week 3",
    "section": "Practice",
    "text": "Practice\nIn-class Exercise & Solutions\nProf. Roger Grosse’s notes on autodiff and word embeddings\nNotes on Backpropagation https://cs231n.github.io/optimization-2/\nAutomatic Differentiation in Machine Learning: a Survey (2018) https://arxiv.org/pdf/1502.05767.pdf",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#perform",
    "href": "weeks/week-3.html#perform",
    "title": "Week 3",
    "section": "Perform",
    "text": "Perform\nIn-person lab session: numerical gradients / word embeddings lab02\nLec3 pre-recorded videos with quizzes\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Week 4 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#prepare",
    "href": "weeks/week-4.html#prepare",
    "title": "Week 4",
    "section": "",
    "text": "Week 4 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\nSlides & Recordings\nDemo: Colab Notebook",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#practice",
    "href": "weeks/week-4.html#practice",
    "title": "Week 4",
    "section": "Practice",
    "text": "Practice\nIn-class Exercise & Solutions",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\nIn-person lab session: classification and medical MNIST lab03\nLecture 4 pre-recorded videos and quizzes\nMath Assignment 1 posted\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Week 7 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#prepare",
    "href": "weeks/week-7.html#prepare",
    "title": "Week 7",
    "section": "",
    "text": "Week 7 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\nSlides & Recordings\nTroubleshooting Deep Neural Networks",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#practice",
    "href": "weeks/week-7.html#practice",
    "title": "Week 7",
    "section": "Practice",
    "text": "Practice\nIn-class Exercise & Solutions",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#perform",
    "href": "weeks/week-7.html#perform",
    "title": "Week 7",
    "section": "Perform",
    "text": "Perform\nIn-person midterm #1 which covers weeks 1-5. Held during the tutorial / lab sessions\nMath assignment 2 posted\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weekly-material.html",
    "href": "weekly-material.html",
    "title": "Weekly Materials",
    "section": "",
    "text": "Software Installations\n\n\n\nSlides & Recordings\n\n\n\nNo tutorials/labs this week but please complete the Pre-Requisite Math Problems\nSuggested Review :\n\nLinear Algebra review \nProbability review"
  },
  {
    "objectID": "weekly-material.html#week-1",
    "href": "weekly-material.html#week-1",
    "title": "Weekly Materials",
    "section": "",
    "text": "Software Installations\n\n\n\nSlides & Recordings\n\n\n\nNo tutorials/labs this week but please complete the Pre-Requisite Math Problems\nSuggested Review :\n\nLinear Algebra review \nProbability review"
  },
  {
    "objectID": "weekly-material.html#week-2",
    "href": "weekly-material.html#week-2",
    "title": "Weekly Materials",
    "section": "Week 2",
    "text": "Week 2\n\nLecture\nSlides & Recordings\nProf. Roger Grosse’s notes on backdrop\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: PyTorch basics with linear models lab01\n\n\nQuiz\nLec2 pre-recorded videos with quizzes"
  },
  {
    "objectID": "weekly-material.html#week-3",
    "href": "weekly-material.html#week-3",
    "title": "Weekly Materials",
    "section": "Week 3",
    "text": "Week 3\n\nLecture\nGloVe embedding demo \nVideo on autodiff\nSlides & Recordings\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: numerical gradients / word embeddings lab02\n\n\nQuiz\nLec3 pre-recorded videos with quizzes\n\n\nAdditional Resources\nProf. Roger Grosse’s notes on autodiff and word embeddings\nNotes on Backpropagation https://cs231n.github.io/optimization-2/\nAutomatic Differentiation in Machine Learning: a Survey (2018) https://arxiv.org/pdf/1502.05767.pdf"
  },
  {
    "objectID": "weekly-material.html#week-4",
    "href": "weekly-material.html#week-4",
    "title": "Weekly Materials",
    "section": "Week 4",
    "text": "Week 4\n\nLecture\nSlides & Recordings\nDemo: Colab Notebook\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: classification and medical MNIST lab03\n\n\nQuiz\n\n\nAssignment\nMath Assignment 1 posted"
  },
  {
    "objectID": "weekly-material.html#week-5",
    "href": "weekly-material.html#week-5",
    "title": "Weekly Materials",
    "section": "Week 5",
    "text": "Week 5\n\nLecture\nSlides & Recordings\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nTutorial: how to implement SGD with momentum neural network optimization\n\n\nQuiz\nLecture 5 pre-recorded videos and quizzes\n\n\nAdditional Resources\nTaylor Series https://www.youtube.com/watch?v=3d6DsjIBzJ4"
  },
  {
    "objectID": "weekly-material.html#week-6",
    "href": "weekly-material.html#week-6",
    "title": "Weekly Materials",
    "section": "Week 6",
    "text": "Week 6\n\nLecture\nSlides & Recordings\nEnsembling code skeleton\nThe definition of differential privacy\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: optimization and differential privacy lab05\n\n\nQuiz\nLecture 6 pre-recorded videos and quizzes\n\n\nAssignment\nMath assignment 1 due"
  },
  {
    "objectID": "weekly-material.html#week-7",
    "href": "weekly-material.html#week-7",
    "title": "Weekly Materials",
    "section": "Week 7",
    "text": "Week 7\n\nLecture\nSlides & Recordings\nTroubleshooting Deep Neural Networks\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person midterm #1 which covers weeks 1-5. Held during the tutorial / lab sessions\n\n\nAssignment\nMath assignment 2 posted"
  },
  {
    "objectID": "weekly-material.html#week-8",
    "href": "weekly-material.html#week-8",
    "title": "Weekly Materials",
    "section": "Week 8",
    "text": "Week 8\n\nLecture\nSlides\nMissing recording due to technical issue\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: transfer learning and double descent lab07\n\n\nQuiz\nLecture 8 pre-recorded videos and quizzes"
  },
  {
    "objectID": "weekly-material.html#week-9",
    "href": "weekly-material.html#week-9",
    "title": "Weekly Materials",
    "section": "Week 9",
    "text": "Week 9\nFall reading week. No lecture.\nMath assignment 2 due."
  },
  {
    "objectID": "weekly-material.html#week-10",
    "href": "weekly-material.html#week-10",
    "title": "Weekly Materials",
    "section": "Week 10",
    "text": "Week 10\n\nLecture\nSlides & Recordings\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: gradcam and input gradients lab09\n\n\nFinal Project\nDeadline to form project groups on Markus, and submit a project proposal\n\n\nAdditional Resources\nTransformer video: https://www.youtube.com/watch?v=XSSTuhyAmnI\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html\nhttp://peterbloem.nl/blog/transformers\nLast year’s A3 (a seq2seq autoencoder) http://modelai.gettysburg.edu/2021/headlines/"
  },
  {
    "objectID": "weekly-material.html#week-11",
    "href": "weekly-material.html#week-11",
    "title": "Weekly Materials",
    "section": "Week 11",
    "text": "Week 11\n\nLecture\nSlides & Recordings\nAutoencoder Notebook\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person midterm #2 which covers weeks 6-9. Held during the tutorial / lab sessions\n\n\nQuiz\nLecture 10 pre-recorded videos and quizzes\n\n\nFinal Project\nWritten feedback on project proposals sent by TAs and instructors."
  },
  {
    "objectID": "weekly-material.html#week-12",
    "href": "weekly-material.html#week-12",
    "title": "Weekly Materials",
    "section": "Week 12",
    "text": "Week 12\n\nLecture\nSlides & Recordings\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: RNN text classification lab10"
  },
  {
    "objectID": "weekly-material.html#week-13",
    "href": "weekly-material.html#week-13",
    "title": "Weekly Materials",
    "section": "Week 13",
    "text": "Week 13\n\nLecture\nSlides & Recordings\nIf you are interested in both groups’ research, it is worth attending or watching both lectures this week.\n\n\nTutorials/Labs\nIn-person lab session: text generation with transformers lab11"
  },
  {
    "objectID": "weekly-material.html#week-14",
    "href": "weekly-material.html#week-14",
    "title": "Weekly Materials",
    "section": "Week 14",
    "text": "Week 14\nFinal project submission"
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful Resources and Links",
    "section": "",
    "text": "Recommended Simulators\nYou are encouraged to use the simplest possible simulator to accomplish the task you are interested in. You can submit links to simulators not included here by opening a github issue.\n\n\n\nSimulately\nA detailed wiki comparing various widely-used robotics simulators. Read this first. The rest of this table shows simulators and environments not mentioned by Simulately.\n\n\nIsaac Lab (formerly Isaac Orbit / Isaac Gym)\nLayer of abstraction and tools to make using Isaac Sim easier.\n\n\nDrake Simulator\nA framework of simulation, analysis and control tools for robotics.\n\n\nDeepmind Control Suite\nSet of robotics environments on top of Mujoco.\n\n\nMujoco Menagerie\nHigh-quality description files and assets for robots, built on top of Mujoco.\n\n\nOpenAI Gym\nAtari, Mujoco, classic control, and third-party environments for RL.\n\n\nRoboSuite\nRobotics simulation environments on top of Mujoco. Also a benchmark.\n\n\nKlampt\nModeling, simulating, planning, and optimization for complex robots, particularly for manipulation and locomotion tasks.\n\n\nDART\nPhysics simulator for robotics and animation.\n\n\nCARLA\nSelf-driving environment and benchmarks on top of the Unreal simulation engine.\n\n\nAirSim\nRobotics simulation environments for flying and driving, built on top of Unreal engine.\n\n\ngym-pybullet-drones\nRobotics simulation environments and tools for quadrotors on top of PyBullet.\n\n\nHabitat 3.0\nSimulation of indoor scenes, humans, and robots. Good for visual navigation and social navigation tasks.\n\n\nGPUDrive\nGPU-accelerated multi-agent driving simulator.\n\n\nProcGen\nProcedurally generated simulation environments (not robotics, but useful).\n\n\nRaiSim\nRigid body physics engine. Supports biomechanics of human motion, as well as quadrupeds.\n\n\nFlightmare\nSimulation environment for flying vehicles built on top of the Unity simulation engine.\n\n\nIKEA Furniture Assembly\nIKEA furniture assembly environment.\n\n\nFurnitureBench\nSimulators, datasets, and real environments for furniture assembly\n\n\nRLBench\nSimulation environments for manipulation, built on top of the CoppeliaSim simulator.\n\n\nALFRED\nSimulation environments for visual and language-based navigation and manipulation tasks.\n\n\nMyoSuite\nMuscosceletal simulation environments for biomechanics, based on Mujoco.\n\n\nMetaWorld\nMulti-task RL environments and benchmarks.\n\n\nBimanual Manipulation Gym\nBimanual manipulation environments\n\n\n\n\n\nRecommended datasets\n\n\n\nSimulately\nA detailed wiki comparing various widely-used robotics datasets. Read this first. The rest of this table shows datasets not mentioned by Simulately.\n\n\nD4RL\nManipulation and navigation datasets for offline RL\n\n\nRoboMimic\nManipulation datasets and imitation learning algorithms\n\n\nMimicGen\nAutomatic augmentation of manipulation datasets starting from human demonstrations\n\n\nOptimus\nAutomatically generating long-horizon manipulation dataset from Task and Motion Planners.\n\n\nDROID\nManipulation dataset across various labs and robots\n\n\nD4RL\nManipulation and navigation datasets for offline RL\n\n\n\n\n\nRecommended RL, IL, trajectory optimization, and motion planning libraries\n\n\n\nSimulately\nA detailed wiki comparing various widely-used RL libaries. Read this first. The rest of this table shows libraries not mentioned by Simulately.\n\n\nRSL RL\nRL library used for training quadrupeds at the RSL lab at ETHZ. Used in Isaac Lab.\n\n\nSTORM\nMPC motion planner on the GPU\n\n\nOMPL\nOpen motion planning library\n\n\nMink\nInverse kinematics library, built on top of pink and pinocchio\n\n\nPureJaxRL\nRL library in JAX, with training and environments running fully on GPU\n\n\nCleanRL\nClean implementations of Online RL baselines\n\n\nClean Offline RL\nClean implementations of Offline RL baselines\n\n\nrliable\nMethod and library for reliable evaluation of RL algorithms\n\n\nDiffusion policy\nImplementation of diffusion policy in action space for imitation learning\n\n\nImplicit behavior cloning\nImplementation of behavior cloning with energy based models\n\n\nTheseus\nA library for differentiable nonlinear optimization in Pytorch\n\n\nModel-based RL algorithms\nList of model-based RL algorithms"
  },
  {
    "objectID": "labs/lab09.html",
    "href": "labs/lab09.html",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "",
    "text": "We have seen that convolutional neural networks (CNN) are successful in many computer vision tasks, including classification, object detection and others. However, it is not immediately clear how CNNs work, and how one can explain the predictions made by CNNs. A deeper understanding of how CNNs work can also help us identify reasons why CNNs may fail to produce correct predictions for some samples.\nA line of work started to visualize and interpret computed features of convolutional neural networks. CAM and Grad-CAM are two influential and fundamental works to find which parts of the input have the most impact on the final output of the models by analyzing the model’s extracted feature maps.\nBy the end of this lab, you will be able to:\nAcknowledgements: 2. We have borrowed some codes from CAM Official Repo. 3. We have borrowd texts, figures and formulas from main papers of CAM and Grad-CAM."
  },
  {
    "objectID": "labs/lab09.html#submission",
    "href": "labs/lab09.html#submission",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab09.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your implementation of the predict function (1 point)\nPart 1. Your implementation of the get_resnet_features function (1 point)\nPart 1. Your implementation of the compute_cam function (3 point)\nPart 1. Your interpretation of the grad cam outputs (2 point)\nPart 2. Your implementation of the compute_gradcam function (3 point)"
  },
  {
    "objectID": "labs/lab09.html#part-1.-class-activation-maps-cam",
    "href": "labs/lab09.html#part-1.-class-activation-maps-cam",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "Part 1. Class Activation Maps (CAM)",
    "text": "Part 1. Class Activation Maps (CAM)\nCAM and its extension Grad-CAM takes the approach of identifying the regions of the an image that contributes most to the model’s prediction. This information can be visualized as a heat map, and provides a way to interpret a model’s prediction: did the model predict that the image is that of a “boat” because of the shape of the ears, or because of the water in the background?\nWe discussed, during lecture, that convolutional layers preserve the geometry of the image, and that these convolutional layers actually behave as feature/ object detectors of various complexity. Since the geometry of the output of a CNN layer corresponds to the geometry of input image, it is straightforward to locate the region of the image that corresponds to a particularly high activation value. This is because the computations that we use in a CNN (convolutions, max pooling, activations) are all geometry preserving (equivariant).\nHowever, fully-connected layers are typically used for classification in the final layers of a CNN. These fully-connected layers are not geometry preserving, thus information about the locations of discriminating features are lost when fully-connected layers are used for classification.\nThe idea behind CAM is to avoid using these fully-connected layers for classification, so that we can reconstruct location information in a straightforward way. Instead of fully-connected layers, we use:\n\nA global average pooling (GAP) layer. This layer will take as input the output of a CNN layer (e.g., of shape H x W x C) and perform an average operation for each channel along the entire activation height/width (producing an output vector of shape C).\nA single linear layer to map this vector (of length C) into the output space.\n\nSince both the pooling and linear layers have straightforward computation, it is possible to assign credit for a output score for a class back to specific activation values of the CNN output.\nThe framework of the Class Activation Mapping is as below (from https://github.com/zhoubolei/CAM):\n\nIn this part of the lab, we will implement CAM to produce a heatmap of the contribution to locations in the image to a predicted class. We will use the pre-trained convolutional neural network ResNet, chosen because this model’s architecture uses global average pooling (GAP). ResNet is trained on the ImageNet data set.\n\nfrom torchvision import models, transforms\nimport torch.nn.functional as F\nimport torch\nimport json\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Download resnet pretrained weights\nresnet = models.resnet18(pretrained=True)\n\n# CAM can only be used when the models are in \"evaluation phase\".\nresnet.eval()\n\n# Print model architecture\n# We will use the CNN activations computed after layer 4, and before GAP.\nresnet\n\nThe ImageNet labels are a bit challenging to read. We will download a list of human-readable labels from here:\n\n!wget https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\n\n\n# Load the imagenet category list\nwith open('imagenet-simple-labels.json') as f:\n    classes = json.load(f)\n\nTo remind ourselves of how ResNet works, let’s predict what class these images belongs to:\n \n\n!wget https://www.cs.toronto.edu/~lczhang/413/cat.jpg\n!wget https://www.cs.toronto.edu/~lczhang/413/boat.jpg\n\n\nfrom PIL import Image\n\ndef process_input(image_file):\n    # open the image\n    img = Image.open(image_file)\n\n    # transform the images by resizing and normalizing\n    preprocess = transforms.Compose([\n       transforms.Resize((224,224)),\n       transforms.ToTensor(),\n       transforms.Normalize(\n          mean=[0.485, 0.456, 0.406],\n          std=[0.229, 0.224, 0.225])])\n\n    return preprocess(img).unsqueeze(0)\n\nGraded Task: Write a function that takes a model and an image file and produces a list of the top 5 predictions with the corresponding probability score.\n\ndef predict(model, image_file):\n    \"\"\"\n    Return the top 5 class predictions with the corresponding probability score.\n\n    Parameters:\n        `model`      - nn.Module\n        `image_file` - file path to the image\n\n    Returns: a list of 5 (string, int, float) pairs: \n        the string is the predicted ResNet class name (see classes above),\n        the int is the predicted ResNet class id,\n        and the float is the prediction probability. The list should be ordered\n        so that the highest probabilty score appears first.\n\n    Example:\n        &gt;&gt;&gt; predict(resnet, 'cat.jpg')\n        [('prison', 743, 0.23517875373363495),\n         ('shopping cart', 791, 0.07393667101860046),\n         ('rocking chair', 765, 0.06884343922138214),\n         ('wheelbarrow', 428, 0.06603048741817474),\n         ('ring-tailed lemur', 383, 0.0434008426964283)]\n    \"\"\"\n    x = process_input(image_file)\n\n    result = None # TODO\n\n    return result\n\nPlease include the output of the below cell in your submission.\n\npredict(resnet, 'cat.jpg')\n\nNow that we can use ResNet to make predictions, we need two additional pieces of information for CAM.\nFirst, given an image, we need to be able to compute the features/activations of the last convolutional layer. This feature map is the input to the GAP layer. Although this information is computed in a forward pass, we will need to write some code to extract this information.\nSecond, we will need the weights of the final fully-connected layer in ResNet.\nGraded Task: Complete the following function that takes an image file and produces the weights of the finally fully-connected layer in Resnet. You may find the named_children() method of resnet helpful, as it produces a sequence of (named) layers. We would like the feature map directly before the global average pooling layer.\n\nfor (name, model) in resnet.named_children():\n    print(name)\n\n\ndef get_resnet_features(image_file):\n    \"\"\"\n    Return the final CNN layer (layer4) feature map in resnet\n\n    Parameters:\n        `image_file` - file path to the image\n\n    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n    \"\"\"\n\n    x = process_input(image_file)\n\n    result = None # TODO\n    for (name, model) in resnet.named_children():\n        # TODO -- update result\n        if name == 'layer4':\n            break\n    return result\n\n\nfets = get_resnet_features('cat.jpg')\nprint(fets.shape) # should be [1, 512, 7, 7]\n\nTask: Assign the variable fc_weight to the weights of the final fully-connected layer in resnet.\n\nweights = None # TODO\nprint(weights.shape) # should be [1000, 512]\n\nGraded Task: Complete the function compute_cam, which takes the CNN feature map (from the get_resnet_features function), the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the following approach.\nWe will use the notation \\({\\bf X}\\) to denote the CNN feature map (the input to the GAP), with \\(X_{i,j,c}\\) being the activation at location \\((i, j)\\) and channel \\(c\\). Here, \\({\\bf X}\\) is a tensor with shape \\(H \\times W \\times C\\), where \\(H \\times W\\) is the height and width of the feature map and \\(C\\) is the number of channels. We will use the vector \\({\\bf h}\\) to denote the output of the GAP, so that \\(h_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{i,j,c}\\). Finally, we will use \\({\\bf W}\\) to denote the finally fully connected layer weights, and \\({\\bf z}\\) to the denote the prediction score, so that \\({\\bf z} = {\\bf W}{\\bf h}\\).\nNow, we would like to relate the features \\(X_{i,j,c}\\) to the scores \\(z_k\\), so that we can compute the contribution of the features at position \\((i,j)\\) to the score \\(k\\).\nFor an output class \\(k\\), we have:\n\\[z_k  = \\sum_{c=1}^{C} w_{k,c} h_c\\]\nSubstituing \\(h_c\\) for its definition, we have:\n\\[z_k  = \\sum_{c=1}^{C} w_{k,c} \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W X_{i,j,c}\\]\nRearranging the sums, we have:\n\\[z_k  = \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W \\sum_{c=1}^C w_{k,c} X_{i,j,c}\\]\nThe inner term \\(\\sum_c w_{k,c} X_{i,j,c}\\) is exactly what we are looking for: this term indicates how much the value in location \\((i, j)\\) of the feature map \\({\\bf X}\\) attributes to class \\(k\\).\n\ndef compute_cam(features, label):\n    \"\"\"\n    Computes the contribution of each location in `features` towards \n    `label` using CAM.\n\n    Parameters:\n        `features`: PyTorch Tensor of shape [1, 512, 7, 7] representing\n                    final layer feature map in ResNet (e.g., from calling\n                    `get_resnet_features`)\n        `label`   : resnet label, integer between 0-999\n\n    Returns: PyTorch Tensor of shape [7, 7]\n    \"\"\"\n    features = features.squeeze(0) # remove the first dimension\n    result = None # TODO \n    return result\n\nTask: Run the below code, which superimposes the result of the compute_cam operation on the image.\n\ndef visualize_cam(image_file, label):\n    # open the image\n    img = Image.open(image_file)\n\n    # compute CAM features\n    fets = get_resnet_features('cat.jpg')\n    m = compute_cam(fets, label)\n\n    # normalize \"m\"\n    m = m - m.min()\n    m = m / m.max()\n    # convert \"m\" into pixel intensities\n    m = np.uint8(255 * m.detach().numpy())\n    # apply a color map\n    m = cv2.resize(m, img.size)\n    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n\n    plt.figure(figsize=(6, 6))\n    plt.title(\"%s %s\" % (image_file, classes[label]))\n    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n    plt.show()\n\n\nvisualize_cam('cat.jpg', 743)\nvisualize_cam('cat.jpg', 383)\n\nGraded Task Compare the above two outputs, and explain what conclusion you may be able to draw about the contribution of the pixel locations to those two classes. Why do you think the model misclassified the image?\n\n# TODO: your explanation goes here"
  },
  {
    "objectID": "labs/lab09.html#part-2.-grad-cam",
    "href": "labs/lab09.html#part-2.-grad-cam",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "Part 2. Grad-CAM",
    "text": "Part 2. Grad-CAM\nAlthough CAM was an important step toward understanding convolutional neural networks, the technique is only applicable to convolutional networks with GAP and a single fully-connected layer. Recall that it leveraged the following relationship between the output of the GAP layer \\({\\bf h}\\) and the score for the output class \\(z_k\\):\n\\[z_k  = \\sum_c w_{k,c} h_c\\]\nWhere \\(w_{k,c}\\) is the fully connected layer weight that describes the strength of the connection betwee \\(h_c\\) and \\(z_k\\). In other words, \\(w_{k,c}\\) describes the following gradient:\n\\[\\frac{\\partial z_k}{\\partial h_c}\\]\nWith this in mind, you may be able to see how CAM may be generalized so that \\({\\bf z}\\) may be a more complex function of \\({\\bf h}\\)—e.g., a MLP or even an RNN!\nGradient-weighted Class Activation Mapping (Grad-CAM) is a generalized form of CAM, and can be applied to any convolutional neural network. In Grad-CAM, we use the gradient \\(\\frac{\\partial z_k}{\\partial h_c}\\) in place of \\(w_{k,c}\\) when attributing class scores to locations \\((i, j)\\). In other words, the below term indicates how much the value in location \\((i, j)\\) of the feature map \\({\\bf X}\\) attributes to class \\(k\\).\n\\[ReLU(\\sum_c \\frac{\\partial z_k}{\\partial h_c} X_{i,j,c})\\]\nThe addition of the ReLU activation only allows positive contributions to be visualized.\nSidenote: To generalize this result even further, we can replace \\(z_k\\) with any target we would like! Grad-CAM has been used on neural networks that performs image caption generation: a model with a CNN encoder and an RNN decoder. We can use use the gradients of any target concept (say “dog” in a classification network or a sequence of words in a captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Taking a look at this video helps you to understand the power of Grad-CAM.\nLet’s explore GradCAM with the VGG network:\n\nvgg19 = models.vgg19(pretrained=True)\nvgg19.eval()\nvgg19\n\n\npredict(vgg19, 'cat.jpg')\n\nTask Just like with CAM, we will need to extract the feature map obtained from the last convolutional layer. This step is actually very straightforward with VGG since vgg19 splits the network into a features network and a classifier network.\n\ndef get_vgg_features(image_file):\n    \"\"\"\n    Return the output of `vgg19.features` network for the image\n\n    Parameters:\n        `image_file` - file path to the image\n\n    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n    \"\"\"\n\n    x = process_input(image_file)\n    result = None # TODO\n    return result\n\n\nget_vgg_features('cat.jpg').shape\n\nTask: Read the forward method of the VGG model here. https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py What other steps are remaining in the forward pass?\n\n# TODO: Explain the remaining steps here\n\nGraded Task: Complete the function compute_gradcam, which takes an image file path, the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the GradCAM approach described at the beginning of Part 2.\n\ndef compute_gradcam(image_file, label):\n    \"\"\"\n    Computes the contribution of each location in `features` towards \n    `label` using GradCAM.\n\n    Parameters:\n        `image_file` - file path to the image\n        `label`   : resnet label, integer between 0-999\n\n    Returns: PyTorch Tensor of shape [7, 7]\n    \"\"\"\n    # obtain the image input features\n    x = process_input(image_file)\n\n    # obtain the output of the features network in the CNN\n    fets = vgg19.features(x)\n\n    # tell PyTorch to compute the gradients with respect\n    # to \"fets\"\n    fets.retain_grad()\n\n    # TODO: compute the rest of the vgg19 forward pass from `fets`\n    out = None # should be the output of the classifier network\n\n    z_k = out.squeeze(0)[label] # identify the target output class\n    z_k.backward()              # backpropagation to compute gradients\n\n    features_grad = fets.grad   # identify the gradient of z_k with respect to fets\n\n    # account for the pooling operation, so that \"pooled_grad\"\n    # aligns with the notation used\n    n, c, h, w = features_grad.shape\n    features_grad = torch.reshape(features_grad, (c, h*w))\n    pooled_grad = features_grad.sum(dim=1)\n\n    # rearrange \"fets\" so that \"X\" aligns with the notation\n    # used above\n    X = fets.squeeze(0).permute((1, 2, 0))\n\n    # TODO: Compute the heatmap using the gradcam\n    m = None\n    m = F.relu(m) # apply the ReLU operation\n    return m\n\nTask: Run the below code, which superimposes the result of the compute_gradcam operation on the image.\n\ndef visualize_gradcam(image_file, label):\n    # open the image\n    img = Image.open(image_file)\n\n    # compute CAM features\n    m = compute_gradcam(image_file, label)\n\n    # normalize \"m\"\n    m = m - m.min()\n    m = m / m.max()\n    # convert \"m\" into pixel intensities\n    m = np.uint8(255 * m.detach().numpy())\n    # apply a color map\n    m = cv2.resize(m, img.size)\n    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n\n    plt.figure(figsize=(6, 6))\n    plt.title(\"%s %s\" % (image_file, classes[label]))\n    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n    plt.show()\n\n\nvisualize_cam('cat.jpg', 743)\n\n\nvisualize_cam('cat.jpg', 383)\n\n\nvisualize_cam('boat.jpg', 536)"
  },
  {
    "objectID": "labs/lab09.html#just-for-fun",
    "href": "labs/lab09.html#just-for-fun",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "Just For Fun",
    "text": "Just For Fun\nAs you might have seen in the video, Grad-CAM can be applied to text-generating models. For example, in image-captioning tasks, a text is generated describing the given image. Some methods first feed the image to a convolutional neural network to extract features, and then feed the extracted features to an RNN, to generate the text. Neuraltalk2 was one of the earliest models using this approach. Similar to the classification task, it is enough to compute the gradient of the score (what is the score in an image-captioning task?) with respect to the last convolutional layer.\nIf you are interested in how neuraltalk2 functions you can check this project. Moreover, if you are looking for more hands-on experience, this repo has implemented many image-captioning methods, and you can easily apply Grad-CAM on them (especially show and tell).\nHint: There is a file which re-implements the forward pipeline of ResNet101, where you can store the features."
  },
  {
    "objectID": "labs/lab11.html",
    "href": "labs/lab11.html",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "",
    "text": "In this lab, we will build a generative transformer to generate new lines that emulates the TV show Friends. In order to do so, we will leverage Andrej Karpathy’s implementation of GPT2 called the nanoGPT. This particular implementation uses a small number of components and focuses on the essential ideas behind the GPT2 model.\nInstead of training a GPT model from scratch, we will fine-tune a pre-trained model. This reduces training time necessary to achieve a reasonable measure of performance.\nBy the end of this lab, you will be able to:\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab11.html#submission",
    "href": "labs/lab11.html#submission",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab11.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your code to generate the list uts. (1 point)\nPart 1. Your explanation for why the tokenization method needs to be consistent. (1 point)\nPart 1. Your explanation of why the target tensor is an “offset” of the input tensor. (1 point)\nPart 2. Your explanation of the relationship between lm_head and wte. (1 point)\nPart 2. Your result for the shape of x in the GPT.forward() method. (1 point)\nPart 2. Your computation of the shape of (q @ k.transpose(-2, -1)) in the causal self attention module (1 point)\nPart 2. Your explantion of why masking makes sense intuitively for causal self attention. (1 point)\nPart 2. Your computation of the number of parameters in a GPT2 model. (3 points)"
  },
  {
    "objectID": "labs/lab11.html#part-1.-data",
    "href": "labs/lab11.html#part-1.-data",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nThe “Friends Corpus” can be downloaded through the ConvKit package on Python. You can read more about the data here.\nLet’s install this package, and explore the data set.\n\n%pip install convokit\n\n\nimport convokit\n\n\ncorpus = convokit.Corpus(convokit.download('friends-corpus'))\n\nTask: Run the code below, which iterates through the first 10 utterances of the show. How is each utterance formatted? What do the speaker and text field mean?\n\nfor i, utterance in enumerate(corpus.iter_utterances()):\n    print(utterance)\n    if (i &gt;= 10): \n        break\n\nGraded Task: Create a list of strings called uts that contains all utterances made by your favourite (of the 6) main character of the show.\n\ncharacter = 'Monica Geller' # OR 'Chandler Bing' OR 'Phoebe Buffay' OR ...\n\nuts = []\n\nfor utterance in corpus.iter_utterances():\n    pass # TODO\n\nPlease include the output of this next line in your solution\n\nprint(len(uts)) \n\nTask: Run the below code. This code combines these lines into a large string for training, and a large string for validation. We will index ranges in this large string use in a minibatch—i.e. a minibatch of data will consist of a substring in this large string. This substring may start in the middle of an utterance and may contain multiple utterances—and that turns out to be okay! Our neural network still manages to learn what an utterance looks like and emulate it.\nSince this approach simpler to implement than the batching approach seen in the previous lab, it is more often seen\nWe will use 90% of the data for training, and 10% of the data for validation.\n\ntrain_split = 0.9\nn = len(uts)\n\ntrain_data_str = '\\n'.join(uts[:int(n*train_split)])\nval_data_str = '\\n'.join(uts[int(n*train_split):])\n\nTask: Notice that we split the utterances so that the earlier utterances are in the training set, and the later utterances (i.e., later in the TV show) is in the validation set. Why is this method preferable to randomly splitting the utterances into training and validation?\n\n# Include your explanation here\n\nTask: Why do we not set aside a test set?\n\n# Include your explanation here\n\nJust like in the previous lab, we will tokenize our text. Modern models use a tokenization strategy called Byte Pair Encoding, which tokenize text into common into sub-word tokens. Sub-word tokens split words into commonly occuring (and thus meaningful) parts. For example, the word “utterance” could be split into “utter” and “ance”. The model would learn about these constinuent tokens in different contexts, helping the model generalize better.\nWe will use the Byte Pair Encoding (BPE) tokenizer from the tiktoken library. Let’s install and import this library.\n\n%pip install tiktoken\n\n\nimport tiktoken\n\nGraded Task: We will be fine-tuning a pre-trained GPT2 model. Explain why it is important for us to use the same tokenization method as is used in the original GPT2 model whose weights we will be using.\n\n# Your explanation goes here\n\nNow, let’s retireve the original GPT2 model.\n\nenc = tiktoken.get_encoding(\"gpt2\")\n\ntrain_ids = enc.encode_ordinary(train_data_str)\nval_ids = enc.encode_ordinary(val_data_str)\n\nTask: How many tokens are in the training and validation sets? How does this compare with the number of words in each data set (computed using the str.split() method)? What about the number of character?\n\n# TODO\n\nTask: Run the below code, which will save the above numpy arrays in a file. This way, we can use np.memmap function, which creates a memory-map to an array stored in a binary file on disk. This approach is useful for accessing segments of a large file on disk, which we will be doing.\n\nimport numpy as np\nimport os\n\ndata_dir = 'friends_gpt2'\nos.makedirs(data_dir, exist_ok=True)\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile(os.path.join(data_dir, 'train.bin'))\nval_ids.tofile(os.path.join(data_dir, 'val.bin'))\n\n\n# create a memory map\ntrain_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\nval_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n\nTask: Use the get_batch function below to extract a sample input/output from this data set. Here, we will be using the approach shown in the generative RNN lecture, where the model generates the next token given the previous context.\n\nimport torch\n\ndef get_batch(data, block_size, batch_size, device):\n    \"\"\"\n    Return a minibatch of data. This function is not deterministic.\n    Calling this function multiple times will result in multiple different\n    return values.\n\n    Parameters:\n        `data` - a numpy array (e.g., created via a call to np.memmap)\n        `block_size` - the length of each sequence\n        `batch_size` - the number of sequences in the batch\n        `device` - the device to place the returned PyTorch tensor\n\n    Returns: A tuple of PyTorch tensors (x, t), where\n        `x` - represents the input tokens, with shape (batch_size, block_size)\n        `y` - represents the target output tokens, with shape (batch_size, block_size)\n    \"\"\"\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    t = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if 'cuda' in device:\n        # pin arrays x,t, which allows us to move them to GPU asynchronously\n        #  (non_blocking=True)\n        x, t = x.pin_memory().to(device, non_blocking=True), t.pin_memory().to(device, non_blocking=True)\n    else:\n        x, t = x.to(device), t.to(device)\n    return x, t\n\n\n# TODO: get and print a single batch from the training set\ndevice = 'cuda' if torch.cuda.is_available()  else 'cpu'\n\nGraded Task: Once again, we will be using the approach shown in the generative RNN lecture, where the model’s goal is to generate the next token given the previous context. With that in mind, explain why the target output tokens is very similar to the input tokens, just offset by 1 along the block_size dimension.\n\n# TODO: Your explanation goes here."
  },
  {
    "objectID": "labs/lab11.html#part-2.-model",
    "href": "labs/lab11.html#part-2.-model",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "Part 2. Model",
    "text": "Part 2. Model\nNow that we have our data set in mind, it is time to set up our GPT2 model. We will use the code provided in the nanoGPT repository, slightly modified here for succinctness. Thus, we will not re-implement the GPT2 model. Instead, let’s use the nanoGPT implementation to understand, step-by-step, what happens in a GPT model.\nLISATODO add a blurb about GPT2 model history, paper, and changes since GPT1.\nWe will explore the components of the GPT2 model first in a top-down manner, to get an intuition as to how the pieces connect. Then, we will explore the same components in a bottom-up manner, so that we can fully understand the role of each component.\n\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport inspect\n\nWe begin with the GPTConfig class, which contains model architecture settings for our GPT2 model. The settings specify:\n\nblock_size: the input sequence length. Shorter sequences can be padded (with a padding token as seen in the previous lab), and longer sequences must be cut shorter. During training, we will generate batches with sequences that are exactly the block size\nvocab_size: the number of unique tokens in our vocabulary. This affects the size of the initial embedding layer.\nn_layer: the number of transformer layers.\nn_head: the number of attention heads to use in the causal self-attention layer.\nn_embd: the embedding size used throughout the model. You can think of each token position as being represented as a vector of length n_embd.\ndropout: for dropout.\nbias: whether to use a bias parameter in certain layers.\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\nTask: Which of these settings do you think would affect the total number of trainable parameters in a GPT model? Which of them do you think have the largest impact on the number of trainable parameters? Please write down your guess before continuing, and we will check back here later.\n\n# TODO: Write down your thoughts here.\n\nWith the setting in mind, we can set up a GPT model. A GPT model will take a GPTConfig object as a parameter. Pay particular attention to the __init__() and forward() methods. These are the methods that we will study in more detail.\nThe code uses a more PyTorch features that we have not discussed, but these features are mostly cosmetic and do not provide significantly different functionality: the use of nn.ModuleDict allows us to access modules in the GPT class in a straightforward way, and nn.ModuleList allows us to create a list of modules. We have not yet defined the PyTorch neural network modules Block and LayerNorm, but we will do so soon.\nIf you see a PyTorch feature used that you don’t understand, you can always look it up in the PyTorch documentation. However, you don’t try to understand everything at one go. It is normal to read code in multiple “passes”, and focus on the big picture in the first pass.\nTask: Begin with a first pass read of the __init__() and forward() methods of the GPT module.\n\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t &lt;= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n\n        return logits, loss\n\n    def crop_block_size(self, block_size):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert block_size &lt;= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            if hasattr(block.attn, 'bias'):\n                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        override_args = override_args or {} # default to empty dict\n        # only dropout can be overridden see more notes below\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        config_args['bias'] = True # always True for GPT model checkpoints\n        # we can override the dropout rate, if desired\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        # start with all of the candidate parameters\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        # filter out those that do not require grad\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() &gt;= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() &lt; 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt) # per second\n        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) &lt;= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits &lt; v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n\nDoing a first-pass read on both the __init__() and forward() methods, you should see that the GPT model has the following components (ignoring dropout):\n\ntransformer.wte, which is an embedding layer that maps tokens to a vector embedding.\ntransformer.wtp, is also an embedding layer, but this one maps token position indices to a vector embedding. This index is required to inject position information into the embedding—otherwise transformer computation would be invariant to the reordering of input tokens (i.e., the computation would not change if the order of the input tokens change). Since the length of a sequence is at most block_size, so there are at most block_size indices to embed.\nA sequence of Blocks — to be defined. The output from the previous block is taken as the input of the next block.\nA final LayerNorm layer after the last block. This layer is also yet to be defined, and the name suggests that this is a normalization layer (similar to batch normalization) that does not change the shape of the features (i.e., the output shape is the same as the input shape).\nlm_head, which is a linear layer that maps embeddings back to a distribution over the possible otuput tokens.\n\nTask: Compute the number of parameters in the wte embedding layer of the GPT2 model. (For these and other questions that specifically mention GPT2 model, please use the config settings above and provide an actual numbers.)\n\n# TODO: Perform this computation by hand.\n\nTask: Compute the number of parameters in the wtp embedding layer of the GPT2 model.\n\n# TODO: Perform this computation by hand.\n\nGraded Task: Explain why the linear layer lm_head has the same number of parameters as the embedding layer wte. Provide an intuitive explanation for why weight tying—i.e., using the same set of weights for both layers, just transposed—would be reasonable. The weight tying is done to reduce the total number of parameters in the GPT2 model.\n\n# TODO: Include your explanations here\n\nTask: Explain why it is that in the forward() method, the tensor tok_emb has the shape (b, t, n_embd), where b is the batch size, t is the sequence length (max block_size), and n_embd is the embedding size.\n\n# TODO: Include your explanations here\n\nTask: Notice that in the forward() method, the tensor pos_emb has the shape (t, n_embd). In other words, we embed the position only once for each batch, and then rely on PyTorch tensor broadcasting to perform the addition tok_emb + pos_emb. Why is this ok?\n\n# TODO: Include your explanations here\n\nTask: What is the shape of tok_emb + pos_emb in the forward() method in a GPT2 model? This question is not trivial because the two addend tensors are not of the same shape. Thus, the addition uses broadcasting. PyTorch broadcasting works similarly to that of Numpy’s. You can look up “PyTorch broadcasting” to find resources related to how broadcasting works.\n\n# TODO: Perform this computation by hand.\n\nGraded Task: What is the shape of x in the forward() method? This is an important shape to remember, since it is the shape of the feature map consistent in most of the transformer network.\n\n# TODO: Perform this computation by hand.\n\nTask: What is the shape of logits in the forward() method?\n\n# TODO: Perform this computation by hand.\n\nThese questions above should give you a clear idea of the main components of the transformer model, the expected input and output tensor shapes, and the shapes of intermediate tensors. With this in mind, let’s explore the two modules referenced by GPT.\nWe’ll start with the simple one. The LayerNorm layer is intended to be similar to PyTorch’s LayerNorm layer.\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nTask: How many parameters are in a LayerNorm layer?\n\n# TODO: Perform this computation by hand.\n\nTask: Read the description of the LayerNorm layer in PyTorch at https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm. Then, explain how layer normalization differs from batch normalization.\n\n# TODO: Perform this computation by hand.\n\nLet’s move on to the Block module. Recall that here are several Block modules in a GPT model, and the output of one module is the input of the next.\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\nThis module is actually quite succinct, but it also refers to modules that are yet to be defined. It consists of:\n\nA layer normalization layer.\nA causal self attention layer (to be defined). This is the heart of the GPT model.\nAnother layer normalization layer.\nAn MLP layer (to be defined).\n\nTask: Judging by the Block.forward() method above, why must the CausalSelfAttention and the MLP layers preserve the shape of the features?\n\n# TODO: Include your explanation here.\n\nTask: How might the skip-connections in the Block.forward() method help with gradient flow? An intuitive explanation is sufficient here.\n\n# TODO: Include your explanation here.\n\nWith the GPT2 Block in mind, we will define the MLP module next.\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nImmediately, we see that this MLP consists of two linear layers. The activation function used between these two layers is the Gaussian Error Linear Units function. You can read more about it in this paper.\nTask: Compute the number of parameters in a MLP layer in a GPT2 model.\n\n# TODO: Perform this computation by hand\n\nTask: Recall that the input of the MLP layer is a tensor with the usual dimension computed earlier. What is the shape of self.c_fc(x) in the MLP.forward() method? What about the shape of the return value in this method?\n\n# TODO: Perform this computation by hand\n\nTask: Explain why this MLP layer is also called the “pointwise feed forward” layer. (Hint: a “point” here refers to a single token or position in the input sequence)\n\n# TODO: Include your explanation here.\n\nFinally, let’s study the definition of the CausalSelfAttention layer. This is the heart of the GPT model and is also the most complex module.\nTask: Begin with a first pass read of the __init__() and forward() methods of CausalSelfAttention module. We will then trace through the case where self.flash is False, since the code provides more detailed explanation for the computation steps.\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nTask: Compute the number of parameters in the c_attn layer in a GPT2 model.\n\n# TODO: Perform this computation by hand\n\nTask: Like the comment in the __init__() method suggests, we can think of the c_attn layer as a combination of three nn.Linear(config.n_embd, config.n_embd, bias=config.bias) modules. These three networks projects the input embedding into three parts: q (for query), k (for key), and v (for *value).\nWhat is the shape of self.c_attn(x) in the forward() method? Use this answer to show that self.c_attn(x).split(self.n_embd, dim=2) gives us the same q, k, v values had we used three separate networks.\n\n# TODO: Perform the computation by hand, then include your explaination.\n\nTask: Explain why config.n_head must be a factor of config.n_embd.\n\n# TODO: Your explanation goes here.\n\nWe will explore the manual implementation of attention in the next few questions. For this part, it helps to first consider the case where the batch size B=1, and n_head=1. For a larger batch size and number of heads, the attention computation is repeated for every sequence in the batch and every attention head. Thus, the shapes of the three important tensors are:\n\nq: (1, 1, T, n_embd)\nk: (1, 1, T, n_embd)\nv: (1, 1, T, n_embd)\n\nLike discussed in the lectures, you can think of the attention mechanism as a “soft” dictionary lookup. Instead of obtaining a single key/values for a given query, attention gives us a probability distribution over the possible keys/values. We can then use this probability distribution to obtain a weighted sum (akin to an expected value) of the lookup value. Moreover, instead of having strings, numbers, or other objects as keys/values, a key is a vector (of shape n_embd), and a value is also a vector (of shape n_embd). This is consistent with what we have seen in neural networks—everything is represented using a vector! The tensors k and v contains these keys and values, and there is one vector at every token position. The tensor q contains the queries— analogues to the item (a possible key) that we are searching for in a regular dictionary lookup. There is also one query vector for each token position: for each token position, we want to look up a corresponding (weighted sum of) values that contains information pertinent to understanding the meaning of the token in this position.\nWith that in mind, let’s go through the mathematical computations.\nGraded Task: What is the shape of (q @ k.transpose(-2, -1))? For this and the following questions, assume that q, k, v have the shape above, where we have assumed that batch size and num heads are both 1.\n\n# TODO: Perform this computation by hand\n\nTask: What is the value of math.sqrt(k.size(-1))?\n\n# TODO: Perform this computation by hand\n\nTask: Argue that the line att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) is computing a “distance” or “similarity” metric between the query at each token position and the key at each token position.\n\n# TODO: Your explanation goes here\n\nThe following line of code references a self.bias parameter, which is defined in the last line of the __init__() method. Since block_size is quite large, we can understand what self.bias looks like by running a similar piece of code below with a smaller block_size value.\n\nbias = torch.tril(torch.ones(5, 5)).view(1, 1, 5, 5)\nbias\n\nTask: Explain what the above code returns. Explain how PyTorch broadcasting may be useful for computations involving this tensor—i.e., why is it okay that the first two dimensions of this tensor are 1, thus assuming that batch size = 1 and num heads = 1?\n\n# TODO: Your explanation goes here\n\nTask: We will use a similar technique of running a modified version of the next two lines of code in the forward() method to better understand what it does. Run the below code, and explain what the masked_fill function does.\n\nattn = torch.rand(1, 1, 5, 5)\nattn\n\n\nmasked = attn.masked_fill(bias[:,:,:5, :5] == 0, float('-inf'))\nmasked\n\n\nout = F.softmax(masked, dim=-1)\nout\n\n\n# Your explanation goes here\n\nGraded Task: This masking is in place so that query tokens cannot “look up” key/values that at a position with a larger index. Explain why this limitation means our GPT model cannot use information in subsequent/later tokens to form an understand of what what is in the current token. (Note: this masking is the “Causal” part of Causal Self-Attention!)\n\n# Your explanation goes here\n\nTask: Your answer above explains which positions in the out tensor need to be set to zero. Explain why setting the corresponding value of pre-softmax tensor masked to -inf is necessary. Why can’t we set the value of masked to 0 in these positions?\n\n# Your argument goes here\n\nTask: Argue that out[0,0,0,0] must always be 1.\n\n# Your argument goes here\n\nTask: Now, out in our example is akin to the final value of att in the CausalSelfAttention.forward() method. Explain why the operation y = att @ v computes a weighted sum of values at each token position, where the weights are defined by att.\n\n# Your explanation goes here\n\nTask: The above explanation pertain to a single attention head. Explain why using multiple attention heads allows a token position to consider information from various other positions. Alternatively, explain why using multiple heads might help the network learn different ways in which the meaning at one token could depend on other tokens.\n\n# Your explanation goes here\n\nGraded Task: Compute the total number of parameters in a GPT2 model by computing the following. Please use actual numbers in each case, assuming the GPT2 configuration from above.\n\nThe number of parameters in a CausalSelfAttention model.\nThe number of parameters in a MLP module.\nThe number of parameters in a Block module.\nThe number of parameters in all Block modules in a GPT2 model.\nThe number of parameters in the wte embedding layer in a GPT2 model.\nThe total number of parameters in a GPT2 model.\n\nPlease perform the computation either by hand (and show your work), or with a function that clearly shows the computations.\nYou should see that approximately 30% of the GPT2 weight comes from the wte embedding layer. This is why weight tying is used in the GPT module!\n\n# TODO: Your work goes here"
  },
  {
    "objectID": "labs/lab11.html#part-3.-training",
    "href": "labs/lab11.html#part-3.-training",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "Part 3. Training",
    "text": "Part 3. Training\nWe are ready to finetune our GPT2 model on the “Friends” data set! There is no graded task in this section since training this model can take some time to achieve reasonable performance.\nTo run this part of the lab, you will need to use a GPU. On Google Colab, you can select a session with a GPU by navigating to the “Runtime” menu, selecting “Change runtime type”, and then selecting the “T4 GPU” option.\nWe will set up a config object to make it easier to store and use configs.\n\nimport easydict\nimport math\nimport time\n\nfinetune_config_dict = {\n  'gradient_accumulation_steps': 32,\n  'block_size': 256,\n  'dropout': 0.2,\n  'bias': False,\n  'learning_rate': 3e-5,\n  'weight_decay': 0.1,\n  'beta1': 0.9,\n  'beta2': 0.99,\n  'grad_clip': 1.0,\n  'decay_lr': False,\n  'warmup_iters': 100,\n  'lr_decay_iters': 5000,\n  'min_lr': 0.0001}\nconfig = easydict.EasyDict(finetune_config_dict)\n\nFirst, we need to load the GPT2 weights.\n\n# initialize from OpenAI GPT-2 weights\noverride_args = dict(dropout=config.dropout)\nmodel = GPT.from_pretrained('gpt2', override_args)\n\n# crop down the model block size using model surgery\nif config.block_size &lt; model.config.block_size:\n    model.crop_block_size(config.block_size)\n\ndevice = 'cuda' if torch.cuda.is_available()  else 'cpu'\nmodel.to(device)\n\nTask: Explain why reducing the block_size do not significantly reduce the number of parameters, but does significantly reduce memory usage.\n\n# TODO: Include your explanation here\n\nThere are some additional helpers to improve training.\n\n# initialize a GradScaler. If enabled=False scaler is a no-op\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# learning rate decay scheduler (cosine with warmup)\ndef get_lr(config, it):\n    # 1) linear warmup for warmup_iters steps\n    if it &lt; config.warmup_iters:\n        return config.learning_rate * it / config.warmup_iters\n    # 2) if it &gt; lr_decay_iters, return min learning rate\n    if it &gt; config.lr_decay_iters:\n        return config.min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n    assert 0 &lt;= decay_ratio &lt;= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n\n# helps estimate an arbitrarily accurate loss over either split using many batches\n@torch.no_grad()\ndef estimate_loss(model, train_dataset, val_dataset, block_size):\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        dataset = train_dataset if split == 'train' else val_dataset\n        for k in range(eval_iters):\n            X, Y = get_batch(dataset, block_size, batch_size, device)\n            with ctx:\n                logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nNow we can begin the training loop. You may need to increase max_iter to obtain good results.\n\niter_num = 0\nbest_val_loss = 1e9\neval_interval = 10\nlog_interval = 10\neval_iters = 40\nmax_iters = 500\nbatch_size = 1\n\n# optimizer\noptimizer = model.configure_optimizers(config.weight_decay, config.learning_rate,\n   (config.beta1, config.beta2), device)\n\n# training loop\nX, Y = get_batch(train_data, config.block_size, batch_size, device) # fetch the very first batch\nt0 = time.time()\nlocal_iter_num = 0 # number of iterations in the lifetime of this process\nraw_model = model # unwrap DDP container if needed\nrunning_mfu = -1.0\nwhile True:\n    # determine and set the learning rate for this iteration\n    lr = get_lr(config, iter_num) if config.decay_lr else config.learning_rate\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    # evaluate the loss on train/val sets and write checkpoints\n    if iter_num % eval_interval == 0:\n        losses = estimate_loss(model, train_data, val_data, config.block_size)\n        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # forward backward update, with optional gradient accumulation to simulate larger batch size\n    # and using the GradScaler if data type is float16\n    for micro_step in range(config.gradient_accumulation_steps):\n        with ctx:\n            logits, loss = model(X, Y)\n            loss = loss / config.gradient_accumulation_steps # scale the loss to account for gradient accumulation\n        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n        X, Y = get_batch(train_data, config.block_size, batch_size, device)\n        # backward pass, with gradient scaling if training in fp16\n        scaler.scale(loss).backward()\n\n    # clip the gradient\n    if config.grad_clip != 0.0:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n\n    # step the optimizer and scaler if training in fp16\n    scaler.step(optimizer)\n    scaler.update()\n\n    # flush the gradients as soon as we can, no need for this memory anymore\n    optimizer.zero_grad(set_to_none=True)\n\n    # timing and logging\n    t1 = time.time()\n    dt = t1 - t0\n    t0 = t1\n    if iter_num % log_interval == 0:\n        # get loss as float. note: this is a CPU-GPU sync point\n        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n        lossf = loss.item() * config.gradient_accumulation_steps\n        if local_iter_num &gt;= 5: # let the training loop settle a bit\n            mfu = raw_model.estimate_mfu(batch_size * config.gradient_accumulation_steps, dt)\n            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n\n    iter_num += 1\n    local_iter_num += 1\n\n    # termination conditions\n    if iter_num &gt; max_iters:\n        break\n\nHere is some code you can use to generate a sequence using the fine-tuned GPT2 model.\n\ninit_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\nout_dir = 'out' # ignored if init_from is not 'resume'\nstart = \"\\n\" # or \"&lt;|endoftext|&gt;\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\nnum_samples = 10 # number of samples to draw\nmax_new_tokens = 500 # number of tokens generated in each sample\ntemperature = 0.8 # 1.0 = no change, &lt; 1.0 = less random, &gt; 1.0 = more random, in predictions\ntop_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n\n\nenc = tiktoken.get_encoding(\"gpt2\")\nencode = lambda s: enc.encode(s, allowed_special={\"&lt;|endoftext|&gt;\"})\ndecode = lambda l: enc.decode(l)\n\nstart_ids = encode(start)\nx = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n\n#model = finetuned_model\n# run generation\nwith torch.no_grad():\n    with ctx:\n        for k in range(num_samples):\n            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n            print(decode(y[0].tolist()))\n            print('---------------')"
  },
  {
    "objectID": "labs/lab05.html",
    "href": "labs/lab05.html",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "In this lab, we will explore how training a neural network with some of the optimization methods discussed in the lecture can cause models to capture more information about the training data than we might intend. We will discuss why this may be problematic from a privacy perspective, and introduce the idea of differential privacy.\nFinally, this lab introduces an optimization strategy called DP-SGD or differentially private stochastic gradient descent. This strategy has some provable properties about the amount of information captured.\nThis lab also serves as an introductory guide to implementing optimization models that are presented in research papers. We hope that the techniques used in this lab build skills so that you can implement new techniques and ideas presented in other papers.\nBy the end of this lab, you will be able to:\nPlease work in groups of 1-2 during the lab.\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab05.html#submission",
    "href": "labs/lab05.html#submission",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab05.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 2: Description of the difference between the non-dp model predictions over data in/out of training (1 point)\nPart 3: Explanation of why the “average height” model is not \\(\\epsilon\\)-DP (1 point)\nPart 4: Explanation of T_max in CosineAnnealingLR (1 point)\nPart 4: Explanation why max_grad_norm &gt;= 7.49 causes gradient clipping to remain unchanged in the example (1 point)\nPart 4: Implementation of dp_grads function (4 points)\nPart 4: Explanation of the difference in the histogram of the dp and non-dp models (1 point)\nPart 4: Analysis of the impact of privacy breach on a vulnerable individual (1 point)"
  },
  {
    "objectID": "labs/lab05.html#google-colab-setup",
    "href": "labs/lab05.html#google-colab-setup",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Google Colab Setup",
    "text": "Google Colab Setup\nLike last week, we will be using the medmnist data set, which is available as a Python package. We will also be using opacus, which is a differential privacy library.\nRecall that on Google Colab, we use “!” to run shell commands. Below, we use such commands to install the Python packages.\n\n!pip install medmnist\n!pip install opacus"
  },
  {
    "objectID": "labs/lab05.html#part-1.-data-and-model",
    "href": "labs/lab05.html#part-1.-data-and-model",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Part 1. Data and Model",
    "text": "Part 1. Data and Model\nWe will be using the same data and model as in lab 3, with modifications in the way that the training, validation, and test sets are split. These modifications are necessary to be able to showcase differential privacy issues using a small model and limited data set size to ensure that models do not take an overwhelming amount of time to train.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport opacus\nimport medmnist\nfrom medmnist import PneumoniaMNIST\nimport torchvision.transforms as transforms\n\nimport torch.utils.data as data_utils\n\nmedmnist.INFO['pneumoniamnist']\n\nTask Use code from lab 3 to re-acquaint yourself with the training data. What do the inputs look like? What about the targets? What is the distribution of the targets? Intuitively, how difficult is the classification problem?\n\n# TODO: Run/revise the data exploration code from lab 3 so\n# that you can describe the dataset and the difference between\n# the two classes\n\nTask: Using the standard train/validation/test split provided by the dataset, what percentage of the training set had the label 0? What about the validation set?\n\n# TODO: Write code to compute the figures here\n\nThese statistics differ significantly between the training, validation and test sets for our DP demonstration to work well with our small MLP model. Thus, we will perform our own split of the training, validation, and test sets.\nIn addition, we will split the data into four sets: training, validation, test, and a memorization assessment set. This data set will be the same size as our training set. Practitioners sometimes call this set a second or unused training set, even though this data set is not used for training. The idea is that we want to see if there is a difference between data that we actually used for training, vs another data that we could have used for training.\nTask: Run the following code to obtain the four datasets.\n\n# Load the training, validation, and test sets\n# We will normalize each data set to mean 0.5 and std 0.5: this\n# improves training speed\ndata_transform = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize(mean=[.5], std=[.5])])\ntrain_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=True)\nvalid_dataset = PneumoniaMNIST(split='val', transform=data_transform, download=True)\ntest_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=True)\n\n# Combine the training and validation\ncombined_data = train_dataset + valid_dataset\n\n# Re-split the data into training,  memory assessment\ntrain_dataset, mem_asses_dataset, valid_dataset = torch.utils.data.random_split(combined_data, [0.4, 0.4, 0.2])\n\nTask: What percentage of the training set had the label 0? What about the memorization assessment set? What about the validation set?\n\n# TODO: Run code to complete your solution here.\n\nNow that our data is ready, we can set up the model and training code similar to lab 3.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLPModel(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=600):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        out = self.sigmoid(out)\n        out = self.fc3(out)\n        return out\n\nTo explore the distribution of predicted logits, the following code is written for you: it produces both the predictions and ground-truth labels across a dataset. There is also another utility function that can be used to measure the accuracy.\n\ndef get_predictions(model, data):\n    \"\"\"\n    Return the ground truth and predicted value across a dataset.\n    Unlike the get_prediction function in lab 3, this dataset\n    will produce the predictions for the *entire* dataset (no sampling)\n\n    Parameters:\n        `model` - A PyTorch model\n        `data` - A PyTorch dataset of MedMNIST images\n\n    Returns: A tuple `(ys, ts)` where:\n        `ys` is a list of prediction probabilities, same length as `data`\n        `ts` is a list of ground-truth labels, same length as `data`\n    \"\"\"\n    ys, ts = [], []\n    loader = torch.utils.data.DataLoader(data, batch_size=100)\n    for X, t in loader:\n        z = model(X.reshape(-1, 784))\n        ys += [float(y) for y in torch.sigmoid(z)]\n        ts += [float(t_) for t_ in t]\n    return ys, ts\n\ndef accuracy(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch model\n        `dataset` - A PyTorch dataset of MedMNIST images\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    ys, ts = get_predictions(model, dataset)\n    predicted = np.ndarray.round(np.array(ys))\n    return np.mean(predicted == ts)\n\nThe training code below is analogues to the training code used in lab 3, with some differences. One difference is that we use the Adam optimizer rather than SGD. The Adam optimizer combines ideas from momentum and RMSProp and is able to train our model to a suitable accuracy with much fewer iterations.\nTask: Run the code below to train our (non-private) model.\n\ndef train_model(model,                # a PyTorch model\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=1e-2,\n                batch_size=100,\n                num_epochs=45,\n                plot_every=20,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    for e in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n              z = model(images.reshape(-1, 784))\n              loss = criterion(z, labels.to(torch.float))\n              loss.backward()\n              optimizer.step()\n              optimizer.zero_grad()\n\n              iter_count += 1\n              if iter_count % plot_every == 0:\n                  iters.append(iter_count)\n                  ta = accuracy(model, train_data)\n                  va = accuracy(model, val_data)\n                  train_loss.append(float(loss))\n                  train_acc.append(ta)\n                  val_acc.append(va)\n                  print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n\n    if plot:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n \n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel_np = MLPModel()\ntrain_model(model_np, train_dataset, valid_dataset)"
  },
  {
    "objectID": "labs/lab05.html#part-2.-privacy-issues-in-our-model",
    "href": "labs/lab05.html#part-2.-privacy-issues-in-our-model",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Part 2. Privacy issues in our model",
    "text": "Part 2. Privacy issues in our model\nIn this part of the lab, we will show that our trained model captures more information about the training data than we might intend. In particular, we show that the model predictions have different patterns for images used in training (compared to images that are not used in training). Specifically, the prediction logits follow a different distribution for training images and images not used for training.\nIn more general applications, the patterns in the logit distributions can be used to build classifiers that can predict whether an image was used in training a neural network.\nTask: Suppose that you are a patient in a medical study, who consented for their data to be used to train a machine learning model to detect the strains of certain diseases (i.e., strain A or B of the same disease). Explain why you might not want it be known that your data was used to build the model.\n\n# TODO: Your explanation goes here.\n\nRecall that we used train_dataset to train our model, but did not use the mem_asses_dataset. To show that our model behaves differently for data used in training (vs not), we will plot the histogram of prediction probabilities across these two datasets.\nTask: Run the code below, which produces cumulative histogram plots showing showing the log model predictions of negative and positive samples (truth label=0 vs true label=1). The predictions for data point in the training set is shown in blue. The predictions for data points not in the training set (in the memorization assessment set) is in red.\n\ndef plot_hist(model, in_dataset, out_dataset):\n    \"\"\"\n    Plots the histogram (cumulative, in the log space) of the predicted\n    probabilities for datasets that is in the training set vs out. The\n    histograms are separated by the true labels.\n\n    Parameters:\n        `model` - A PyTorch model\n        `in_dataset` - A PyTorch dataset used for training \n                       (i.e. *in* the training set)\n        `out_dataset` - A PyTorch dataset not used for training \n                       (i.e. *out* the training set)\n    \"\"\"\n    # Obtain the prediction for data points in both data sets\n    ys_in, ts_in  = get_predictions(model, in_dataset)\n    ys_out, ts_out = get_predictions(model, out_dataset)\n\n    # Compute the negative log() of these predictions, separated by the\n    # ground truth labels. An epsilon is added to the prediction for\n    # numerical stability\n    epsilon = 1e-10\n    conf_in_0 = [-np.log(y + epsilon) for t, y in zip(ts_in, ys_in) if t == 0]\n    conf_in_1 = [-np.log(1 - y + epsilon) for t, y in zip(ts_in, ys_in) if t == 1]\n    conf_out_0 = [-np.log(y + epsilon) for t, y in zip(ts_out, ys_out) if t == 0]\n    conf_out_1 = [-np.log(1 - y + epsilon) for t, y in zip(ts_out, ys_out) if t == 1]\n\n    # Bins used for the density/histogram\n    bins_0 = np.linspace(0,max(max(conf_in_0),max(conf_out_0)),500)\n    bins_1 = np.linspace(0,max(max(conf_in_1),max(conf_out_1)),500)\n\n    # Plot the histogram for the predicted probabilities for true label = 0\n    plt.subplot(2, 1, 1)\n    plt.hist(conf_out_0, bins_0, color='r', label='out', alpha=0.5,cumulative=True, density=True)\n    plt.hist(conf_in_0, bins_0, color='b', label='in', alpha=0.5,cumulative=True, density=True)\n    plt.legend()\n    plt.ylabel('CDF')\n    plt.xlabel('-log(Pr(pred=1))')\n    plt.title(\"True label=0\")\n\n    # Plot the histogram for the predicted probabilities for true label = 1\n    plt.subplot(2, 1, 2)\n    plt.hist(conf_out_1, bins_1, color='r', label='out', alpha=0.5,cumulative=True, density=True)\n    plt.hist(conf_in_1, bins_1, color='b', label='in', alpha=0.5,cumulative=True, density=True)\n    plt.legend()\n    plt.title(\"True label=1\")\n    plt.ylabel('CDF')\n    plt.xlabel('-log(Pr(pred=0))')\n\n    plt.subplots_adjust(hspace=1)\n    plt.show()\n\nplot_hist(model_np, train_dataset, mem_asses_dataset)\n\nGraded Task: What difference do you notice between the histograms of the data points in the training set, vs those not in the training set? Explain how this difference is indicative of overfitting.\n\n# TODO: Your answer goes here"
  },
  {
    "objectID": "labs/lab05.html#part-3.-differential-privacy",
    "href": "labs/lab05.html#part-3.-differential-privacy",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Part 3. Differential Privacy",
    "text": "Part 3. Differential Privacy\nIn the previous section, we observed that a model’s prediction confidence for the samples inside its training set can be different from those outside the training set. We already know that this disparity has a negative impact on the model’s performance during test time. This phenomenon is known as overfitting and we know several techniques on how to measure and reduce the overfitting. In this part, we want to argue that this disparity has a negative impact on privacy of the training samples.\nAssume the designed model in the previous section is published for the public as a classification model for Pneumonia. Assume that the training set consists of individuals’ X-ray. Even though the participants have consented to participate in the research, their privacy should still be protected. If an attacker can determine whether a specific individual’s data is part of the research dataset, it might lead to unintended privacy breaches. Participants might not want their involvement in such a study to be public knowledge due to the stigma associated with certain medical conditions. This disparity may also help an attacker to reconstruct a participant’s data. This example shows that the disparity between the model’s confidence lets the adversary infer the membership of a sample from the predictions. This is alarming! In the next part, we discuss how to mitigate this risk by introducing the fundamental concept of Differential Privacy.\nDifferential privacy (DP) is a data privacy framework that aims to provide strong privacy guarantees when analyzing or sharing sensitive data. We say an ML algorithm satisfies Differential Privacy if changing one of the training samples does not change the output of the algorithm significantly. DP is an interesting property: assume you want to give a hospital access to your X-ray. If the hospital uses a DP ML algorithm, then, you are guaranteed that your presence does not affect the output significantly. This is promising and motivates people to give access to their data for the purpose of data analysis.\nNext, we discuss how to formalize DP.\nAssume we have a dataset \\(S=\\{(x_1,y_1),\\dots,(x_n,y_n)\\}\\) which consists of \\(n\\) individuals data. Consider the neighboring dataset \\(S'=\\{(x_1,y_1),\\dots,(x'_n, y'_n)\\}\\) which differs from \\(S\\) in only one sample. Then, we say a randomized algorithm \\(\\mathcal{A}\\) satisfies \\(\\epsilon\\)-DP if for all the output \\(y\\) in the range of \\(\\mathcal{A}\\) it satisfies\n\\[\n\\mathbb{Pr}\\left(\\mathcal{A}(S) =y \\right) \\leq \\exp(\\epsilon)  \\mathbb{Pr}\\left(\\mathcal{A}(S’) =y \\right).\n\\]\nBut what is the intuition behind this equation?\n\\(\\epsilon\\) is called the privacy budget. Privacy budget captures how strong our privacy guarantees are, by showing that the outcome is indistinguishable in two neighboring datasets. This can be shown by setting \\(\\epsilon = 0\\), the probability the analysis having an outcome is the same with or without you in the database. So if we set \\(\\epsilon\\) to some small value, we can get good guarantees that the output will not differ much.\nA common property of privacy-preserving algorithms is randomness. To see why it is the case assume we are interested in the average height of the students enrolled in CSC413while preserving their privacy. Consider a deterministic algorithm that reports the average height of the students. We argue that there exists no finite \\(\\epsilon\\) for which this algorithm is DP. For this example, it can be shown that by adding a Gaussian noise to the average height we can preserve privacy of the individuals.\nGraded Task: Explain why the algorithm that reports the average height of the students is not \\(\\epsilon\\)-DP for any finite \\(\\epsilon&gt;0\\).\n\n# TODO: Include your explanation here\n\nWe hope that the basic intuition behind differential privacy is now clear. DP is now a widely-used method to preserve privacy. It has been used in companies like Google and Apple to gather the user’s data. It is also recently used for the US Census. See the following [video]{https://www.youtube.com/watch?v=nVPE1dbA394} to get more information."
  },
  {
    "objectID": "labs/lab05.html#part-4.-differentially-private-sgd",
    "href": "labs/lab05.html#part-4.-differentially-private-sgd",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Part 4. Differentially-Private SGD",
    "text": "Part 4. Differentially-Private SGD\nIn this section, we discuss how we can make stochastic gradient descent differentially private and implement it. We will follow the algorithmic description of DP-SGD forom https://arxiv.org/pdf/1607.00133.pdf that we reproduce below. Start by reading the words/headings in the description, then we will discuss the algorithm line by line.\nAlgorithm Outline for Differentially Private SGD (DP-SGD)\nInput Examples \\(\\left\\{x_1, \\ldots, x_N\\right\\}\\), loss function \\(\\mathcal{L}(\\theta)=\\) \\(\\frac{1}{N} \\sum_i \\mathcal{L}\\left(\\theta, x_i\\right)\\).\nParameters: learning rate \\(\\eta_t\\), noise scale, \\(\\sigma\\), group size \\(L\\), gradient norm bound \\(C\\). \\\nInitialize \\(\\theta_0\\) randomly\nfor \\(t \\in[T]\\) do * Take a random sample \\(L_t\\) with sampling probability \\(L / N\\) * Compute gradient \\(\\quad\\) For each \\(i \\in L_t\\), compute \\(\\mathbf{g}_t\\left(x_i\\right) \\leftarrow \\nabla_{\\theta_t} \\mathcal{L}\\left(\\theta_t, x_i\\right)\\) * Clip gradient \\(\\overline{\\mathbf{g}}_t\\left(x_i\\right) \\leftarrow \\mathbf{g}_t\\left(x_i\\right) / \\max \\left(1, \\frac{\\left\\|\\mathbf{g}_t\\left(x_i\\right)\\right\\|_2}{C}\\right)\\) * Add noise \\(\\tilde{\\mathbf{g}}_t \\leftarrow \\frac{1}{L}\\left(\\sum_i \\overline{\\mathbf{g}}_t\\left(x_i\\right)+\\mathcal{N}\\left(0, \\sigma^2 C^2 \\mathbf{I}\\right)\\right)\\) \\ * Descent \\(\\theta_{t+1} \\leftarrow \\theta_t-\\eta_t \\tilde{\\mathbf{g}}_t\\)\nOutput \\(\\theta_T\\) and compute the overall privacy cost \\((\\varepsilon, \\delta)\\) using a privacy accounting method.\nTask: Read the algorithm above. Write down, for each symbol/notation used in the algorithm, what it represents. Pay particular attention to the symbol \\(\\mathbf{g}_t\\left(x_i\\right)\\) and its various modifications.\n\n# TODO: Make sure you understand the notation before moving on to the\n# detailed descriptions.\n\nNow, let’s discuss the algorithm line by line.\n\nSampling: The sampling mechanism used in DP-SGD is different from SGD. In non-private SGD, we choose a random permutation at the beginning of each epoch. However, in DP-SGD at each iteration we select a sample with probability (batchsize/number of samples) to be a member of the batch at the current iteration. This sampling mechanism is also called Poisson subsampling. We will not implement this sampling ourselves; we will use Opacus software package implement of it.\nGradient Computation: This step is analogous to gradient computation in SGD. However, the gradients of each sample in the batch is computed separately.\nClipping Gradients: You should be able to show that, mathematically, that clipping ensures that for each data point, the gradient vector of that data point has a maximum norm of \\(C\\). Why is this useful? Assume there is an outlier for which the gradient is very large. Without clipping, the impact of the outlier on the algorithm will be unbounded. DP-SGD performs clipping to each individual gradient point separately, which limits the contribution of each data point to the parameter update.\nAdding Noise: In order to achieve a specific level of privacy determined by \\(\\epsilon\\), we need to select the minimum amount of noise to be added in each iteration (\\(\\sigma\\) in the algorithm description). Since determining the exact amount requires a very technical calculation, there are software packages which can be used. Here, we use the Opacus software package from Meta research. It provides a function which takes as input the privacy level \\(\\epsilon\\), batchsize and the number of training points, and outputs the variance of the noise. There is another input, i.e., \\(\\delta\\). Do not make any changes to it. If you are interested to know what it means please read the following lecture note.\n\nTask: Notice that the scale of the noise added to the gradient is related to the clipping parameter. Does the amount of the noise added increase or decrease if we allow a larger maximum norm \\(C\\) during clipping? (Reasoning about these differences is one way to make sense of mathematical equations like these.)\n\n# TODO: Your answer goes here\n\nIn the next few tasks, we will describe the pieces of code that we will need to implement DP-SGD.\nTask: Run the following code to compare the batches produced by the usual Dataloader vs. via Poisson Sampling. What do you noitce?\n\n# Create a dataset with 20 numbers\nx = torch.arange(20)\nprint(x)\ndataset = torch.utils.data.TensorDataset(x)\n\nprint('PyTorch DataLoader')\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=4,shuffle=True)\nfor _ in range(2): # run for 2 epochs\n    for x_b in data_loader:\n        print(x_b)\n    print('-------------------')\n\nprint('Poisson Sampling')\ndp_data_loader = opacus.data_loader.DPDataLoader(dataset, sample_rate = 5/20)\nfor _ in range(2): # run for 2 epochs\n    for x_b in dp_data_loader:\n        print(x_b)\n    print('-------------------')\n\nIn addition to using a different sampling method, we will use CosineAnnealingLR learning rate scheduler in pytorch. You need to understand how this learning rate scheduler works and how it can be updated.\nGraded Task Read the PyTorch documentation on CosineAnnealingLR and explain what the parameter T_max represent.\n\n# TODO: Write your explanation here.\n\nThe most challenging part of implementing DP-SGD is that we will need to implement our own optimization process to modify the default gradient descent behaviour. However, Pytorch has a nice feature that we saw in lab 1: each parameter in a model stores its own gradient as an attribute. In particular, consider the following snippet which can be used to print the name and gradient of the parameters in a model.\n\nfor name, param in model_np.named_parameters(): \n    print(name)\n    print(param.grad)\n\nWhat we didn’t see in lab 1 is that we can anually change the gradient of each parameter!\nThis is powerful, because the optimizers in PyTorch uses the .grad attributes of each parameter to perform model updates. Thus, changing the .grad attributes provides a way to override the default gradient descent behaviour.\nTask: Run the below code, which demonstrates how the .grad attribute can be modified.\n\nmodel = nn.Linear(5, 1) # linear model with input dim = 5, and a single output\nprint(list(model.parameters())) # print the current parameters\n\n# manually set the optimizers\noptimizer = torch.optim.SGD(model.parameters(), 0.1)\nmodel.weight.grad = torch.nn.parameter.Parameter(torch.Tensor([[1, 2, 3, 4, 5.]]))\nmodel.bias.grad = torch.nn.parameter.Parameter(torch.Tensor([1.]))\noptimizer.step()\n\n# what would you expect the output to be?\nprint(list(model.parameters()))\n\nTo implement DP-SGD, We will need to manually modify the .grad attribute in a few ways. One of the steps to DP-SGD is gradient clipping. Fortunately, PyTorch actually comes with an implementation of gradient clipping through the function torch.nn.utils.clip_grad_norm_.\nTask: Run this code to see how gradient clipping works. Notice that the gradient direction is unchanged, only the magnitude.\n\nmodel = nn.Linear(5, 1) # linear model with input dim = 5, and a single output\nmodel.weight.grad = torch.Tensor([[1, 2, 3, 4, 5.]])\nmodel.bias.grad = torch.Tensor([1.])\nmax_grad_norm = 0.5\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\nprint(model.weight.grad, model.bias.grad)\n\nGraded Task: Explain why if we set max_grad_norm &gt;= 7.49 above, the gradient will be unchanged. Your explanation should demonstrate the calculation of where the number 7.49 comes from.\n\n# TODO: Include your explanation and calculation here.\n\nGraded Task: Now that we have the pieces we need to implement our DP-SGD gradient computation, Complete the code below, which performs one iteration of DP-SGD update for a batch of data. You may wish to look ahead to see how this function will be used in DP-SGD training.\n\ndef dp_grads(model, batch_data, criterion, max_grad_norm, noise_multiplier):\n    \"\"\"\n    Compute gradients for an iteration of DP-SGD training by setting the\n    .grad attribute of each parameter in model.named_parameters()\n    according to the DP-SGD algorithm.\n\n    Parameters:\n        - `model` - A PyTorch model\n        - `batch_data` - A list of tuples (x, t) representing a batch of data\n        - `criterion` - A PyTorch loss function\n        - `max_grad_norm` - The maximum gradient norm, used for gradient clipping\n                            (C in the algorithm description)\n        - `noise_multiplier` - The noise multiplier, used for adding noise\n                               (sigma in the algorithm description)\n\n    Returns: A dictionary `clipped_noisy_grads` that maps the names of each\n             parameter in `model.named_parameters()` to its modified gradient\n             computed according to DP-SGD\n    \"\"\"\n    # Create the mapping of each parameter in our model to\n    # what will evetually be the noisy gradients\n    clipped_noisy_grads = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n\n    # Iterate over the data points in each batch. This is unfortunately\n    # necessary so that we can perform gradient clipping separtely for each\n    # data point\n    for xi, ti in batch_data:\n        zi = None # TODO: compute the model prediction (logit)\n        lossi = None # TODO: compute the loss for this data point\n\n        # TODO: perform the backward pass\n\n        # TODO: perform gradient clipping\n\n        # accumulate the clipped gradients in `clipped_noisy_grads`\n        for name, param in model.named_parameters():\n            clipped_noisy_grads[name] += param.grad\n\n        # TODO: clear the gradients in the model's computation graph\n\n    # Now, we iterate over the name parameters to add noise\n    for name, param in model.named_parameters():\n        # TODO: Read the equation in the \"Add Noise\" section of the\n        #      algorithm description, and implement it. You may find\n        #      the function `torch.normal` helpful.\n        clipped_noisy_grads[name] += 0 # TODO\n\n\n    return clipped_noisy_grads\n\nPlease include the output of the tests below in your submission. (What should the output of the test be?)\n\nmodel = nn.Linear(5, 1)\nmodel.weight = nn.Parameter(torch.Tensor([[1, 1, 0, 0, 0.]]))\nmodel.bias = nn.Parameter(torch.Tensor([0.]))\nbatch_data = [(torch.Tensor([[1, 1, 1, 0, 0.]]), torch.Tensor([1.])),\n              (torch.Tensor([[1, 0, 1, 0, 0.]]), torch.Tensor([0.]))]\ncriterion = nn.BCEWithLogitsLoss()\n\n# no noise and a large max_grad_norm\nprint(dp_grads(model, batch_data, criterion, max_grad_norm=1000, noise_multiplier=0))\nprint('-----------')\n\n# no noise and a small max_grad_norm\nprint(dp_grads(model, batch_data, criterion, max_grad_norm=0.5, noise_multiplier=3))\nprint('-----------')\n\n# small max_grad_norm and some noise (STD should be ~0.5x3/2)\nfor i in range(10):\n    print(dp_grads(model, batch_data, criterion, max_grad_norm=0.5, noise_multiplier=3))\n\nTask Now that we have DP-SGD in place, run the below code to train a differentially private model.\n\ndef train_model_private(model, traindata, valdata, learning_rate=2e-1,\n                        batch_size=500, num_epochs=25, plot_every=20,\n                        epsilon=0.5, max_grad_norm=6):\n    # Compute the noise multiplier\n    N = len(traindata)\n    noise_multiplier = opacus.accountants.utils.get_noise_multiplier(\n        target_epsilon=epsilon,\n        target_delta=1/N,\n        sample_rate=batch_size/N,\n        epochs=num_epochs)\n\n    # Use the differentially private data loader\n    train_loader = opacus.data_loader.DPDataLoader(\n        dataset=traindata,\n        sample_rate=batch_size/N)\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N//batch_size * num_epochs)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n \n    for e in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n            images = images.reshape(-1, 28*28)\n            # get the clipped noisy gradients from the function you wrote\n            clipped_noisy_grads = dp_grads(model,\n                                           batch_data=list(zip(images,labels)),\n                                           criterion=criterion,\n                                           max_grad_norm=max_grad_norm,\n                                           noise_multiplier=noise_multiplier)\n            # manually update the gradients\n            for name, param in model.named_parameters():\n                param.grad = clipped_noisy_grads[name]\n            optimizer.step() # update the parameters\n            scheduler.step() # update the learning rate scheduler\n            optimizer.zero_grad() # clean up accumualted gradients\n\n            iter_count += 1\n            if iter_count % plot_every == 0:\n                # forward pass to compute the loss\n                z = model(images.reshape(-1, 784))\n                loss = criterion(z, labels.to(torch.float))\n                optimizer.zero_grad()\n\n                iters.append(iter_count)\n                ta = accuracy(model, traindata)\n                va = accuracy(model, valdata)\n                train_loss.append(float(loss))\n                train_acc.append(ta)\n                val_acc.append(va)\n                print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n   \n    plt.figure()\n    plt.plot(iters[:len(train_loss)], train_loss)\n    plt.title(\"Loss over iterations\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n\n    plt.figure()\n    plt.plot(iters[:len(train_acc)], train_acc)\n    plt.plot(iters[:len(val_acc)], val_acc)\n    plt.title(\"Accuracy over iterations\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"Train\", \"Validation\"])\n\n\nmodel_priv = MLPModel()\ntrain_model_private(model_priv, train_dataset, valid_dataset)\n\nGraded Task: Plot the histogram of the model prediction for this differentially private model. How does this histogram differ from that of model_np from above?\n\nplot_hist(model_priv, train_dataset, mem_asses_dataset)\n\n\n# TODO: Explain how the histogram differs from that of model_np\n\nTask How does the accuracy differ from that of model_no mentioned above? Explain what you observe.\n\n# TODO: Your answer goes here\n\nGraded Task: Suppose that an attacker recognizes that your friend Taylor is in this data set, means that their X-ray was taken at some point during a hospitalization, and that Taylor provided researchers consent to be included in the study dataset. If this information is sold to a third-party (e.g., a credit reporting agency, an employer, or a landlord), how might this affect Taylor?\n\n# TODO: Your answer goes here\n\nIf you are interested in DP, we suggest performing hyperparameter tuning over batch size and max grad norm. In, DP-SGD usually larger batch size would help. So, for instance for two values of \\(\\varepsilon \\in \\{0.5,1,5\\}\\), try to find the best model for \\(\\text{batchsize}\\in \\{100,500\\}\\) and \\(\\text{gradnorm}\\in\\{4,8,16\\}\\)."
  },
  {
    "objectID": "labs/lab05.html#appendix",
    "href": "labs/lab05.html#appendix",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Appendix",
    "text": "Appendix\n\n[Differential Privacy and the Census] https://www.youtube.com/watch?v=nVPE1dbA394\nMain DP-SGD Paper\nCS 860 - Algorithms for Private Data Analysis at UWaterloo"
  },
  {
    "objectID": "labs/lab03.html",
    "href": "labs/lab03.html",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "",
    "text": "MedMNIST’s PneumoniaMNIST data set. We will now transition fully to using PyTorch for our labs going forward.\nBy the end of this lab, you will be able to:\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab03.html#submission",
    "href": "labs/lab03.html#submission",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab03.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 2. Your expression that computes the number of trainable parameters in the MLPModel (1 point)\nPart 2. Your implementation of accuracy. (1 point)\nPart 2. Your implementation of train_model. (2 points)\nPart 3. Your implementation of precision and recall. (2 points)\nPart 3. Your interpretation of the confusion matrix for m_once (1 point)\nPart 4. Your completion of the grid search, along with the output (2 point)\nPart 4. Your description of why a model with high AUC may still perform poorly for some groups (1 point)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "labs/lab03.html#google-colab-setup",
    "href": "labs/lab03.html#google-colab-setup",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Google Colab Setup",
    "text": "Google Colab Setup\nWe will be using the medmnist data set, which is available as a Python package. Recall that on Google Colab, we use “!” to run shell commands.\n\n!pip install medmnist"
  },
  {
    "objectID": "labs/lab03.html#part-1.-data",
    "href": "labs/lab03.html#part-1.-data",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nWe will use the MedMNIST data set, which is described here: https://medmnist.com/. We will use the PneumoniaMNIST images, which are greyscale chest X-ray images that has been resized to 28x28. The task is to predict, given one of these X-ray images, whether the patient has pneumonia or not—a binary classification task. We chose this dataset both because it is lightweight, and because it allows us to discuss the sensitive nature of biomedical images.\nLet’s begin by printing some information about the PneumoniaMNIST data set:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport medmnist\nfrom medmnist import PneumoniaMNIST\n\nmedmnist.INFO['pneumoniamnist']\n\nTask: The dataset providers already split the data into training, validation, and test sets. How many samples are there in the training, validation, and test sets?\n\n# TODO: Write your answer here.\n\nLet’s visually inspect the first element of the training data:\n\ntrain_data_imgs = PneumoniaMNIST(split='train', download=True)\n\nfor img, target in train_data_imgs:\n    plt.imshow(img, cmap='gray')\n    print(np.array(img)) # img is a numpy array of shape 28x28 , with integer values between 0-255\n    print(target)        # the target\n    break\n\nTask: Based on the code above, what is the type of the data structure train_data?\n\n# TODO: Write your answer here.\n\nTask: The code below plots 5 images from each class: normal and pneumonia. Do you notice qualitative differences between these two sets of images? It is always important to qualitatively assess your data prior to training, so that you can develop intuition as to what features may or may not be important for your model. Understanding your data also helps to estimate how challenging the classification problem may be and identify incorrect implementations (e.g., a surprisingly high model accuracy could indicate issues with training set leakage into the test set).\n\n# normal images\nplt.figure()\nn = 0\nfor img, target in train_data_imgs:\n    if int(target) == 0:\n      plt.subplot(1, 5, n+1)\n      plt.title(\"normal\")\n      plt.imshow(img, cmap='gray')\n      n += 1\n    if n &gt;= 5:\n      break\n# pneumonia images\nplt.figure()\nn = 0\nfor img, target in train_data_imgs:\n    if int(target) == 1:\n      plt.subplot(1, 5, n+1)\n      plt.title(\"pneumonia\")\n      plt.imshow(img, cmap='gray')\n      n += 1\n    if n &gt;= 5:\n      break\n\n\n# TODO: Write your explanation here.\n\nPyTorch makes it easy to apply pre-processing transformations to the data, for example to normalize the data prior to using for training. We will use the standard preprocessing functions to transform the images into tensors for PyTorch to be able to use. This transformation also changes the values to be floating-point numbers between 0 and 1.\n\nimport torchvision.transforms as transforms # contains a collection of transformations\n\ntrain_data = PneumoniaMNIST(split='train', download=True, transform=transforms.ToTensor())\nval_data = PneumoniaMNIST(split='val', download=True, transform=transforms.ToTensor())\ntest_data = PneumoniaMNIST(split='test', download=True, transform=transforms.ToTensor())\n\nfor img, target in train_data:\n    print(img)    # img is a PyTorch tensor fo shape 1x28x28\n    print(target) # the target\n    break\n\nTask: How many X-ray images are in the training set with pneumonia? What about without pneumonia? What about the validation/test sets? What does your answer say about the data balance?\n\n# TODO: Write code to find the answer here."
  },
  {
    "objectID": "labs/lab03.html#part-2.-model-and-training",
    "href": "labs/lab03.html#part-2.-model-and-training",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Part 2. Model and Training",
    "text": "Part 2. Model and Training\nWe will build our own PyTorch model, which will be a subclass of nn.Module. This subclass provides the important methods that we used in the training loop in lab 1, including the methods that allow us to compute the forward pass by calling the model object, and other methods used under the hood to compute the backward pass.\nOur model will be a three-layer MLP with the following architecture: ACTUALTODO—the model architecture may change!\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLPModel(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=100):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        return out\n\nGraded Task: How many trainable parameters are in this model? Express your answer in terms of input_dim and num_hidden.\n\n# TODO: Compute the number of trainable parameters in MLPModel\n\nIn order to assess model performance, we will begin by implementing the accuracy function, which computes the accuracy of the model across a dataset.\nGraded Task: Complete the accuracy function. Keep in mind that this function will be slightly different from the accuracy function in lab 1, since we are working on a binary classification problem and prediction here is a single logit value (rather than a vector).\n\ndef accuracy(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    correct, total = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n\n        correct += int(torch.sum(t == y))\n        total   += t.shape[0]\n    return correct / total\n\nBecause we are working with binary classification, we will be using a different implementation of the cross-entropy loss function, implemented via PyTorch in a class called BCEWithLogitsLoss (short for Binary Cross Entropy with Logits loss).\n\ncriterion = nn.BCEWithLogitsLoss()\n\nThis loss function takes a predicted logit (pre-softmax activation) and the ground-truth label. The use of pre-softmax logits rather than prediction probabilities is due to numerical stability reasons.\n\nprint(criterion(torch.tensor([2.5]),  # predicted\n                torch.tensor([1.])))  # actual\n\nprint(criterion(torch.tensor([-2.5]), # predicted\n                torch.tensor([1.])))  # actual\n\nTask: Explain why the second printed value above is larger than the first. In other words, why does it make sense that we think of the second prediction (logit of z=-2.5) as “worse” than the first (logit of z=2.5)?\n\n# TODO: Your explanation goes here.\n\nGraded Task: Complete the following code to be used for training.\n\ndef train_model(model,                # an instance of MLPModel\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=0.1,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=50,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (images, labels) in enumerate(train_loader):\n                z = None # TODO\n  \n                loss = None # TODO\n  \n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n  \n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    ta = accuracy(model, train_data)\n                    va = accuracy(model, val_data)\n                    train_loss.append(float(loss))\n                    train_acc.append(ta)\n                    val_acc.append(va)\n                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        if plot:\n            plt.figure()\n            plt.plot(iters[:len(train_loss)], train_loss)\n            plt.title(\"Loss over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n\n            plt.figure()\n            plt.plot(iters[:len(train_acc)], train_acc)\n            plt.plot(iters[:len(val_acc)], val_acc)\n            plt.title(\"Accuracy over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Accuracy\")\n            plt.legend([\"Train\", \"Validation\"])\n\n# Please include the output of this cell for grading\nmodel = MLPModel()\ntrain_model(model, train_data, val_data)\n\nTask: Suppose that a model has a validation accuracy of 74% for this binary classification task. Why would this model be considered a very bad model? Your answer should illustrate why accuracy may not be an excellent tool to use.\n\n# TODO: Write your explanation here"
  },
  {
    "objectID": "labs/lab03.html#part-3.-performance-metrics",
    "href": "labs/lab03.html#part-3.-performance-metrics",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Part 3. Performance Metrics",
    "text": "Part 3. Performance Metrics\nWe often use accuracy as a go-to metric when evaluating the performance of a classification model. However, the accuracy measure weighs all errors equally. A deeper look into the types of errors made can provide a more complete picture of model performance, especially when there is data imbalance and—when applying models in real situations—when some errors may be associated with more serious impacts to users than others.\nTo start our explorations, we’ll look at the decisions we made well, i.e. the:\n\nTrue Positives (TP), or positive outcomes that were correctly predicted as positive.\nTrue Negatives (TN), or negative outcomes that were correctly predicted as negative.\n\nThen we will look at our mistakes, i.e. the:\n\nFalse Positives (FP, or Type I errors), or negative outcomes that were predicted as positive. In our case, this occurs when our model predicts that a person has heart disease, but they do not.\nFalse Negatives (FN, or Type II errors), or positive outcomes that were predicted as negative. In our case, this occurs when our model predicts that a person does not have heart disease, but they do.\n\nWe can then use the metrics above to calculate:\n\nPrecision (or True Positive Rate, or Positive Predicive Value): \\(\\frac{TP}{TP + FP}\\). The answers the question: out of all the examples that we predicted as positive, how many are really positive?\nRecall (or Sensitivity): \\(\\frac{TP}{TP + FN}\\). The answers the question: out of all the positive examples in the data set, how many did we predict as positive?\nFalse Positive Rate (or Negative Predicive Value): \\(\\frac{TN}{TN + FN}\\). The answers the question: out of all the examples that we predicted as negative, how many are really negative?\n\nGraded Task: Complete the functions precision and recall:\n\ndef precision(model, dataset):\n    \"\"\"\n    Compute the precision of `model` over the `dataset`.  We will take the\n    **most probable class** as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    true_pos, total_pred_pos = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n\n        # TODO: update total_pred_pos and true_pos\n    return true_pos / total_pred_pos\n\n\ndef recall(model, dataset):\n    \"\"\"\n    Compute the recall (or sensitivity) of `model` over the `dataset`.  We will\n    take the **most probable class** as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    true_pos, total_actual_pos = 0, 0 # track the true and false positive\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n\n        # TODO: update total_pos and true_pos\n    return true_pos / total_actual_pos\n\nprint(\"Precision(Training)\", precision(model, train_data))\nprint(\"Recall(Training)\", recall(model, train_data))\nprint(\"Precision(Validation)\", precision(model, val_data))\nprint(\"Recall(Validation)\", recall(model, val_data))\n\nA confusion matrix is a table that shows the number of TP, TN, FP, and FN. A confusion matrix can be a valuable tool in understanding why a model makes the mistake that it makes.\nTask Run the code below to display the confusion matrix for your model for the validation data.\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef get_prediction(model, data, sample=1000):\n    loader = torch.utils.data.DataLoader(data, batch_size=sample, shuffle=True)\n    for X, t in loader:\n        z = model(X.view(-1, 784))\n        y = torch.sigmoid(z)\n        break\n    y = y.detach().numpy()\n    t = t.detach().numpy()\n    return y, t\n\ny, t = get_prediction(model, val_data)\ny = y &gt; 0.5\ncm = confusion_matrix(t, y)\ncmp = ConfusionMatrixDisplay(cm, display_labels=[\"0\", \"1\"])\ncmp.plot()\nplt.title(\"Confusion Matrix (Val Data)\")\n\nTask: The code below trains a MLPModel for a very few number of iterations. You should see that this model achieves a 74% accuracy. Display the confusion matrix for this model by running the code below.\n\nm_once = MLPModel()\ntrain_model(m_once, train_data, val_data, learning_rate=0.5, batch_size=500, num_epochs=1)\nprint(\"Training Accuracy:\", accuracy(m_once, train_data))\nprint(\"Validation Accuracy:\", accuracy(m_once, val_data))\n\ny, t = get_prediction(m_once, val_data)\ny = y &gt; 0.5\nConfusionMatrixDisplay(confusion_matrix(t, y), display_labels=[\"0\", \"1\"]).plot()\nplt.title(\"Confusion Matrix (Val Data)\")\n\nGraded Task: What does the confusion matrix tell you about how the m_once model is achieving 74% accuracy?\n\n# TODO: Your explanation goes here.\n\nWe have been choosing a threshold of 0.5 for turning our continuous predicted probabilities into a discrete prediction. However, this can be an arbitrary choice.\nTask: Explain why, in practical application, it may be reasonable to use a different threshold value. In what situation might you want the threshold to be set very high in order to make a positive prediction? What about a negative prediction?\n\n# TODO: Your explanation goes here.\n\nA receiver operating characteristic curve (or ROC) shows how the True Positive Rate and False Positive Rate vary based on our choice of the decision making threshold used to binarize predictions. By default, this threshold is 0.5, but it can be changed to any value between 0 and 1. Different thresholds will result in different TP and FP rates, all of which are illustrated on our graph. we can calculate the area underneath this curve in order to get a sense as to how our classifiers might work across a wide range of different thresholds. This calcution of area can also be used as a metric of our model’s “goodness”, and it is called AUC (or “Area Under Curve”).\nThe AUC metric is particularly useful for machine learning practitioners because it does not depend on the choice of the threshold value used for making discrete predicions. The metric is also resistant to measurement.\nTask: Is it better for the AUC to be larger or smaller? Explain why.\n\n# TODO: Your explanation goes here\n\nThe code below plots the ROC curve for a model.\n\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, auc\n\ny, t = get_prediction(model, val_data)\n\nfpr, tpr, thresholds = roc_curve(t, y)\nroc_auc = auc(fpr, tpr)\nrocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\nrocp.plot()\nplt.title(\"Validation ROC Curve\")\n\n\ny, t = get_prediction(model, train_data)\n\nfpr, tpr, thresholds = roc_curve(t, y)\nroc_auc = auc(fpr, tpr)\nrocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\nrocp.plot()\nplt.title(\"Training ROC Curve\")\n\nHere is a function you can use to estimate the auc:\n\ndef get_auc(model, data):\n    y, t = get_prediction(model, data)\n    fpr, tpr, thresholds = roc_curve(t, y)\n    return auc(fpr, tpr)"
  },
  {
    "objectID": "labs/lab03.html#part-4.-hyperparameter-tuning-via-grid-search",
    "href": "labs/lab03.html#part-4.-hyperparameter-tuning-via-grid-search",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Part 4. Hyperparameter Tuning via Grid Search",
    "text": "Part 4. Hyperparameter Tuning via Grid Search\nAs we mentioned in lab 1, hyperparameter choices matter significantly, and these hyperparameter choices interact with one another. Practitioners use a strategy called grid search to try all variations of hyperparameters from a set of hyperparameters.\nOne very important hyperparameter is the number of hidden units in our MLPModel. This setting affects the number of parameters (weights/biases) used in our model.\nThe use of ReLU vs sigmoid activation function is another hyperparameter that we will explore.\nFinally, optimization parameters like the batch size and the learning rate can also significantly affect the learning process.\n\nclass MLPModelSigmoid(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=100):\n        super(MLPModelSigmoid, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sig(out)\n        out = self.fc2(out)\n        out = self.sig(out)\n        out = self.fc3(out)\n        return out\n\nGraded Task: Complete the code below, which performs grid search over the following hyperparameter values of the:\n\nhidden size\nactivation function (ReLu vs sigmoid activation)\nbatch size\nlearning rate\n\nDo so by creating a new model and train it with the appropriate settings, then assessing the final training/validation accuracy, precision, recall, and AUC score. You may use to use the flag plot=False when calling train_model. You might also set plot_every to a large value and visualize the training curve as a separate step for hyperparameter values that you’re interested in.\nPlease include all your output in your submission.\n(There is one more graded task below that you can complete while the hyperparameter tuning is running.)\n\ngridsearch = {}\nfor num_hidden in [25, 100, 250]:\n    for act in [\"relu\", \"sigmoid\"]:\n        for bs in [10, 100, 500]:\n            for lr in [0.01, 0.1]:\n                # Adjust num_epoch based on the batch size, so that we \n                # train for the same number of iterations irrespective\n                # of batch size\n                ne = int(20 * (bs/100))\n\n                modelname = f\"num_hidden: {num_hidden}, activation: {act}, batch_size: {bs}, learning_rate: {lr}\"\n                print(f\"========={modelname}\")\n\n                # TODO: create and train the model with the appropriate settings\n\n                # Update and display metrics. This part is done for you.\n                metrics = {\n                    \"acc_train\": accuracy(m, train_data),\n                    \"acc_val\": accuracy(m, val_data),\n                    \"precision_train\": precision(m, train_data),\n                    \"precision_val\": precision(m, val_data),\n                    \"recall_train\": recall(m, train_data),\n                    \"recall_val\": recall(m, val_data),\n                    \"auc_train\": get_auc(m, train_data),\n                    \"auc_val\": get_auc(m, val_data),\n                }\n                gridsearch[modelname] = metrics\n                print(f'Accuracy (train):{metrics[\"acc_train\"]} (val):{metrics[\"acc_val\"]}')\n                print(f'Precision (train):{metrics[\"precision_train\"]} (val):{metrics[\"precision_val\"]}')\n                print(f'Recall (train):{metrics[\"recall_train\"]} (val):{metrics[\"recall_val\"]}')\n                print(f'AUC (train):{metrics[\"auc_train\"]} (val):{metrics[\"auc_val\"]}')\n\nPlease include the below output in your submission\n\nprint(gridsearch)\n\nTask: Which hyperparameter choice is the “best”? You should base this answer on the validation AUC. Use the other metrics as a guide to understand the kinds of predictions and mistakes that your model is likely make. Train a final model with those hyperparameter values.\n\n# TODO\n\nTask: Report the test accuracy and AUC for this model, and plot the confusion matrix over the test set.\n\n# TODO\n\nGraded Task: Explain why a model with high AUC may still produce consistently poor predictions for a subset of the population. You might find this article interesting: Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis; in particular, Figure 1 shows how test AUC differs male/female patients depending on the training set used.\n\n# TODO"
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "",
    "text": "In this lab, we will review linear models for classification, which was discussed in depth in CSC311. We will use this as an opportunity to review key ideas, including the splitting of the dataset into train/validation/test sets, optimization methods using Stochastic Gradient Descent, and so on. This lab reviews Python libraries that you have used in CSC311, including numpy, matplotlib and others.\nBut the main aim of this lab is to introduce a new Python library that we will be using throughout this course: PyTorch. PyTorch provides automatic differentiation capabilities and other neural network tools. This means that we do not need to compute gradients ourselves! Instead, we rely on PyTorch to build the computation graph and compute gradients. PyTorch can do this because it knows how to compute gradients for simple operations like addition, multiplication, ReLU activation, and common functions like exponentials, logarithms, and so on. The neural networks we build require computation that are combinations of these simple operations.\nFor now, we will we solve a multi-class classification problem in two ways: first with numpy, and then with PyTorch.\nBy the end of this lab, you will be able to:\nPlease work in groups of 1-2 during the lab, but submit your own solution individually."
  },
  {
    "objectID": "labs/lab01.html#submission",
    "href": "labs/lab01.html#submission",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab01.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your explanation of the purpose of the training and validation sets. (1 point)\nPart 2. Your implementation of accuracy_basic. (2 point)\nPart 2. Your implementation of accuracy_vectorized. (1 point)\nPart 2. Your implementation of accuracy. (1 point)\nPart 3. Your computation of model_bias_grad. (2 points)\nPart 4. Your completion of train_model. (1 point)\nPart 5. Your list of hyperparameters. (1 point)\nPart 5. Your explanation of what happens if the learning rate is too large. (1 point)"
  },
  {
    "objectID": "labs/lab01.html#google-colab-setup",
    "href": "labs/lab01.html#google-colab-setup",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Google Colab Setup",
    "text": "Google Colab Setup\nWe will use Google Colab to open IPython Notebook (ipynb) file. This tool allows us to write and execute Python code through our browser, without any environmental setup.\nHere are the steps to open ipynb file on Google Colab.\n\nDownload lab01.ipynb, available from the Quercus course website.\nClick on the following link to open Google Colab: https://colab.research.google.com/\nClick “Upload”, then choose the file which has been downloaded in step 1.\n\nAnd that’s it! Now we can start writing the codes, creating the new code or text cell, etc.\nHere are some basic functionalities and features that you might find useful.\n\nRunning a cell\nClick the run button on the left side of the code cell (looks like a “play” button with a triangle in a circle)\nor\npress SHIFT + ENTER.\nInstalling libraries using Bash Commands\nAlthough most of the commonly used libraries (e.g. NumPy, Pandas, Matplotlib) are pre-installed, we may occasionally ask you to install new libraries or run other bash commands. Bash commands can be run by prefixing instructions in a code cell with ‘!’ in Google Colab (One exception: ‘cd’ command can be run by prefixing with ‘%’), e.g. !pip install [package name]\nMounting Google Drive\nYou may optionally mount Google Drive. Click the files button on the left pane, then click on ‘mount drive’ button (looks like a file icon with a google drive logo).\nor\nRun the following code snippet: from google.colab import drive     drive.mount('/content/drive') By mounting the drive, we can use any files or folders in our drive by using the path as follows: /content/drive/MyDrive/[folder name] For example, we can read the csv file uploaded in the drive using Pandas library as follows: pd.read_csv('/content/drive/MyDrive/myfolder/myfile.csv')\n\nNow, we are ready to import the necessary packages and begin our lab.\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "labs/lab01.html#part-1.-data",
    "href": "labs/lab01.html#part-1.-data",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nWe will use the MNIST data set, which consists of hand-written digits. This dataset is available within the torchvision.datasets library. The dataset creators divided the MNIST imgages into a training and test set, so that different researchers report test accuracy on a consistent set of images. (Recall that the test set is to be set aside and not used during training or to make any model decisions, and that it is used to estimate how well your models generalize to new data that it has never seen before.)\n\nfrom torchvision.datasets import MNIST\n\nmnist_train = MNIST(root=\".\",      # where on the disk to store the data\n                    download=True, # download the data if it does not already exist\n                    train=True)    # use the training set (rather than the test set)\n\nTask: If different practitioners are exploring machine learning models for the same task and data set, why is it important that they use these practitioners report their test performance (e.g., accuracy) on the same test set?\n\n# TODO: Write your answer here\n\nIt is always a good idea to visually inspect our data before working with it. First, let’s take a look at the first element of the training set:\n\nprint(mnist_train[0]) # a tuple consisting of the image, and the label (5)\n\nThe image can be displayed on Google Colab using matplotlib:\n\nplt.imshow(mnist_train[0][0], cmap='gray') # display the image\n\nIt is important to note that images are represented using numbers on your machine. Converting this image into a numpy array shows a representation of the image using 28x28 numbers, each representing a pixel value.\n\nnp.array(mnist_train[0][0])\n\nTask: What does the numerical value 0 (smallest possible value) mean in the image? What about the largest possible value, 255?\n\n# TODO: Write your answer here\n\nFor our purposes, we will only use the first 5000 elements of the training set. This is to make training faster.\nPyTorch also makes it easy to apply pre-processing transformations to the data, for example to normalize the data prior to using for training. We will use the standard preprocessing functions to transform the images into tensors for PyTorch to be able to use. This transformation also changes the values to be floating-point numbers between 0 and 1. Performing this transformation to PyTorch tensors now makes it easier to use PyTorch functionalities to help us create minibatches with this data.\n\nimport torchvision.transforms as transforms\n\nmnist_data = MNIST(root=\".\",      # where on the disk to store the data\n                   download=True, # download the data if it does not already exist\n                   train=True,    # use the canonical training set (rather than the test set)\n                   transform=transforms.ToTensor()) # transforms the images into PyTorch tensors\nmnist_data = list(mnist_data)[:5000]\n\nprint(mnist_data[0]) # a tuple consisting of a PyTorch tensor of shape (1, 28, 28) and an integer target label\n\nWe will split the data set into 3000 for training, 1000 for validation, and 1000 for test:\n\ntrain_data = mnist_data[:3000]\nval_data   = mnist_data[3000:4000]\ntest_data  = mnist_data[4000:]\n\nGraded Task: We described, above, that the purpose of the test set is to estimate how well our models would generalize to new data that it has never seen before. What are the purposes of the training and validation sets?\n\n# TODO: Write your answer here"
  },
  {
    "objectID": "labs/lab01.html#part-2.-a-linear-model-in-pytorch",
    "href": "labs/lab01.html#part-2.-a-linear-model-in-pytorch",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 2. A Linear Model in PyTorch",
    "text": "Part 2. A Linear Model in PyTorch\nYou may recall from CSC311 that in machine learning, we often describe a model by first describing how to make predictions with a model (e.g., multi-class classification), and then describe how to find appropriate weights (e.g., an optimization method like gradient descent). We will follow that process here.\nRecall that, mathematically, the multi-class classification model can be written as follows:\n\\[{\\bf y} = \\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})\\]\nWhere the \\[{\\bf x}\\] vector represents the input (i.e., the vector representation of the MNIST image), and the \\[{\\bf y}\\] vector contains the predicted probably of the image being in each class (i.e., predicted probability of the image being of each digit 0-9).\nTask: In the MNIST image classification tas, what are the shapes of the quantities \\[{\\bf x}\\], \\[{\\bf W}\\], \\[{\\bf b}\\] and \\[{\\bf y}\\]?\n\n# TODO: Write your answer here\n\nThe matrix \\[{\\bf W}\\] and \\[{\\bf b}\\] are the parameters of the model. The matrix \\[{\\bf W}\\] is sometimes called the weight matrix and the vector \\[{\\bf b}\\] the bias vector, but these parameters taken together are often referred to collectively as the weights. “Good” values of the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are those that would produce values of the vector \\[{\\bf y}\\] that more accurately match the actual target label. We will need to discuss what “good” means and how to measure “good”ness and optimize it. For now, let’s begin by applying and analyzing a “bad” model: a model where the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are chosen randomly.\nNotice that there are two parts to the computation \\[\\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})\\]: there is a linear transformation on \\[{\\bf X}\\], and then there is a softmax operation. PyTorch models these two components separately.\nThe linear transformation is modeled as a Python class in PyTorch. Since the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are parameters of the linear transformation portion of the computation, the weights and biases are class attributes. The output of the linear transformation step is typically denoted using the symbol \\[{\\bf z}\\], and is called the logit (or unnormalized logits).\n\nimport torch.nn as nn\n\nmodel = nn.Linear(in_features=784,\n                  out_features=10)\n\nprint(model.weight) # the weight parameter, initialized to random values\nprint(model.bias)   # the bias parameter, initialized to random values\n\nThis model object can be called like a function to perform the computation \\[{\\bf W}{\\bf x} + {\\bf b}\\]:\n\n# reshape the input image into the shape [1, 784]\n# PyTorch always expects inputs of shape [batch_size, num_features]\nx = train_data[0][0].reshape(1, 784)\n\n# Computes z = Wx + b, also called the *logit*\nz = model(x)\nprint(z)\n\nThe softmax operation has no trainable parameters (i.e., no numbers that we tune that would effect the predictions of our model). Torch ahs a function torch.softmax that performs this operation, which normalizes the prediction so that the prediction represents a probability distribution. The dim parameter to the function tells PyTorch which dimension represent the different label classes (as opposed to, say, the dimension that represents different images in a batch).\n\ny = torch.softmax(z, dim=1)\nprint(y) # notice that this represents a probability distribution!\n\nIf we are looking for a discrete/point prediction rather than a probability distribution, we will typically choose the label that the model believes to be most probable:\n\npred = torch.argmax(y, axis=1)\nprint(pred) # a prediction of which class/digit the image belong to\n\nNote that if all we wanted was a discrete prediction, we need not compute the softmax! (Why is that? How can we prove this property mathematically, using the definition of the softmax operation?)\n\npred = torch.argmax(z, axis=1)\nprint(pred)\n\nGraded Task: Complete the function below, which computes the accuracy of a PyTorch model over a dataset. The accuracy metric is the proportion of predictions made that is correct, or:\n\\[\\frac{\\textrm{the number of correct predictions}}{\\textrm{total number of predictions made}}\\]\n\ndef accuracy_basic(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A torch.nn model. We will only be passing `nn.Linear` models.\n                  However, to make your code more generally useful, do not access\n                  `model.weight` and `model.bias` parameters directly. These\n                  class attributes may not exist for other kinds of models.\n        `dataset` - A list of 2-tuples of the form (x, t), where `x` is a PyTorch\n                  tensor of shape [1, 28, 28] representing an MNIST image,\n                  and `t` is the corresponding target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    total = 0      # count the total number of predictions made\n    correct = 0    # count the number of correct predictions made\n    for img, t in dataset:\n        x = None # TODO - what should the input be? Recall that\n        z = None # TODO - how can we compute z = Wx + b using `model`?\n        pred = None # TODO - how can we obtain a point prediction?\n        if t == pred:\n            correct += 1\n        total += 1\n    return correct / total\n\n\nprint(\"Accuracy over the training set:\")\nprint(accuracy_basic(model, train_data))\n\nTask: Explain why we would expect the training accuracy above to be poor.\n\n# TODO: Your explanation goes here.\n\nOne other nice thing about our nn.Linear model is that PyTorch vectorizes computations for us: we can make predictions for many images at the same time. Here is a rudimentary example where we make predictions for the first three images of our training set.\n\nx1 = train_data[0][0].reshape(1, 784)\nx2 = train_data[1][0].reshape(1, 784)\nx3 = train_data[2][0].reshape(1, 784)\n\nX = torch.cat([x1, x2, x3]).reshape(-1, 784) # note the -1 value here, explained below\nprint(X.shape) # Pytorch figures out that the shape of this tensor needs to be [3, 784]\n\nThe above code uses a features of PyTorch’s reshape() method that allows you to use the size -1 as a placeholder value and let PyTorch figure out what the correct size should be.\nNow, we can make predictions for all 3 images simultaneously\n\nz = model(X)\ny = torch.softmax(z, dim=1)\nprint(y)\n\nTask Complete the function accuracy_vectorized that outputs the same result as accuracy_basic, but uses vectorization to compute predictions for all inputs in the dataset simultaneously.\n\ndef accuracy_vectorized(model, dataset):\n    \"\"\"\n    Same signature as the `accuracy_basic()` function, but the call to model() is vectorized\n    \"\"\"\n    X = torch.concat([x for x, t in dataset])\n    t = torch.Tensor([t for x, t in dataset])\n    z = None # TODO: use a single call to model() to compute the prediction for all inputs\n    pred = None # TODO: `pred` should have the same shape as `t`\n    correct = int(torch.sum(t == pred)) # count the number of correct predictions\n    total = t.shape[0]                  # count the total number of predictions made\n    return correct / total\n\nTask: Compare the runtime of accuracy_basic and accuracy_vectorized by running the two cells below. The line %%time prints the amount of time that Colab takes to run the code in the cell. The function call is repeated 100 times so that we can more clearly see the difference in runtime. Using what you learned from CSC311, explain why accuracy_vectorized is faster.\n\nimport time\n\nstart_time = time.time()\nfor i in range(100):\n    accuracy_basic(model, train_data)\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n\n\nimport time\n\nstart_time = time.time()\nfor i in range(100):\n    accuracy_vectorized(model, train_data)\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n\n\n# TODO: Your explanation goes here\n\nIf our data set is large, feeding all inputs into the model at the same time may result in an out of memory error. Thus, we may find PyTorch’s DataLoader useful. This class takes our data set and the desired batch size, and splits the data into mini-batches of that size. We can use a loop to iterate over the minibatches:\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=100)\nfor X, t in train_loader:\n    print(X.shape)\n    print(t.shape)\n    break\n\nTask: Complete the definition of the accuracy function below:\n\ndef accuracy(model, dataset):\n    \"\"\"\n    Same signature as the `accuracy_basic()` function, but we will use a DataLoader and process\n    100 images at a time\n    \"\"\"\n    correct, total = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for X, t in loader:\n        z = None # TODO: use a single call to `model()` here as before\n        pred = None # TODO: `pred` should have the same shape as `t`\n        # TODO: update `correct` and `total`\n    return correct / total\n\naccuracy(model, train_data)"
  },
  {
    "objectID": "labs/lab01.html#part-3.-cross-entropy-loss-and-automatic-gradient-computation",
    "href": "labs/lab01.html#part-3.-cross-entropy-loss-and-automatic-gradient-computation",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 3. Cross Entropy Loss and Automatic Gradient Computation",
    "text": "Part 3. Cross Entropy Loss and Automatic Gradient Computation\nNow that we understand how a linear model makes predictions, we can explore how to modify its parameters to produce a model that makes “better” predictions. To do this, we will define a measure of “good”ness (or rather, “bad”ness) of a model that is differentiable with respect to the parameters. Differentiability is important, because it allows us to compute derivatives with respect to these parameters, which tells us how to tune the parameters to decrease “bad”ness.\nThis “badness” metric is called a loss function. A loss function compares the model prediction against the ground-truth target and produces a value representing how different the prediction is from the target. Like in CSC311, we will use the Cross-Entropy Loss for multi-class classification. In statistics courses you may learn about the theoretical reasons why the cross-entropy loss is appropriate for the multi-class classification task.\n\\[\\mathcal{L}(y, t) = - t \\log(y) - (1-t) \\log(1-y)\\]\nYou can read more about PyTorch’s implementation of the Cross-entropy loss here: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html One key thing to note is that the cross entropy loss takes as input the unnormalized logits (the z’s) and not the post-softmax, normalized probabilities (the y’s).\n\ncriterion = nn.CrossEntropyLoss()\n\nBefore we go further, let’s get some more intuition about the cross-entropy loss. We first demonstrate the cross entropy loss in action by computing hte “badness”\n\nx = train_data[0][0].reshape(1, 784)        # input\nt = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\nz = model(x.reshape(-1, 784))               # prediction logit\n\nloss = criterion(z, t)\n\nprint(loss)\n\nGraded Task: Consider the code below. What value of label would produce the lowest cross-entropy loss? Why? To answer this question, start by exploring the arguments passed to the call to criterion, and form an understanding of what those arugments represent. (Why are there 3 possible values of label? What do the 3 floating-point values in the first argument to criterion represent?)\n\nlabel = None # 0, 1, or 2?\n\nloss = criterion(torch.Tensor([[1.5, 2.2, 3.2]]),\n                 torch.Tensor([label]).long())\nprint(loss)\n\n\n# TODO: Explain why the label choice produces the lowest loss.\n\nNow that we have an understanding of the cross-entropy loss, we can begin to understand how PyTorch computes gradients. Notice that when we print the variable loss above, the value printed is not only a numerical value, but also has other information attached. These information help PyTorch compute gradients, and these gradients can be propagated backwards using the loss.backward() method.\nUnder the hood, PyTorch performs backpropagation, which we discussed in CSC311 and will review again in the coming weeks. After loss.backward() is computed, tensors like model.bias and model.weights will have gradients.\n\n# recreate the model to clean up some hidden variables\nmodel = nn.Linear(in_features=784, out_features=10)\n\nx = train_data[0][0].reshape(1, 784)        # input\nt = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\nz = model(x.reshape(-1, 784))               # prediction logit\n\nloss = criterion(z, t)\nprint(loss)\n\nloss.backward() # propagate the gradients with respect to the loss\n\n\nprint(model.bias.grad) # the gradient of the loss with respect to the bias vector\n\n\nprint(model.weight.grad) # the gradient of the loss with respect to the weight matrix\n\nGraded Task: Verify that model.bias.grad is correct by computing this gradient explicitly in PyTorch. You may wish to begin by reviewing your past CSC311 notes, or by using calculus to find an expression to represent this quantity.\n\nt_onehot = torch.eye(10)[t] # You may find this useful. (Why? What does this quantity represent)\nmodel_bias_grad = None      # TODO\n\nprint(model_bias_grad) # should be the same as model.bias.grad\n\nThe gradient computation works in a vectorized setting as well.\n\n# create a data set with 3 inputs, 3 targets\n\nx1 = train_data[0][0].reshape(1, 784)\nx2 = train_data[1][0].reshape(1, 784)\nx3 = train_data[1][0].reshape(1, 784)\nX = torch.cat([x1, x2, x3]).reshape(-1, 784)\nt = torch.Tensor([train_data[0][1], train_data[1][1], train_data[2][1]]).long()\n\nprint(X.shape)\nprint(t)\n\nThe average loss (mean) across the data points are shown:\n\nmodel = nn.Linear(in_features=784, out_features=10)\nz = model(X)\nloss = criterion(z, t)\nprint(loss)\n\nloss.backward() # propagate the gradients with respect to the loss\n\nGraded Task: Verify that model.bias.grad is correct when using vectorized input by computing this gradient explicitly in PyTorch. Again, we recommend working this out by hand first. After you have done so, you may find the function torch.sum() or torch.mean() helpful.\n\nt_onehot = torch.eye(10)[t] # TODO: understand the shape of this quantity before using it\nmodel_bias_grad = None      # TODO\n\nprint(model_bias_grad)\nprint(model.bias.grad) # should be the same as above"
  },
  {
    "objectID": "labs/lab01.html#part-4.-neural-network-training-via-stochastic-gradient-descent.",
    "href": "labs/lab01.html#part-4.-neural-network-training-via-stochastic-gradient-descent.",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 4. Neural Network Training via Stochastic Gradient Descent.",
    "text": "Part 4. Neural Network Training via Stochastic Gradient Descent.\nWe are almost ready to use PyTorch to train the model. One last piece that we need is an optimizer that updates the model parameter based on the gradient. This update can be done in different ways, and the most basic approach discussed in CSC311 was using gradient descent.\n\\[{\\bf W} \\leftarrow {\\bf W} - \\lambda \\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}}\\]\nTODO: description of gradient descent here!!\nWhen we use the entire training data set to compute the gradient of the mean loss (with respect to each parameter), we call the approach full batch gradient descent. However, this approach is expensive if we have a large training set. Typically, we approximate this mean loss using a minibatch, or a small sample of the training set. Using gradient descent with this approximate gradient is called stochastic gradient descent.\nPyTorch has built-in classes inside the package torch.optim to perform these gradient update steps. We will use the SGD class. Initializing this class requires a few things, including the list of model parameters that we want to optimize. For us, the list of parameters to optimize include model.weight and model.bias, which we can obtain by calling model.parameters().\n\nimport torch.optim as optim\n\noptimizer = optim.SGD(model.parameters(), # the parameters to optimize\n                      lr=0.005)           # the learning rate\n\nThere are two important optimizer methods that we will use. First is the optimizer.zero_grad() method, which clears the .grad attribute of the parameters. Let’s see how it works:\n\nprint(model.bias.grad) # should be a nonzero value from above\n\noptimizer.zero_grad()\n\nprint(model.bias.grad) # should be cleared\n\nThe other method is the step() method, which performs the actual gradient descent update.\n\nmodel = nn.Linear(in_features=784, out_features=10)\noptimizer = optim.SGD(model.parameters(), lr=0.005)\n\nprint(model.bias)\n\nz = model(X)\nloss = criterion(z, t)\nloss.backward()\n\nprint(model.bias.grad) # gradient\nprint(model.bias - 0.005 * model.bias.grad) # this should be the updated value of model.bias\n\noptimizer.step()\n\nprint(model.bias) # should be different compared to above\n\nNow that we have everything in place, we are ready to write the training loop:\nGraded Task: Complete the function below, which trains the model.\n\ndef train_model(model,\n                train_data,\n                val_data,\n                learning_rate=0.005,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=10):\n    \"\"\"\n    Train the PyTorch model `model` using the training data `train_data` and the\n    corresponding hyperparameters. Report training loss, training accuracy, and\n    validation accuracy every `plot_every` iterations.\n    \"\"\"\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (images, labels) in enumerate(train_loader):\n                z = None # TODO\n\n                loss = None # TODO\n\n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    train_loss.append(float(loss))\n                    train_acc.append(accuracy(model, train_data))\n                    val_acc.append(accuracy(model, val_data))\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel = nn.Linear(784, 10)\ntrain_model(model, train_data, val_data)"
  },
  {
    "objectID": "labs/lab01.html#part-5.-hyperparameter-tuning",
    "href": "labs/lab01.html#part-5.-hyperparameter-tuning",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 5. Hyperparameter Tuning",
    "text": "Part 5. Hyperparameter Tuning\nOur training process is not yet complete. In general, the performance of machine learning models depend heavily on the hyperparameter settings used. Hyperparameters are settings that cannot be tuned via gradient descent in a straightforward way. These settings can affect the model architecture, but may also affect the optimization process.\nGraded Task: What are some examples of hyperparameters that affect the model architecture of a model? You may consider some hyperparameters that you learned about in CSC311. List at least 3 examples.\n\n# TODO: List at least 3 examples\n\nTask: What are some examples of hyperparameters that affect the optimization process?\n\n# TODO: List at least 2 examples\n\nModel architecture related hyperparameters are important to tune and should not be neglected in practical application. However, since we are working with a linear model right now, we are limited in this lab to exploring the optimization hyperparameters.\nTask: What happens if the learning rate is too small? Provide an example training curve by calling train_model with a low learning rate, and describe the features of the training curve that you see.\n\n# TODO\n\nGraded Task: What happens if the learning rate is too large? Provide an example training curve by calling train_model with a large learning rate, and describe the features of the training curve that you see.\n\n# TODO\n\nHyperparameter choices interact with one another. Thus, practitioners use a strategy called grid search to try all variations of hyperparameters from a set of hyperparameters. We will not do that here since linear models don’t yet have many hyperparameters to work with.\nTask: Choose the best model that you have trained. Typically we make this choice using the validation accuracy. To understand how well the model you choose would generalize to unseen data, we use the test data. Compute the test accuracy for this model by calling the function accuracy() on the model and the test data.\n\n# TODO: Compute the test accuracy"
  },
  {
    "objectID": "labs/lab07.html",
    "href": "labs/lab07.html",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "",
    "text": "Transfer learning is a technique where we use neural network weights trained to complete one task to complet a different task. In this tutorial, we will go through an example of transfer learning to detect American Sign Language (ASL) gestures letters A-I. Although we could train a CNN from scratch, you will see that using CNN weights that are pretrained on a larger dataset and more complex task provides much better results, all with less training.\nAmerican Sign Language (ASL) is a complete, complex language that employs signs made by moving the hands combined with facial expressions and postures of the body. It is the primary language of many North Americans who are deaf and is one of several communication options used by people who are deaf or hard-of-hearing.\nThe hand gestures representing English alphabets are shown below. This lab focuses on classifying a subset of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand showing one of the letters A-I, we want to detect which letter is being represented.\nBy the end of this lab, you will be able to:\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab07.html#submission",
    "href": "labs/lab07.html#submission",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab07.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your answer to the question about the splitting of the data into train/validation/test sets. (1 point)\nPart 2. Your comparison of the CNN model with and without batch normalization. (1 point)\nPart 2. Your comparison of BatchNorm1d vs BatchNorm2d. (1 point)\nPart 2. Your analysis of the effect of varying the CNN model width. (1 point)\nPart 2. Your analysis of the effect of varying weight decay parameter. (1 point)\nPart 2. Your analysis of the training curve that illustrates double descent. (1 point)\nPart 3. Your implementation of LinearModel for transfer learning. (1 point)\nPart 3. Your comparison of transfer learning vs the CNN model. (1 point)\nPart 4. Your analysis of the confusion matrix. (1 point)\nPart 4. Your explanation for how to mitigate an issue we notice by visually inspecting misclassified images. (1 point)\n\n\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models, torchvision.datasets\n\n%matplotlib inline"
  },
  {
    "objectID": "labs/lab07.html#part-1.-data",
    "href": "labs/lab07.html#part-1.-data",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nWe will begin by downloading the data onto Google Colab.\n\n# Download lab data file\n!wget https://www.cs.toronto.edu/~lczhang/413/asl_data.zip\n!unzip asl_data.zip\n\nThe file structure we use is intentional, so that we can use torchvision.datasets.ImageFolder to help load our data and create labels.\nYou can read what torchvision.datasets.ImageFolder does for us here https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\n\ntrain_path = \"asl_data/train/\" # edit me\nvalid_path = \"asl_data/valid/\" # edit me\ntest_path = \"asl_data/test/\"   # edit me\n\ntrain_data = torchvision.datasets.ImageFolder(train_path, transform=torchvision.transforms.ToTensor())\nvalid_data = torchvision.datasets.ImageFolder(valid_path, transform=torchvision.transforms.ToTensor())\ntest_data = torchvision.datasets.ImageFolder(test_path, transform=torchvision.transforms.ToTensor())\n\nAs in previous labs, we can iterate through the one training data point at a time like this:\n\nfor x, t in train_data:\n    print(x, t)\n    plt.imshow(x.transpose(2, 0).transpose(0, 1).numpy()) # display an image\n    break # uncomment if you'd like\n\nTask: What do the variables x and t contain? What is the shape of our images? What are our labels? Based on what you learned in Part (a), how were the labels generated from the folder structure?\n\n# Your explanation goes here\n\nWe saw in the earlier tutorials that PyTorch has a utility to help us creat minibatches with our data. We can use the same DataLoader helper here:\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)\n\nfor x, t in train_loader:\n    print(x, t)\n    break # uncomment if you'd like\n\nTask: What do the variables x and t contain? What are their shapes? What data do they contain?\n\n# Your explanation goes here\n\nTask: How many images are there in the training, validation, and test sets?\n\n# Your explanation goes here\n\nNotice that there are fewer images in the training set, compared to the validation and test sets. This is so that we can explore the effect of having a limited training set.\nGraded Task: The data set is generated by students taking pictures of their hand while making the corresponding gestures. We therefor split the training, validation, and test sets were split so that images generated by a student all belongs in a single data set. In other words, we avoid cases where some students’ images are in the training set and others end up in the test set. Why do you think this important for obtaining a representative test accuracy?\n\n# Your explanation goes here"
  },
  {
    "objectID": "labs/lab07.html#part-2.-training-a-cnn-model",
    "href": "labs/lab07.html#part-2.-training-a-cnn-model",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 2. Training a CNN Model",
    "text": "Part 2. Training a CNN Model\nFor this part, we will be working with this CNN network.\n\nclass CNN(nn.Module):\n    def __init__(self, width=4, bn=True):\n        \"\"\"\n        A 4-layer convolutional neural network. The first layer has\n        `width` number of channels, and with each layer we half the\n        feature width/height and double the number of channels.\n\n        If `bn` is set to False, then batch normalization will not run.\n        \"\"\"\n        super(CNN, self).__init__()\n        self.width = width\n        self.bn = bn\n        # define all the conv layers\n        self.conv1 = nn.Conv2d(in_channels=3,\n                               out_channels=self.width,\n                               kernel_size=3,\n                               padding=1)\n        self.conv2 = nn.Conv2d(in_channels=self.width,\n                               out_channels=self.width*2,\n                               kernel_size=3,\n                               padding=1)\n        self.conv3 = nn.Conv2d(in_channels=self.width*2,\n                               out_channels=self.width*4,\n                               kernel_size=3,\n                               padding=1)\n        self.conv4 = nn.Conv2d(in_channels=self.width*4,\n                               out_channels=self.width*8,\n                               kernel_size=3,\n                               padding=1)\n        # define all the BN layers\n        if bn:\n            self.bn1 = nn.BatchNorm2d(self.width)\n            self.bn2 = nn.BatchNorm2d(self.width*2)\n            self.bn3 = nn.BatchNorm2d(self.width*4)\n            self.bn4 = nn.BatchNorm2d(self.width*8)\n        # pooling layer has no parameter, so one such layer\n        # can be shared across all conv layers\n        self.pool = nn.MaxPool2d(2, 2)\n        # FC layers\n        self.fc1 = nn.Linear(self.width * 8 * 14 * 14, 100)\n        self.fc2 = nn.Linear(100, 9)\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        if self.bn:\n            x = self.bn1(x)\n        x = self.pool(torch.relu(self.conv2(x)))\n        if self.bn:\n            x = self.bn2(x)\n        x = self.pool(torch.relu(self.conv3(x)))\n        if self.bn:\n            x = self.bn3(x)\n        x = self.pool(torch.relu(self.conv4(x)))\n        if self.bn:\n            x = self.bn4(x)\n        x = x.view(-1, self.width * 8 * 14 * 14)\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\nTask:\nThe training code is written for you. Train the CNN() model for at least 6 epochs, and report on the maximum validation accuracy that you can attain.\nAs your model is training, you might want to move on to the next question.\n\ndef get_accuracy(model, data, device=\"cpu\"):\n    loader = torch.utils.data.DataLoader(data, batch_size=256)\n    model.to(device)\n    model.eval() # annotate model for evaluation (important for batch normalization)\n    correct = 0\n    total = 0\n    for imgs, labels in loader:\n        output = model(imgs.to(device))\n        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(labels.view_as(pred)).sum().item()\n        total += imgs.shape[0]\n    return correct / total\n\ndef train_model(model,\n                train_data,\n                valid_data,\n                batch_size=64,\n                weight_decay=0.0,\n                learning_rate=0.001,\n                num_epochs=50,\n                plot_every=20,\n                plot=True,\n                device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True)\n    model = model.to(device) # move model to GPU if applicable\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(),\n                           lr=learning_rate,\n                           weight_decay=weight_decay)\n    # for plotting\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for epoch in range(num_epochs):\n            for imgs, labels in iter(train_loader):\n                if imgs.size()[0] &lt; batch_size:\n                    continue\n\n                model.train()\n                out = model(imgs)\n                loss = criterion(out, labels)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    loss = float(loss)\n                    tacc = get_accuracy(model, train_data, device)\n                    vacc = get_accuracy(model, valid_data, device)\n                    print(\"Iter %d; Loss %f; Train Acc %.3f; Val Acc %.3f\" % (iter_count, loss, tacc, vacc))\n\n                    iters.append(iter_count)\n                    train_loss.append(loss)\n                    train_acc.append(tacc)\n                    val_acc.append(vacc)\n    finally:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend([\"Train\", \"Validation\"])\n\nTask: Run the training code below. What validation accuracy can be achieved by this CNN?\n\ncnn = CNN(width=4)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)"
  },
  {
    "objectID": "labs/lab07.html#part-2.-model-architecture-biasvariance-and-double-descent",
    "href": "labs/lab07.html#part-2.-model-architecture-biasvariance-and-double-descent",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 2. Model Architecture, Bias/Variance and Double Descent",
    "text": "Part 2. Model Architecture, Bias/Variance and Double Descent\nIn this section, we will explore the effect of various aspects of a CNN model architecture. We will pay particluar attention to architecture decisions that affect the bias and variance of the model. Finally, we explore a phenomenon called double descent.\nTo begin, let’s explore the effect of batch normalization.\nTask: Run the training code below to explore the effect of training without batch normalization.\n\ncnn = CNN(bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\n\nGraded Task: Compare the two sets of training curves above for the CNN model with and without batch normalization. What is the effect of batch normalization on the training loss and accuracy? What about the validation accuracy?\n\n# TODO: Include your analysis here\n\nGraded Task: We used the layer called BatchNorm2d in our CNN. What do you think is the difference between BatchNorm2d and BatchNorm1d? Why are we using BatchNorm2d in our CNN? Why would we use BatchNorm1d in an MLP? You may wish to consult the PyTorch documentation. (How can you find it?)\n\n# Explain your answer here\n\nTask: Run the training code below to explore the effect of varying the model width for this particular data set.\n\ncnn = CNN(width=2, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\n\n\ncnn = CNN(width=4, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\n\n\ncnn = CNN(width=16, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\n\nGraded Task: What is the effect of varying the model width above for this particular data set? Do you notice an effect on the training loss? What about the training/validation accuracy? The final validation accuracy? (Your answer may or may not match your expectations. Please answer based on the actual results above.)\n\n# TODO: Include your analysis here\n\nTask: Run the training code below to explore the effect of weight decay when training a large model.\n\ncnn = CNN(width=16, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)\n\n\ncnn = CNN(width=16, bn=True) # try with batch norm on\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)\n\n\ncnn = CNN(width=16, bn=True) # try decreasing weight decay parameter\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.0001)\n\nGraded Task: What is the effect of setting weight decay to the above value? Do you notice an effect on the training loss? What about the training/validation accuracy? The final validation accuracy? (Again, your answer may or may not match your expectations. Please answer based on the actual results above.)\n\n# TODO: Include your analysis here\n\nTask: Note that there is quite a bit of noise in the results that we might obtain above. That is, if you run the same code twice, you may obtain different answers. Why might that be? What are two sources of noise/randomness?\n\n# TODO: Include your explanation here\n\nThese settings that we have been exporting are hyperparameters that should be tuned when you train a neural network. These hyperparameters interact with one another, and thus we should tune them using the grid search strategy mentioned in previous labs.\nYou are not required to perform grid search for this lab, so that we can explore a few other phenomena.\nOne interesting phenomenon is called double descent. In statistical learning theory, we expect validation error to decrease with increase model capacity, and then increase as the model overfits to the number of data points available for training. In practise, in neural networks, we often see that as model capacity increases, validation error first decreases, then increase, and then decrease again—hence the name “double descent”.\nIn fact, the increase in validation error is actually quite subtle. However, what is readily apparent is that in most cases, increasing model capacity does not result in a decrease in validation accuracy.\nOptional Task: To illustrate that validation accuracy is unlikely to decrease with increased model parameter, train the below network.\n\n# Uncomment to run. \n# cnn = CNN(width=40, bn=True)\n# train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=50)\n\nDouble descent is actually not that mysterious. It comes from the fact that when capacity is large enough there are many parameter choices that achieves 100% training accuracy, the neural network optimization procedure is effectively choosing a best parameters out of the many that can achieve this perfect training accuracy. This differs from when capacity is low, where the optimization process needs to find a set of parameter choices that best fits the training data—since no choice of parameters fits the training data perfectly. When the capacity is just large enough to be able to find parameters that fit the data, but too small for there be a range of parameter choices available to be able to select a “best” one.\nThis twitter thread written by biostatistics professor Daniela Witten also provides an intuitive explanation, using polynomial curve fitting as an example: https://twitter.com/daniela_witten/status/1292293102103748609\nDouble descent explored in depth in this paper here: https://openreview.net/pdf?id=B1g5sA4twr This paper highlights that the increase in validation/test error occurs when the training accuracy approximates 100%. Moreover, the double descent phenomena is noticable when varying model capacity (e.g. number of parameters) and when varying the number of iterations/epochs of training.\nWe will attempt to explore the latter effect—i.e. we will train a large model, use a small numer of training data points, and explore how each iteration of training impacts validation accuracy. The effect is subtle and, depending on your neural network initialization, you may not see an effect. So, a training curve is also provided for you to analyze.\nOptional Task: Run the code below to try and reproduce the “double descent” phenomena. This code will take a while to run, so you may wish to continue with the remaining questions while it runs.\n\n# use a subset of the training data\n# uncomment to train\n\n# train_data_subset, _ =  random_split(train_data, [50, len(train_data)-50])\n# cnn = CNN(width=20)\n# train_model(cnn,\n#             train_data_subset,\n#             valid_data,\n#             batch_size=50, # set batch_size=len(train_data_subset) to minimize training noise\n#             num_epochs=200,\n#             plot_every=1,  # plot every epoch (this is slow)\n#             learning_rate=0.0001)  # choose a low learning rate\n\nFor reference, here is the our training curve showing the loss and accuracy over 200 iterations:\n \nWe are not able to consistently reproduce this result (e.g., due to initialization), so it is totally reasonable for your figure to look different!\nTask: In the provided training curve, during which iterations do the validation accuracy initially increase (i.e. validation error decrease)?\n\n# TODO: Include your answer here\n\nGraded Task: In the provided training curve, during which iterations do the validation accuracy decrease slightly? Approximately what training accuracy is achieved at this piont?\n\n# TODO: Include your answer here\n\nTask: In the provided training curve, during which iterations do the validation accuracy increase for a second time (i.e. validation error descends for a second time)?\n\n# TODO: Include your answer here"
  },
  {
    "objectID": "labs/lab07.html#part-3.-transfer-learning",
    "href": "labs/lab07.html#part-3.-transfer-learning",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 3. Transfer Learning",
    "text": "Part 3. Transfer Learning\nFor many image classification tasks, it is generally not a good idea to train a very large deep neural network model from scratch due to the enormous compute requirements and lack of sufficient amounts of training data.\nA better option is to try using an existing model that performs a similar task to the one you need to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed Transfer Learning. In this assignment, we will use Transfer Learning to extract features from the hand gesture images. Then, train a smaller network to use these features as input and classify the hand gestures.\nAs you have learned from the CNN lecture, convolution layers extract various features from the images which get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal role in establishing Deep Neural Nets as a go-to tool for image classification problems and we will use an ImageNet pre-trained AlexNet model to extract features in this assignment.\nHere is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch will download the pretrained weights from the internet.\n\nimport torchvision.models\nalexnet = torchvision.models.alexnet(pretrained=True)\n\nprint(alexnet)\n\nAs you can see, the alexnet model is split up into two components: alexnet.features and alexnet.classifier. The first neural network component, alexnet.features, is used to computed convolutional features, which is taken as input in alexnet.classifier.\nThe neural network alexnet.features expects an image tensor of shape Nx3x224x224 as inputs and it will output a tensor of shape Nx256x6x6 . (N = batch size).\nHere is an example code snippet showing how you can compute the AlexNet features for some images (your actual code might be different):\n\nimg, label = train_data[0]\nfeatures = alexnet.features(img.unsqueeze(0)).detach()\n\nprint(features.shape)\n\nNote that the .detach() at the end will be necessary in your code. The reason is that PyTorch automatically builds computation graphs to be able to backpropagate graidents. If we did not explicitly “detach” this tensor from the AlexNet portion of the computation graph, PyTorch might try to backpropagate gradients to the AlexNet weight and tune the AlexNet weights.\nTODO Compute the AlexNet features for each of your training, validation, and test data by completing the function compute_features. The code below creates three new arrays called train_data_fets, valid_data_fets and test_data_fets. Each of these arrays contains tuples of the form (alexnet_features, label).\n\ndef compute_features(data):\n    fets = []\n    for img, t in data:\n        features = None  # TODO\n        fets.append((features, t),)\n    return fets\n\ntrain_data_fets = compute_features(train_data)\nvalid_data_fets = compute_features(valid_data)\ntest_data_fets = compute_features(test_data)\n\nIn the rest of this part of the lab, we will test two models that will take as input these AlexNet features, and make a prediction for which letter the hand gesture represents. The two models are a linear model, a two-layer MLP. We will compare the performance of these two models.\nGraded Task: Complete the definition of the LinearModel class, which is a linear model (e.g., logistic regression). This model should as input these AlexNet features, and make a prediction for which letter the hand gesture represents.\n\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super(LinearModel, self).__init__()\n        # TODO: What layer need to be initialized?\n\n    def forward(self, x):\n        x = x.view(-1, 256 * 6 * 6) # flatten the input\n        z = None # TODO: What computation needs to be performed?\n        return z\n\nm_linear = LinearModel()\nm_linear(train_data_fets[0][0]) # this should produce a(n unnormalized) prediction\n\nTask: Train a LinearModel() for at least 6 epochs, and report on the maximum validation accuracy that you can attain. We should still be able to use the train_model function, but make sure to provide the AlexNet features as input (and not the image features).\n\nm_linear = LinearModel()\n# TODO: Train the linear model. Include your output in your submission\n\nGraded Task: Compare this model with the CNN() models that we trained earlier. How does this model perform in terms of validation accuracy? What about in terms of the time it took to train this model?\n\n# TODO: Your observation goes here\n\nTask: We decide to use AlexNet features as input to our MLP, and avoided tuning AlexNet weights. However, we could have considered AlexNet to be a part of our model, and continue to tune AlexNet weights to improve our model performance. What are the advantages and disadvantages of continuing to tune AlexNet weights?\n\n# TODO"
  },
  {
    "objectID": "labs/lab07.html#part-4.-data",
    "href": "labs/lab07.html#part-4.-data",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 4. Data",
    "text": "Part 4. Data\nTask: Report the test accuracy on this transfer learning model.\n\n# TODO\n\nTask: Use this code below to construct the confusion matrix for this model over the test set.\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nimport sklearn\nlabel = \"ABCDEFGHI\"\ndef plot_confusion(model, data):\n    n = 0\n    ts = []\n    ys = []\n    for x, t in data:\n        z = model(x.unsqueeze(0))\n        y = int(torch.argmax(z))\n        ts.append(t)\n        ys.append(y)\n\n    cm = confusion_matrix(ts, ys)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label)\n    plt.figure()\n    disp.plot()\n\nplot_confusion(m_linear, test_data_fets)\n\nGraded Task: Which class is most likely mistaken as another? Is this reasonable? (i.e. is that class particularly challenging, or very similar to another class?)\n\n# TODO: Include your analysis here\n\nTask: In order to understand where errors come from, it is crucial that we explore why and how our models fail. A first step is to visually inspect the test data points where failure occurs. That way, we can identify what we can do to prevent/fix errors before our models are deployed.\nRun the below code to display images in the test set that our model misclassifies:\n\nfor i, (x, t) in enumerate(test_data_fets):\n    y = int(torch.argmax(m_linear(x)))\n    if not (y == t):\n        plt.figure()\n        plt.imshow(test_data[i][0].transpose(0,1).transpose(1,2).numpy())\n\nTask: By visually inspecting these misclassified images, we see that there are two main reasons for misclassification. What reason for misclassification is due to a mistake in the formatting of the test set images?\n\n# TODO\n\nGraded Task: We also see a much more serious issue, where gestures made by individuals with darker skin tones may be more frequently misclasified. This result suggests that errors in the model may impact some groups more than others. What steps should we take to mitigate this issue?\n\n# TODO"
  },
  {
    "objectID": "labs/lab02.html",
    "href": "labs/lab02.html",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "",
    "text": "In this lab, we will build a neural network that can predict the next word in a sentence given the previous three. We will apply an idea called weight sharing to go beyond the multi-layer perceptrons that we discussed in class.\nWe will also solve this problem problem twice: once in numpy, and once using PyTorch. When using numpy, you’ll implement the backpropagation computation manually.\nThe prediction task is not very interesting on its own, but in learning to predict subsequent words given the previous three, our neural networks will learn about how to represent words. In the last part of the lab, we’ll explore the vector representations of words that our model produces, and analyze these representations.\nAcknowledgements:"
  },
  {
    "objectID": "labs/lab02.html#submission",
    "href": "labs/lab02.html#submission",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Submission",
    "text": "Submission\nClick “Working Alone” on Markus.\nSubmit the ipynb file lab02.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your implementation of convert_words_to_indices (1 point)\nPart 2. Your implementation of do_forward_pass (4 points)\nPart 3. Your implementation of do_backward_pass (3 points)\nPart 3. The output of the gradient checking code (1 point)\nPart 4. Your explanation of why each row of model.Ww corresponds to a word representation (1 point)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "labs/lab02.html#part-1.-data",
    "href": "labs/lab02.html#part-1.-data",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nWe will begin by downloading the data onto Google Colab.\n\n# Download lab data file\n!wget https://www.cs.toronto.edu/~lczhang/413/raw_sentences.txt\n\nWith any machine learning problem, the first thing that we would want to do is to get an intuitive understanding of what our data looks like. The following code reads the sentences in our file, split each sentence into its individual words, and stores the sentences (list of words) in the variable sentences.\n\nsentences = []\nfor line in open('raw_sentences.txt'):\n    words = line.split()\n    sentence = [word.lower() for word in words]\n    sentences.append(sentence)\n\nThere are 97,162 sentences in total, and these sentences are composed of 250 distinct words.\n\nvocab = set([w for s in sentences for w in s])\nprint(len(sentences)) # 97162\nprint(len(vocab)) # 250\n\nWe will separate our data into training, validation, and test. We will use 10,000 sentences for test, 10,000 for validation, and the remaining for training.\n\n# First, randomly shuffle the sentences in case these sentences are\n# temporally correlated (i.e., so that our train/val/test sets have\n# equal probability of getting the earlier vs later sentences)\nimport random\nrandom.seed(10)\nrandom.shuffle(sentences)\n\ntest, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]\n\nTask: To get an understanding of the data set that we are working with, start by printing 10 sentences in the training set. How are punctuated treated in this word representation? What about words with apostrophes?\n\n# TODO: Your code goes here\n\nIt is also a good idea to explore the distributional properties of the data.\nTask: The length of the sentences affects the types of modeling we can perform on the data. If the sentences are too short, then using a model that depends on many previous words would not make sense. Run the below code, which computes the length of the shortest, average, and longest sentences in the training set.\n\nsentence_lengths = [len(s) for s in train]\nprint(\"Shortest Sentence\", np.min(sentence_lengths))\nprint(\"Average Sentence\", np.mean(sentence_lengths))\nprint(\"Longest Sentence\", np.max(sentence_lengths))\n\nTask: How many words are in the training set? In general, there may be words in the validation/test sets that are not in training!\n\n# TODO: Write code to perform the computation\n\nTask: What is the most common word in the training set? How often does this word appear in the training set? This information is useful to know since it helps us understand the difficult of the word prediction problem. In other words, this figure represents the accuracy of a “baseline” model that simply returns the most common word as the prediction for what the next word should be!\n\nfrom collections import Counter\n\ntotal_words = 0        # count number of words in the training set\nword_count = Counter() # count the occurrence of each word\nfor s in train:\n    for w in s:\n        word_count[w] += 1\n        total_words += 1\n\n# TODO: find the most common word\n\nNow that we understand a bit about the distributional properties of our data, we can move on to representing our data numerically in a way that a neural network can use.\nWe will use a one-hot encoding to represent words. Alternatively, you can think of what we’re doing as assigning each word to a unique integer index. We will need some functions that converts sentences into the corresponding word indices.\nGraded Task: Complete the helper functions convert_words_to_indices. The functions generate_4grams and process_data have been written for you. The process_data function will take a list of sentences (i.e. list of list of words), and generate an \\(N \\times 4\\) numpy matrix containing indices of 4 words that appear next to each other. You can use the constants vocab, vocab_itos, and vocab_stoi in your code.\n\n# A list of all the words in the data set. We will assign a unique\n# identifier for each of these words.\nvocab = sorted(list(set([w for s in train for w in s])))\n# A mapping of index =&gt; word (string)\nvocab_itos = dict(enumerate(vocab))\n# A mapping of word =&gt; its index\nvocab_stoi = {word:index for index, word in vocab_itos.items()}\n\ndef convert_words_to_indices(sents):\n    \"\"\"\n    This function takes a list of sentences (list of list of words)\n    and returns a new list with the same structure, but where each word\n    is replaced by its index in `vocab_stoi`.\n\n    Example:\n    &gt;&gt;&gt; convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'],\n                                  ['other', 'one', 'since', 'yesterday'],\n                                  ['you']])\n    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n    \"\"\"\n    indices = []\n    # TODO: Write your code here\n    return indices\n\ndef generate_4grams(seqs):\n    \"\"\"\n    This function takes a list of sentences (list of lists) and returns\n    a new list containing the 4-grams (four consecutively occurring words)\n    that appear in the sentences. Note that a unique 4-gram can appear multiple\n    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n\n    Example:\n\n    &gt;&gt;&gt; generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n    &gt;&gt;&gt; generate_4grams([[1, 1, 1, 1, 1]])\n    [[1, 1, 1, 1], [1, 1, 1, 1]]\n    \"\"\"\n    grams = []\n    for seq in seqs:\n        for i in range(len(seq) - 4):\n            grams.append(seq[i:i+4])\n    return grams\n\ndef process_data(sents):\n    \"\"\"\n    This function takes a list of sentences (list of lists), and generates an\n    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n    \"\"\"\n    indices = convert_words_to_indices(sents)\n    fourgrams = generate_4grams(indices)\n    return np.array(fourgrams)\n\ntrain4grams = process_data(train)\nvalid4grams = process_data(valid)\ntest4grams = process_data(test)\n\nTask: We are almost ready to discuss the model. Review the following helper functions, which has been written for you:\n\nmake_onehot, which converts indices\n\n\ndef make_onehot(indicies, total=250):\n    \"\"\"\n    Convert indicies into one-hot vectors.\n\n    Parameters:\n        `indices` - a numpy array of any shape (e.g. `[N, 3]` where `N`\n                    is the batch size)\n        `total` - an integer describing the total number of possible classes\n                  (maximum possible value in `indicies`)\n\n    Returns: a one-hot representation of the input numpy array\n             (If the input is of shape `[X, Y]`, then the output would\n             be of shape `[X, Y, total]` and consists of 0's and 1's)\n    \"\"\"\n    # create an identity matrix of shape [total, total]\n    I = np.eye(total)\n    # index the appropriate columns of that identity matrix\n    return I[indicies]\n\ndef softmax(x):\n    \"\"\"\n    Compute the softmax of vector x, or row-wise for a matrix x.\n    We subtract x.max(axis=0) from each row for numerical stability.\n\n    Parameters:\n        `x` - a numpy array shape `[N, num_classes]`\n\n    Returns: a numpy array of the same shape as the input.\n    \"\"\"\n    x = x.T\n    exps = np.exp(x - x.max(axis=0))\n    probs = exps / np.sum(exps, axis=0)\n    return probs.T\n\nThere is one more data processing function that we need, which turns the four-grams into inputs (consisting of the one-hot representations of the first 3 words), and the target output (the index of the 4th word).\nSince the one-hot representation is not memory efficient, we will only convert data into this representation when required, and only do so at a minibatch level.\n\ndef get_batch(data, range_min, range_max, onehot=True):\n    \"\"\"\n    Convert one batch of data (specifically, `data[range_min:range_max]`)\n    in the form of 4-grams into input and output data and return the\n    training data.\n\n    Parameters:\n        `data` - a numpy array of shape [N, 4] produced by a call\n                 to the function `process_data`\n        `range_min` - the starting index of the minibatch\n        `range_max` - the ending index of the minibatch, with\n                      range_max &gt; range_min and\n                      batch_size = range_max - range_min\n        `onehot` - boolean value, if `True` the targets are also made\n                   to be one-hot vectors rather than indices\n\n    Returns: a tuple `(x, t)` where\n     - `x` is an numpy array of one-hot vectors of shape [batch_size, 3, 250]\n     - `t` is either\n            - a numpy array of shape [batch_size, 250] if onehot is True,\n            - a numpy array of shape [batch_size] containing indicies otherwise\n    \"\"\"\n    x = data[range_min:range_max, :3]\n    x = make_onehot(x)\n    x = x.reshape(-1, 750)\n    t = data[range_min:range_max, 3]\n    if onehot:\n        t = make_onehot(t).reshape(-1, 250)\n    return x, t\n\n# some testing code for illustrative purposes\nx_, t_ = get_batch(train4grams, 0, 10, onehot=False)\nprint(train4grams[0])\npos_index = train4grams[0][0]\nprint(x_[0, pos_index-1]) # should be 0\nprint(x_[0, pos_index])   # should be 1\nprint(x_[0, pos_index+1]) # should be 0\npos_index = train4grams[1][0]\nprint(x_[0, 250 + pos_index-1]) # should be 0\nprint(x_[0, 250 + pos_index])   # should be 1\nprint(x_[0, 250 + pos_index+1]) # should be 0\npos_index = train4grams[2][0]\nprint(x_[0, 500 + pos_index-1]) # should be 0\nprint(x_[0, 500 + pos_index])   # should be 1\nprint(x_[0, 500 + pos_index+1]) # should be 0\n\nFinally, the estimate_accuracy function has been provided to you as well. This function is similar to the accuracy function from lab 1.\n\ndef estimate_accuracy(model, data, batch_size=5000, max_N=10000):\n    \"\"\"\n    Estimate the accuracy of the model on the data. To reduce\n    computation time, use at least `max_N` elements of `data` to\n    produce the estimate.\n\n    Parameters:\n        `model` - an object (e.g. `NNModel`, see below) with a forward()\n                  method that produces predictions for an input\n        `data` - a dataset of 4grams (produced by `process_data`) over\n                 which we compute accuracy\n        `batch_size` - integer batch size to use to produce predictions\n        `max_N` - integer value describing the minimum number of predictions\n                  to make to produce the accuracy estimate\n\n    Returns: a floating point value between 0 and 1\n    \"\"\"\n    num_correct = 0\n    num_preds = 0\n    for i in range(0, data.shape[0], batch_size):\n        xs, ts = get_batch(data, i, i + batch_size, onehot=False)\n        z = model.forward(xs)\n        pred = np.argmax(z, axis=1)\n        num_correct += np.sum(ts == pred)\n        num_preds += ts.shape[0]\n\n        if num_preds &gt;= max_N: # at least max_N predictions have been made\n            break\n    return num_correct / num_preds"
  },
  {
    "objectID": "labs/lab02.html#part-2.-model-building-forward-pass",
    "href": "labs/lab02.html#part-2.-model-building-forward-pass",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Part 2. Model Building: Forward Pass",
    "text": "Part 2. Model Building: Forward Pass\nIn this section, we will build our deep learning model. As we did in the previous lab, we begin by understanding how to make predictions with this model. So, in this part of the lab, we will write the functions required to perform the forward pass operation. We will write the backward-pass and train the model in Part 3 and 4.\nTask: Consider the following two models architectures:\nModel 1: \nModel 2: \nIn Model 1, the input \\(\\bf{x}\\) consists of three one-hot vectors concatenated together. We can think of \\(\\bf{h}\\) as a representation of those three words (all together). However, the model architecture treat the three one-hot vectors from the three words distinctly. However, \\(\\bf{W^{(1)}}\\) needs to learn about the first word separately from the second and third word. In other words, the deep learning model treats these three sets of one-hot features as if they have no semantic connection in common.\nIn Model 2, we use an idea called weight sharing, where we use the sample set of weights \\(\\bf{W}^{(word)}\\) to map the one-hot vectors into a vector representation. This allows us to learn the weights \\(\\bf{W}^{(word)}\\) from informatino from all three words. This model architecture encodes our knowledge that the three sets of one-hot vectors share something in common.\nWe will use model 2 in the rest of this lab. For clarity, here is the forward-pass computation to be performed. (Note that this is not vectorized!)\n\\[\\begin{align*}\n\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\nL &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n\\end{align*}\\]\nThe class NNModel represents this above neural network model. This class stores the weights and biases of our model. Moreover, this class will also have methods that use and modify these weights.\nMost of the class has been implemented for you, including these methods:\n\nThe initializeParams() method, which randomly initializes the weights\nThe loss() method, which computes the average cross-entropy loss\nThe update() method, which performs the gradient updates\nThe cleanup() method, which clears the member variables used in the computation\n\nThe implementation for these methods are incomplete:\n\nThe forward method computes the prediction given a data matrix X. These computations are known as the forward pass. This method also saves some of the intermediate values in the neural network computation, to make gradient computation easier.\nThe backward method computes the gradient of the average loss with respect to various quantities (i.e. the error signals). These computations are known as the backward pass.\n\nYou may assume that during an iteration of gradient descent, the following methods will be called in order:\n\nThe cleanup method to clear information stored from the previous computation\nThe forward method to compute the predictions\nThe backward method to compute the error signals\n(Possibly the loss method to compute the average loss)\nThe update method to move the weights\n\nYou might recognize that the way we set up the class correspond to what PyTorch does.\n\nclass NNModel(object):\n    def __init__(self, vocab_size=250, emb_size=150, num_hidden=100):\n        \"\"\"\n        Initialize the weights and biases of this two-layer MLP.\n        \"\"\"\n        # information about the model architecture\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.num_hidden = num_hidden\n\n        # weights for the embedding layer of the model\n        self.Ww = np.zeros([vocab_size, emb_size])\n\n        # weights and biases for the first layer of the MLP\n        self.W1 = np.zeros([emb_size * 3, num_hidden])\n        self.b1 = np.zeros([num_hidden])\n\n        # weights and biases for the second layer of the MLP\n        self.W2 = np.zeros([num_hidden, vocab_size])\n        self.b2 = np.zeros([vocab_size])\n\n        # initialize the weights and biases\n        self.initializeParams()\n\n        # set all values of intermediate variables (to be used in the\n        # forward/backward passes) to None\n        self.cleanup()\n\n    def initializeParams(self):\n        \"\"\"\n        Initialize the weights and biases of this two-layer MLP to be random.\n        This random initialization is necessary to break the symmetry in the\n        gradient descent update for our hidden weights and biases. If all our\n        weights were initialized to the same value, then their gradients will\n        all be the same!\n        \"\"\"\n        self.Ww = np.random.normal(0, 2/self.vocab_size, self.Ww.shape)\n        self.W1 = np.random.normal(0, 2/(3*self.emb_size), self.W1.shape)\n        self.b1 = np.random.normal(0, 2/(3*self.emb_size), self.b1.shape)\n        self.W2 = np.random.normal(0, 2/self.num_hidden, self.W2.shape)\n        self.b2 = np.random.normal(0, 2/self.num_hidden, self.b2.shape)\n\n    def forward(self, X):\n        \"\"\"\n        Compute the forward pass to produce prediction logits.\n\n        Parameters:\n            `X` - A numpy array of shape (N, self.vocab_size * 3)\n\n        Returns: A numpy array of logit predictions of shape\n                 (N, self.vocab_size)\n        \"\"\"\n        return do_forward_pass(self, X) # To be implemented below\n\n    def backward(self, ts):\n        \"\"\"\n        Compute the backward pass, given the ground-truth, one-hot targets.\n\n        You may assume that the `forward()` method has been called for the\n        corresponding input `X`, so that the quantities computed in the\n        `forward()` method is accessible.\n\n        Parameters:\n            `ts` - A numpy array of shape (N, self.vocab_size)\n        \"\"\"\n        return do_backward_pass(self, ts)\n\n    def loss(self, ts):\n        \"\"\"\n        Compute the average cross-entropy loss, given the ground-truth, one-hot targets.\n\n        You may assume that the `forward()` method has been called for the\n        corresponding input `X`, so that the quantities computed in the\n        `forward()` method is accessible.\n\n        Parameters:\n            `ts` - A numpy array of shape (N, self.num_classes)\n        \"\"\"\n        return np.sum(-ts * np.log(self.y)) / ts.shape[0]\n\n    def update(self, alpha):\n        \"\"\"\n        Compute the gradient descent update for the parameters of this model.\n\n        Parameters:\n            `alpha` - A number representing the learning rate\n        \"\"\"\n        self.Ww = self.Ww - alpha * self.Ww_bar\n        self.W1 = self.W1 - alpha * self.W1_bar\n        self.b1 = self.b1 - alpha * self.b1_bar\n        self.W2 = self.W2 - alpha * self.W2_bar\n        self.b2 = self.b2 - alpha * self.b2_bar\n\n    def cleanup(self):\n        \"\"\"\n        Erase the values of the variables that we use in our computation.\n        \"\"\"\n        # To be filled in during the forward pass\n        self.N = None # Number of data points in the batch\n        self.xa = None # word (a)'s one-hot encoding\n        self.xb = None # word (b)'s one-hot encoding\n        self.xc = None # word (c)'s one-hot encoding\n        self.va = None # word (a)'s embedding\n        self.vb = None # word (b)'s embedding\n        self.vc = None # word (c)'s embedding\n        self.v = None  # concatenated embedding\n        self.m = None  # pre-activation hidden state\n        self.h = None  # post-activation hidden state\n        self.z = None  # prediction logit\n        self.y = None  # prediction softmax\n\n        # To be filled in during the backward pass\n        self.z_bar  = None # The error signal for self.z\n        self.W2_bar = None # The error signal for self.W2\n        self.b2_bar = None # The error signal for self.b2\n        self.h_bar  = None # The error signal for self.h\n        self.m_bar  = None # The error signal for self.z1\n        self.W1_bar = None # The error signal for self.W1\n        self.b1_bar = None # The error signal for self.b1\n        self.v_bar  = None # The error signal for self.v\n        self.va_bar = None # The error signal for self.va\n        self.vb_bar = None # The error signal for self.vb\n        self.vc_bar = None # The error signal for self.vc\n        self.Ww_bar = None # The error signal for self.Ww\n\nGraded Task: Complete the implementation of the do_forward_pass method, which computes the predictions given a NNModel and a batch of input data.\nWe recommend that you reason about your approach on paper before writing any numpy code. Track the shapes of your quantities carefully! When you finally write your numpy code, print out the shapes of your quantities as you go along, and reason about whether these shapes match your initial expectations.\n\ndef do_forward_pass(model, X):\n    \"\"\"\n    Compute the forward pass to produce prediction logits.\n\n    This function also keeps some of the intermediate values in\n    the neural network computation, to make computing gradients easier.\n\n    For the ReLU activation, you may find the function `np.maximum` helpful\n\n    Parameters:\n        `model` - An instance of the class NNModel\n        `X` - A numpy array of shape (N, model.vocab_size)\n\n    Returns: A numpy array of logit predictions of shape\n             (N, model.vocab_size)\n    \"\"\"\n    # populate the input attributes necessary for the\n    # backward pass\n    model.N = X.shape[0]\n    model.X = X\n\n    # for xa, xb, xc, we index the appropriate range of X\n    # (recall that the tensor X has shape [batch_size, 3*vocab_size])\n    model.xa = X[:, :model.vocab_size]\n    model.xb = X[:, model.vocab_size:model.vocab_size*2]\n    model.xc = X[:, model.vocab_size*2:]\n\n    # compute the embeddings\n    model.va = None # TODO\n    model.vb = None # TODO\n    model.vc = None # TODO\n    model.v = np.concatenate([model.va, model.vb, model.vc], axis=1)\n\n    # compute the remaining part of the forward pass\n    model.m = None # TODO - the hidden state value (pre-activation)\n    model.h = None # TODO - the hidden state value (post ReLU activation)\n    model.z = None # TODO - the logit scores (pre-activation)\n    model.y = None # TODO - the class probabilities (post-activation)\n    return model.z\n\nTask: One way important way to check your implementation is to run the forward() method to ensure that the shapes of your quantities are correct. Run the below code. If you run into shape mismatch issues, print out the shapes of the quantities that you are working with (e.g. print(model.va.shape)) and ensure that these shapes are what you expect them to be.\n\n# Create a batch of data that we will use for gradient checking\n# we will use a small batch size of 8. This number is chosen\n# because it is small, but also because this shape does not\n# appear elsewhere in our architecture (e.g. vocab size, num hidden)\n# so that shape mismatch issues are easier to identify.\nx_, t_ = get_batch(train4grams, 0, 8)\nmodel = NNModel()\ny = model.forward(x_)\n\n# TODO: Check that these shapes are correct. What should these shapes be?\nprint(model.va.shape, model.vb.shape, model.vc.shape)\nprint(model.v.shape)\nprint(model.m.shape, model.h.shape)\nprint(model.z.shape, model.y.shape)\n\nAt this point, we can work with a pre-trained model by loading weights that are provided to you via the link below. If you would like, you can jump to part 4 first and explore the interesting properties of this model before tackling backpropagation and model training.\n\n!wget https://www.cs.toronto.edu/~lczhang/413/sentence_pretrained.pk\n\n\ndef load_pretrained(model):\n    import pickle\n    assert(model.vocab_size == 250)\n    assert(model.emb_size   == 150)\n    assert(model.num_hidden == 100)\n    Ww, W1, b1, W2, b2 = pickle.load(open(\"sentence_pretrained.pk\", \"rb\"))\n    model.Ww = Ww\n    model.W1 = W1\n    model.b1 = b1\n    model.W2 = W2\n    model.b2 = b2\n    model.cleanup()\n    return model\n\nmodel = load_pretrained(NNModel())"
  },
  {
    "objectID": "labs/lab02.html#part-3.-model-building-backwards-pass",
    "href": "labs/lab02.html#part-3.-model-building-backwards-pass",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Part 3. Model Building: Backwards Pass",
    "text": "Part 3. Model Building: Backwards Pass\nWe are ready to complete the function that computes the backward pass of our model!\nYou should start by reviewing the lecture slides on backpropagation. One difference between the slides and our implementation here is that the slides express the required computations for computing the gradients of the loss for a single data point. However, our implementation of backpropagation is further vectorized to compute gradients of the loss for a batch consisting of multiple data points.\nWe begin with applying the backpropagation algorithm on our forward pass steps from earlier. Recall that our model’s forward pass is as follows:\n\\[\\begin{align*}\n\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\nL &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n\\end{align*}\\]\nFollowing the steps discussed in this week’s lecture, we should get the following backward-pass computation (verify this yourself!): \\[\\begin{align*}\n\\overline{{\\bf z}}  &= {\\bf y} - {\\bf t} \\\\\n\\overline{W^{(2)}}  &= \\overline{{\\bf z}}{\\bf h}^T \\\\\n\\overline{{\\bf b^{(2)}}}  &= \\overline{{\\bf z}} \\\\\n\\overline{{\\bf h}}  &= {W^{(2)}}^T\\overline{z} \\\\\n\\overline{W^{(1)}} &= \\overline{{\\bf m}} {\\bf v}^T \\\\\n\\overline{{\\bf b}^{(1)}} &= \\overline{{\\bf m}} \\\\\n\\overline{{\\bf m}}  &= \\overline{{\\bf h}}\\circ \\textrm{ReLU}'({\\bf m}) \\\\\n\\overline{{\\bf v}} &=  {W^{(1)}}^T \\overline{{\\bf m}} \\\\\n\\overline{{\\bf v_a}} &= \\dots \\\\\n\\overline{{\\bf v_b}} &= \\dots \\\\\n\\overline{{\\bf v_c}} &= \\dots \\\\\n\\overline{{\\bf W^{(word)}}} &= \\dots \\\\\n\\end{align*}\\]\nTask: What is the error signal \\(\\overline{{\\bf v_a}}\\)? How does this quantity relate to \\(\\overline{{\\bf v}}\\)? To answer this question, reason about the scalars that make up the elements of \\(\\overline{{\\bf v}}\\). Which of these scalars also appear in \\(\\overline{{\\bf v_a}}\\)?\nExpress your answer by computing va_bar (representing the quantity \\(\\overline{{\\bf v_a}}\\)) given v_bar (representing the quantity \\(\\overline{{\\bf v}}\\)).\n\nN = 10\nemb_size = 100\nv_bar = np.random.rand(N, emb_size * 3)\n\nva_bar = None # TODO\nvb_bar = None # TODO\nvc_bar = None # TODO\n\nTask: What is the derivative \\(\\overline{{\\bf W^{(word)}}}\\)? You may find it helpful to draw a computation graph, and then remember the multivariate chain rule. If \\(\\overline{{\\bf W^{(word)}}}\\) affects the loss in 3 different paths, what do we do with those 3 gradients?\n\n# TODO: Work out the derivative on paper.\n\nWe are still not done: the gradient computation is for a single input \\({\\bf x}\\). We will need to vectorize each of these computations so that they work for an entire batch of inputs \\({\\bf X}\\) of shape \\(N \\times 3\\textrm{vocab_size}\\).\nFor some quantities, vectorizing the backward-pass computation is just as straightforward as the forward-pass computation, requiring the same techniques. For example, each input \\({\\bf x}\\) in a batch will have its own corresponding value of \\({\\bf z}\\) and thus $. (If this sentence is confusing, check that your description of the shape for z_bar from Part 2 has the batch size N in there somewhere.)\nFor other quantities, vectorizing requires the use of the multivariate chain rule. For example, there is a single weight matrix \\(W^{(2)}\\), used for all inputs in a batch. Thus, a change in \\(W^{(2)}\\) will affect the predictions for all inputs. (If this sentence is confusing, check that your description of the shape for W2_bar from Part 2 does not have batch size N in there.)\nThe vectorization for the quantities consistent with those of a MLP is already provided to you in the do_backward_pass function. However, the rest of this function is incomplete.\nGraded Task: Complete the implementation of the do_backward_pass function, which performs backpropagation given a NNModel, given the ground-truth one-hot targets ts. This function assumes that the forward pass method had been called on the input X corresponding to those one-hot targets.\nOnce again, we recommend that you reason about your approach on paper before writing any numpy code! In particular, understand the vectorization strategies discussed in the previous weeks and above before proceeding. Track the shapes of your quantities carefully! When you finally write your numpy code, print out the shapes of your quantities as you go along, and reason about whether these shapes match your initial expectations.\n\ndef do_backward_pass(model, ts):\n    \"\"\"\n    Compute the backward pass, given the ground-truth, one-hot targets.\n\n    You may assume that `model.forward()` has been called for the\n    corresponding input `X`, so that the quantities computed in the\n    `forward()` method is accessible.\n\n    The member variables you store here will be used in the `update()`\n    method. Check that the shapes match what you wrote in Part 2.\n\n    Parameters:\n        `model` - An instance of the class NNModel\n        `ts` - A numpy array of shape (N, model.num_classes)\n    \"\"\"\n    # The gradient signal for the MLP part of this is given\n    # to you (or worked out together from above, TODO)\n    model.z_bar = (model.y - ts) / model.N\n    model.W2_bar = np.dot(model.h.T, model.z_bar)\n    model.b2_bar = np.dot(np.ones(model.N).T, model.z_bar)\n    model.h_bar = np.matmul(model.z_bar, model.W2.T)\n    model.m_bar = model.h_bar * (model.m &gt; 0)\n    model.W1_bar = np.dot(model.v.T, model.m_bar)\n    model.b1_bar = np.dot(np.ones(model.N).T, model.m_bar)\n    model.v_bar = np.matmul(model.m_bar, model.W1.T)\n\n    # Refer to your answer above\n    model.va_bar = None # TODO\n    model.vb_bar = None # TODO\n    model.vc_bar = None # TODO\n\n    # Refer to your answer above\n    model.Ww_bar = None\n\nAs we saw in CSC311, debugging machine learning code can be extremely challenging. It helps to be systematic about testing, and to test every helper function as we write it. It is important to test do_backward_pass before using it for training, so that we can isolates issues related to computing gradients vs. other training issues (e.g. those related to poor hyperparameter choices).\nTask: As in the forward pass, start by making sure that the shapes match. Again, If you run into shape mismatch issues, print out the shapes of the quantities that you are working with.\n\nx_, t_ = get_batch(train4grams, 0, 8)\nmodel = NNModel()\n\nmodel.forward(x_)\nmodel.backward(t_)\nmodel.update(0.001)\n\nThe above step checks that the shapes match. But we also saw, in CSC311, that one way to check the gradient computation is through finite difference. Recall the definition of a derivative. For a function \\(g(w): \\mathbb{R} \\rightarrow \\mathbb{R}\\),\n\\[g'(w) = \\lim_{h \\rightarrow 0} \\frac{g(w+h) - g(w)}{h}\\]\nThis above rule tells us that if we have a way to evaluate g and would like to test our implementation of \\(g'\\), we can choose an \\(h\\) small enough, and check if:\n\\[g'(w) \\approx \\frac{g(w+h) - g(w)}{h}\\]\nIn our case, we have that for any parameter \\(w_j\\) and an \\(h\\) small enough, we should have for our loss \\[\\mathcal{E}\\]:\n\\[\\frac{\\partial \\mathcal{E}}{\\partial w_j} \\approx \\frac{\\mathcal{E}(w_0, w_1, \\dots, w_{j-1}, w_j + h, w_{j+1}, \\dots, w_D) - \\mathcal{E}(w_0, w_1, \\dots, w_D)}{h}\\]\n(A word about notation: here we are enumerating over all scalar weights \\(w_0 \\dots w_D\\) in our model. You will often see this in machine learning textbooks and papers, where we ignore the fact that these scalar weights come from several different weight matrices and bias vectors. This notation might feel strange/imprecise as first, but keep in mind that mathematical notations is a form of language whose purpose is to communicate ideas. Practitioners choose different notations, and even introduce new notation, with the goal of clearly communicating a specific idea. Here, the idea is that we should be able to test the gradient computation or a single scalar weight by computing the loss function twice: once with a slight perturbation on that scalar weight.)\nGraded Task: Run the below code to spot test that the gradients Ww_bar is computed correctly. Include the output of the code in your submission.\n\n# We will opt to use a large batch size to test the gradients `Ww_bar`\n# with a large batch size. Why do you think this is? (Why might we\n# be more likely to have gradients of value 0 if the batch size is\n# small?)\nx_, t_ = get_batch(train4grams, 0, 800)\n\nmodel = NNModel()\nmodel.forward(x_)\n\n# Check the gradient for Ww_bar[3, 10].\n# You should spot check other indices too!\nmodel.backward(t_)\ngradient = model.Ww_bar[3, 10]\n\n# we should have\n# gradient ~= (loss_perturbed - loss_initial) / h\n# where loss_perturbed is the loss if we perturb\n# model.Ww_bar[3, 10] by a small value h\n\nloss_initial = model.loss(t_)\n\nh = 0.01\nmodel.Ww[3, 10] += h\n\nmodel.cleanup()\nmodel.forward(x_)\nloss_perturbed = model.loss(t_)\n\n# These two values should be close\nprint(gradient)\nprint((loss_perturbed - loss_initial) / h)\n\nIf gradient checking succeeds, we are ready to train our model. The function train_model is written for you. Run the code below with the default hyperparameters. Although hyperparameter tuning is an important step in machine learning, we have chosen reasonable hyperparameters to you to keep this lab a reasonable size.\n\ndef train_model(model,\n                train_data=train4grams,\n                validation_data=valid4grams,\n                batch_size=50,\n                learning_rate=0.3,\n                max_iters=20000,\n                plot_every=1000):\n    \"\"\"\n    Use gradient descent to train the numpy model on the dataset train4grams.\n    \"\"\"\n    iters, train_loss, train_acc, val_acc = [], [], [], [] # for the training curve\n    iter_count = 0  # count the number of iterations\n    try:\n        while iter_count &lt; max_iters:\n            # shuffle the training data, and break early if we don't have\n            # enough data to remaining in the batch\n            np.random.shuffle(train_data)\n            for i in range(0, train_data.shape[0], batch_size):\n                if (i + batch_size) &gt; train_data.shape[0]:\n                    break\n\n                # get the input and targets of a minibatch\n                xs, ts = get_batch(train_data, i, i + batch_size, onehot=True)\n\n                # erase any accumulated gradients\n                model.cleanup()\n\n                # forward pass: compute prediction\n                ys = model.forward(xs)\n\n                # backward pass: compute error\n                model.backward(ts)\n                model.update(learning_rate)\n\n                # increment the iteration count\n                iter_count += 1\n\n                # compute and plot the *validation* loss and accuracy\n                if (iter_count % plot_every == 0):\n                    iters.append(iter_count)\n                    train_loss.append(model.loss(ts))\n                    train_acc.append(estimate_accuracy(model, train_data))\n                    val_acc.append(estimate_accuracy(model, validation_data))\n                    model.cleanup()\n                    print(f\"Iter {iter_count}. Acc [val:{val_acc[-1]}, train:{train_acc[-1]}] Loss {train_loss[-1]}]\")\n            #   if iter_count &gt;= max_iters:\n            #       break\n    finally:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel= NNModel()\ntrain_model(model)"
  },
  {
    "objectID": "labs/lab02.html#part-4.-applying-the-model",
    "href": "labs/lab02.html#part-4.-applying-the-model",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Part 4. Applying the Model",
    "text": "Part 4. Applying the Model\nIn this section, we will use apply the model for sentence completion, and to explore model embeddings. If you do not have a trained model, you may use the trained weights provided as part of the assignment.\n\n# model = load_pretrained(NNModel())\n\nTask: The function make_prediction has been written for you. It takes as parameters a NNModel model and sentence (a list of words), and produces a prediction for the next word in the sentence.\nRun the following code to predict what the next word should be in each of the following sentences:\n\ndef make_prediction(model, sentence):\n    \"\"\"\n    Use the model to make a prediction for the next word in the\n    sentence using the last 3 words (sentence[-3:])\n    \"\"\"\n    global vocab_itos\n    indices = convert_words_to_indices([sentence[-3:]])\n    X = make_onehot(indices).reshape(-1, 750)\n    z = model.forward(X)\n    i = np.argmax(z)\n    return vocab_itos[i]\n\nprint(make_prediction(model, ['you', 'are', 'a']))\nprint(make_prediction(model, ['there', 'are', 'no']))\nprint(make_prediction(model, ['yesterday', 'the', 'federal']))\n\nDo your predictions make sense? (If all of your predictions are the same, train your model for more iterations, or change the hyper parameters in your model.\n\n# TODO: Your analysis goes here\n\nWhile training the NNModel, we trained the weight model.Ww, which takes a one-hot representation of a word in our vocabulary, and returns a low-dimensional vector representation of that word. These representations, also called word embeddings have interesting properties.\nGraded Task: Explain why each row of model.Ww contains the vector representing of a word. For example model.Ww[vocab_stoi[\"any\"],:] contains the vector representation of the word “any”.\n\n# TODO: Write your explanation here\n\nOne interesting thing about these word embeddings is that distances in these vector representations of words make some sense! To show this, we have provided code below that computes the cosine similarity of every pair of words in our vocabulary.\n\nnorms = np.linalg.norm(model.Ww, axis=1)\nword_emb_norm = (model.Ww.T / norms).T\nsimilarities = np.matmul(word_emb_norm, word_emb_norm.T)\n\n# Some example distances. The first one should be larger than the second\nprint(similarities[vocab_stoi['any'], vocab_stoi['many']])\nprint(similarities[vocab_stoi['any'], vocab_stoi['government']])\n\nTask: Run the below code, which computes the 5 closest words to each of the following words. Replace these words with words of your choice to explore the distances in the word embeddings.\n\ndef get_closest(word):\n    dst = [(w, similarities[vocab_stoi[word], idx])\n           for w, idx in vocab_stoi.items()]\n    dst = sorted(dst, key=lambda x: x[1], reverse=True)\n    return dst[1:6]\n\nprint(get_closest(\"four\"))\nprint(get_closest(\"go\"))\nprint(get_closest(\"should\"))\nprint(get_closest(\"yesterday\"))\n\nNotice that similar words provided above tend to occur in similar surrounding words in a sentence. Why do you think this might be? Consider the architecture used in this model, and what this model is trained to do. (How would replacing a word with another word with a similar embedding change the neural network prediction?)\nWe can also visualize the word embeddings by reducing the dimensionality of the word vectors to 2D. There are many dimensionality reduction techniques that we could use, and we will use an algorithm called t-SNE. (You don’t need to know what this is for the lab). Nearby points in this 2-D space are meant to correspond to nearby points in the original, high-dimensional space.\nThe following code runs the t-SNE algorithm and plots the result. Look at the plot and find two clusters of related words. What do the words in each cluster have in common?\nNote that there is randomness in the initialization of the t-SNE algorithm. If you re-run this code, you may get a different image.\n\nimport sklearn.manifold\ntsne = sklearn.manifold.TSNE()\nY = tsne.fit_transform(word_emb_norm)\n\nplt.figure(figsize=(10, 10))\nplt.xlim(Y[:,0].min(), Y[:, 0].max())\nplt.ylim(Y[:,1].min(), Y[:, 1].max())\nfor i, w in enumerate(vocab):\n    plt.text(Y[i, 0], Y[i, 1], w)\nplt.show()"
  },
  {
    "objectID": "labs/lab10.html",
    "href": "labs/lab10.html",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "",
    "text": "Sentiment Analysis is the problem of identifying the writer’s sentiment given a piece of text. Sentiment Analysis can be applied to movie reviews, feedback of other forms, emails, tweets, course evaluations, and much more.\nIn this lab, we will build an RNN to classify positive vs negative tweets We use the Sentiment140 data set, which contains tweets with either a positive or negative emoticon. Our goal is to determine whether which type of emoticon the tweet (with the emoticon removed) contained. The dataset was actually collected by a group of students, much like you, who are doing their first machine learning projects.\nBy the end of this lab, you will be able to:\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab10.html#submission",
    "href": "labs/lab10.html#submission",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab10.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your output showing several positive tweets. (1 point)\nPart 2. Your explanation of the shapes of wordemb. (1 point)\nPart 2. Your explanation of the shapes of h and out. (2 points)\nPart 2. Your explanation of why computing the mean and max of hidden states across all time steps is likely more informative than using the final output state. (1 point)\nPart 3. Your demonstration of the model’s ability to “overfit” on a data set. (1 point)\nPart 3. Your output from training the model on the full data set. (1 point)\nPart 4. Your explanation of why MyGloveRNN requires fewer iteration to obtain “good” accuracy. (1 point)\nPart 4. Your comparison of MyGloveRNN and MyRNN in low data settings.. (1 point)\nPart 4. Your explanation of where the biases in embeddings come from, and whether our model will have the same sorts of baises.. (1 point)"
  },
  {
    "objectID": "labs/lab10.html#part-1.-data",
    "href": "labs/lab10.html#part-1.-data",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nStart by running these two lines of code to download the data on to Google Colab.\n\n# Download tutorial data files.\n!wget https://www.cs.toronto.edu/~lczhang/413/sample_tweets.csv\n\nAs always, we start by understanding what our data looks like. Notice that the test set has been set aside for us. Both the training and test set files follow the same format. Each line in the csv file contains the tweet text, the string label “4” (positive) or “0” (negative), and some additional information about the tweet.\n\nimport csv\ndatafile = \"sample_tweets.csv\"\n\n# Training/Validation set\ndata = csv.reader(open(datafile))\nfor i, line in enumerate(data):\n    print(line)\n    if i &gt; 10:\n        break\n\nTask: How many positive and negative tweets are in this file?\n\n# TODO\nprint(Counter(x[0] for x in csv.reader(open(datafile))))\n\nGraded Task: We have printed several negative tweets above. Print 10 positive tweets.\n\n# TODO: Please make sure to include both your code and the\n# printed output\n\nWe will now split the dataset into training, validation, and test sets:\n\n# read the data; convert labels into integers\ndata = [(review, int(label=='4'))  # label 1 = positive, 0 = negative\n        for label, _, _, _, _, review in csv.reader(open(datafile))]\n\n# shuffle the data, since the file stores all negative tweets first\nimport random\nrandom.seed(42)\nrandom.shuffle(data)\n\ntrain_data = data[:50000] \nval_data = data[50000:60000] \ntest_data = data[60000:]\n\nIn order to be able to use neural networks to make predictions about these tweets, we need to begin by convert these tweets into sequences of numbers, each representing a words. This is akin to a one-hot encoding: each word will be converted into an a number representing the unique index of that word.\nAlthough we could do this conversion by writing our own python code, torch has a package called torchtext that has utilities useful for text classification and generation tasks. In particular, the Vocab class and build_vocab_from_iterator will be useful for us for building the mapping from words to indices.\n\nimport torchtext\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import Vocab, build_vocab_from_iterator\n\n# we will *tokenize* each word by using a tokenzier from \n# https://pytorch.org/text/stable/data_utils.html#get-tokenizer\n\ntokenizer = get_tokenizer(\"basic_english\")\ntrain_data_words = [tokenizer(x) for x, t in train_data]\n\n# build the vocabulary object. the parameters to this function\n# is described below\nvocab = build_vocab_from_iterator(train_data_words,\n                                  specials=['&lt;bos&gt;', '&lt;eos&gt;', '&lt;unk&gt;', '&lt;pad&gt;'],\n                                  min_freq=10)\n\n# set the index of a word not in the vocabulary\nvocab.set_default_index(2) # this is the index of the `&lt;unk&gt;` keyword\n\nNow, vocab is an object of class Vocab (see more here https://pytorch.org/text/stable/vocab.html ) that provides functionalities for converting words into their indices. In addition to words appearing in the training set, ther are four special tokens that we use, akin to placeholder words:\n\n&lt;bos&gt;, to indicate the beginning of a sequence.\n&lt;eos&gt;, to indicate the end of a sequence.\n&lt;unk&gt;, to indicate a word that is not in the vocabulary. This includes words that appear too infrequently to be included in the vocabulary, and any other words in the validation/test sets that are not see in training.\n&lt;pad&gt;, used for padding shorter sequences in a batch: since each tweet may have different length, the shorter tweets in each batch will be padded with the &lt;pad&gt; token so that each sequence (tweet) in a batch has the same length.\n\nThe min_freq parameter identifies the minimum number of times a word must appear in the training set in order to be included in the vocabulary.\nHere you can see the vocab object in action:\n\n# Print the number of words in the vocabulary\nprint(len(vocab))\n\n# Convert a tweet into a sequence of word indices.\ntweet = 'The movie Pneumonoultramicroscopicsilicovolcanoconiosis is a good movie, it is very funny'\ntokens = tokenizer(f'&lt;bos&gt; {tweet} &lt;eos&gt;')\nprint(tokens)\nindices = vocab.forward(tokens)\nprint(indices)\n\nTask: What is the index of the &lt;pad&gt; token?\n\n# TODO: write code to identify the index of the `&lt;pad&gt;` token\n\nNow let’s apply this transformation to the entire set of training, validation, and test data.\n\n\ndef convert_indices(data, vocab):\n    \"\"\"Convert data of form [(tweet, label)...] where tweet is a string\n    into an equivalent list, but where the tweets represented as a list\n    of word indices.\n    \"\"\"\n    return [(vocab.forward(tokenizer(f'&lt;bos&gt; {text} &lt;eos&gt;')), label)\n            for (text, label) in data]\n\ntrain_data_indices = convert_indices(train_data, vocab)\nval_data_indices = convert_indices(val_data, vocab)\ntest_data_indices = convert_indices(test_data, vocab)\n\nWe have seen that PyTorch’s DataLoader provides an easy way to form minibatches when we worked with image data. However, text and sequence data is more challenging to work with since the sequences may not be the same length.\nAlthough we can (and will!) continue to use DataLoader for our text data, we need to provide a function that merges sequences of various lengths into two PyTorch tensors correspondingg to the inputs and targets for that batch.\nTask: Following the instructions below, complete the collate_batch function, which creates the input and target tensors for a batch of data.\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_batch(batch):\n    \"\"\"\n    Returns the input and target tensors for a batch of data\n\n    Parameters:\n        `batch` - An iterable data structure of tuples (indices, label),\n                  where `indices` is a sequence of word indices, and \n                  `label` is either 1 or 0.\n\n    Returns: a tuple `(X, t)`, where \n        - `X` is a PyTorch tensor of shape (batch_size, sequence_length)\n        - `t` is a PyTorch tensor of shape (batch_size)\n    where `sequence_length` is the length of the longest sequence in the batch\n    \"\"\"\n\n    text_list = []  # collect each sample's sequence of word indices\n    label_list = [] # collect each sample's target labels\n    for (text_indices, label) in batch:\n        text_list.append(torch.tensor(text_indices))\n        # TODO: what do we need to do with `label`?\n\n    X = pad_sequence(text_list, padding_value=3).transpose(0, 1)\n    t = None # TODO\n    return X, t\n\n\ntrain_dataloader = DataLoader(train_data_indices, batch_size=10, shuffle=True,\n                              collate_fn=collate_batch)\n\nWith the above code in mind, we should be able to extract batches from train_dataloader. Notice that X.shape is different in each batch. You should also see that the index 3 is used to pad shorter sequences in in a batch.\n\nfor i, (X, t) in enumerate(train_dataloader):\n    print(X.shape, t.shape)\n    if i &gt;= 10:\n        break\n\nprint(X)\n\nTask: Why does each sequence begin with the token 0, and end with the token 1 (ignoring the paddings).\n\n# TODO: Your explanation goes here"
  },
  {
    "objectID": "labs/lab10.html#part-2.-model",
    "href": "labs/lab10.html#part-2.-model",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Part 2. Model",
    "text": "Part 2. Model\nWe will use a recurrent neural network model to classify positive vs negative sentiments. Our RNN model will have three components that are typical in a sequence classification model:\n\nAn embedding layer, which will map each word index (akin to a one-hot embedding) into a low-dimensional vector. This layer as having the same functionality as the weights \\(W^{(word)}\\) from lab 2.\nA recurrent layer, which performs the recurrent neural network computation. The input to this layer is the low-dimensional embedding vectors for each word in the sequence.\nA fully connected layer, which computes the final binary classification using features computed from the recurrent layer. In our case, we concatenate the max and mean of the hidden units across the time steps (i.e. across each word).\n\nLet’s define the model that we will use, and then explore it step by step.\n\nimport torch.nn as nn\n\nclass MyRNN(nn.Module):\n    def __init__(self, vocab_size, emb_size, hidden_size, num_classes):\n        super(MyRNN, self).__init__()\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.emb = nn.Embedding(vocab_size, emb_size)\n        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, X):\n        # Look up the embedding\n        wordemb = self.emb(X)\n        # Forward propagate the RNN\n        h, out = self.rnn(wordemb)\n        # combine the hidden features computed from *each* time step of\n        # the RNN. we do this by \n        features = torch.cat([torch.amax(h, dim=1),\n                              torch.mean(h, dim=1)], axis=-1)\n        # Compute the final prediction\n        z = self.fc(features)\n        return z\n\nmodel = MyRNN(len(vocab), 128, 64, 2)\n\nTo explore exactly what this model is doing, let’s grab one batch of data from the data loader we created. We will observe, step-by-step, what computation will be performed on the input X to obtain the final prediction. We do this by emulating the forward method of the MyRNN function.\n\nX, t = next(iter(train_dataloader))\n\nprint(X.shape)\n\nGraded Task: Run the code below to check the shape of wordemb. What shape does this tensor have? Explain what each dimension in this shape means.\n\nwordemb = model.emb(X)\n\nprint(wordemb.shape)\n\n# TODO: Include your explanation here\n\nGraded Task: Run the code below, which computes the RNN forward pass, with wordemb as input. What shape do the tensors h and out have? Explain what these tensors correspond to. (See the RNN reference https://pytorch.org/docs/stable/generated/torch.nn.RNN.html on the PyTorch documentation page.)\n\nh, out = model.rnn(wordemb)\n\nprint(h.shape)\nprint(out.shape)\n\n# The tensors `h` and `out` are related. To see the relation,\n# choose an index in the batch and compare the following two\n# vectors in `h` and `out`.\nindex = 2 # choose an index to iterate through the batch\nprint(h[index, -1, :])\nprint(out[0, index, :])\n\n# TODO: Include your explanation here\n\nGraded Task: There is a step in the MyRNN forward pass that combines the features from each time step of the RNN by computing:\n\nthe maximum value of each position in the hidden vector.\nthe mean value of each position in the hidden vector.\nconcatenating the resulting two vectors.\n\n(Note that in the demo below, we are working with a minibatch. Thus, each of out1, out2, and features below are matrices containing the vectors from each minibatch)\nThis method typically performs better than, say, taking the hidden state at the last time step (the value out from above). Explain, intuitively, why you might expect this performance to be the case for a sentiment analysis task.\n\nout1 = torch.amax(h, dim=1)\nout2 = torch.mean(h, dim=1)\nfeatures = torch.cat([out1, out2], axis=-1)\n\n# Compare, for a single input in the batch, the connection between\n# `h`, `out1`, `out2` and `features`:\nprint(h[index, :, :])\nprint(out1[index, :])\nprint(out2[index, :])\nprint(features[index, :])\n\n# TODO: Include your explanation here\n\nTask: Finally, the model uses the features tensor to compute the prediction for each element in the batch. Run the code below to complete this step.\n\nprint(model.fc(features))\n\nThere is one more thing we need to do before training the model, which is to write a function to estimate the accuracy of the model. This is done for you below.\n\ndef accuracy(model, dataset, max=1000):\n    \"\"\"\n    Estimate the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model`   - An object of class nn.Module\n        `dataset` - A dataset of the same type as `train_data`.\n        `max`     - The max number of samples to use to estimate \n                    model accuracy\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    correct, total = 0, 0\n    dataloader = DataLoader(dataset,\n                            batch_size=1,  # use batch size 1 to prevent padding\n                            collate_fn=collate_batch)\n    for i, (x, t) in enumerate(dataloader):\n        z = model(x)\n        y = torch.argmax(z, axis=1)\n        correct += int(torch.sum(t == y))\n        total   += 1\n        if i &gt;= max:\n            break\n    return correct / total\n\naccuracy(model, train_data_indices) # should be close to half"
  },
  {
    "objectID": "labs/lab10.html#part-3.-training",
    "href": "labs/lab10.html#part-3.-training",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Part 3. Training",
    "text": "Part 3. Training\nIn this section, we will train the MyRNN model to classify tweets. As the models that we are building begin to increase in complexity, it is important to use good debugging techniques. In this section, we will introduce the technique of checking whether the model and training code is able to overfit on a small training set. This is a way to check for bugs in the implementation.\nTask: Complete the training code below\n\nimport torch.optim as optim \nimport matplotlib.pyplot as plt\n\ndef train_model(model,                # an instance of MLPModel\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=0.001,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=50,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               collate_fn=collate_batch,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (texts, labels) in enumerate(train_loader):\n                z = None # TODO\n\n                loss = None # TODO\n\n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    ta = accuracy(model, train_data)\n                    va = accuracy(model, val_data)\n                    train_loss.append(float(loss))\n                    train_acc.append(ta)\n                    val_acc.append(va)\n                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        if plot:\n            plt.figure()\n            plt.plot(iters[:len(train_loss)], train_loss)\n            plt.title(\"Loss over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n\n            plt.figure()\n            plt.plot(iters[:len(train_acc)], train_acc)\n            plt.plot(iters[:len(val_acc)], val_acc)\n            plt.title(\"Accuracy over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n            plt.legend([\"Train\", \"Validation\"])\n\nGraded Task: As a way to check the model and training code, check if your model can obtain a 100% training accuracy relatively quickly (e.g. within less than a minute of training time), when training on only the first 20 element of the training data.\n\nmodel = MyRNN(vocab_size=len(vocab),\n              emb_size=300,\n              hidden_size=64,\n              num_classes=2)\n# TODO: Include your code and output \n\nTask: Will this model that you trained above have a high accuracy over the validation set? Explain why or why not.\n\n# TODO: Your explanation goes here\n\nGraded Task: Train your model on the full data set. What validation accuracy can you achieve?\n\n# TODO: Include your code here. Try a few hyperparameter choices until you\n# are satisfied that your model performance is reasonable (i.e. no obviously\n# poor hyperparameter choices)\n\nInstead of a (vanilla) RNN model, PyTorch also makes available nn.LSTM and nn.GRU units. They can be used in place of nn.RNN without further changes to the MyRNN code.\nIn general, gated units like LSTM’s are much more frequently used than vanilla RNNs, although transformers are much more popular now as well."
  },
  {
    "objectID": "labs/lab10.html#part-4.-pretrained-embeddings",
    "href": "labs/lab10.html#part-4.-pretrained-embeddings",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Part 4. Pretrained Embeddings",
    "text": "Part 4. Pretrained Embeddings\nAs we saw in the previous lab on images, transfer learning is a useful technique in practical machine learning, especially in low-data settings: instead of training an entire neural network from scratch, we use (part of) a model that is pretrained on large amounts of similar data. We use the intermediate state of this pretrained model as features to our model—i.e. we use the pretrained models to compute features.\nJust like with images, using a pretrained model is an important strategy for working with text. Large language models is an excellent demonstration of how generalizable pretrained features can be.\nIn this part of the lab, we will use a slightly older idea of using pretrained word embeddings. In particular, instead of training our own nn.Embedding layer, we will use GloVe embeddings (2014) https://nlp.stanford.edu/projects/glove/ trained on a large data set containing all of Wikipedia and other webpages.\nNowadays, large language model (LLMs), including those with APIs provided by various organizations, can also be used to map words/sentences into embeddings. However, the basic idea of using pretrained models in low-data settings remains similar. We will also identify some bias issues with pretrained word embeddings. There is evidence that these types of bias issues continues to persist in LLMs as well.\n\nfrom torchtext.vocab import GloVe\n\nglove = torchtext.vocab.GloVe(name=\"6B\", dim=300)\n\nTask: Run the below code to print the GloVe word embedding for the word “cat”.\n\nprint(glove['cat'])\n\nUnfortunately, it is not straightforward to add the &lt;pad&gt;, &lt;unk&gt;, &lt;bos&gt; and &lt;eos&gt; tokens. So we will do without them.\nTask: Run the below code to look up GloVe word indices for the training, validation, and test sets.\n\ndef convert_indices_glove(data, default=len(glove)-1):\n    result = []\n    for text, label in data:\n        words = tokenizer(text) # for simplicity, we wont use &lt;bos&gt; and &lt;eos&gt;\n        indices = []\n        for w in words:\n            if w in glove.stoi:\n                indices.append(glove.stoi[w])\n            else:\n                # this is a bit of a hack, but we will repurpose *last* word\n                # (least common word) appearing in the GloVe vocabluary as our\n                # '&lt;pad&gt;' token\n                indices.append(default)\n        result.append((indices, label),)\n    return result\n\ntrain_data_glove = convert_indices_glove(train_data)\nval_data_glove = convert_indices_glove(val_data)\ntest_data_glove = convert_indices_glove(test_data)\n\nNow, we will modify the MyRNN to use the pretrained GloVe vectors:\n\nclass MyGloveRNN(nn.Module):\n    def __init__(self,  hidden_size, num_classes):\n        super(MyGloveRNN, self).__init__()\n        self.vocab_size, self.emb_size = glove.vectors.shape\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n        self.emb.requires_grad=False # do *not* update the glove embeddings\n        self.rnn = nn.RNN(self.emb_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, X):\n        # Look up the embedding\n        wordemb = self.emb(X)\n        # Forward propagate the RNN\n        h, out = self.rnn(wordemb)\n        # combine the hidden features computed from *each* time step of\n        # the RNN. we do this by \n        features = torch.cat([torch.amax(h, dim=1),\n                              torch.mean(h, dim=1)], axis=-1)\n        # Compute the final prediction\n        z = self.fc(features)\n        return z\n\n    def parameters(self):\n        # do not return the parameters of self.emb \n        # so the optimizer will not update the parameters of self.emb\n        return (p for p in super(MyGloveRNN, self).parameters() if p.requires_grad)\n\n\nmodel = MyGloveRNN(64, 2)\n\nTask Train this model. Use comparable hyperparameters so that you can compare your result against MyRNN.\n\n# TODO: Train your model here, and include the output\n\nGraded Task: You might notice that a very smaller number of iterations will be required to train this model to a reasonable performance (e.g. &gt;70% validation accuracy). Why might this be?\n\n# TODO: Include your explanation here\n\nGraded Task: Train both MyGloveRNN and MyRNN models using the corresponding embeddings (pretrained vs. not), but only with the first 200 data points in the training set. How do the validation accuracies compare between these two models?\n\n# TODO: Training code for MyGloveRNN.\n# Include outputs and training curves in your submission\n\n\n# TODO: Training code for MyRNN\n# Include outputs and training curves in your submission\n\n\n# TODO: Compare the validation accuaries here\n\nMachine learning models have an air of “fairness” about them, since models make decisions without human intervention. However, models can and do learn whatever bias is present in the training data. GloVe vectors seems innocuous enough: they are just representations of words in some embedding space. Even so, we will show that the structure of the GloVe vectors encodes the everyday biases present in the texts that they are trained on.\nWe start with an example analogy to demonstrate the power of GloVe embeddings that allows us to complete analogies by applying arithmetic operations to the word vectors.\n\\[doctor - man + woman \\approx ??\\]\nTo find the answers to the above analogy, we will compute the following vector, and then find the word whose vector representation is closest to it.\n\nv = glove['doctor'] - glove['man'] + glove['woman']\n\nTask: Run the code below to find the closets word. You should see the word “nurse” fairly high up in that list.\n\ndef print_closest_words(vec, n=5):\n    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n    for idx, difference in lst[1:n+1]:                         # take the top n\n        print(glove.itos[idx], difference)\n\nprint_closest_words(v)\n\nTask: To compare, use a similar method to find the answer to this analogy: \\[doctor - woman + man \\approx ??\\]\nIn other words, we go the opposite direction in the “gender” axis to check if similarly concerning analogies exist.\n\nprint_closest_words(glove['doctor'] - glove['woman'] + glove['man'])\n\nTask: Compare the following two outputs.\n\nprint_closest_words(glove['programmer'] - glove['man'] + glove['woman'])\n\n\nprint_closest_words(glove['programmer'] - glove['woman'] + glove['man'])\n\nTask: Compare the following two outputs.\n\nprint_closest_words(glove['professor'] - glove['man'] + glove['woman'])\n\n\nprint_closest_words(glove['professor'] - glove['woman'] + glove['man'])\n\nTask: Compare the following two outputs.\n\nprint_closest_words(glove['engineer'] - glove['man'] + glove['woman'])\n\n\nprint_closest_words(glove['engineer'] - glove['woman'] + glove['man'])\n\nGraded Task: Explain where the bias in these embeddings come from. Would you expect our word embeddings (trained on tweets) to be similarly problematic? Why or why not?\n\n# TODO: Your explanation goes here"
  },
  {
    "objectID": "labs/neural_network_optimization.html",
    "href": "labs/neural_network_optimization.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "This notebook demonstrates the implementation of a three-layer neural network and compares the performance of different optimization methods: Gradient Descent (GD), Momentum, and Adam. The neural network is trained on a two-moon-shaped dataset using various optimization techniques to find the optimal parameters that minimize the cost function and improve the accuracy of the model.\n\n\nThe dataset used in this notebook is generated using the make_moons() function from the sklearn.datasets module. The dataset consists of two classes that form two moon shapes. It is a non-linearly separable dataset, making it a suitable scenario for testing different optimization methods for training a neural network.\n\n\n\nThe neural network uses two activation functions:\n\nReLU (Rectified Linear Unit): The ReLU activation function is used in the hidden layers of the neural network. It allows the model to handle non-linearities efficiently and avoids the vanishing gradient problem, which can occur with sigmoid activation.\nSigmoid: The sigmoid activation function is used in the output layer to compute the final probability of the binary classification task. It maps the input to the range (0, 1), representing the probability of the input belonging to class 1.\n\n\n\n\nThe neural network architecture is defined as a three-layer model with the following layer dimensions: [input size, 5, 2, 1]. The input size corresponds to the features of the dataset, and the output size is set to 1 for binary classification.\n\n\n\n\n\nGradient Descent is a first-order optimization algorithm that updates the model’s parameters in the opposite direction of the gradient of the cost function. The magnitude of the update is controlled by the learning rate. While GD is a simple and intuitive optimization method, it may suffer from slow convergence, especially for large datasets or in cases where the cost function has high curvatures.\n\n\n\nMomentum is an extension of GD that introduces a moving average of the gradients to accelerate convergence. It accumulates the past gradients’ information to continue moving in the same direction even when the gradients change direction frequently. This helps in faster convergence and reduces oscillations in the cost function. The momentum hyperparameter controls the influence of the past gradients.\n\n\n\nAdam is a popular optimization algorithm that combines the ideas of both Momentum and RMSprop. It uses moving averages of the past gradients and squared gradients to adapt the learning rate for each parameter. The algorithm automatically adjusts the learning rate based on the history of the gradients and their magnitudes. This adaptive learning rate makes Adam robust and efficient in practice, requiring minimal hyperparameter tuning.\n\n\n\n\nThe neural network is trained using each of the three optimization methods. For each optimization method, the model’s parameters are updated iteratively over a specified number of epochs using mini-batch gradient descent. At the end of each epoch, the cost function is computed and printed to monitor the training progress. The final trained model’s accuracy on the training data is also calculated and displayed.\n\n\n\nThe notebook provides an insight into the effectiveness of different optimization methods in training neural networks. By comparing the performance of GD, Momentum, and Adam, we can gain a better understanding of how these algorithms handle the optimization process and improve the neural network’s convergence and accuracy on non-linear datasets like the two-moon-shaped dataset used here.\nSure, let’s delve into the details and theory of each optimization method along with their advantages and disadvantages.\n\n\n\nTheory: Gradient Descent is a first-order optimization algorithm used to minimize the cost function of a neural network. It works by iteratively updating the model’s parameters in the opposite direction of the gradient of the cost function with respect to those parameters. The gradient points in the direction of steepest ascent, so taking the opposite direction allows the algorithm to move towards the minimum of the cost function.\nAlgorithm: 1. Initialize the model’s parameters randomly. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: parameter = parameter - learning_rate * gradient, where the learning_rate controls the size of the update step.\nAdvantages: - Simple and easy to implement. - Can be applied to large datasets since it processes one data point at a time. - Can handle non-convex cost functions.\nDisadvantages: - Convergence can be slow, especially for large datasets or complex cost functions. - Sensitive to the learning rate choice; a large learning rate may lead to overshooting the minimum, while a small learning rate may result in slow convergence.\n\n\n\nTheory: Momentum is an extension of GD that aims to accelerate the convergence of the optimization process. It introduces a moving average of the past gradients to continue moving in the same direction even when the gradients change direction frequently. This helps to overcome oscillations in the cost function and speeds up convergence.\nAlgorithm: 1. Initialize the model’s parameters and the velocity (initialized as zeros) for each parameter. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: velocity = beta * velocity + (1 - beta) * gradient    parameter = parameter - learning_rate * velocity where beta is the momentum hyperparameter.\nAdvantages: - Accelerates convergence, especially in areas with high curvature or noisy gradients. - Reduces oscillations and overshooting, leading to more stable updates.\nDisadvantages: - Momentum may accumulate too much velocity in flat regions, making it harder to escape local minima. - May overshoot and oscillate when the learning rate is too large.\n\n\n\nTheory: Adam is an adaptive learning rate optimization algorithm that combines the ideas of both Momentum and RMSprop. It uses moving averages of the past gradients and squared gradients to adapt the learning rate for each parameter. The algorithm automatically adjusts the learning rate based on the history of the gradients and their magnitudes.\nAlgorithm: 1. Initialize the model’s parameters and the first and second moment estimates (initialized as zeros) for each parameter. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: first_moment = beta1 * first_moment + (1 - beta1) * gradient    second_moment = beta2 * second_moment + (1 - beta2) * gradient^2    first_moment_corrected = first_moment / (1 - beta1^t)    second_moment_corrected = second_moment / (1 - beta2^t)    parameter = parameter - learning_rate * first_moment_corrected / sqrt(second_moment_corrected + epsilon) where beta1 and beta2 are the moment hyperparameters, and epsilon is a small constant to prevent division by zero.\nAdvantages: - Adaptive learning rate for each parameter, reducing the need for extensive learning rate tuning. - Efficient and robust in practice, suitable for a wide range of neural network architectures and cost functions. - Fast convergence and good generalization on various datasets.\nDisadvantages: - Adam may exhibit slow convergence on certain non-stationary objectives or saddle points. - Requires more memory to store the additional moving average parameters.\n\n\n\nEach optimization method has its strengths and weaknesses, and the choice of the optimization algorithm may depend on the specific problem, dataset, and neural network architecture. While Gradient Descent is a simple baseline algorithm, Momentum and Adam often offer faster convergence and more stable updates in practice. However, Adam may require more memory due to the additional moving average parameters.\nIt is recommended to experiment with different optimization methods and hyperparameter values to find the optimal combination that yields the best convergence and accuracy for a given neural network task. Additionally, other optimization techniques, such as Adagrad, RMSprop, and Nesterov Accelerated Gradient (NAG), are also widely used in practice and may be worth exploring for specific scenarios.\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport h5py \nimport scipy.io \nimport sklearn \nimport sklearn.datasets\n\n\n#creat L-layer neural net with different optimizations \n\n\ndef sigmoid (x) : \n    s = 1/(1+np.exp(-x))\n    return s\n\n\ndef relu(x):\n    s = np.maximum(0,x)\n    return s\n\n\n\ndef load_params_and_grads(seed=1):\n    np.random.seed(seed)\n    W1 = np.random.randn(2,3)\n    b1 = np.random.randn(2,1)\n    W2 = np.random.randn(3,3)\n    b2 = np.random.randn(3,1)\n\n    dW1 = np.random.randn(2,3)\n    db1 = np.random.randn(2,1)\n    dW2 = np.random.randn(3,3)\n    db2 = np.random.randn(3,1)\n    \n    return W1, b1, W2, b2, dW1, db1, dW2, db2\n\n\ndef initialize_parameters(layer_dims) : \n    np.random.seed(3)\n    parameters = {}\n    L = len (layer_dims)\n\n    for i in range (1 , L) : \n        parameters['W' + str (i)] = np.random.rand ( layer_dims[i] ,layer_dims[i-1]) * np.sqrt (2/layer_dims[i-1])\n        parameters['b' + str(i)] = np.zeros(shape=(layer_dims[i] ,1 ))\n\n    return parameters \n\n\ndef compute_cost (a3 , y) : \n    m = max (np.shape(y))\n    cost = 1./m * np.sum(np.multiply (-np.log(a3),y) + np.multiply(-np.log(1 - a3), 1 - y))\n    return cost \n\n\ndef forward_propagation(X , parameters) : \n        \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n    z1 = np.dot(W1, X) + b1\n    a1 = relu(z1)\n    z2 = np.dot(W2, a1) + b2\n    a2 = relu(z2)\n    z3 = np.dot(W3, a2) + b3\n    a3 = sigmoid(z3)\n    \n    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n    \n    return a3, cache \n\n\ndef backward_propagation(X, Y, cache):\n    m = X.shape[1]\n    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n    \n    dz3 = 1./m * (a3 - Y)\n    dW3 = np.dot(dz3, a2.T)\n    db3 = np.sum(dz3, axis=1, keepdims = True)\n    \n    da2 = np.dot(W3.T, dz3)\n    dz2 = np.multiply(da2, np.int64(a2 &gt; 0))\n    dW2 = np.dot(dz2, a1.T)\n    db2 = np.sum(dz2, axis=1, keepdims = True)\n    \n    da1 = np.dot(W2.T, dz2)\n    dz1 = np.multiply(da1, np.int64(a1 &gt; 0))\n    dW1 = np.dot(dz1, X.T)\n    db1 = np.sum(dz1, axis=1, keepdims = True)\n    \n    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\n\ndef predict(X, y, parameters):\n\n    \n    m = X.shape[1]\n    p = np.zeros((1,m))\n    \n    # Forward propagation\n    a3, caches = forward_propagation(X, parameters)\n    \n    # convert probas to 0/1 predictions\n    for i in range(0, a3.shape[1]):\n        if a3[0,i] &gt; 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n\n    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n    \n    return p\n\n\ndef predict_dec(parameters, X):\n    \"\"\"\n    Used for plotting decision boundary.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (m, K)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Predict using forward propagation and a classification threshold of 0.5\n    a3, cache = forward_propagation(X, parameters)\n    predictions = (a3 &gt; 0.5)\n    return predictions\n\ndef load_dataset():\n    np.random.seed(3)\n    train_X, train_Y = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n    # Visualize the data\n    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n    train_X = train_X.T\n    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n    \n    return train_X, train_Y\n\n\ntrain_X, train_Y = load_dataset()\n\n\n\n\n\n\n\n\n\n\ndef update_parameters_with_gd(parameters, grads, learning_rate):\n   \n    L = len(parameters) // 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n        \n    return parameters\n\n\nimport math\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch &lt; mini_batch_size)\n    if m % mini_batch_size != 0:\n        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\nmini_batches = random_mini_batches(train_X, train_Y, mini_batch_size = 128 )\nprint (\"shape of the X_train is \" , np.shape (train_X))\nprint(\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\nprint(\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\nprint(\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\nprint(\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\nprint(\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \nprint(\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\nprint(\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n\nshape of the X_train is  (2, 300)\nshape of the 1st mini_batch_X: (2, 128)\nshape of the 2nd mini_batch_X: (2, 128)\nshape of the 3rd mini_batch_X: (2, 44)\nshape of the 1st mini_batch_Y: (1, 128)\nshape of the 2nd mini_batch_Y: (1, 128)\nshape of the 3rd mini_batch_Y: (1, 44)\nmini batch sanity check: [-0.14656235  0.22452308  1.38239247]\n\n\n\nMomentum\n\n\ndef initialize_velocity(parameters):\n  \n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    \n    # Initialize velocity\n    for l in range(L):\n        ### START CODE HERE ### (approx. 2 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n        ### END CODE HERE ###\n        \n    return v\n\n\n\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate) :\n\n    L = len(parameters) // 2 # number of layers in the neural networks\n    \n    # Momentum update for each parameter\n    for l in range(L):\n        \n        # compute velocities\n        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n        # update parameters\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n        \n    return parameters, v\n\n# ADAM Optimizer\n\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n    ### START CODE HERE ### (approx. 4 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n\n        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n    ### END CODE HERE ###\n    \n    return v, s\n\n\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s[\"dW\" + str(l + 1)] + epsilon)\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s[\"db\" + str(l + 1)] + epsilon)\n\n    return parameters, v, s\n\n\ndef model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, beta=0.9,\n          beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after epoch %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer=\"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\nCost after epoch 0: 0.776085\nCost after epoch 1000: 0.709262\nCost after epoch 2000: 0.663615\nCost after epoch 3000: 0.663728\nCost after epoch 4000: 0.597440\nCost after epoch 5000: 0.507172\nCost after epoch 6000: 0.487885\nCost after epoch 7000: 0.346078\nCost after epoch 8000: 0.372416\nCost after epoch 9000: 0.498869\n\n\n\n\n\n\n\n\n\nAccuracy: 0.85\n\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, beta=0.9, optimizer=\"momentum\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\nCost after epoch 0: 0.776325\nCost after epoch 1000: 0.709252\nCost after epoch 2000: 0.663705\nCost after epoch 3000: 0.663868\nCost after epoch 4000: 0.597699\nCost after epoch 5000: 0.507593\nCost after epoch 6000: 0.488211\nCost after epoch 7000: 0.346261\nCost after epoch 8000: 0.372549\nCost after epoch 9000: 0.498889\n\n\n\n\n\n\n\n\n\nAccuracy: 0.85\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer=\"adam\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\n\nCost after epoch 0: 0.706337\nCost after epoch 1000: 0.173407\nCost after epoch 2000: 0.191965\nCost after epoch 3000: 0.049790\nCost after epoch 4000: 0.147109\nCost after epoch 5000: 0.108286\nCost after epoch 6000: 0.122491\nCost after epoch 7000: 0.028581\nCost after epoch 8000: 0.131736\nCost after epoch 9000: 0.159093\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9433333333333334"
  },
  {
    "objectID": "labs/neural_network_optimization.html#neural-network-with-optimization-methods-comparison",
    "href": "labs/neural_network_optimization.html#neural-network-with-optimization-methods-comparison",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "This notebook demonstrates the implementation of a three-layer neural network and compares the performance of different optimization methods: Gradient Descent (GD), Momentum, and Adam. The neural network is trained on a two-moon-shaped dataset using various optimization techniques to find the optimal parameters that minimize the cost function and improve the accuracy of the model.\n\n\nThe dataset used in this notebook is generated using the make_moons() function from the sklearn.datasets module. The dataset consists of two classes that form two moon shapes. It is a non-linearly separable dataset, making it a suitable scenario for testing different optimization methods for training a neural network.\n\n\n\nThe neural network uses two activation functions:\n\nReLU (Rectified Linear Unit): The ReLU activation function is used in the hidden layers of the neural network. It allows the model to handle non-linearities efficiently and avoids the vanishing gradient problem, which can occur with sigmoid activation.\nSigmoid: The sigmoid activation function is used in the output layer to compute the final probability of the binary classification task. It maps the input to the range (0, 1), representing the probability of the input belonging to class 1.\n\n\n\n\nThe neural network architecture is defined as a three-layer model with the following layer dimensions: [input size, 5, 2, 1]. The input size corresponds to the features of the dataset, and the output size is set to 1 for binary classification.\n\n\n\n\n\nGradient Descent is a first-order optimization algorithm that updates the model’s parameters in the opposite direction of the gradient of the cost function. The magnitude of the update is controlled by the learning rate. While GD is a simple and intuitive optimization method, it may suffer from slow convergence, especially for large datasets or in cases where the cost function has high curvatures.\n\n\n\nMomentum is an extension of GD that introduces a moving average of the gradients to accelerate convergence. It accumulates the past gradients’ information to continue moving in the same direction even when the gradients change direction frequently. This helps in faster convergence and reduces oscillations in the cost function. The momentum hyperparameter controls the influence of the past gradients.\n\n\n\nAdam is a popular optimization algorithm that combines the ideas of both Momentum and RMSprop. It uses moving averages of the past gradients and squared gradients to adapt the learning rate for each parameter. The algorithm automatically adjusts the learning rate based on the history of the gradients and their magnitudes. This adaptive learning rate makes Adam robust and efficient in practice, requiring minimal hyperparameter tuning.\n\n\n\n\nThe neural network is trained using each of the three optimization methods. For each optimization method, the model’s parameters are updated iteratively over a specified number of epochs using mini-batch gradient descent. At the end of each epoch, the cost function is computed and printed to monitor the training progress. The final trained model’s accuracy on the training data is also calculated and displayed.\n\n\n\nThe notebook provides an insight into the effectiveness of different optimization methods in training neural networks. By comparing the performance of GD, Momentum, and Adam, we can gain a better understanding of how these algorithms handle the optimization process and improve the neural network’s convergence and accuracy on non-linear datasets like the two-moon-shaped dataset used here.\nSure, let’s delve into the details and theory of each optimization method along with their advantages and disadvantages.\n\n\n\nTheory: Gradient Descent is a first-order optimization algorithm used to minimize the cost function of a neural network. It works by iteratively updating the model’s parameters in the opposite direction of the gradient of the cost function with respect to those parameters. The gradient points in the direction of steepest ascent, so taking the opposite direction allows the algorithm to move towards the minimum of the cost function.\nAlgorithm: 1. Initialize the model’s parameters randomly. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: parameter = parameter - learning_rate * gradient, where the learning_rate controls the size of the update step.\nAdvantages: - Simple and easy to implement. - Can be applied to large datasets since it processes one data point at a time. - Can handle non-convex cost functions.\nDisadvantages: - Convergence can be slow, especially for large datasets or complex cost functions. - Sensitive to the learning rate choice; a large learning rate may lead to overshooting the minimum, while a small learning rate may result in slow convergence.\n\n\n\nTheory: Momentum is an extension of GD that aims to accelerate the convergence of the optimization process. It introduces a moving average of the past gradients to continue moving in the same direction even when the gradients change direction frequently. This helps to overcome oscillations in the cost function and speeds up convergence.\nAlgorithm: 1. Initialize the model’s parameters and the velocity (initialized as zeros) for each parameter. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: velocity = beta * velocity + (1 - beta) * gradient    parameter = parameter - learning_rate * velocity where beta is the momentum hyperparameter.\nAdvantages: - Accelerates convergence, especially in areas with high curvature or noisy gradients. - Reduces oscillations and overshooting, leading to more stable updates.\nDisadvantages: - Momentum may accumulate too much velocity in flat regions, making it harder to escape local minima. - May overshoot and oscillate when the learning rate is too large.\n\n\n\nTheory: Adam is an adaptive learning rate optimization algorithm that combines the ideas of both Momentum and RMSprop. It uses moving averages of the past gradients and squared gradients to adapt the learning rate for each parameter. The algorithm automatically adjusts the learning rate based on the history of the gradients and their magnitudes.\nAlgorithm: 1. Initialize the model’s parameters and the first and second moment estimates (initialized as zeros) for each parameter. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: first_moment = beta1 * first_moment + (1 - beta1) * gradient    second_moment = beta2 * second_moment + (1 - beta2) * gradient^2    first_moment_corrected = first_moment / (1 - beta1^t)    second_moment_corrected = second_moment / (1 - beta2^t)    parameter = parameter - learning_rate * first_moment_corrected / sqrt(second_moment_corrected + epsilon) where beta1 and beta2 are the moment hyperparameters, and epsilon is a small constant to prevent division by zero.\nAdvantages: - Adaptive learning rate for each parameter, reducing the need for extensive learning rate tuning. - Efficient and robust in practice, suitable for a wide range of neural network architectures and cost functions. - Fast convergence and good generalization on various datasets.\nDisadvantages: - Adam may exhibit slow convergence on certain non-stationary objectives or saddle points. - Requires more memory to store the additional moving average parameters.\n\n\n\nEach optimization method has its strengths and weaknesses, and the choice of the optimization algorithm may depend on the specific problem, dataset, and neural network architecture. While Gradient Descent is a simple baseline algorithm, Momentum and Adam often offer faster convergence and more stable updates in practice. However, Adam may require more memory due to the additional moving average parameters.\nIt is recommended to experiment with different optimization methods and hyperparameter values to find the optimal combination that yields the best convergence and accuracy for a given neural network task. Additionally, other optimization techniques, such as Adagrad, RMSprop, and Nesterov Accelerated Gradient (NAG), are also widely used in practice and may be worth exploring for specific scenarios.\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport h5py \nimport scipy.io \nimport sklearn \nimport sklearn.datasets\n\n\n#creat L-layer neural net with different optimizations \n\n\ndef sigmoid (x) : \n    s = 1/(1+np.exp(-x))\n    return s\n\n\ndef relu(x):\n    s = np.maximum(0,x)\n    return s\n\n\n\ndef load_params_and_grads(seed=1):\n    np.random.seed(seed)\n    W1 = np.random.randn(2,3)\n    b1 = np.random.randn(2,1)\n    W2 = np.random.randn(3,3)\n    b2 = np.random.randn(3,1)\n\n    dW1 = np.random.randn(2,3)\n    db1 = np.random.randn(2,1)\n    dW2 = np.random.randn(3,3)\n    db2 = np.random.randn(3,1)\n    \n    return W1, b1, W2, b2, dW1, db1, dW2, db2\n\n\ndef initialize_parameters(layer_dims) : \n    np.random.seed(3)\n    parameters = {}\n    L = len (layer_dims)\n\n    for i in range (1 , L) : \n        parameters['W' + str (i)] = np.random.rand ( layer_dims[i] ,layer_dims[i-1]) * np.sqrt (2/layer_dims[i-1])\n        parameters['b' + str(i)] = np.zeros(shape=(layer_dims[i] ,1 ))\n\n    return parameters \n\n\ndef compute_cost (a3 , y) : \n    m = max (np.shape(y))\n    cost = 1./m * np.sum(np.multiply (-np.log(a3),y) + np.multiply(-np.log(1 - a3), 1 - y))\n    return cost \n\n\ndef forward_propagation(X , parameters) : \n        \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n    z1 = np.dot(W1, X) + b1\n    a1 = relu(z1)\n    z2 = np.dot(W2, a1) + b2\n    a2 = relu(z2)\n    z3 = np.dot(W3, a2) + b3\n    a3 = sigmoid(z3)\n    \n    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n    \n    return a3, cache \n\n\ndef backward_propagation(X, Y, cache):\n    m = X.shape[1]\n    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n    \n    dz3 = 1./m * (a3 - Y)\n    dW3 = np.dot(dz3, a2.T)\n    db3 = np.sum(dz3, axis=1, keepdims = True)\n    \n    da2 = np.dot(W3.T, dz3)\n    dz2 = np.multiply(da2, np.int64(a2 &gt; 0))\n    dW2 = np.dot(dz2, a1.T)\n    db2 = np.sum(dz2, axis=1, keepdims = True)\n    \n    da1 = np.dot(W2.T, dz2)\n    dz1 = np.multiply(da1, np.int64(a1 &gt; 0))\n    dW1 = np.dot(dz1, X.T)\n    db1 = np.sum(dz1, axis=1, keepdims = True)\n    \n    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\n\ndef predict(X, y, parameters):\n\n    \n    m = X.shape[1]\n    p = np.zeros((1,m))\n    \n    # Forward propagation\n    a3, caches = forward_propagation(X, parameters)\n    \n    # convert probas to 0/1 predictions\n    for i in range(0, a3.shape[1]):\n        if a3[0,i] &gt; 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n\n    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n    \n    return p\n\n\ndef predict_dec(parameters, X):\n    \"\"\"\n    Used for plotting decision boundary.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (m, K)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Predict using forward propagation and a classification threshold of 0.5\n    a3, cache = forward_propagation(X, parameters)\n    predictions = (a3 &gt; 0.5)\n    return predictions\n\ndef load_dataset():\n    np.random.seed(3)\n    train_X, train_Y = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n    # Visualize the data\n    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n    train_X = train_X.T\n    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n    \n    return train_X, train_Y\n\n\ntrain_X, train_Y = load_dataset()\n\n\n\n\n\n\n\n\n\n\ndef update_parameters_with_gd(parameters, grads, learning_rate):\n   \n    L = len(parameters) // 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n        \n    return parameters\n\n\nimport math\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch &lt; mini_batch_size)\n    if m % mini_batch_size != 0:\n        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\nmini_batches = random_mini_batches(train_X, train_Y, mini_batch_size = 128 )\nprint (\"shape of the X_train is \" , np.shape (train_X))\nprint(\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\nprint(\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\nprint(\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\nprint(\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\nprint(\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \nprint(\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\nprint(\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n\nshape of the X_train is  (2, 300)\nshape of the 1st mini_batch_X: (2, 128)\nshape of the 2nd mini_batch_X: (2, 128)\nshape of the 3rd mini_batch_X: (2, 44)\nshape of the 1st mini_batch_Y: (1, 128)\nshape of the 2nd mini_batch_Y: (1, 128)\nshape of the 3rd mini_batch_Y: (1, 44)\nmini batch sanity check: [-0.14656235  0.22452308  1.38239247]\n\n\n\nMomentum\n\n\ndef initialize_velocity(parameters):\n  \n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    \n    # Initialize velocity\n    for l in range(L):\n        ### START CODE HERE ### (approx. 2 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n        ### END CODE HERE ###\n        \n    return v\n\n\n\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate) :\n\n    L = len(parameters) // 2 # number of layers in the neural networks\n    \n    # Momentum update for each parameter\n    for l in range(L):\n        \n        # compute velocities\n        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n        # update parameters\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n        \n    return parameters, v\n\n# ADAM Optimizer\n\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n    ### START CODE HERE ### (approx. 4 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n\n        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n    ### END CODE HERE ###\n    \n    return v, s\n\n\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s[\"dW\" + str(l + 1)] + epsilon)\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s[\"db\" + str(l + 1)] + epsilon)\n\n    return parameters, v, s\n\n\ndef model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, beta=0.9,\n          beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after epoch %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer=\"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\nCost after epoch 0: 0.776085\nCost after epoch 1000: 0.709262\nCost after epoch 2000: 0.663615\nCost after epoch 3000: 0.663728\nCost after epoch 4000: 0.597440\nCost after epoch 5000: 0.507172\nCost after epoch 6000: 0.487885\nCost after epoch 7000: 0.346078\nCost after epoch 8000: 0.372416\nCost after epoch 9000: 0.498869\n\n\n\n\n\n\n\n\n\nAccuracy: 0.85\n\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, beta=0.9, optimizer=\"momentum\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\nCost after epoch 0: 0.776325\nCost after epoch 1000: 0.709252\nCost after epoch 2000: 0.663705\nCost after epoch 3000: 0.663868\nCost after epoch 4000: 0.597699\nCost after epoch 5000: 0.507593\nCost after epoch 6000: 0.488211\nCost after epoch 7000: 0.346261\nCost after epoch 8000: 0.372549\nCost after epoch 9000: 0.498889\n\n\n\n\n\n\n\n\n\nAccuracy: 0.85\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer=\"adam\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\n\nCost after epoch 0: 0.706337\nCost after epoch 1000: 0.173407\nCost after epoch 2000: 0.191965\nCost after epoch 3000: 0.049790\nCost after epoch 4000: 0.147109\nCost after epoch 5000: 0.108286\nCost after epoch 6000: 0.122491\nCost after epoch 7000: 0.028581\nCost after epoch 8000: 0.131736\nCost after epoch 9000: 0.159093\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9433333333333334"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "No preparation this week",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#prepare",
    "href": "weeks/week-6.html#prepare",
    "title": "Week 6",
    "section": "",
    "text": "No preparation this week",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\nSlides & Recordings\nEnsembling code skeleton\nThe definition of differential privacy",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#practice",
    "href": "weeks/week-6.html#practice",
    "title": "Week 6",
    "section": "Practice",
    "text": "Practice\nIn-class Exercise & Solutions",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#perform",
    "href": "weeks/week-6.html#perform",
    "title": "Week 6",
    "section": "Perform",
    "text": "Perform\nIn-person lab session: optimization and differential privacy lab05\nLecture 6 pre-recorded videos and quizzes\nMath assignment 1 due\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Week 5 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#prepare",
    "href": "weeks/week-5.html#prepare",
    "title": "Week 5",
    "section": "",
    "text": "Week 5 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\nSlides & Recordings",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#practice",
    "href": "weeks/week-5.html#practice",
    "title": "Week 5",
    "section": "Practice",
    "text": "Practice\nIn-class Exercise & Solutions\nTaylor Series https://www.youtube.com/watch?v=3d6DsjIBzJ4",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#perform",
    "href": "weeks/week-5.html#perform",
    "title": "Week 5",
    "section": "Perform",
    "text": "Perform\nTutorial: how to implement SGD with momentum neural network optimization\nLecture 5 pre-recorded videos and quizzes\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Read the syllabus\nWeek 1 Overview\nSoftware Installations",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#prepare",
    "href": "weeks/week-1.html#prepare",
    "title": "Week 1",
    "section": "",
    "text": "Read the syllabus\nWeek 1 Overview\nSoftware Installations",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\nLecture 1 & Recordings",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#practice",
    "href": "weeks/week-1.html#practice",
    "title": "Week 1",
    "section": "Practice",
    "text": "Practice\nNo practices this week",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#perform",
    "href": "weeks/week-1.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\nNo tutorials/labs this week but please complete the Pre-Requisite Math Problems\nSuggested Review :\n\nLinear Algebra review \nProbability review\n\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Week 2 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#prepare",
    "href": "weeks/week-2.html#prepare",
    "title": "Week 2",
    "section": "",
    "text": "Week 2 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\nSlides & Recordings\nProf. Roger Grosse’s notes on backdrop",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#practice",
    "href": "weeks/week-2.html#practice",
    "title": "Week 2",
    "section": "Practice",
    "text": "Practice\nNo exercises this week",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\nIn-person lab session: PyTorch basics with linear models lab01\nLec2 pre-recorded videos with quizzes\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "No additional readings this week. Catch up with previously assigned readings if you’ve fallen behind.",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#prepare",
    "href": "weeks/week-11.html#prepare",
    "title": "Week 11",
    "section": "",
    "text": "No additional readings this week. Catch up with previously assigned readings if you’ve fallen behind.",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\nSlides & Recordings\nAutoencoder Notebook",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#practice",
    "href": "weeks/week-11.html#practice",
    "title": "Week 11",
    "section": "Practice",
    "text": "Practice\nIn-class Exercise & Solutions",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#perform",
    "href": "weeks/week-11.html#perform",
    "title": "Week 11",
    "section": "Perform",
    "text": "Perform\nIn-person midterm #2 which covers weeks 6-9. Held during the tutorial / lab sessions\nLecture 10 pre-recorded videos and quizzes\nFinal Project - Written feedback on project proposals sent by TAs and instructors\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "No Preparation for this week",
    "crumbs": [
      "Weekly materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#prepare",
    "href": "weeks/week-12.html#prepare",
    "title": "Week 12",
    "section": "",
    "text": "No Preparation for this week",
    "crumbs": [
      "Weekly materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\nSlides & Recordings",
    "crumbs": [
      "Weekly materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#practice",
    "href": "weeks/week-12.html#practice",
    "title": "Week 12",
    "section": "Practice",
    "text": "Practice\nIn-class Exercise & Solutions",
    "crumbs": [
      "Weekly materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#perform",
    "href": "weeks/week-12.html#perform",
    "title": "Week 12",
    "section": "Perform",
    "text": "Perform\nIn-person lab session: RNN text classification lab10\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Week 8 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-8.html#prepare",
    "href": "weeks/week-8.html#prepare",
    "title": "Week 8",
    "section": "",
    "text": "Week 8 Overview",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\nSlides\nMissing recording due to technical issue",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-8.html#practice",
    "href": "weeks/week-8.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\nIn-class Exercise & Solutions",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-8.html#perform",
    "href": "weeks/week-8.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\nIn-person lab session: transfer learning and double descent lab07\nLecture 8 pre-recorded videos and quizzes\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "Final project submission\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week-14.html#perform",
    "href": "weeks/week-14.html#perform",
    "title": "Week 14",
    "section": "",
    "text": "Final project submission\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 14"
    ]
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#data-sources",
    "href": "project-tips-resources.html#data-sources",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %&gt;%\n  count(manufacturer) %&gt;%\n  mutate(manufacturer = str_to_title(manufacturer)) %&gt;%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "ex/w03/exercises03_solution.html",
    "href": "ex/w03/exercises03_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean.\n\n\nFirst, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters). The product on the right hand side is also known as the likelihood function. In practice, we usually work with the log-likelihood function instead. Because the logarithm is monotonously increasing, the resulting estimators are the same, i.e. \\[\n\\hte_{\\mathrm{MLE}}\n= \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n= \\argmax_{\\te \\in \\Te} \\sum_{i=1}^N \\log \\li( f(\\fx_i | \\te)\\ri).\n\\]\nSecond, let’s remember that the probability density function of a multivariate Gaussian distribution is given by \\[\nf(\\fx_i | \\mu, \\Si)\n= \\frac{1}{(2 \\pi)^{n/2} |\\Si|^{1/2}}\n   \\exp \\li(\n     - \\frac{1}{2}\n     (\\fx_i - \\mu)^\\top\n     \\Si^{-1}\n     (\\fx_i - \\mu)\n   \\ri)\n\\] with parameters \\(\\te=(\\mu, \\Si)\\), where \\(\\mu\\in\\R^n\\) is the mean vector and \\(\\Si\\in\\R^{n\\times n}\\) is the covariance matrix. Moreover, the notation \\(|\\Si|\\) denotes the determinant of \\(\\Si\\).\nOur goal is to maximize the log-likelihood function which in this case is \\[\\begin{aligned}\nl(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n  & := \\sum_{i=1}^N \\log f(\\fx_i | \\mu, \\Si) \\\\\n  & = \\sum_{i=1}^N \\li(\n    - \\frac{n}{2} \\log (2 \\pi)\n    - \\frac{1}{2} \\log |\\Si|  \n    - \\frac{1}{2}  (\\fx_i - \\mu)^\\top \\Si^{-1} (\\fx_i - \\mu)\n  \\ri).\n\\end{aligned}\\] For obtaining the MLE of \\(\\mu\\), we can simply take the gradient of \\(l\\) with respect to \\(\\mu\\) and set it to zero resulting in: \\[\n\\nabla_\\mu l(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n= \\sum_{i=1}^N  \\Si^{-1} ( \\fx_i - \\mu )\n= 0\n\\] Since \\(\\Si\\) is a covariance matrix, it is positive definite. Thus, we can multiply both sides of the equation by \\(\\Si\\) and obtain \\[\n\\begin{aligned}\n0 & = N \\mu - \\sum_{i=1}^N  \\fx_i,\n\\\\\n\\Rightarrow \\hmu_{\\mathrm{MLE}} &=  \\frac{1}{N} \\sum_{i=1}^N \\fx_i .\n\\end{aligned}\n\\] Conveniently, \\(\\Si\\) disappears and thus we do not have to worry about it."
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-1---maximum-likelihood-estimation-refresher",
    "href": "ex/w03/exercises03_solution.html#exercise-1---maximum-likelihood-estimation-refresher",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean.\n\n\nFirst, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters). The product on the right hand side is also known as the likelihood function. In practice, we usually work with the log-likelihood function instead. Because the logarithm is monotonously increasing, the resulting estimators are the same, i.e. \\[\n\\hte_{\\mathrm{MLE}}\n= \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n= \\argmax_{\\te \\in \\Te} \\sum_{i=1}^N \\log \\li( f(\\fx_i | \\te)\\ri).\n\\]\nSecond, let’s remember that the probability density function of a multivariate Gaussian distribution is given by \\[\nf(\\fx_i | \\mu, \\Si)\n= \\frac{1}{(2 \\pi)^{n/2} |\\Si|^{1/2}}\n   \\exp \\li(\n     - \\frac{1}{2}\n     (\\fx_i - \\mu)^\\top\n     \\Si^{-1}\n     (\\fx_i - \\mu)\n   \\ri)\n\\] with parameters \\(\\te=(\\mu, \\Si)\\), where \\(\\mu\\in\\R^n\\) is the mean vector and \\(\\Si\\in\\R^{n\\times n}\\) is the covariance matrix. Moreover, the notation \\(|\\Si|\\) denotes the determinant of \\(\\Si\\).\nOur goal is to maximize the log-likelihood function which in this case is \\[\\begin{aligned}\nl(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n  & := \\sum_{i=1}^N \\log f(\\fx_i | \\mu, \\Si) \\\\\n  & = \\sum_{i=1}^N \\li(\n    - \\frac{n}{2} \\log (2 \\pi)\n    - \\frac{1}{2} \\log |\\Si|  \n    - \\frac{1}{2}  (\\fx_i - \\mu)^\\top \\Si^{-1} (\\fx_i - \\mu)\n  \\ri).\n\\end{aligned}\\] For obtaining the MLE of \\(\\mu\\), we can simply take the gradient of \\(l\\) with respect to \\(\\mu\\) and set it to zero resulting in: \\[\n\\nabla_\\mu l(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n= \\sum_{i=1}^N  \\Si^{-1} ( \\fx_i - \\mu )\n= 0\n\\] Since \\(\\Si\\) is a covariance matrix, it is positive definite. Thus, we can multiply both sides of the equation by \\(\\Si\\) and obtain \\[\n\\begin{aligned}\n0 & = N \\mu - \\sum_{i=1}^N  \\fx_i,\n\\\\\n\\Rightarrow \\hmu_{\\mathrm{MLE}} &=  \\frac{1}{N} \\sum_{i=1}^N \\fx_i .\n\\end{aligned}\n\\] Conveniently, \\(\\Si\\) disappears and thus we do not have to worry about it."
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-2---more-gradients",
    "href": "ex/w03/exercises03_solution.html#exercise-2---more-gradients",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - More Gradients",
    "text": "Exercise 2 - More Gradients\nYou are an ML Engineer at Googlezon where you are working on an internal ML framework called TorchsorFlow. You are tasked with implementing a new layer known as BatchNormalization. The idea of this layer is as follows:\nDuring training, consider the outputs of the previous layer \\(\\fa_i=(a_i^{(1)}, \\ldots, a_i^{(N)})\\) for each element \\(i\\in \\{1, \\ldots, M\\}\\) of the input batch. Compute the mean \\(\\mu_j\\) and variance \\(\\si_j^2\\) over each input dimension \\(j\\). Use the resulting statistics to normalize the output of the previous layer. Finally, rescale the resulting vector with a learned constant \\(\\gm\\) and shift it by another learned constant \\(\\be\\).\n\nWrite down the mathematical expression for the BatchNormalization layer. What are its learnable parameters?\nCompute the gradient of the loss \\(\\mcL\\) with respect to the input of the BatchNormalization \\(\\fa_i\\) layer.\nAt test time, the batch size is usually 1. So, it is not meaningful (or even possible) to compute mean / variance. How would you implement a layer like this?\n\n\nSolution\nFor the purpose of batch normalization, we can consider each output neuron individually. Thus, we will simplify our notation and write \\(a_i\\), \\(y_i\\), … instead of \\(a_i^{(j)}\\), \\(y_i^{(j)}\\), … respectively.\n\nThe forward pass is given by the following equations \\[\n\\begin{aligned}\n\\mu_B\n  &:= \\frac{1}{M}\\sum_{i=1}^m a_i,\n  &\\text{(mini-batch mean)}  \\\\\n\\sigma_B^2\n  &:=\\frac{1}{M}\\sum_{i=1}^M (a_i-\\mu_B)^2,\n  &\\text{(mini-batch variance)}\\\\\n\\ha_i\n  &:= \\frac{a_i-\\mu_B}{\\sqrt{\\si_B^2+\\epsilon}},   \n&\\text{(normalize)}\\\\\n  y_i\n  &:= BN_{\\gamma, \\beta}((a_i)_{i=1}^M)\n  := \\gamma\\ha_i + \\beta.\n  &\\text{(scale and shift)}\n\\end{aligned}\n\\] The entire layer is defined as \\[\nBN(\\fa_1, \\ldots \\fa_M)=\\big(\n  BN_{\\gm^{(1)}, \\be^{(1)}}\\big((a_i^{(1)})_{i=1}^M\\big),\n  \\ldots,\n  BN_{\\gm^{(N)}, \\be^{(N)}}\\big((a_i^{(N)})_{i=1}^M\\big)\n  \\big)\n\\] where \\(\\gm^{(1)}, \\ldots, \\gm^{(N)}\\) and \\(\\be^{(1)}, \\ldots, \\be^{(N)}\\) are learnable parameters.\nThe derivatives can be expressed using the chain rule where we obtain \\(\\mu_B\\), \\(\\si_B\\), \\(\\ha_i\\), and \\(y_i\\) during the forward pass while \\(\\partial \\mcL/\\partial y_i\\) is obtained from earlier steps of the backward pass. The remaining derivatives are: \\[\n\\begin{aligned}\n\\frac{\\partial\\mcL}{\\partial \\ha_i} &= \\fr{\\partial \\mcL}{y_i}\\cdot \\gm, \\\\\n\\frac{\\partial\\mcL}{\\partial \\si_B^2}\n  &=\n  \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\\cdot(a_i-\\mu_B)\\cdot\n  \\frac{-1}{2}(\\si_B^2+\\epsilon)^{-3/2}, \\\\\n\\fr{\\partial\\mcL}{\\partial\\mu_B}\n  &=\n  \\bigg(\\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot\\frac{-1}{\\sqrt{\\sigma_B^2+\\epsilon}}\\bigg)\n  +\\fr{\\partial\\mcL}{ \\partial \\si_B^2}\n  \\cdot\\frac{\\sum_{i=1}^M  -2(a_i-\\mu_B)}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial a_i}\n  &=\n  \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot \\frac{1}{\\sqrt{\\sigma_B^2+\\epsilon}}  + \\fr{\\partial \\mcL}{\\partial \\sigma_B^2} \\cdot \\frac{2(a_i-\\mu_B)}{M} + \\fr{\\partial \\mcL}{\\partial \\mu_B}\\cdot \\frac{1}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial\\gamma}&= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i} \\cdot \\ha_i, \\\\\n\\fr{\\partial\\mcL}{\\partial\\beta} &= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i}.\n\\end{aligned}\n\\] Here \\(\\epsilon\\) is a small constant which is added in practice to the variance to avoid division by zero. It is actually not part of the derivative."
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-3---autodiff-modes",
    "href": "ex/w03/exercises03_solution.html#exercise-3---autodiff-modes",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Autodiff Modes",
    "text": "Exercise 3 - Autodiff Modes\nConsider the function \\(F(x) = f_3(f_2(f_1(x))\\) and assume you also know the derivatives \\(f_i'\\) for all \\(f_i\\).\n\nApply the chain rule to express \\(F'(x)\\) in terms of \\(f_i'\\)s and \\(f_i\\).\nWrite down the pseudocode for computing \\(F'(x)\\) using the forward mode and the reverse mode respectively. Assume all functions to be scalar functions of a scalar variable, i.e. \\(f_i: \\R \\rightarrow \\R\\).\nIf you simply ask your interpreter / compiler to evaluate the expression in (a), will the computation be in forward mode, reverse mode, or neither of the modes? Why? You can assume that your interpreter / compiler does not do any caching or optimization and simply evaluates the expression from left to right. Does anything change if you assume that your interpreter caches results that have been computed before?\n\n\nSolution\n\nFor better readability we will write \\((g\\circ h)(x)\\) for \\(g(h(x))\\). By applying the chain rule, we obtain \\[\n\\begin{aligned}\nF'(x)\n&= (f_3 \\circ f_2 \\circ f_1)' (x) \\\\\n&= (f_2 \\circ f_1)'(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\\\\\n&= f_1'(x) \\cdot (f_2' \\circ f_1)(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\n\\end{aligned}\n\\]\nFirst, let’s start with the forward mode\nd = f1'(x)\nv = f1(x)\nd = f2'(x) * d\nv = f2(v)\nd = f3'(v) * d\nNow, for the reverse mode, we first do a “forward pass” before computing gradients:\nv1 = f1(x)\nv2 = f2(v1)\nd = f3'(v2)\nd = d*f2'(v1)\nd = d*f1'(x)\nSimply evaluating the expression in (a) is not in line with any of the modes. It also involves repeated computations because \\(f_1(x)\\) will be computed twice. Now, if we allow for caching of ingtermediate results, this doubling of compugtaiton disappears. The order written above will then be in line with forward move automatic differentiation. However, this is specific to our example and in general not true."
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-4---glove-embeddings",
    "href": "ex/w03/exercises03_solution.html#exercise-4---glove-embeddings",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - GloVe Embeddings",
    "text": "Exercise 4 - GloVe Embeddings\nOpen the notebook presented in class and work through it by trying some of the ideas presented therein for different word combinations."
  },
  {
    "objectID": "ex/w03/questions/autodiff-modes-sol.html",
    "href": "ex/w03/questions/autodiff-modes-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "For better readability we will write \\((g\\circ h)(x)\\) for \\(g(h(x))\\). By applying the chain rule, we obtain \\[\n\\begin{aligned}\nF'(x)\n&= (f_3 \\circ f_2 \\circ f_1)' (x) \\\\\n&= (f_2 \\circ f_1)'(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\\\\\n&= f_1'(x) \\cdot (f_2' \\circ f_1)(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\n\\end{aligned}\n\\]\nFirst, let’s start with the forward mode\nd = f1'(x)\nv = f1(x)\nd = f2'(x) * d\nv = f2(v)\nd = f3'(v) * d\nNow, for the reverse mode, we first do a “forward pass” before computing gradients:\nv1 = f1(x)\nv2 = f2(v1)\nd = f3'(v2)\nd = d*f2'(v1)\nd = d*f1'(x)\nSimply evaluating the expression in (a) is not in line with any of the modes. It also involves repeated computations because \\(f_1(x)\\) will be computed twice. Now, if we allow for caching of ingtermediate results, this doubling of compugtaiton disappears. The order written above will then be in line with forward move automatic differentiation. However, this is specific to our example and in general not true."
  },
  {
    "objectID": "ex/w03/questions/backprop-batchnorm.html",
    "href": "ex/w03/questions/backprop-batchnorm.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are an ML Engineer at Googlezon where you are working on an internal ML framework called TorchsorFlow. You are tasked with implementing a new layer known as BatchNormalization. The idea of this layer is as follows:\nDuring training, consider the outputs of the previous layer \\(\\fa_i=(a_i^{(1)}, \\ldots, a_i^{(N)})\\) for each element \\(i\\in \\{1, \\ldots, M\\}\\) of the input batch. Compute the mean \\(\\mu_j\\) and variance \\(\\si_j^2\\) over each input dimension \\(j\\). Use the resulting statistics to normalize the output of the previous layer. Finally, rescale the resulting vector with a learned constant \\(\\gm\\) and shift it by another learned constant \\(\\be\\).\n\nWrite down the mathematical expression for the BatchNormalization layer. What are its learnable parameters?\nCompute the gradient of the loss \\(\\mcL\\) with respect to the input of the BatchNormalization \\(\\fa_i\\) layer.\nAt test time, the batch size is usually 1. So, it is not meaningful (or even possible) to compute mean / variance. How would you implement a layer like this?"
  },
  {
    "objectID": "ex/w03/questions/backprop-batchnorm-sol.html",
    "href": "ex/w03/questions/backprop-batchnorm-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "For the purpose of batch normalization, we can consider each output neuron individually. Thus, we will simplify our notation and write \\(a_i\\), \\(y_i\\), … instead of \\(a_i^{(j)}\\), \\(y_i^{(j)}\\), … respectively.\n\nThe forward pass is given by the following equations \\[\n\\begin{aligned}\n\\mu_B\n  &:= \\frac{1}{M}\\sum_{i=1}^m a_i,\n  &\\text{(mini-batch mean)}  \\\\\n\\sigma_B^2\n  &:=\\frac{1}{M}\\sum_{i=1}^M (a_i-\\mu_B)^2,\n  &\\text{(mini-batch variance)}\\\\\n\\ha_i\n  &:= \\frac{a_i-\\mu_B}{\\sqrt{\\si_B^2+\\epsilon}},   \n&\\text{(normalize)}\\\\\n  y_i\n  &:= BN_{\\gamma, \\beta}((a_i)_{i=1}^M)\n  := \\gamma\\ha_i + \\beta.\n  &\\text{(scale and shift)}\n\\end{aligned}\n\\] The entire layer is defined as \\[\nBN(\\fa_1, \\ldots \\fa_M)=\\big(\n  BN_{\\gm^{(1)}, \\be^{(1)}}\\big((a_i^{(1)})_{i=1}^M\\big),\n  \\ldots,\n  BN_{\\gm^{(N)}, \\be^{(N)}}\\big((a_i^{(N)})_{i=1}^M\\big)\n  \\big)\n\\] where \\(\\gm^{(1)}, \\ldots, \\gm^{(N)}\\) and \\(\\be^{(1)}, \\ldots, \\be^{(N)}\\) are learnable parameters.\nThe derivatives can be expressed using the chain rule where we obtain \\(\\mu_B\\), \\(\\si_B\\), \\(\\ha_i\\), and \\(y_i\\) during the forward pass while \\(\\partial \\mcL/\\partial y_i\\) is obtained from earlier steps of the backward pass. The remaining derivatives are: \\[\n\\begin{aligned}\n\\frac{\\partial\\mcL}{\\partial \\ha_i} &= \\fr{\\partial \\mcL}{y_i}\\cdot \\gm, \\\\\n\\frac{\\partial\\mcL}{\\partial \\si_B^2}\n  &=\n  \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\\cdot(a_i-\\mu_B)\\cdot\n  \\frac{-1}{2}(\\si_B^2+\\epsilon)^{-3/2}, \\\\\n\\fr{\\partial\\mcL}{\\partial\\mu_B}\n  &=\n  \\bigg(\\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot\\frac{-1}{\\sqrt{\\sigma_B^2+\\epsilon}}\\bigg)\n  +\\fr{\\partial\\mcL}{ \\partial \\si_B^2}\n  \\cdot\\frac{\\sum_{i=1}^M  -2(a_i-\\mu_B)}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial a_i}\n  &=\n  \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot \\frac{1}{\\sqrt{\\sigma_B^2+\\epsilon}}  + \\fr{\\partial \\mcL}{\\partial \\sigma_B^2} \\cdot \\frac{2(a_i-\\mu_B)}{M} + \\fr{\\partial \\mcL}{\\partial \\mu_B}\\cdot \\frac{1}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial\\gamma}&= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i} \\cdot \\ha_i, \\\\\n\\fr{\\partial\\mcL}{\\partial\\beta} &= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i}.\n\\end{aligned}\n\\] Here \\(\\epsilon\\) is a small constant which is added in practice to the variance to avoid division by zero. It is actually not part of the derivative."
  },
  {
    "objectID": "ex/w04/exercises04-notes.html",
    "href": "ex/w04/exercises04-notes.html",
    "title": "Solution Computations",
    "section": "",
    "text": "This file is for internal use where we compute the solutions to the exercises. I also compute the solutions for seemingly easy exercises here to use the code later for automation of the process."
  },
  {
    "objectID": "ex/w04/exercises04-notes.html#cnn-size-exercise-solutions",
    "href": "ex/w04/exercises04-notes.html#cnn-size-exercise-solutions",
    "title": "Solution Computations",
    "section": "CNN Size exercise solutions",
    "text": "CNN Size exercise solutions\n\nimport torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0, bias=False)\n\nx = torch.tensor([\n    [ 1, 0, -2, 1],\n    [ 0, 1, 1, 0],\n    [ 0, 1, 0, 1],\n    [ -3, 4, 0, 0]\n])\n\nk = torch.tensor([\n    [ 2, 1],\n    [ 0, 1]\n])\n\nconv.weight.data = k.view(1, 1, 2, 2).float()\n\nprint(conv(x.view(1, 1, 4, 4).float()))\n\n\n\ntensor([[[[ 3., -1., -3.],\n          [ 2.,  3.,  3.],\n          [ 5.,  2.,  1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=0, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 3., -3.],\n          [ 5.,  1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# PyTorch uses zero-padding by default\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=1, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1., -2.,  0.],\n          [ 0.,  3.,  0.],\n          [-3.,  8.,  0.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)"
  },
  {
    "objectID": "ex/w04/exercises04-notes.html#exercise-3-mlp-sizes",
    "href": "ex/w04/exercises04-notes.html#exercise-3-mlp-sizes",
    "title": "Solution Computations",
    "section": "Exercise 3 MLP Sizes",
    "text": "Exercise 3 MLP Sizes\n\na = nn.Sequential(\n    nn.Linear(10,5),\n    nn.Linear(5,10),\n    nn.Linear(10,5)\n)\n\nfor i in a:\n    print(sum(p.numel() for p in i.parameters()))\n\n55\n60\n55"
  },
  {
    "objectID": "ex/w04/exercises04-notes.html#exercise-4-cnn-sizes",
    "href": "ex/w04/exercises04-notes.html#exercise-4-cnn-sizes",
    "title": "Solution Computations",
    "section": "Exercise 4 CNN Sizes",
    "text": "Exercise 4 CNN Sizes\n\ninp = torch.randn(1, 3, 100, 100)\narch = nn.Sequential(\n    nn.Conv2d(3, 5, kernel_size=3),\n    nn.MaxPool2d(2, stride=2),\n    nn.Conv2d(5, 10, kernel_size=3, stride=1),\n    nn.MaxPool2d(2, stride=2),\n    nn.Conv2d(10, 5, kernel_size=3, stride=1),\n    nn.Flatten(),\n    nn.Linear(2205, 20),\n    nn.Linear(20,10)\n)\n\ncur = inp\nfor num, layer in enumerate(arch):\n    cur = layer(cur)\n    print(f\"{num+1}. Activation: {cur.shape}, Params: {sum(p.numel() for p in layer.parameters())}\") \n\n1. Activation: torch.Size([1, 5, 98, 98]), Params: 140\n2. Activation: torch.Size([1, 5, 49, 49]), Params: 0\n3. Activation: torch.Size([1, 10, 47, 47]), Params: 460\n4. Activation: torch.Size([1, 10, 23, 23]), Params: 0\n5. Activation: torch.Size([1, 5, 21, 21]), Params: 455\n6. Activation: torch.Size([1, 2205]), Params: 0\n7. Activation: torch.Size([1, 20]), Params: 44120\n8. Activation: torch.Size([1, 10]), Params: 210\n\n\n\n\n\n0 5\n1 4\n2 2"
  },
  {
    "objectID": "ex/w04/questions/mlp-sizes-sol.html",
    "href": "ex/w04/questions/mlp-sizes-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The number of parameters for each neuron is the number of weights plus one for the biaas term. The number of weights corresponds to the number of inputs / activations from the previous layer. So for the first layer, we have 10 inputs and thus 11 parameters per neuron resulting in 55 parameters total per layer.\nA similar computation gives 60 and 55 as the number of parameters for the next two layers. Thus, the network has a total of 170 parameters."
  },
  {
    "objectID": "ex/w04/questions/cnn-sizes.html",
    "href": "ex/w04/questions/cnn-sizes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are givne a neural network with the following architecture:\nInput: 100 x 100 x 3 Image\n\nLayers:\n1. Conv(in_channels=3, out_channels=5, kernel_size=3, stride=1, padding=0)\n2. MaxPool2d(kernel_size=2, stride=2, padding=0)\n3. Conv(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=0)\n4. MaxPool2d(kernel_size=2, stride=2, padding=0)\n5. Conv(in_channels=10, out_channels=5, kernel_size=3, stride=1, padding=0)\n6. Flatten()\n7. MLP(neurons=20)\n8. MLP(neurons=10)\n\nWhat is the dimensionality of the activations after each layer.\nHow many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/questions/mlp-sizes.html",
    "href": "ex/w04/questions/mlp-sizes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given an MLP with ReLU activations. It has 3 layers consisting of 5, 10, and 5 neurons respectively. The input is a vector of size 10. How many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/questions/cnn-by_hand.html",
    "href": "ex/w04/questions/cnn-by_hand.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Consider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & -2 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n-3 & 4 & 0 & 0\n\\epmat, \\qquad\n%\nK = \\bpmat\n2, & 1 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w05/exercises05-notes.html",
    "href": "ex/w05/exercises05-notes.html",
    "title": "Ex 2: Eigenvalues and Eigenvectors to matrices",
    "section": "",
    "text": "Part (a) is straightforward. One needs to simply concatenate the given eigenvalues in a matrix.\n\nimport numpy as np\n\nla1 = 2\nla2 = 3\nev1 = np.array([1, 0])\nev2 = np.array([0, -1])\n\nLa = np.diag([la1, la2])\nO = np.array([ev1, ev2])\n\nnp.matmul(np.matmul(O, La), O.T)\n\narray([[2, 0],\n       [0, 3]])\n\n\nPart (b) has a catch. The eigenvectors are not normalized. So, we need to normalize them first before concatenating them in a matrix.\n\nla1 = 2\nla2 = 3\nev1 = np.array([1, 1])\nev2 = np.array([1, -1])\n\nLa = np.diag([la1, la2])\nO = np.array([ev1, ev2])\n\nprint(np.matmul(np.matmul(O, La), np.linalg.inv(O)))\nprint(np.matmul(np.matmul(O, La), (O).T)/2) # Sanity check\n\n[[ 2.5 -0.5]\n [-0.5  2.5]]\n[[ 2.5 -0.5]\n [-0.5  2.5]]\n1.4142135623730951\n\n\n\nnp.linalg.norm(ev2)\n\n2.8284271247461903\n\n\n\nEx 3: SGD w. Momentum Implementation\nFirst, let’s define an objective function, its gradient, and a starting point for the optimizer. In our case, this is simply \\(f(x) = x^2\\) and \\(x_{init}\\)=1.\n\ndef objective(x):\n    return x**2\n\ndef obj_grad(x):\n    return 2*x\n\nNow, we implement the actual optimizer.\n\ndef sgd_with_momentum(obj, grad, x_init, learning_rate, momentum, max_iter):\n  x = x_init\n  update = 0\n  for i in range(max_iter):\n    update = momentum * update - learning_rate * grad(x)\n    x = x + update\n\n    print('&gt;%d f(%s) = %.5f' % (i, x, obj(x)))\n  return x\n\n\nsgd_with_momentum(\n    objective, obj_grad, x_init=3.0, learning_rate=0.1, momentum=0.5, \n    max_iter=20\n    )\n\n&gt;0 f(2.4) = 5.76000\n&gt;1 f(1.6199999999999999) = 2.62440\n&gt;2 f(0.9059999999999999) = 0.82084\n&gt;3 f(0.3677999999999999) = 0.13528\n&gt;4 f(0.02513999999999994) = 0.00063\n&gt;5 f(-0.15121800000000002) = 0.02287\n&gt;6 f(-0.2091534) = 0.04375\n&gt;7 f(-0.19629041999999997) = 0.03853\n&gt;8 f(-0.15060084599999995) = 0.02268\n&gt;9 f(-0.09763588979999996) = 0.00953\n&gt;10 f(-0.05162623373999997) = 0.00267\n&gt;11 f(-0.018296158961999986) = 0.00033\n&gt;12 f(0.0020281102194000047) = 0.00000\n&gt;13 f(0.011784622766219999) = 0.00014\n&gt;14 f(0.014305954486385997) = 0.00020\n&gt;15 f(0.012705429449191796) = 0.00016\n&gt;16 f(0.009364081040756336) = 0.00009\n&gt;17 f(0.005820590628387338) = 0.00003\n&gt;18 f(0.002884727296525372) = 0.00001\n&gt;19 f(0.0008398501712893144) = 0.00000\n\n\n0.0008398501712893144"
  },
  {
    "objectID": "ex/w05/exercises05.html",
    "href": "ex/w05/exercises05.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\)."
  },
  {
    "objectID": "ex/w05/exercises05.html#exercise-1---taylor-series",
    "href": "ex/w05/exercises05.html#exercise-1---taylor-series",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\)."
  },
  {
    "objectID": "ex/w05/exercises05.html#exercise-2---eigenvalues-eigenvectors",
    "href": "ex/w05/exercises05.html#exercise-2---eigenvalues-eigenvectors",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Eigenvalues, Eigenvectors",
    "text": "Exercise 2 - Eigenvalues, Eigenvectors\nYou are given the sets of eigevalues and eigenvectors. Compute the corresponding matrix.\n\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,0)^\\top\\), \\(\\fv_2 = (0,1)^\\top\\).\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,1)^\\top\\), \\(\\fv_2 = (1,-1)^\\top\\)."
  },
  {
    "objectID": "ex/w05/exercises05.html#exercise-3---sgd-with-momentum",
    "href": "ex/w05/exercises05.html#exercise-3---sgd-with-momentum",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - SGD with Momentum",
    "text": "Exercise 3 - SGD with Momentum\nImplement stochastic gradient descent with momentum and apply it to optimize some elementary functions in 1d and 2d."
  },
  {
    "objectID": "ex/w05/questions/calc-taylor.html",
    "href": "ex/w05/questions/calc-taylor.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\)."
  },
  {
    "objectID": "ex/w05/questions/linalg-evs_to_mat-sol.html",
    "href": "ex/w05/questions/linalg-evs_to_mat-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\n\nHere, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 2 & 0 \\\\\n0 & 3\n\\epmat\n  \\]\nHere, we have to normalize the eigenvectors first. Each has length \\(\\sqrt{2}\\), so we have to divide each of them by \\(\\sqrt{2}\\), i.e. we set \\(\\fte_i:=\\fe_i/\\sqrt{2}\\). With this, we can construct an orthogonal matrix of eigenvalues as \\(\\fQ = (\\fte_1, \\fte_2)\\). The resulting matrix \\(\\fA\\) is \\[\n  \\fA = \\bpmat\n2.5 & -0.5 \\\\\n-0.5 & 2.5\n\\epmat\n  \\]"
  },
  {
    "objectID": "ex/w05/questions/opt-momentum.html",
    "href": "ex/w05/questions/opt-momentum.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Implement stochastic gradient descent with momentum and apply it to optimize some elementary functions in 1d and 2d."
  },
  {
    "objectID": "ex/w02/exercises02_solution.html",
    "href": "ex/w02/exercises02_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue.\n\n\n\nBecause \\(\\la_1=\\la_2\\), we will write \\(\\la\\) for simplicity. The result is obtained by applying the definition of eigenvalues and distributivity via \\[\\begin{equation*}\n\\fA\\fu\n  = \\fA (\\al \\fu_1 + \\be \\fu_2)\n  =   \\al \\fA \\fu_1 + \\be \\fA \\fu_2\n  =   \\al \\la \\fu_1 + \\be \\la \\fu_2\n  =   \\la (\\al\\fu_1 + \\be \\fu_2)\n  =   \\la \\fu .\n\\end{equation*}\\]"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-1---eigenvectors-and-eigenvalues",
    "href": "ex/w02/exercises02_solution.html#exercise-1---eigenvectors-and-eigenvalues",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue.\n\n\n\nBecause \\(\\la_1=\\la_2\\), we will write \\(\\la\\) for simplicity. The result is obtained by applying the definition of eigenvalues and distributivity via \\[\\begin{equation*}\n\\fA\\fu\n  = \\fA (\\al \\fu_1 + \\be \\fu_2)\n  =   \\al \\fA \\fu_1 + \\be \\fA \\fu_2\n  =   \\al \\la \\fu_1 + \\be \\la \\fu_2\n  =   \\la (\\al\\fu_1 + \\be \\fu_2)\n  =   \\la \\fu .\n\\end{equation*}\\]"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-2---variance-and-expectation",
    "href": "ex/w02/exercises02_solution.html#exercise-2---variance-and-expectation",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Variance and Expectation",
    "text": "Exercise 2 - Variance and Expectation\n\nGiven a set of vectors \\(\\{\\fx_i\\}_{i=1}^N\\). Show that their empirical mean is equivalent to \\[\\hmu=\\argmin_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2.\\]\nThere are two equivalent definitons of variance of a random variable. The first one is \\(\\Var(X) := \\E[(X - \\E[X])^2]\\) and the second is \\(\\Var(X) = \\E[X^2] - \\E[X]^2\\). Show that these two definitions actually are equivalent.\n\n\nSolution\n\nThe key idea here is to compute the gradient of the objective function and solve for \\(\\fmu\\). The gradient is obtained by applying the chain rule resulting in \\[0=\\nabla_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2 = -2 \\sum_i (\\fx_i - \\fmu) .\\] Now, we solve this for \\(\\fmu\\) to obtain \\[\\fmu = \\frac{1}{N} \\sum_i \\fx_i .\\]\nHere, we simply need to apply some algebraic manipulations to show that the two definitions are equivalent. We start with the first definition and expand the square: \\[\\begin{align*}\n\\E[(X - \\E[X])^2]\n  &= \\E\\li[X^2 - 2X\\E[X]+\\E[X]^2\\ri] \\\\\n  &= \\E\\li[X^2\\ri] - \\E[2X\\E[X]]+\\E\\li[\\E[X]^2\\ri]\\\\\n  &= \\E\\li[X^2\\ri] - 2\\E[X]\\E[X]+\\E[X]^2\\\\\n  &= \\E\\li[X^2\\ri] - \\E[X]^2\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-3---linear-regression",
    "href": "ex/w02/exercises02_solution.html#exercise-3---linear-regression",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Linear Regression",
    "text": "Exercise 3 - Linear Regression\n\n\nIn the linear regression model with one feature, we have the following model/hypothesis: \\[y = f(x) = w x + b\\] ​with parameters, \\(w\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}(w, b) = \\frac{1}{2N}\\sum_i ((w x^{(i)} + b) - t^{(i)})^2\\] What are the derivatives \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\) and \\(\\frac{\\partial \\mathcal{E}}{\\partial b}\\)?\nIn the linear regression model with many features, we have the following model/hypothesis: \\[y = f(x) = {\\bf w}^\\top {\\bf x}+ b\\] with parameters, \\({\\bf w} = [w_1, w_2, \\dots w_d]^T\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_i ((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})^2\\] What is the derivative \\(\\frac{\\partial \\mathcal{E}}{\\partial w_j}\\) for a weight \\(w_j\\)?\n\n\nSolution\n\nWe obtain the derivative with respect to \\(w\\) directly using the chain rule resulting in \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w}\n  = \\frac{1}{N}\\sum_i x^{(i)}((w x^{(i)} + b) - t^{(i)})\n\\] Similarly, the derivative with respect to \\(b\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial b}\n  = \\frac{1}{N}\\sum_i ((w x^{(i)} + b) - t^{(i)})\n\\]\nThe derivative with respect to \\(w_j\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w_j}\n  = \\frac{1}{N}\\sum_i x_j^{(i)}((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})\n\\]"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-4---gradients-and-computation-graphs",
    "href": "ex/w02/exercises02_solution.html#exercise-4---gradients-and-computation-graphs",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - Gradients and Computation Graphs",
    "text": "Exercise 4 - Gradients and Computation Graphs\n\n\nCompute the \\(\\frac{\\partial \\mathcal{L}}{\\partial w_j}\\) gradient of \\(\\mathcal{L}\\) with respect to a \\(w_j\\) in the following computation: \\[\\begin{align*}\n  \\mathcal{L}(y, t) &= - t \\log(y) - (1-t) \\log(1-y) ,\n& y &= \\sigma(z) ,\n&  z &= {\\bf w}^\\top {\\bf x} .\n  \\end{align*}\\]\nDraw the computation graph for the following neural network, showing the relevant scalar quantities. Assume that \\(\\fy, \\fh, \\fx \\in \\mathbb{R}^2\\) \\[\\begin{align*}\n  \\mathcal{L} &= \\frac{1}{2}\\sum_k (y_k - t_k)^2 ,\n& y_k &= \\sum_i w_{ki}^{(2)} h_i + b_k^{(2)} ,\n& h_i &= \\sigma(z_i) ,\n& z_i &= \\sum_j w_{ij}^{(1)} x_j + b_i^{(1)} .\n  \\end{align*}\\]\n\n\nSolution\n\nApplying the chain rule, we have\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial w_j} \\]\nLooking at each term individually yields \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial y}\n  &= \\frac{\\partial}{\\partial y} [-t \\log(y) - (1 - t) \\log(1 - y)]\n  = - \\frac{t}{y} + \\frac{1 - t}{1 - y}\\\\\n\\frac{\\partial y}{\\partial z}\n  &= \\frac{\\partial \\sigma(z)}{\\partial z}\n  = \\sigma(z) (1 - \\sigma(z))\n  = y (1 - y)\\\\\n\\frac{\\partial z}{\\partial w_j}\n  &= \\frac{\\partial}{\\partial w_j} (w^\\top x) = x_j\n\\end{aligned}\n\\]\nBringing it all together yields: \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial w_j}\n  &= \\left( - \\frac{t}{y} + \\frac{1 - t}{1 - y} \\right) \\cdot y (1 - y) \\cdot x_j \\\\\n  &= (-t + ty + 1 - t - y + ty) x_j \\\\\n  &= (y - t) x_j\n\\end{aligned}\n\\]\nThe computation graph is given in the figure below.\n\n\n\n\nComputation graph for exercise 4 (b)"
  },
  {
    "objectID": "ex/w02/questions/nn-compgraph.html",
    "href": "ex/w02/questions/nn-compgraph.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Compute the \\(\\frac{\\partial \\mathcal{L}}{\\partial w_j}\\) gradient of \\(\\mathcal{L}\\) with respect to a \\(w_j\\) in the following computation: \\[\\begin{align*}\n  \\mathcal{L}(y, t) &= - t \\log(y) - (1-t) \\log(1-y) ,\n& y &= \\sigma(z) ,\n&  z &= {\\bf w}^\\top {\\bf x} .\n  \\end{align*}\\]\nDraw the computation graph for the following neural network, showing the relevant scalar quantities. Assume that \\(\\fy, \\fh, \\fx \\in \\mathbb{R}^2\\) \\[\\begin{align*}\n  \\mathcal{L} &= \\frac{1}{2}\\sum_k (y_k - t_k)^2 ,\n& y_k &= \\sum_i w_{ki}^{(2)} h_i + b_k^{(2)} ,\n& h_i &= \\sigma(z_i) ,\n& z_i &= \\sum_j w_{ij}^{(1)} x_j + b_i^{(1)} .\n  \\end{align*}\\]"
  },
  {
    "objectID": "ex/w02/questions/linreg-sol.html",
    "href": "ex/w02/questions/linreg-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "We obtain the derivative with respect to \\(w\\) directly using the chain rule resulting in \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w}\n  = \\frac{1}{N}\\sum_i x^{(i)}((w x^{(i)} + b) - t^{(i)})\n\\] Similarly, the derivative with respect to \\(b\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial b}\n  = \\frac{1}{N}\\sum_i ((w x^{(i)} + b) - t^{(i)})\n\\]\nThe derivative with respect to \\(w_j\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w_j}\n  = \\frac{1}{N}\\sum_i x_j^{(i)}((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})\n\\]"
  },
  {
    "objectID": "ex/w02/questions/linreg.html",
    "href": "ex/w02/questions/linreg.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "In the linear regression model with one feature, we have the following model/hypothesis: \\[y = f(x) = w x + b\\] ​with parameters, \\(w\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}(w, b) = \\frac{1}{2N}\\sum_i ((w x^{(i)} + b) - t^{(i)})^2\\] What are the derivatives \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\) and \\(\\frac{\\partial \\mathcal{E}}{\\partial b}\\)?\nIn the linear regression model with many features, we have the following model/hypothesis: \\[y = f(x) = {\\bf w}^\\top {\\bf x}+ b\\] with parameters, \\({\\bf w} = [w_1, w_2, \\dots w_d]^T\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_i ((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})^2\\] What is the derivative \\(\\frac{\\partial \\mathcal{E}}{\\partial w_j}\\) for a weight \\(w_j\\)?"
  },
  {
    "objectID": "ex/w02/questions/prob-evvar.html",
    "href": "ex/w02/questions/prob-evvar.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Given a set of vectors \\(\\{\\fx_i\\}_{i=1}^N\\). Show that their empirical mean is equivalent to \\[\\hmu=\\argmin_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2.\\]\nThere are two equivalent definitons of variance of a random variable. The first one is \\(\\Var(X) := \\E[(X - \\E[X])^2]\\) and the second is \\(\\Var(X) = \\E[X^2] - \\E[X]^2\\). Show that these two definitions actually are equivalent."
  },
  {
    "objectID": "ex/w11/exercises11.html",
    "href": "ex/w11/exercises11.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\)."
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-1---eigenvalues-and-eigenvectors",
    "href": "ex/w11/exercises11.html#exercise-1---eigenvalues-and-eigenvectors",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\)."
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-2---parameter-counting",
    "href": "ex/w11/exercises11.html#exercise-2---parameter-counting",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Parameter Counting",
    "text": "Exercise 2 - Parameter Counting\nUse PyTorch to load the alexnet model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-3---convolutional-layers",
    "href": "ex/w11/exercises11.html#exercise-3---convolutional-layers",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Convolutional Layers",
    "text": "Exercise 3 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & 1 & -1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 3 & 0 & 1 \\\\\n1 & -1 & 0 & 1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 2 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-4---scaled-dot-product-attention",
    "href": "ex/w11/exercises11.html#exercise-4---scaled-dot-product-attention",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - Scaled Dot-Product Attention",
    "text": "Exercise 4 - Scaled Dot-Product Attention\nConsider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 3\\\\\n0 & 1\n\\epmat,\\quad\nK = \\bpmat\n1 & 1\\\\\n1 & 2\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 0 & -2\\\\\n2 & 1 & 2 \\\\\n0 & 3 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w11/questions/pytorch-parameter_count.html",
    "href": "ex/w11/questions/pytorch-parameter_count.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Use PyTorch to load the alexnet model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w11/questions/attn-transformers_by_hand-notes.html",
    "href": "ex/w11/questions/attn-transformers_by_hand-notes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\n\n\nQ = torch.tensor([\n    [1, 3], \n    [0, 1]]).float()\n\nK = torch.tensor([\n    [1, 1], \n    [1, 2],\n    [0, 1]]).float()\n\nV = torch.tensor([\n    [1, 0, -2],\n    [2, 1, 2], \n    [0, 3, -1]]).float()\n\n\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\n\ntensor([[1.7981, 0.9986, 1.4429],\n        [1.2552, 1.2483, 0.2622]])\n\n\nLazy version\n\nF.scaled_dot_product_attention(Q, K, V)\n\ntensor([[1.7981, 0.9986, 1.4429],\n        [1.2552, 1.2483, 0.2622]])"
  },
  {
    "objectID": "ex/w11/questions/linalg-evs_to_mat.html",
    "href": "ex/w11/questions/linalg-evs_to_mat.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\)."
  },
  {
    "objectID": "ex/w11/questions/pytorch-parameter_count-notes.html",
    "href": "ex/w11/questions/pytorch-parameter_count-notes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "import torchvision\nalexnet = torchvision.models.alexnet()\n\nprint(\n    f\"Total number of parameters: {sum(p.numel() for p in alexnet.parameters())}\")\n\nprint(\"\\n\\nParameter Overview in the backbone:\")\nfor layer in alexnet.features:\n    print(f\"{layer}: {sum(p.numel() for p in layer.parameters())}\")\n\nprint(\"\\n\\nParameter Ovewview in the head:\")\nfor layer in alexnet.classifier:\n    print(layer, sum(p.numel() for p in layer.parameters()))\n\nTotal number of parameters: 61100840\n\n\nParameter Overview in the backbone:\nConv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)): 23296\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)): 307392\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 663936\nReLU(inplace=True): 0\nConv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 884992\nReLU(inplace=True): 0\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 590080\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False): 0\n\n\nParameter Ovewview in the head:\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=9216, out_features=4096, bias=True) 37752832\nReLU(inplace=True) 0\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=4096, out_features=4096, bias=True) 16781312\nReLU(inplace=True) 0\nLinear(in_features=4096, out_features=1000, bias=True) 4097000"
  },
  {
    "objectID": "ex/w11/questions/linalg-evs_to_mat-notes.html",
    "href": "ex/w11/questions/linalg-evs_to_mat-notes.html",
    "title": "Eigenvalues and Eigenvectors to matrices",
    "section": "",
    "text": "Part (a) is straightforward. One needs to simply concatenate the given eigenvalues in a matrix.\n\nimport numpy as np\n\nla1 = 1\nla2 = 2\nev1 = np.array([np.sqrt(.5), np.sqrt(.5)])\nev2 = np.array([np.sqrt(.5), -np.sqrt(.5)])\n\nLa = np.diag([la1, la2])\nO = np.array([ev1, ev2])\n\n# Check orthogonality\nprint(np.dot(ev1, ev2))\n\nnp.matmul(np.matmul(O, La), O.T)\n\n4.266421588589642e-17\n\n\narray([[ 1.5, -0.5],\n       [-0.5,  1.5]])\n\n\nPart (b) has a catch. The eigenvectors are not normalized. So, we need to normalize them first before concatenating them in a matrix.\n\nla1 = 2\nla2 = 3\nla3 = 4\nev1 = np.array([np.sqrt(1/3), np.sqrt(1/3), np.sqrt(1/3)])\nev2 = np.array([np.sqrt(.5), -np.sqrt(.5), 0])\nev3 = np.array([np.sqrt(1/6), np.sqrt(1/6), -np.sqrt(2/3)])\n\nLa = np.diag([la1, la2, la3])\nO = np.array([ev1, ev2, ev3])\n\nprint(np.matmul(O, O.T))\nprint(np.matmul(np.matmul(O, La), np.linalg.inv(O)))\n\n[[ 1.00000000e+00 -3.39032612e-18  2.15314570e-17]\n [-3.39032612e-18  1.00000000e+00  1.84419141e-17]\n [ 2.15314570e-17  1.84419141e-17  1.00000000e+00]]\n[[ 3.         -0.40824829 -0.70710678]\n [-0.40824829  2.5        -0.28867513]\n [-0.70710678 -0.28867513  3.5       ]]\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb Cell 5 line 1\n      &lt;a href='vscode-notebook-cell:/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb#W4sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt; O = np.array([ev1, ev2, ev3])\n     &lt;a href='vscode-notebook-cell:/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb#W4sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt; print(np.matmul(O, O.T))\n---&gt; &lt;a href='vscode-notebook-cell:/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb#W4sZmlsZQ%3D%3D?line=11'&gt;12&lt;/a&gt; print(np.matmul(np.matmul(O, La), np.linalg.inv(O)))*np.sqrt(.5)\n\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'float'"
  },
  {
    "objectID": "ex/w11/questions/cnn-by_hand-notes.html",
    "href": "ex/w11/questions/cnn-by_hand-notes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0, bias=False)\n\nx = torch.tensor([\n    [ 1, 0, 1, -1],\n    [ 1, 0, 1, 0],\n    [ 0, 3, 0, 1],\n    [ 1, -1, 0, 1]\n])\n\nk = torch.tensor([\n    [ 1, 2],\n    [ 0, 1]\n])\n\nconv.weight.data = k.view(1, 1, 2, 2).float()\n\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1.,  3., -1.],\n          [ 4.,  2.,  2.],\n          [ 5.,  3.,  3.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=0, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1., -1.],\n          [ 5.,  3.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# PyTorch uses zero-padding by default\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=1, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1.,  1.,  0.],\n          [ 2.,  2.,  0.],\n          [ 2., -1.,  1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)"
  },
  {
    "objectID": "ex/w10/exercises10.html",
    "href": "ex/w10/exercises10.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)"
  },
  {
    "objectID": "ex/w10/exercises10.html#exercise-1---transposed-convolution-output-sizes",
    "href": "ex/w10/exercises10.html#exercise-1---transposed-convolution-output-sizes",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)"
  },
  {
    "objectID": "ex/w10/exercises10.html#exercise-2---transposed-convolution-parameter-sizes",
    "href": "ex/w10/exercises10.html#exercise-2---transposed-convolution-parameter-sizes",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Transposed Convolution Parameter Sizes",
    "text": "Exercise 2 - Transposed Convolution Parameter Sizes\nWhat is the number of learnable parameters for each of the following transposed convolution layers defined in PyTorch. Try to calculate those by hand first and use pytorch later to verify your results.\n\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=1)\nnn.ConvTranspose2d(in_channels=3, out_channels=10, kernel_size=3, stride=2)\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=4, stride=5)\nnn.ConvTranspose2d(in_channels=3, out_channels=4, kernel_size=3, stride=23)"
  },
  {
    "objectID": "ex/w10/exercises10.html#exercise-3---transposed-convolution-by-hand",
    "href": "ex/w10/exercises10.html#exercise-3---transposed-convolution-by-hand",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Transposed Convolution by Hand",
    "text": "Exercise 3 - Transposed Convolution by Hand\nYou are given an input matrix \\(X\\) (consisting of a single channel) and a kernel \\(K\\) as follows: \\[\nX = \\bpmat\n1 & 0 & 2 \\\\\n2 & 3 & 0 \\\\\n-1 & 0 & 3\n\\epmat, \\quad\nK = \\bpmat\n1 & 0 \\\\\n1 & 2\n\\epmat\n\\]\n\nCompute the transposed convolution by hand assuming stride 2.\nCompute the transposed convolution by hand assuming stride 1.\nUse PyTorch to verify your answers.\nImplement the transposed convolution in PyTorch without using its own implementaiton. You can assume no bias term a square kernel and no separate batch dimension. I.e. your task is to implement the following function\n\ndef conv_transpose2d(inp, weight, stride=1):\n    # inp - input of shape (C_in, H, W)\n    # weight - kernel of shape (C_in, C_out, K, K)\n    # stride - stride of the transposed convolution\n    # RETURNS\n    # output - output of shape (C_out, H_out, W_out)\n    #\n    # YOUR CODE HERE\n    return output"
  },
  {
    "objectID": "ex/w10/questions/tconv-by_hand.html",
    "href": "ex/w10/questions/tconv-by_hand.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given an input matrix \\(X\\) (consisting of a single channel) and a kernel \\(K\\) as follows: \\[\nX = \\bpmat\n1 & 0 & 2 \\\\\n2 & 3 & 0 \\\\\n-1 & 0 & 3\n\\epmat, \\quad\nK = \\bpmat\n1 & 0 \\\\\n1 & 2\n\\epmat\n\\]\n\nCompute the transposed convolution by hand assuming stride 2.\nCompute the transposed convolution by hand assuming stride 1.\nUse PyTorch to verify your answers.\nImplement the transposed convolution in PyTorch without using its own implementaiton. You can assume no bias term a square kernel and no separate batch dimension. I.e. your task is to implement the following function\n\ndef conv_transpose2d(inp, weight, stride=1):\n    # inp - input of shape (C_in, H, W)\n    # weight - kernel of shape (C_in, C_out, K, K)\n    # stride - stride of the transposed convolution\n    # RETURNS\n    # output - output of shape (C_out, H_out, W_out)\n    #\n    # YOUR CODE HERE\n    return output"
  },
  {
    "objectID": "ex/w10/questions/tconv-params-notes.html",
    "href": "ex/w10/questions/tconv-params-notes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\n\ntconv = nn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3)\nprint([p.numel() for p in tconv.parameters()])\n\ntconv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=10, kernel_size=3)\nprint([p.numel() for p in tconv.parameters()])\n\ntconv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=2, kernel_size=4)\nprint([p.numel() for p in tconv.parameters()])\n\ntconv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=4, kernel_size=3)\nprint([p.numel() for p in tconv.parameters()])\n\n[54, 2]\n[270, 10]\n[96, 2]\n[108, 4]"
  },
  {
    "objectID": "ex/w10/questions/tconv-sizes-notes.html",
    "href": "ex/w10/questions/tconv-sizes-notes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\nCompute the output sizes for the exercises.\n\nx = torch.randn(1, 3, 2, 2)\ntransp_conv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=2, kernel_size=3, stride=1)\ntransp_conv(x).shape\n\ntorch.Size([1, 2, 4, 4])\n\n\n\nx = torch.randn(1, 3, 5, 5)\ntransp_conv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=4, kernel_size=2, stride=2)\ntransp_conv(x).shape\n\ntorch.Size([1, 4, 10, 10])"
  },
  {
    "objectID": "ex/w10/questions/tconv-sizes.html",
    "href": "ex/w10/questions/tconv-sizes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)"
  },
  {
    "objectID": "ex/w07/exercises07.html",
    "href": "ex/w07/exercises07.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The exercises this week involve some old material so you can check your learning and understanding."
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-1---maximum-likelihood-estimator",
    "href": "ex/w07/exercises07.html#exercise-1---maximum-likelihood-estimator",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 1 - Maximum Likelihood Estimator",
    "text": "Exercise 1 - Maximum Likelihood Estimator\nAssume you are given datapoints \\((x_i)_{i=1}^N\\) with \\(x_i\\in\\R\\) coming from a Exponential distribution. The probability density function of a exponential distribution is given by \\(f(x) = \\la \\exp(-\\la x)\\) with \\(x\\in\\R\\). Derive the maximum likelihood estimator of the parameter \\(\\la\\)."
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-2---convolutional-layers",
    "href": "ex/w07/exercises07.html#exercise-2---convolutional-layers",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 2 & -1 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 2 \\\\\n2 & 1 & 0 & -1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 0 \\\\\n2, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-3---computational-parameter-counting",
    "href": "ex/w07/exercises07.html#exercise-3---computational-parameter-counting",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Computational Parameter Counting",
    "text": "Exercise 3 - Computational Parameter Counting\nUse PyTorch to load the vgg11 model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-4---influence-functions",
    "href": "ex/w07/exercises07.html#exercise-4---influence-functions",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 4 - Influence Functions",
    "text": "Exercise 4 - Influence Functions\nLet \\(\\hte\\) and \\(\\hte(\\ve)\\) be as defined in class. Show that the first order Taylor expansion of \\(\\hte(\\ve)\\) around \\(\\ve=0\\) is given by the equation given in class, i.e. by \\[\\begin{align*}\n\\hat{\\theta}({\\epsilon}) \\approx \\hat{\\theta} + \\epsilon\\frac{d\\hat{\\theta}(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0} .\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w07/exercises07-notes.html",
    "href": "ex/w07/exercises07-notes.html",
    "title": "Solution Computations",
    "section": "",
    "text": "This file is for internal use where we compute the solutions to the exercises. I also compute the solutions for seemingly easy exercises here to use the code later for automation of the process."
  },
  {
    "objectID": "ex/w07/exercises07-notes.html#cnn-size",
    "href": "ex/w07/exercises07-notes.html#cnn-size",
    "title": "Solution Computations",
    "section": "CNN Size",
    "text": "CNN Size\n\nimport torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0, bias=False)\n\nx = torch.tensor([\n    [ 1, 2, -1, 1],\n    [ 1, 0, 1, 0],\n    [ 0, 1, 0, 2],\n    [ 2, 1, 0, -1]\n])\n\nk = torch.tensor([\n    [ 1, 0],\n    [ 2, 1]\n])\n\nconv.weight.data = k.view(1, 1, 2, 2).float()\n\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 3.,  3.,  1.],\n          [ 2.,  2.,  3.],\n          [ 5.,  3., -1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=0, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 3.,  1.],\n          [ 5., -1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# PyTorch uses zero-padding by default\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=1, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1.,  3.,  2.],\n          [ 0.,  2.,  4.],\n          [ 0.,  1., -1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)"
  },
  {
    "objectID": "ex/w07/exercises07-notes.html#vgg-weight-count",
    "href": "ex/w07/exercises07-notes.html#vgg-weight-count",
    "title": "Solution Computations",
    "section": "VGG weight count",
    "text": "VGG weight count\n\nimport torchvision\nvgg11 = torchvision.models.vgg.vgg11(pretrained=False)\n\nprint(\n    f\"Total number of parameters: {sum(p.numel() for p in vgg11.parameters())}\")\n\nprint(\"\\n\\nParameter Overview in the backbone:\")\nfor layer in vgg11.features:\n    print(f\"{layer}: {sum(p.numel() for p in layer.parameters())}\")\n\nprint(\"\\n\\nParameter Ovewview in the head:\")\nfor layer in vgg11.classifier:\n    print(layer, sum(p.numel() for p in layer.parameters()))\n\n/Users/igor/Conda/envs/csc413f23/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/igor/Conda/envs/csc413f23/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n\n\nTotal number of parameters: 132863336\n\n\nParameter Overview in the backbone:\nConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 1792\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 73856\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 295168\nReLU(inplace=True): 0\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 590080\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 1180160\nReLU(inplace=True): 0\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 2359808\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 2359808\nReLU(inplace=True): 0\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 2359808\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\n\n\nParameter Ovewview in the head:\nLinear(in_features=25088, out_features=4096, bias=True) 102764544\nReLU(inplace=True) 0\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=4096, out_features=4096, bias=True) 16781312\nReLU(inplace=True) 0\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=4096, out_features=1000, bias=True) 4097000"
  },
  {
    "objectID": "ex/w07/questions/cnn-by_hand-sol.html",
    "href": "ex/w07/questions/cnn-by_hand-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Here, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 &  3 &  1 \\\\\n2 &  2 &  3 \\\\\n5 &  3 & -1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & 1 \\\\\n5 & -1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 &  0 &  0 & 0 \\\\\n0 & 1 & 2 & -1 &  1 & 0 \\\\\n0 & 1 & 0 &  1 &  0 & 0 \\\\\n0 & 0 & 1 &  0 &  2 & 0 \\\\\n0 & 2 & 1 &  0 & -1 & 0 \\\\\n0 & 0 & 0 &  0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 3 & 2 \\\\\n0 & 2 & 4 \\\\\n0 & 1 & -1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w07/questions/pytorch-parameter_count.html",
    "href": "ex/w07/questions/pytorch-parameter_count.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Use PyTorch to load the vgg11 model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w07/questions/pytorch-parameter_count-sol.html",
    "href": "ex/w07/questions/pytorch-parameter_count-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First, we hvae to load the vgg11 model which is part of torchvision as has been shown in the lecture:\nimport torchvision\nvgg11 = torchvision.models.vgg.vgg11(pretrained=False)\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in vgg11.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the vgg11 model. All VGG models are ultimately instantiated by using the VGG class. Its forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in vgg11.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over vgg11.classifier instead of vgg11.features."
  },
  {
    "objectID": "ex/w07/questions/prob-mle_exp_dist.html",
    "href": "ex/w07/questions/prob-mle_exp_dist.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Assume you are given datapoints \\((x_i)_{i=1}^N\\) with \\(x_i\\in\\R\\) coming from a Exponential distribution. The probability density function of a exponential distribution is given by \\(f(x) = \\la \\exp(-\\la x)\\) with \\(x\\in\\R\\). Derive the maximum likelihood estimator of the parameter \\(\\la\\)."
  },
  {
    "objectID": "ex/w09/exercises09_solution.html",
    "href": "ex/w09/exercises09_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\).\n\n\nFirst we compute the dot products between \\(\\fs\\) and the \\(\\fh_i\\) and apply softmax resulting in: \\[\n\\fa = \\text{Softmax}\\li(\n\\bpmat\n1 & 1 & 0\\\\\n2 & 2 & 1\\\\\n3 & 1 & -1\n\\epmat^\\top\n\\cdot\n\\bpmat 1\\\\ 2\\\\ 1\\epmat\n\\ri)\n\\approx\n\\bpmat 0.88\\\\ 0.12\\\\ 0.00\\epmat\n\\] The resulting context vector is then computed as a weighted sum of the \\(\\fh_i\\): \\[\n\\fc\n  = a_1 \\fh_1 + a_2 \\fh_2 + a_3 \\fh_3\n  \\approx \\bpmat 1.00\\\\ 2.00\\\\ 2.76\\epmat\n\\]\nA simple implementation yielding the solution is as follows:\nimport torch\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c)"
  },
  {
    "objectID": "ex/w09/exercises09_solution.html#exercise-1---dot-product-attention",
    "href": "ex/w09/exercises09_solution.html#exercise-1---dot-product-attention",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\).\n\n\nFirst we compute the dot products between \\(\\fs\\) and the \\(\\fh_i\\) and apply softmax resulting in: \\[\n\\fa = \\text{Softmax}\\li(\n\\bpmat\n1 & 1 & 0\\\\\n2 & 2 & 1\\\\\n3 & 1 & -1\n\\epmat^\\top\n\\cdot\n\\bpmat 1\\\\ 2\\\\ 1\\epmat\n\\ri)\n\\approx\n\\bpmat 0.88\\\\ 0.12\\\\ 0.00\\epmat\n\\] The resulting context vector is then computed as a weighted sum of the \\(\\fh_i\\): \\[\n\\fc\n  = a_1 \\fh_1 + a_2 \\fh_2 + a_3 \\fh_3\n  \\approx \\bpmat 1.00\\\\ 2.00\\\\ 2.76\\epmat\n\\]\nA simple implementation yielding the solution is as follows:\nimport torch\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c)"
  },
  {
    "objectID": "ex/w09/exercises09_solution.html#exercise-2---attention-in-transformers",
    "href": "ex/w09/exercises09_solution.html#exercise-2---attention-in-transformers",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Attention in Transformers",
    "text": "Exercise 2 - Attention in Transformers\nTransformers use a scaled dot product attention mechanism given by \\[\nC\n= \\text{attention}(Q, K, V)\n= \\text{softmax}\\left(\\fr{QK^\\top}{\\sqrt{d}}\\right) V,\n\\] where \\(Q\\in\\R^{n_q\\times d_k}\\), \\(K\\in\\R^{n_k\\times d_k}\\), \\(V\\in\\R^{n_k\\times d_v}\\).\n\nIs the softmax function here applied row-wise or column-wise? What is the shape of the result?\nWhat is the value of \\(d\\)? Why is it needed?\nWhat is the computational complexity of this attention mechanism? How many additions and multiplications are required? Assume the canonical matrix multiplcation and not counting \\(\\exp(x)\\) towards computational cost.\nIn the masked variant of the module, a masking matrix is added before the softmax function is applied. What are its values and its shape? For simplicity, assume \\(n_q=n_k\\).\n\n\nSolution\n\nThe softmax function is applied row-wise and the shape of the result is \\(n_q\\times n_k\\). One way to see this is by looking at the shape of the dot product \\(QK^\\top\\) which is \\(n_q\\times n_k\\). Each row represents the pre-softmax scores of all keys and a given query. Because we need to normalize our attention weights per query, the normalization happens along the rows.\nThe value of \\(d\\) is \\(d_k\\). It is needed to scale the dot product so that the gradient of the softmax function does not vanish.\nTo obtain the computational complexity, let’s look at all the operations individually:\n\n\\(QK^\\top\\) requires \\(n_q n_k d_k\\) multiplications and \\(n_qn_k(d_k-1)\\) additions.\nDividing by \\(\\sqrt{d_k}\\) needs to be carried out \\(n_q n_k\\) times.\nApplying the softmax function can be implemented in \\(n_q n_k\\) divisions and \\(n_q(n_k-1)\\) additions.\nThe final matrix multiplication requires \\(n_qd_vn_k\\) multiplications and \\(n_q d_v (n_k-1)\\) additions.\n\nThe masking matrix is a triangular matrix with \\(-\\infty\\) on its top right half. This results in softmax weights being \\(0\\) for all key-query combinations to which \\(-\\infty\\) is added."
  },
  {
    "objectID": "ex/w09/exercises09_solution.html#exercise-3---scaled-dot-product-attention-by-hand",
    "href": "ex/w09/exercises09_solution.html#exercise-3---scaled-dot-product-attention-by-hand",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Scaled Dot-Product Attention by Hand",
    "text": "Exercise 3 - Scaled Dot-Product Attention by Hand\nConsider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 2\\\\\n3 & 1\n\\epmat,\\quad\nK = \\bpmat\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 2 & -2\\\\\n1 & 1 & 2 \\\\\n0 & 1 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention.\n\nSolution\nThe resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n0.86 & 1.58 & -0.72\\\\\n0.99 & 1.88 & -1.56\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers_by_hand-sol.html",
    "href": "ex/w09/questions/attn-transformers_by_hand-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "The resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n0.86 & 1.58 & -0.72\\\\\n0.99 & 1.88 & -1.56\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w09/questions/attn-dot_product_attention.html",
    "href": "ex/w09/questions/attn-dot_product_attention.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\)."
  },
  {
    "objectID": "ex/w09/questions/attn-dot_product_attention-sol.html",
    "href": "ex/w09/questions/attn-dot_product_attention-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First we compute the dot products between \\(\\fs\\) and the \\(\\fh_i\\) and apply softmax resulting in: \\[\n\\fa = \\text{Softmax}\\li(\n\\bpmat\n1 & 1 & 0\\\\\n2 & 2 & 1\\\\\n3 & 1 & -1\n\\epmat^\\top\n\\cdot\n\\bpmat 1\\\\ 2\\\\ 1\\epmat\n\\ri)\n\\approx\n\\bpmat 0.88\\\\ 0.12\\\\ 0.00\\epmat\n\\] The resulting context vector is then computed as a weighted sum of the \\(\\fh_i\\): \\[\n\\fc\n  = a_1 \\fh_1 + a_2 \\fh_2 + a_3 \\fh_3\n  \\approx \\bpmat 1.00\\\\ 2.00\\\\ 2.76\\epmat\n\\]\nA simple implementation yielding the solution is as follows:\nimport torch\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c)"
  },
  {
    "objectID": "ex/w09/questions/attn-dot_product_attention-notes.html",
    "href": "ex/w09/questions/attn-dot_product_attention-notes.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "import torch\nimport torch.nn\nimport torch.nn.functional as F\n\n\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c, a)\n\ntensor([0.9992, 1.9992, 2.7586]) tensor([8.8009e-01, 1.1911e-01, 8.0254e-04])"
  },
  {
    "objectID": "ex/w08/exercises08_solution.html",
    "href": "ex/w08/exercises08_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Suppose we are training a vanilla RNN like below to determine whether a sentence expresses positive or negative sentiment. This RNN will be a character-level RNN where \\(x^{(1)}, \\ldots, x^{(T)}\\) is the sequence of input characters. The RNN is given as follows: \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(U x^{(t)} + W h^{(t-1)} + b\\ri) \\\\\ny &= \\sigma\\li(V h^{(T)} + d\\ri)\n\\end{align*}\\]\n\nHow many times do we need to apply the weight matrix \\(U\\), \\(W\\), and \\(V\\)?\nWhat are the shapes of the matrices \\(U\\), \\(W\\), and \\(V\\)?\nHow many addition and multiplication operations are required to make a prediction? You can assume that no addition and multiplications are performed when applying the tanh and sigmoid activation functions.\n\n\n\n\nWe will need to compute \\(h^{(t)}\\) for \\(t = 1, \\ldots, T\\). Each of this computation requires applying the weight matrices \\(W\\) and \\(T\\) once. The matrix \\(V\\) is only applied once at the end. Therefore, we need to apply \\(W\\) and \\(U\\) \\(T\\) times each and \\(V\\) once.\nThe shape of \\(U\\) is \\(d_h \\times d_x\\), the shape of \\(W\\) is \\(d_h \\times d_h\\), and the shape of \\(V\\) is \\(d_y \\times d_h\\), where \\(d_h\\) is the dimensionality of the \\(h^{(i)}\\) (i.e. \\(h^{(i)}\\in\\R^{d_h}\\)), \\(d_x\\) is the dimensionality of the inputs \\(x^{(i)}\\), and \\(d_y\\) is the dimensionality of the ouput \\(y\\).\nFor each of the \\(T\\) steps, we need to perform two matrix-vector multiplications (one for \\(Ux^{(i)}\\) and one for \\(Uh^{(i)}\\)) and two vector additions. To compute the output, we need one additional matrix-vector multiplication and one vector addition."
  },
  {
    "objectID": "ex/w08/exercises08_solution.html#exercise-1---rnn-for-sentiment-analysis",
    "href": "ex/w08/exercises08_solution.html#exercise-1---rnn-for-sentiment-analysis",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Suppose we are training a vanilla RNN like below to determine whether a sentence expresses positive or negative sentiment. This RNN will be a character-level RNN where \\(x^{(1)}, \\ldots, x^{(T)}\\) is the sequence of input characters. The RNN is given as follows: \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(U x^{(t)} + W h^{(t-1)} + b\\ri) \\\\\ny &= \\sigma\\li(V h^{(T)} + d\\ri)\n\\end{align*}\\]\n\nHow many times do we need to apply the weight matrix \\(U\\), \\(W\\), and \\(V\\)?\nWhat are the shapes of the matrices \\(U\\), \\(W\\), and \\(V\\)?\nHow many addition and multiplication operations are required to make a prediction? You can assume that no addition and multiplications are performed when applying the tanh and sigmoid activation functions.\n\n\n\n\nWe will need to compute \\(h^{(t)}\\) for \\(t = 1, \\ldots, T\\). Each of this computation requires applying the weight matrices \\(W\\) and \\(T\\) once. The matrix \\(V\\) is only applied once at the end. Therefore, we need to apply \\(W\\) and \\(U\\) \\(T\\) times each and \\(V\\) once.\nThe shape of \\(U\\) is \\(d_h \\times d_x\\), the shape of \\(W\\) is \\(d_h \\times d_h\\), and the shape of \\(V\\) is \\(d_y \\times d_h\\), where \\(d_h\\) is the dimensionality of the \\(h^{(i)}\\) (i.e. \\(h^{(i)}\\in\\R^{d_h}\\)), \\(d_x\\) is the dimensionality of the inputs \\(x^{(i)}\\), and \\(d_y\\) is the dimensionality of the ouput \\(y\\).\nFor each of the \\(T\\) steps, we need to perform two matrix-vector multiplications (one for \\(Ux^{(i)}\\) and one for \\(Uh^{(i)}\\)) and two vector additions. To compute the output, we need one additional matrix-vector multiplication and one vector addition."
  },
  {
    "objectID": "ex/w08/exercises08_solution.html#exercise-2---scalar-rnn",
    "href": "ex/w08/exercises08_solution.html#exercise-2---scalar-rnn",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Scalar RNN",
    "text": "Exercise 2 - Scalar RNN\n\nSuppose we have the following vanilla RNN network, where the inputs and hidden units are scalars. \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(w \\cdot h^{(t-1)} + u \\cdot x^{(t-1)} + b_h\\ri) \\\\\ny &= \\sigma\\li(v \\cdot h^{(T)} + b_y\\ri)\n\\end{align*}\\]\n\nShow that if \\(|w| &lt; 1\\), and the number of time steps \\(T\\) is large, then the gradient \\(\\frac{\\partial y}{\\partial x^{(0)}}\\) vanishes.\nWhy is the result from Part (a) troubling?\n\n\nSolution\n\nTo make the sequence length \\(T\\) explicit in the notation, we will write \\(y\\) instead of \\(y_T\\). Formally, what we have to show is \\[\n|w|&lt;1 \\implies \\lim_{T\\to\\infty} \\fr{\\partial y_T}{\\partial x^{(0)}} = 0 .\n\\] For the proof, we expand the derivative of \\(y_T\\) with respect to \\(x^{(0)}\\) using the chain rule: \\[\n\\begin{aligned}\n\\fr{\\partial y_T}{\\partial x^{(0)}}\n  & = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v \\cdot \\fr{\\partial h^{(T)}}{\\partial x^{(0)}} \\\\\n  & =\n  \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot\n      \\underbrace{\n        \\tanh'\\li(w \\cdot h^{(T-1)} + u \\cdot x^{(T-1)} + b_h\\ri)\n      }_{A_{T-1}(x^{(0)})}\n    \\cdot w \\cdot \\fr{\\partial h^{(T-1)}}{\\partial x^{(0)}} \\\\\n& = \\ldots \\\\\n& = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot \\prod_{t=2}^{T-1} A_{t}(x^{(0)})\n    \\cdot w^{T-1} \\cdot \\fr{\\partial h^{(1)}}{\\partial x^{(0)}} .\\\\\n\\end{aligned}\n\\] Using this, we can analyze the absolute value of the derivative \\(\\partial y_T/\\partial x^{(0)}\\). For \\(\\tanh\\) and \\(\\si\\), the absolute value of their respective derivatives is bounded by \\(1\\). Thus, we have \\[\n\\begin{aligned}\n\\li|\\fr{\\partial y_T}{\\partial x^{(0)}} \\ri|\n& =\n\\underbrace{\n  \\li|\\si'\\li(v \\cdot h^{(T)} + b_y\\ri) \\ri|\n}_{\\leq 1}\n\\cdot |v|\n\\cdot \\prod_{t=2}^{T-1}\n  \\underbrace{\\li| A_{t}(x^{(0)}) \\ri|}_{\\leq 1}\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n& \\leq |v|\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n\\end{aligned}\n\\] Because \\(|w|&lt;1\\), this converges to \\(0\\) as \\(T\\to\\infty\\) and thus \\(|\\partial y_T/\\partial x^{(0)}|\\) also converges to \\(0\\), i.e. the gradient vanishes.\nIt implies that in the considered setting, the input has no impact on the output."
  },
  {
    "objectID": "ex/w08/exercises08_solution.html#exercise-3---rnn-addition",
    "href": "ex/w08/exercises08_solution.html#exercise-3---rnn-addition",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - RNN Addition",
    "text": "Exercise 3 - RNN Addition\n\nIn this problem, you will implement a recurrent neural network which implements binary addition. The inputs are given as binary sequences, starting with the significant binary digit. (It is easier to start from the least significant bit, just like how you did addition in grade school.) The sequences will be padded with at least one zero as the most significant digit, so that the output length is the same as the input length. For example, the problem \\(100111 + 110010\\), whose target output value is \\(1011001\\), will be represented as follows: \\[\\begin{align*}\n\\bf{x}^{(1)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(2)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(3)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(4)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(5)} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(6)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(7)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{align*}\\]\nWith the target output: \\[\\begin{align*}\ny^{(1)} = 1,\ny^{(2)} = 0,\ny^{(3)} = 0,\ny^{(4)} = 1,\ny^{(5)} = 1,\ny^{(6)} = 0,\ny^{(7)} = 1,\n\\end{align*}\\]\nThere are two input units corresponding to the two inputs, and one output unit. Therefore, the pattern of inputs and outputs for this example would be:\n\nDesign, by hand, the weights and biases for an RNN which has two input units, three hidden units, and one output unit, which implements binary addition as discussed above. All of the units use the hard threshold activation function (\\(f(x) = 1\\) if \\(x &gt; 0\\) and \\(0\\) otherwise). In particular, specify weight matrices \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{W}\\), bias vector \\(\\mathbf{b}_{\\mathbf{h}}\\), and scalar bias \\(b_y\\) for the following architecture: \\[\\begin{align*}\nh^{(t)} &= f(\\bf{W}h^{(t-1)} + \\bf{U}\\bf{x}^{(t)} + \\bf{b_h}) \\\\\ny^{(t)} &= f(\\bf{v}^T h^{(t)} + b_y)\n\\end{align*}\\]\n\nWhat are the shapes of \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\)?\nCome up with values for \\(\\mathbf{U}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\). Justify your answer. Hint: When performing binary addition, in addition to adding up two digits in a column, we need to track whether there is a digit from the previous column. We will choose one of the three units in \\(\\bf{h}^{(t)}\\), say \\(\\bf{h}_2^{(t)}\\), to represent this carry digit. You may also find it helpful to set \\(\\bf{h}_1\\) to activate if the sum of the 3 digits is at least 1, \\(\\bf{h}_2\\) to activate if the sum is at least 2, and \\(\\bf{h}_3\\) to activate if the sum is at least 3.\nCome up with the values of \\(\\bf{v}\\) and \\(b_y\\). Justify your answer.\n\n\nSolution\n\nSince the inputs \\(\\bf{x}^{(t)}\\) are \\(2 \\times 1\\) and the hidden units \\(\\bf{h}^{(t)}\\) are \\(2 \\times 1\\), we should have:\n\n\\(\\bf{W}\\) is \\(3 \\times 3\\)\n\\(\\bf{U}\\) is \\(3 \\times 2\\)\n\\(\\bf{b}_h\\) is \\(3 \\times 1\\)\n\\(\\bf{v}\\) is \\(3 \\times 1\\)\n\nWe will follow the hint and implement the addition in our RNN such that:\n\nThe first of our hidden units \\(h_1^{(t)}\\) is 1 if and only if the sum \\(S^{(t)} \\doteq x_1^{(t)} + x_2^{(t)} + c^{(t-1)} \\geq 1\\), where by \\(c^{(t-1)}\\) we denote a carry (\\(\\bf{h_2}^{(t-1)}\\) from the previous addition). Note, these \\(S^{(t)}\\) and \\(c^{(t-1)}\\) are not variables of the model, merely our notation to help us to work out the solution.\nThe \\(h_2^{(t)}\\) is 1 iff the sum \\(S^{(t)} \\geq 2\\),\nand \\(h_3^{(t)}\\) is 1 iff the sum \\(S^{(t)}\\) is 3.\n\nNotice that the carry \\(c^{(t-1)}\\) is going to be 1 iff \\(h_2^{(t-1)}=1\\) and 0 otherwise, i.e. when the previous addition was 2 or 3. Therefore to compute \\(h_i^{(t)}\\) we need to first compute the sum \\(S^{(t)} = x_1^{(t)} + x_2^{(t)} + h_2^{(t-1)}\\) and then offset it by \\(-i+1\\) so that after applying the hard threshold function we get the desired value as specified above. This can be achieved with the following set of parameters: \\[\n\\mathbf{U}= \\begin{bmatrix}\n    1 & 1 \\\\\n    1 & 1 \\\\\n    1 & 1 \\end{bmatrix},\\quad\n\\mathbf{W}=\\begin{bmatrix}\n    0 & 1 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 1 & 0\\end{bmatrix},\\quad\n\\mathbf{b_h}= \\begin{bmatrix}\n    -0.5 \\\\\n    -1.5 \\\\\n    -2.5 \\end{bmatrix}.\n\\]\nTo compute the output \\(y^{(t)}\\) we need to check if the \\(S^{(t)}\\) is 1 or 3, that is, if either \\(h_1^{(t)} = 1\\) while all other hidden units are zero or all hidden units are 1. We can accomplish this by setting: \\(\\mathbf{v}=\\begin{bmatrix} 1, -1, 1 \\end{bmatrix}\\) and \\(b_y = -0.5\\)."
  },
  {
    "objectID": "ex/w08/questions/rnn-sentiment.html",
    "href": "ex/w08/questions/rnn-sentiment.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Suppose we are training a vanilla RNN like below to determine whether a sentence expresses positive or negative sentiment. This RNN will be a character-level RNN where \\(x^{(1)}, \\ldots, x^{(T)}\\) is the sequence of input characters. The RNN is given as follows: \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(U x^{(t)} + W h^{(t-1)} + b\\ri) \\\\\ny &= \\sigma\\li(V h^{(T)} + d\\ri)\n\\end{align*}\\]\n\nHow many times do we need to apply the weight matrix \\(U\\), \\(W\\), and \\(V\\)?\nWhat are the shapes of the matrices \\(U\\), \\(W\\), and \\(V\\)?\nHow many addition and multiplication operations are required to make a prediction? You can assume that no addition and multiplications are performed when applying the tanh and sigmoid activation functions."
  },
  {
    "objectID": "ex/w08/questions/rnn-scalar-sol.html",
    "href": "ex/w08/questions/rnn-scalar-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "To make the sequence length \\(T\\) explicit in the notation, we will write \\(y\\) instead of \\(y_T\\). Formally, what we have to show is \\[\n|w|&lt;1 \\implies \\lim_{T\\to\\infty} \\fr{\\partial y_T}{\\partial x^{(0)}} = 0 .\n\\] For the proof, we expand the derivative of \\(y_T\\) with respect to \\(x^{(0)}\\) using the chain rule: \\[\n\\begin{aligned}\n\\fr{\\partial y_T}{\\partial x^{(0)}}\n  & = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v \\cdot \\fr{\\partial h^{(T)}}{\\partial x^{(0)}} \\\\\n  & =\n  \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot\n      \\underbrace{\n        \\tanh'\\li(w \\cdot h^{(T-1)} + u \\cdot x^{(T-1)} + b_h\\ri)\n      }_{A_{T-1}(x^{(0)})}\n    \\cdot w \\cdot \\fr{\\partial h^{(T-1)}}{\\partial x^{(0)}} \\\\\n& = \\ldots \\\\\n& = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot \\prod_{t=2}^{T-1} A_{t}(x^{(0)})\n    \\cdot w^{T-1} \\cdot \\fr{\\partial h^{(1)}}{\\partial x^{(0)}} .\\\\\n\\end{aligned}\n\\] Using this, we can analyze the absolute value of the derivative \\(\\partial y_T/\\partial x^{(0)}\\). For \\(\\tanh\\) and \\(\\si\\), the absolute value of their respective derivatives is bounded by \\(1\\). Thus, we have \\[\n\\begin{aligned}\n\\li|\\fr{\\partial y_T}{\\partial x^{(0)}} \\ri|\n& =\n\\underbrace{\n  \\li|\\si'\\li(v \\cdot h^{(T)} + b_y\\ri) \\ri|\n}_{\\leq 1}\n\\cdot |v|\n\\cdot \\prod_{t=2}^{T-1}\n  \\underbrace{\\li| A_{t}(x^{(0)}) \\ri|}_{\\leq 1}\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n& \\leq |v|\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n\\end{aligned}\n\\] Because \\(|w|&lt;1\\), this converges to \\(0\\) as \\(T\\to\\infty\\) and thus \\(|\\partial y_T/\\partial x^{(0)}|\\) also converges to \\(0\\), i.e. the gradient vanishes.\nIt implies that in the considered setting, the input has no impact on the output."
  },
  {
    "objectID": "ex/w08/questions/rnn-addition-sol.html",
    "href": "ex/w08/questions/rnn-addition-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Since the inputs \\(\\bf{x}^{(t)}\\) are \\(2 \\times 1\\) and the hidden units \\(\\bf{h}^{(t)}\\) are \\(2 \\times 1\\), we should have:\n\n\\(\\bf{W}\\) is \\(3 \\times 3\\)\n\\(\\bf{U}\\) is \\(3 \\times 2\\)\n\\(\\bf{b}_h\\) is \\(3 \\times 1\\)\n\\(\\bf{v}\\) is \\(3 \\times 1\\)\n\nWe will follow the hint and implement the addition in our RNN such that:\n\nThe first of our hidden units \\(h_1^{(t)}\\) is 1 if and only if the sum \\(S^{(t)} \\doteq x_1^{(t)} + x_2^{(t)} + c^{(t-1)} \\geq 1\\), where by \\(c^{(t-1)}\\) we denote a carry (\\(\\bf{h_2}^{(t-1)}\\) from the previous addition). Note, these \\(S^{(t)}\\) and \\(c^{(t-1)}\\) are not variables of the model, merely our notation to help us to work out the solution.\nThe \\(h_2^{(t)}\\) is 1 iff the sum \\(S^{(t)} \\geq 2\\),\nand \\(h_3^{(t)}\\) is 1 iff the sum \\(S^{(t)}\\) is 3.\n\nNotice that the carry \\(c^{(t-1)}\\) is going to be 1 iff \\(h_2^{(t-1)}=1\\) and 0 otherwise, i.e. when the previous addition was 2 or 3. Therefore to compute \\(h_i^{(t)}\\) we need to first compute the sum \\(S^{(t)} = x_1^{(t)} + x_2^{(t)} + h_2^{(t-1)}\\) and then offset it by \\(-i+1\\) so that after applying the hard threshold function we get the desired value as specified above. This can be achieved with the following set of parameters: \\[\n\\mathbf{U}= \\begin{bmatrix}\n    1 & 1 \\\\\n    1 & 1 \\\\\n    1 & 1 \\end{bmatrix},\\quad\n\\mathbf{W}=\\begin{bmatrix}\n    0 & 1 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 1 & 0\\end{bmatrix},\\quad\n\\mathbf{b_h}= \\begin{bmatrix}\n    -0.5 \\\\\n    -1.5 \\\\\n    -2.5 \\end{bmatrix}.\n\\]\nTo compute the output \\(y^{(t)}\\) we need to check if the \\(S^{(t)}\\) is 1 or 3, that is, if either \\(h_1^{(t)} = 1\\) while all other hidden units are zero or all hidden units are 1. We can accomplish this by setting: \\(\\mathbf{v}=\\begin{bmatrix} 1, -1, 1 \\end{bmatrix}\\) and \\(b_y = -0.5\\)."
  },
  {
    "objectID": "ex/w06/exercises06.html",
    "href": "ex/w06/exercises06.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]"
  },
  {
    "objectID": "ex/w06/exercises06.html#exercise-1---variance",
    "href": "ex/w06/exercises06.html#exercise-1---variance",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]"
  },
  {
    "objectID": "ex/w06/exercises06.html#exercise-2---variance-bias-decomposistion",
    "href": "ex/w06/exercises06.html#exercise-2---variance-bias-decomposistion",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Variance / Bias Decomposistion",
    "text": "Exercise 2 - Variance / Bias Decomposistion\nLet \\(D=\\lbrace (x_i,y_i) | i=1 \\ldots n\\rbrace\\) be a dataset obtained from the true underlying data distribution \\(P\\), i.e. \\(D\\sim P^n\\). And let \\(h_D(\\cdot)\\) be a classifier trained on \\(D\\). Show the variance bias decomposition \\[\n\\underbrace{\\mathbb{E}_{D,x,y} \\li[ (h_D(x) - y)^2 \\ri]}_{\\text{Expected test error}}  \n  = \\underbrace{\\mathbb{E}_{D,x} \\li[ (h_D(x) - \\hat{h}(x))^2 \\ri]}_{\\text{Variance}} +\n  \\underbrace{\\mathbb{E}_{x,y} \\li[ (\\hat{y}(x) - y)^2 \\ri]}_{\\text{Noise}}  +  \\underbrace{\\mathbb{E}_{x} \\li[ (\\hat{h}(x) - \\hat{y}(x))^2 \\ri]}_{\\text{Bias}^2}\n\\] where \\(\\hat{h}(x) = \\mathbb{E}_{D \\sim P^n}[h_D(x)]\\) is the expected regressor over possible training sets, given the learning algorithm \\(\\mathcal{A}\\) and \\(\\hat{y}(x) = \\mathbb{E}_{y|x}[y]\\) is the expected label given \\(x\\). As mentioned in the lecture, labels might not be deterministic given x. To carry out the proof, proceed in the following steps:\n\nShow that the following identity holds \\[\\begin{align}\n\\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n= \\E_{D, x}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri].\n   \\end{align}\\]\nNext, show \\[\\begin{align}\nE_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri]\n=E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri]\n\\end{align}\\] which completes the proof by substituting (2) into (1)."
  },
  {
    "objectID": "ex/w06/exercises06.html#exercise-3---ensembling",
    "href": "ex/w06/exercises06.html#exercise-3---ensembling",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Ensembling",
    "text": "Exercise 3 - Ensembling\nDownload the file exercises06-ensembling.ipynb from quercus. It contains basic Pytorch code training a classifier on MNIST. Modify that code such that it trains an ensemble of 5-10 neural networks and computes their average prediction once trained."
  },
  {
    "objectID": "ex/w06/exercises06_solution.html",
    "href": "ex/w06/exercises06_solution.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]\n\n\nFirst, we use the definition of variance and rewrite the left hand side as \\[\n\\Var(aX+bY) = \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2.\n\\] Next, we expand the squares for each of the terms on the right hand side: \\[\\begin{align*}\n\\E\\li[ (aX+bY)^2 \\ri]\n  &= \\E\\li[ a^2X^2 + 2abXY + b^2Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ XY ] + b^2\\E\\li[ Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri] ,\\\\\n\\E[aX+bY]^2\n  &= \\li(a\\E[X] + b\\E[Y]\\ri)^2 \\\\\n  &= a^2\\E[X]^2 + 2ab\\E[X]\\,\\E[Y] + b^2\\E[Y]^2.\n\\end{align*}\\] Subtracting the two terms, we get \\[\\begin{align*}\n& \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri]\n    - a^2\\E[X]^2 - 2ab\\E[X]\\,\\E[Y] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] - a^2\\E[X]^2\n   + b^2\\E\\li[ Y^2 \\ri] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2(\\E\\li[ X^2 \\ri] - \\E[X]^2)\n   + b^2\\li(\\E\\li[ Y^2 \\ri] - \\E[Y]^2\\ri) \\\\\n& \\quad = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w06/exercises06_solution.html#exercise-1---variance",
    "href": "ex/w06/exercises06_solution.html#exercise-1---variance",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]\n\n\nFirst, we use the definition of variance and rewrite the left hand side as \\[\n\\Var(aX+bY) = \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2.\n\\] Next, we expand the squares for each of the terms on the right hand side: \\[\\begin{align*}\n\\E\\li[ (aX+bY)^2 \\ri]\n  &= \\E\\li[ a^2X^2 + 2abXY + b^2Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ XY ] + b^2\\E\\li[ Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri] ,\\\\\n\\E[aX+bY]^2\n  &= \\li(a\\E[X] + b\\E[Y]\\ri)^2 \\\\\n  &= a^2\\E[X]^2 + 2ab\\E[X]\\,\\E[Y] + b^2\\E[Y]^2.\n\\end{align*}\\] Subtracting the two terms, we get \\[\\begin{align*}\n& \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri]\n    - a^2\\E[X]^2 - 2ab\\E[X]\\,\\E[Y] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] - a^2\\E[X]^2\n   + b^2\\E\\li[ Y^2 \\ri] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2(\\E\\li[ X^2 \\ri] - \\E[X]^2)\n   + b^2\\li(\\E\\li[ Y^2 \\ri] - \\E[Y]^2\\ri) \\\\\n& \\quad = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w06/exercises06_solution.html#exercise-2---variance-bias-decomposistion",
    "href": "ex/w06/exercises06_solution.html#exercise-2---variance-bias-decomposistion",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 2 - Variance / Bias Decomposistion",
    "text": "Exercise 2 - Variance / Bias Decomposistion\nLet \\(D=\\lbrace (x_i,y_i) | i=1 \\ldots n\\rbrace\\) be a dataset obtained from the true underlying data distribution \\(P\\), i.e. \\(D\\sim P^n\\). And let \\(h_D(\\cdot)\\) be a classifier trained on \\(D\\). Show the variance bias decomposition \\[\n\\underbrace{\\mathbb{E}_{D,x,y} \\li[ (h_D(x) - y)^2 \\ri]}_{\\text{Expected test error}}  \n  = \\underbrace{\\mathbb{E}_{D,x} \\li[ (h_D(x) - \\hat{h}(x))^2 \\ri]}_{\\text{Variance}} +\n  \\underbrace{\\mathbb{E}_{x,y} \\li[ (\\hat{y}(x) - y)^2 \\ri]}_{\\text{Noise}}  +  \\underbrace{\\mathbb{E}_{x} \\li[ (\\hat{h}(x) - \\hat{y}(x))^2 \\ri]}_{\\text{Bias}^2}\n\\] where \\(\\hat{h}(x) = \\mathbb{E}_{D \\sim P^n}[h_D(x)]\\) is the expected regressor over possible training sets, given the learning algorithm \\(\\mathcal{A}\\) and \\(\\hat{y}(x) = \\mathbb{E}_{y|x}[y]\\) is the expected label given \\(x\\). As mentioned in the lecture, labels might not be deterministic given x. To carry out the proof, proceed in the following steps:\n\nShow that the following identity holds \\[\\begin{align}\n\\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n= \\E_{D, x}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri].\n   \\end{align}\\]\nNext, show \\[\\begin{align}\nE_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri]\n=E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri]\n\\end{align}\\] which completes the proof by substituting (2) into (1).\n\n\nSolution\n\nFirst, we reformulate (1) as \\[\\begin{align*}\n  \\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n&= \\E_{D,x,y}\\li[\\li[\\li(h_{D}(x) - \\hh(x)\\ri) + \\li(\\hh(x) - y\\ri)\\ri]^{2}\\ri] \\nonumber \\\\\n&= \\E_{x, D}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y, D} \\li[\\li(h_{D}(x) - \\hh(x)\\ri)\\li(\\hh(x) - y\\ri)\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri]\n\\end{align*}\\] Next, we note that the second term in the above equation is zero because \\[\\begin{align*}\n\\E_{D,x, y} \\li[\\li(h_{D}(x) - \\hh(x)\\ri) \\li(\\hh(x) - y\\ri)\\ri] &= \\E_{x, y} \\li[\\E_{D} \\li[ h_{D}(x) - \\hh(x)\\ri] \\li(\\hh(x) - y\\ri) \\ri] \\\\\n&= \\E_{x, y} \\li[ \\li( \\E_{D} \\li[ h_{D}(x) \\ri] - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ \\li(\\hh(x) - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ 0 \\ri] \\\\\n&= 0\\ .\n\\end{align*}\\]\nThe proof here, is similar. We start by reformulating the second term in (2) as \\[\\begin{align*}\n\\E_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri] &= \\E_{x, y} \\li[ \\li(\\hh(x) -\\bar y(x) )+(\\bar y(x) - y \\ri)^{2}\\ri]  \\\\\n  &=\\E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + \\E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y} \\li[ \\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri]\n  \\end{align*}\\] Here, the third term is zero which follows from an analogous derivation as in (a). Thus, we have \\[\\begin{align*}\n\\E_{x, y} \\li[\\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri] &= \\E_{x}\\li[\\E_{y \\mid x} \\li[\\hy(x) - y \\ri] \\li(\\hh(x) - \\hy(x) \\ri) \\ri] \\\\\n&= \\E_{x} \\li[ \\E_{y \\mid x} \\li[ \\hy(x) - y\\ri] \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\E_{y \\mid x} \\li [ y \\ri]\\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\hy(x) \\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ 0 \\ri] \\\\\n&= 0\n  \\end{align*}\\]"
  },
  {
    "objectID": "ex/w06/exercises06_solution.html#exercise-3---ensembling",
    "href": "ex/w06/exercises06_solution.html#exercise-3---ensembling",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "Exercise 3 - Ensembling",
    "text": "Exercise 3 - Ensembling\nDownload the file ex06-ensembling.ipynb from quercus. It contains basic Pytorch code training a classifier on MNIST. Modify that code such that it trains an ensemble of 5-10 neural networks and computes their average prediction once trained."
  },
  {
    "objectID": "ex/w06/questions/prob-variance.html",
    "href": "ex/w06/questions/prob-variance.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]"
  },
  {
    "objectID": "ex/w06/questions/ml-variance_bias_decomposition-sol.html",
    "href": "ex/w06/questions/ml-variance_bias_decomposition-sol.html",
    "title": "CSC413 - Fall 2024, UTM",
    "section": "",
    "text": "First, we reformulate (1) as \\[\\begin{align*}\n  \\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n&= \\E_{D,x,y}\\li[\\li[\\li(h_{D}(x) - \\hh(x)\\ri) + \\li(\\hh(x) - y\\ri)\\ri]^{2}\\ri] \\nonumber \\\\\n&= \\E_{x, D}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y, D} \\li[\\li(h_{D}(x) - \\hh(x)\\ri)\\li(\\hh(x) - y\\ri)\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri]\n\\end{align*}\\] Next, we note that the second term in the above equation is zero because \\[\\begin{align*}\n\\E_{D,x, y} \\li[\\li(h_{D}(x) - \\hh(x)\\ri) \\li(\\hh(x) - y\\ri)\\ri] &= \\E_{x, y} \\li[\\E_{D} \\li[ h_{D}(x) - \\hh(x)\\ri] \\li(\\hh(x) - y\\ri) \\ri] \\\\\n&= \\E_{x, y} \\li[ \\li( \\E_{D} \\li[ h_{D}(x) \\ri] - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ \\li(\\hh(x) - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ 0 \\ri] \\\\\n&= 0\\ .\n\\end{align*}\\]\nThe proof here, is similar. We start by reformulating the second term in (2) as \\[\\begin{align*}\n\\E_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri] &= \\E_{x, y} \\li[ \\li(\\hh(x) -\\bar y(x) )+(\\bar y(x) - y \\ri)^{2}\\ri]  \\\\\n  &=\\E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + \\E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y} \\li[ \\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri]\n  \\end{align*}\\] Here, the third term is zero which follows from an analogous derivation as in (a). Thus, we have \\[\\begin{align*}\n\\E_{x, y} \\li[\\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri] &= \\E_{x}\\li[\\E_{y \\mid x} \\li[\\hy(x) - y \\ri] \\li(\\hh(x) - \\hy(x) \\ri) \\ri] \\\\\n&= \\E_{x} \\li[ \\E_{y \\mid x} \\li[ \\hy(x) - y\\ri] \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\E_{y \\mid x} \\li [ y \\ri]\\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\hy(x) \\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ 0 \\ri] \\\\\n&= 0\n  \\end{align*}\\]"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Fall 2024 - CSC 2626: Imitation Learning for Robotics",
    "section": "",
    "text": "Course Overview\nIn the next few decades we are going to witness millions of people, from various backgrounds and levels of technical expertise, needing to effectively interact with robotic technologies on a daily basis. As such, people will need to modify the behavior of their robots without explicitly writing code, but by providing only a small number of kinesthetic or visual demonstrations, or even natural language commands. At the same time, robots should try to infer and predict the human’s intentions and internal objectives from past interactions, in order to provide assistance before it is explicitly asked. This graduate-level course will examine some of the most important papers in imitation learning for robot control, placing more emphasis on developments in the last 10 years. Its purpose is to familiarize students with the frontiers of this research area, to help them identify open problems, and to enable them to make a research contribution.\nThis course will broadly cover the following areas:\n\nImitating the policies of demonstrators (people, expensive algorithms, optimal controllers)\nConnections between imitation learning, optimal control, and reinforcement learning\nLearning the cost functions that best explain a set of demonstrations\nShared autonomy between humans and robots for real-time control\n\n\n\nPrerequisites\nYou need to be comfortable with: introductory machine learning concepts (such as from CSC411/CSC413/ECE521 or equivalent), linear algebra, basic multivariable calculus, intro to probability. You also need to have strong programming skills in Python. Note: if you don’t meet all the prerequisites above please contact the instructor by email. Optional, but recommended: experience with neural networks, such as from CSC321, introductory-level familiarity with reinforcement learning and control.\n\n\nCourse Delivery Details\n\nLectures: In-person, Mondays @ 1pm-4pm ET, Carr Hall 404\nAnnouncements will be posted on Quercus\nDiscussions will take place on Piazza\nZoom recordings will be posted on Quercus after lectures\nAnonymous feedback form for suggested improvements"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Instructor\n\nFlorian Shkurti is an assistant professor in computer science at the University of Toronto, where he leads the Robot Vision and Learning lab. He is a faculty member of the University of Toronto Robotics Institute, the Acceleration Consortium, and a faculty affiliate at Vector Institute. His research group develops methods that enable robots to learn to perceive, reason, plan, and act effectively and safely, particularly in dynamic environments and alongside humans. Application areas of his research include field robotics for environmental monitoring, visual navigation for autonomous vehicles, and mobile manipulation.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMondays 10:00-11:00am ET\nDH3066\n\n\n\n\nIgor Gilitschenski is an assistant professor in computer science at the University of Toronto, where he leads the Toronto Intelligent Systems Lab. His work focuses on developing novel probabilistic and learning-based techniques for robotic perception and decision-making with the ultimate goal of enabling robust interactive autonomy.\n\n\n\nOffice hours\nLocation\n\n\n\n\nTuesdays 7:00-8:00pm ET\nDH3070\n\n\n\n\n\nTeaching Assistants\n\n\n\n\n\n\n\n\n\n\n\nAidan Li\nVivian Chu\nJasper Gerigk\nNavid Hasanzadeh\nLucas Jin\nEnsieh Khazaei\n\n\n\n\n\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday 11-12pm ET\n(Online) Zoom\n\n\nThursday 2-3pm ET\n(Online) Zoom",
    "crumbs": [
      "Teaching Staff"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation 4 and Equation 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation 4 using Equation 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From Equation 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Equation 12 below is mathematically equivalent to Equation 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.1\n✔ parsnip      1.3.2     ✔ yardstick    1.3.2\n✔ recipes      1.3.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 × 9\n   .rownames body_mass_g flipper_length_mm .fitted  .resid    .hat .sigma\n   &lt;chr&gt;           &lt;int&gt;             &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 1                3750               181   3212.  538.   0.00881   394.\n 2 2                3800               186   3461.  339.   0.00622   394.\n 3 3                3250               195   3908. -658.   0.00344   393.\n 4 5                3450               193   3808. -358.   0.00385   394.\n 5 6                3650               190   3659.   -9.43 0.00469   395.\n 6 7                3625               181   3212.  413.   0.00881   394.\n 7 8                4675               195   3908.  767.   0.00344   393.\n 8 9                3475               193   3808. -333.   0.00385   394.\n 9 10               4250               190   3659.  591.   0.00469   394.\n10 11               3300               186   3461. -161.   0.00622   395.\n# ℹ 332 more rows\n# ℹ 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "Fall 2024 - UTM - CSC 413: Neural Networks and Deep Learning",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\nRecordings\nLabs\nQuiz\nAssignments\nTests\nProject\n\n\n\n\n1\nSept 2-6\nLinear Model\n📖\n🖥️\n🎥\n\n\n\n\n\n\n\n2\nSept 9-13\nMulti-Layer Perceptron, Backpropagation, Losses\n📖\n🖥️\n🎥\n💻\n📋\n\n\n\n\n\n3\nSept 16-20\nAutomatic Differentiation, Distributed Representation\n📖\n🖥️\n🎥\n💻\n📋\n\n\n\n\n\n4\nSept 23-27\nConvolutional Neural Networks\n📖\n🖥️\n🎥\n💻\n📋\n✍️\n\n\n\n\n5\nOct 1-4\nOptimization\n📖\n🖥️\n🎥\n💻\n📋\n\n\n\n\n\n6\nOct 7-11\nGeneralization\n📖\n🖥️\n🎥\n💻\n📋\n\n\n\n\n\n7\nOct 14-18\nInterpreting Convolutional Neural Networks\n📖\n🖥️\n🎥\n\n\n✍️\n✅\n\n\n\n8\nOct 21-25\nRecurrent Neural Networks\n📖\n🖥️\n\n💻\n📋\n\n\n\n\n\n9\nOct 28-Nov 1\nFall Reading Week\n📖\n\n\n\n\n\n\n\n\n\n10\nNov 4-8\nAttention and Transformers\n📖\n🖥️\n🎥\n💻\n\n\n\n📂\n\n\n11\nNov 11-15\nAutoencoders and Transpose Convolutions\n📖\n🖥️\n🎥\n\n📋\n\n✅\n\n\n\n12\nNov 18-22\nGenerative Adversarial Networks\n📖\n🖥️\n🎥\n💻\n\n\n\n\n\n\n13\nNov 25-29\nUse of Deep Learning in the Research of Profs Shkurti and Gilitschenski\n📖\n🖥️\n🎥\n💻\n\n\n\n\n\n\n14\nDec 2-6\nFinal project submission\n📖\n\n\n\n\n\n\n📂",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "The project in this course is an opportunity to develop deep learning application in an area of your own choosing. It also provides the chance to complete a deep learning project that is much closer to a real-world application area, for example in medicine, finance, robotics, commerce, biology, chemistry, physics (or other sciences), social media, or other fields.\nWhile this project has some structure, you will be required to deal with the ambiguity and significant decision making that make up the life of a deep learning practitioner.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Final Project Guidelines",
    "section": "Project Proposal",
    "text": "Project Proposal\n\nLogistics\nProjects must be done in groups of 3-4. Please form groups on Markus by March 17, 10pm. Exceptions to this rule can be made only in rare cases provided there is good reason to do so. Email the instructors if this applies to you. If you do not know anyone in class feel free to post a message on Piazza. We will also set aside some time during the tutorial for students who are looking for collaborators to find each other and discuss forming a group.\nA 1-2 page project proposal is due March 21, 10pm. You will also be asked to summarize the data set that you are using for this proposal.\nEach team will submit a github repository page that describes the deep learning model built in the project. The repository should also contain the code that you wrote.\n\n\nProject Requirements\nBy default, your project must either take a sequence (of variable length) as an input, or produce a sequence as an output, or both. If you have a project proposal that does not involve sequences, please contact the instructors.\nYour model should thus involve an RNN or a Transformer component. Students who want to use methods that we have not covered in the course (e.g. diffusion models, neural ODEs) are free to do so, as long as they confirm their methodology with the instructors before they submit this project proposal. There is also flexibility for students to pursue an open research problem. If any groups want to attempt this, they need to discuss this with one of the instructors before the prject proposal deadline.\nHere are some examples of possible projects:\n\nUsing an RNN (or transformer) to classify sequences (e.g. whether a restaurant review is positive or negative)\nUsing a generative RNN to produce sequences (e.g. South Park TV scripts)\nUsing a Siamese network to determine whether two StackOverflow questions are duplicates\nPredict the next item in a sequence (e.g. Stock market)\nPredict the outcome of a patient based on some sequential factors\nPredict the dynamics of objects under contact and collision (e.g. robotics and graphics)\nGenerate molecules, or predict properties of molecules\n\nBefore choosing a project, consider whether there is data available for you. Since the project deadline is about a month away, consider tailoring your project ideas to what data is available to you.\nYou are encouraged to use transfer learning and data augmentation ideas in your project.\nYou can use deep learning packages (e.g. pytorch, huggingface). However, you should be able to explain the steps involved in the forward pass computation of your model.\n\n\nProject Proposal\nA 1-2 page project proposal is due March 21, 10pm. Please use 12-point font and standard margins. You will also be asked to summarize the data set that you are using for this proposal.\nThe proposal should:\n\nClearly describe the task that your model will perform. (2pt)\n\n2/2 for clearly describing the task using standard deep learning terminology\n1.5/2 for describing the task in a way that is understandable to the grader, but that uses non-standard terminology\n1/2 for describing the task generally (e.g. “sequence classification” without stating the exact classes)\n0/2 for a proposal that does not align with the project requirements\n\nClearly describe the model that you intend to use (2pt)\n\n2/2 for clearly describing the model using standard deep learning terminology; the grader can picture exactly how the model could be used.\n1.5/2 for describing the task in a way that is understandable to the grader, but that uses non-standard terminology\n1/2 for describing the models generally (e.g. sequence-to-sequence model, without describing which ones)\n0/2 for a model that does not align with the project requirements\n\nOutline the data set that you intend to use, and provide some statistics about the amount/type of data that is available (4pt)\n\n1 point for convincing the grader that you are able to acquire the data that you need (with the appropriate license/permission for educational use)\n1 point for convincing the grader that the type and amount of data is sufficient (e.g. via summary statistics, examples data set)\n2 points for convincing the grader that you have explored the data, and considered information about your data relevant to your model (like in A1 Q1)\n\nDiscuss any ethical implications of your model—how might the use (or misuse) of this model help or hurt people? (2pt)\n\n2/2 For a thoughtful discussion that considers the ethical implications across many groups of people (that different groups may be impacted differently).\n1/2 For a discussion that is generic, or considers the ethical implications for only one group of people.\n\nDescribe how work will be divided amongst the team members. We recommend pair-coding for parts of the project, but consider the work that it might take to load/format your data, write a first model, “overfit” to a single data point, etc… (2pt)\n\n2/2 The description provides enough detail so that if a team member is replaced, they know exactly what their responsibilities will be.\n1/2 There is clearly an attempt to describe the division of tasks, but the communication is unclear and/or only the tasks listed above are assigned.\n0/2 Only vague assertions are made (e.g. “we will divide the work equally”, “everyone will work on everything”, or “we will determine who will work on what as the project progresses).\n\nProper formatting (2pt)\n\n2/2 Proposal is 1-2 pages. The proposal is formatted so that readers can find specific information quickly (e.g. via the use of paragraphs and topic sentences)\n1/2 Proposal is slightly over the length limit. There was clearly an attempt to format the proposal, but information is still scattered in various places.\n0/2 Proposal runs extremely long. It is difficult to understand the structure of the proposal.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "project-description.html#final-project",
    "href": "project-description.html#final-project",
    "title": "Final Project Guidelines",
    "section": "Final Project",
    "text": "Final Project\n\nSubmission\nPlease submit a file called github.txt containing a link to the github repository. If your repository will be private, please email the instructors by April 7, 10pm so that TAs and instructors can be added—even if you use tokens.\n\n\nRepository Content\nThe repository should contain:\n\nThe code you used to pre-process the data, but not the data itself. It is generally a bad idea to include data in your github repository, since git is great for lots of small files, but a poor choice for sharing large files. Moreover, most groups are using data collected by other people. While you should share the source of your data, you should generally not share a copy of the data.\nThe code you used to train your model. You may opt to share model weights, or not.\nA README file with the following component:\n\n\nIntroduction that states the deep learning model that you are building\nModel:\n\nA figure/diagram of the model architecture that demonstrates understanding of the steps involved in computing the forward pass\nCount the number of parameters in the model, and a description of where the parameters come from\nExamples of how the model performs on two actual examples from the test set: one successful and one unsuccessful\n\nData:\n\nDescribe the source of your data\nProvide summary statistics of your data to help interpret your results (similar to in the proposal)\nDescribe how you transformed the data (e.g. any data augmentation techniques)\nIf appropriate to your project, describe how the train/validation/test set was split. (Note that splitting the training/validation/test set is not always straightforward!)\n\nTraining:\n\nThe training curve of your final model\nA description how you tuned hyper-parameters\n\nResults:\n\nDescribe the quantitative measure that you are using to evaluate your result\nDescribe the quantitative and qualitative results\nA justification that your implemented method performed reasonably, given the difficulty of the problem—or a hypothesis for why it doesn’t (this is extremely important)\n\nEthical Consideration:\n\nDescription of a use of the system that could give rise to ethical issues. Are there limitations of your model? Your training data?\n\nAuthors\n\nA description of how the work was split—i.e. who did what in this project.\n\n\n\n\nMarking Scheme\nHere is the marking scheme that we will use. Note that you model must be able to make reasonable predictions for your project to receive a passing project grade. In particular, without a reasonable model, you won’t be able to earn credit for Model Examples, Training Curve, Hyperparameter Tuning, Qualitative/Quantitative Results, etc.\nREADME/Writeup (70 points)\n\nIntroduction (4 points): What deep learning model are you building? We are looking for a clear and concise description that uses standard deep learning terminology. Clearly describe the type of task that you are solving, and what your input/outputs are.\nModel Figure (4 points): A figure/diagram of the model architecture that demonstrates understanding of the steps involved in computing the forward pass. We are looking to see if you understand the steps involved in the model computation (i.e. are you treating the model as a black box or do you understand what it’s doing?)\nModel Parameters(4 points): Count the number of parameters in the model, and a description of where the parameters come from. Again, we are looking to see if you understand what the model is doing, and what parameters are being tuned.\nModel Examples (4 points): Examples of how the model performs on two actual examples from the test set: one successful and one unsuccessful.\nData Source (1 point): Describe the source of your data.\nData Summary (4 points): Provide summary statistics of your data to help interpret your results, similar to in the proposal. Please review the feedback provided in the proposal for some guidance on what information is helpful for interpreting your model behaviour.\nData Transformation (3 points): Describe how you transformed the data, i.e. the steps you took to turn the data from what you downloaded, to something that a neural network can use as input. We are looking for a concise description that has just enough information for another person to replicate your process.\nData Split (2 points): If appropriate to your project, describe how the train/validation/test set was split. Note that splitting strategy is not always straightforward, so we are looking to see a split that can be justified.\nTraining Curve (4 points): The training curve of your final model. We are looking for a curve that shows both training and validation performance (if applicable). Your training curve should look reasonable for the problem that you are solving.\nHyperparamter Tuning (4 points): A description how you tuned hyper-parameters. We are looking for hyperparameter choices that makes sense.\nQuantitative Measures (2 points): A description and justification of the quantitative measure that you are using to evaluate your results. For some problems this will be straightforward. For others, please justify the measure that you chose.\nQuantitative and Qualitative Results (8 points): Describe the quantitative and qualitative results. You may choose to use a table or figure to aid in your description. We are looking for both a clear presentation, and a result that makes sense given your data summary. (As an extreme example, you should not have a result that performs worse than a model that, say, predicts the most common class.)\nJustification of Results (20 points): A justification that your implemented method performed reasonably, given the difficulty of the problem—or a hypothesis for why it doesn’t. This is extremely important. We are looking for an interpretation of the result. You may want to refer to your data summary and hyperparameter choices to make your argument.\nEthical Consideration (4 points): Description of a use of the system that could give rise to ethical issues. Are there limitations of your model? Your training data? Please review the feedback provided in the proposal for some guidance on how to think deeply about these issues.\nAuthors (2 points): A description of how the work was split—i.e. who did what in this project. If there are significant issues with the way that work is split, we may follow up with individual teams, and not award equal points to all team members.\n\nCode/Documentation (20 points) We are looking for whether TAs can generally understand what your code does, how it is organized, and the steps that needs to be taken to replicate your model and results. Your code must be in working order (otherwise the TA will not be able to replicate your results)\nAdvanced Concept (10 points). Your project involves at least one of the following:\n\nData Augmentation applied in a way that makes sense for your domain\nTransformer\nGenerative Model, Sequence-to-Sequence Architecture (e.g. that uses teacher-forcing)",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you’re having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you’ll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course forum to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. They’ll be able to help diagnose the issue."
  },
  {
    "objectID": "lecs/w04/lec04.html#last-week",
    "href": "lecs/w04/lec04.html#last-week",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Last Week",
    "text": "Last Week\n\nAutomatic differentiation\nDistributed representations\nGloVe embeddings"
  },
  {
    "objectID": "lecs/w04/lec04.html#this-week",
    "href": "lecs/w04/lec04.html#this-week",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "This week",
    "text": "This week\n\nComputer Vision\nConvolutional Layers\nDownsampling\nTraining a ConvNet\nExamples and Applications"
  },
  {
    "objectID": "lecs/w04/lec04.html#computer-vision-is-hard",
    "href": "lecs/w04/lec04.html#computer-vision-is-hard",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computer vision is hard",
    "text": "Computer vision is hard\n\n \n\n\n\nIn 2012, when Deep Learning had its most recent comeback it was because of computer vision.\nObject change in pose, size, viewpoint, background, illumination\nSome objects are hidden behind others: occlusion"
  },
  {
    "objectID": "lecs/w04/lec04.html#computer-vision-is-really-hard",
    "href": "lecs/w04/lec04.html#computer-vision-is-really-hard",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computer vision is really hard",
    "text": "Computer vision is really hard\n\n\n\nHow can you “hard code” an algorithm that still recognizes that this is a cat?"
  },
  {
    "objectID": "lecs/w04/lec04.html#working-with-small-images",
    "href": "lecs/w04/lec04.html#working-with-small-images",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Working with Small Images",
    "text": "Working with Small Images\nIn the week 3 tutorial, we worked with small, MNIST images, which are \\(28 \\times 28\\) pixels, black and white.\n\n\n\nHow do our models work?"
  },
  {
    "objectID": "lecs/w04/lec04.html#notebook-demo---logistic-regression-weights",
    "href": "lecs/w04/lec04.html#notebook-demo---logistic-regression-weights",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Notebook Demo - Logistic Regression Weights",
    "text": "Notebook Demo - Logistic Regression Weights"
  },
  {
    "objectID": "lecs/w04/lec04.html#notebook-demo---mlp-weights-first-layer",
    "href": "lecs/w04/lec04.html#notebook-demo---mlp-weights-first-layer",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Notebook Demo - MLP Weights (first layer)",
    "text": "Notebook Demo - MLP Weights (first layer)"
  },
  {
    "objectID": "lecs/w04/lec04.html#working-with-large-images",
    "href": "lecs/w04/lec04.html#working-with-large-images",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Working with Large Images",
    "text": "Working with Large Images\n\nSuppose you have an image that is 200 pixels x 200 pixels\nThere are 500 units in the first hidden layer\n\nQ: How many parameters will there be in the first layer?\n\nA: \\(200 \\times 200 \\times 500 + 500 =\\) over 20 million!"
  },
  {
    "objectID": "lecs/w04/lec04.html#working-with-large-images-ii",
    "href": "lecs/w04/lec04.html#working-with-large-images-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Working with Large Images II",
    "text": "Working with Large Images II\nQ: Why might using a fully connected layer be problematic?\n\ncomputing predictions (forward pass) will take a long time\nlarge number of weights requires a lot of training data to avoid overfitting\nsmall shift in image can result in large change in prediction"
  },
  {
    "objectID": "lecs/w04/lec04.html#biological-influence",
    "href": "lecs/w04/lec04.html#biological-influence",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Biological Influence",
    "text": "Biological Influence\n\n\n\nThere is evidence that biological neurons in the visual cortex have locally-connected connections\nSee Hubel and Wiesel Cat Experiment (Note: there is an anesthetised cat in the video that some may find disturbing).\n\n\nHarvard neurophysiologists David H. Hubel / Torsten Wiesel\nInserted a microelectrode into primary visual cortex of a cat.\nProjected patterns of light and dark on a screen in front of the cat\nFound that some neurons fired rapidly when presented with lines at one angle, while others responded best to another angle.\nMore on this here"
  },
  {
    "objectID": "lecs/w04/lec04.html#convolutional-neural-network",
    "href": "lecs/w04/lec04.html#convolutional-neural-network",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolutional Neural Network",
    "text": "Convolutional Neural Network\n\nLocally-connected layers: compute local features based on small regions of the image\n\nExamples of features:\n\na horizontal edge in an area\na vertical edge in an area\na blob (no edges) in the area\na circular shape in the area\n\n\nWeight-sharing: detect the same local features across the entire image"
  },
  {
    "objectID": "lecs/w04/lec04.html#locally-connected-layers",
    "href": "lecs/w04/lec04.html#locally-connected-layers",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Locally Connected Layers",
    "text": "Locally Connected Layers\n\n\n\nEach hidden unit connects to a small region of the input (in this case a \\(3 \\times 3\\) region)"
  },
  {
    "objectID": "lecs/w04/lec04.html#locally-connected-layers-1",
    "href": "lecs/w04/lec04.html#locally-connected-layers-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Locally Connected Layers",
    "text": "Locally Connected Layers\n\n (Remove lines for readability)"
  },
  {
    "objectID": "lecs/w04/lec04.html#locally-connected-layers-2",
    "href": "lecs/w04/lec04.html#locally-connected-layers-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Locally Connected Layers",
    "text": "Locally Connected Layers\n\n\n\nHidden unit geometry has a 2D geometry consistent with the input."
  },
  {
    "objectID": "lecs/w04/lec04.html#locally-connected-layers-3",
    "href": "lecs/w04/lec04.html#locally-connected-layers-3",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Locally Connected Layers",
    "text": "Locally Connected Layers"
  },
  {
    "objectID": "lecs/w04/lec04.html#locally-connected-layers-4",
    "href": "lecs/w04/lec04.html#locally-connected-layers-4",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Locally Connected Layers",
    "text": "Locally Connected Layers"
  },
  {
    "objectID": "lecs/w04/lec04.html#locally-connected-layers-5",
    "href": "lecs/w04/lec04.html#locally-connected-layers-5",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Locally Connected Layers",
    "text": "Locally Connected Layers"
  },
  {
    "objectID": "lecs/w04/lec04.html#locally-connected-layers-6",
    "href": "lecs/w04/lec04.html#locally-connected-layers-6",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Locally Connected Layers",
    "text": "Locally Connected Layers"
  },
  {
    "objectID": "lecs/w04/lec04.html#locally-connected-layers-7",
    "href": "lecs/w04/lec04.html#locally-connected-layers-7",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Locally Connected Layers",
    "text": "Locally Connected Layers\n\n\n\nQ: Which region of the input is this hidden unit connected to?"
  },
  {
    "objectID": "lecs/w04/lec04.html#locally-connected-layers-8",
    "href": "lecs/w04/lec04.html#locally-connected-layers-8",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Locally Connected Layers",
    "text": "Locally Connected Layers"
  },
  {
    "objectID": "lecs/w04/lec04.html#summary",
    "href": "lecs/w04/lec04.html#summary",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Summary",
    "text": "Summary\nFully-connected layers:\n\n\n\n\nLocally connected layers:"
  },
  {
    "objectID": "lecs/w04/lec04.html#weight-sharing",
    "href": "lecs/w04/lec04.html#weight-sharing",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weight Sharing",
    "text": "Weight Sharing\n\n\nLocally connected layers\n\n\n\n\n\nConvolutional layers\n\n\n\n\nUse the same weights across each region (each colour represents the same weight)"
  },
  {
    "objectID": "lecs/w04/lec04.html#convolution-computation",
    "href": "lecs/w04/lec04.html#convolution-computation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolution Computation",
    "text": "Convolution Computation\n\n\n\n\\[\\begin{align*}\n300 = & 100 \\times 1 + 100 \\times 2  + 100 \\times 1 +  \\\\\n      & 100 \\times 0 + 100 \\times 0 + 100 \\times 0 + \\\\\n      & 100 \\times (-1) + 0 \\times (-2) + 0 \\times (-1)\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w04/lec04.html#convolution-computation-ii",
    "href": "lecs/w04/lec04.html#convolution-computation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolution Computation II",
    "text": "Convolution Computation II\n\\[\\begin{align*}\n300 = & 100 \\times 1 + 100 \\times 2  + 100 \\times 1 +  \\\\\n      & 100 \\times 0 + 100 \\times 0 + 100 \\times 0 + \\\\\n      & 100 \\times (-1) + 0 \\times (-2) + 0 \\times (-1)\n\\end{align*}\\]\n\nThe kernel or filter (middle) contains the trainable weights\nIn our example, the kernel size is \\(3\\times3\\)\nThe “convolved features” is another term for the output hidden activation"
  },
  {
    "objectID": "lecs/w04/lec04.html#convolution-computation-1",
    "href": "lecs/w04/lec04.html#convolution-computation-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolution Computation",
    "text": "Convolution Computation\n\n\n\n\\[\\begin{align*}\n300 = &100 \\times 1 + 100 \\times 2  + 100 \\times 1 +  \\\\\n      &100 \\times 0 + 100 \\times 0 + 100 \\times 0 + \\\\\n      &0 \\times (-1) + 0 \\times (-2) + 100 \\times (-1)\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w04/lec04.html#convolution-computation-2",
    "href": "lecs/w04/lec04.html#convolution-computation-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolution Computation",
    "text": "Convolution Computation\n\n\n\nQ: What is the value of the highlighted hidden activation?\n\n\nPause here and try to solve this for yourself."
  },
  {
    "objectID": "lecs/w04/lec04.html#convolution-computation-3",
    "href": "lecs/w04/lec04.html#convolution-computation-3",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolution Computation",
    "text": "Convolution Computation\n\n\n\n\\[\\begin{align*}\n100 = &100 \\times 1 + 100 \\times 2  + 100 \\times 1 +  \\\\\n      &100 \\times 0 + 100 \\times 0 + 100 \\times 0 + \\\\\n      &0 \\times (-1) + 100 \\times (-2) + 100 \\times (-1)\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w04/lec04.html#convolution-computation-4",
    "href": "lecs/w04/lec04.html#convolution-computation-4",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolution Computation",
    "text": "Convolution Computation"
  },
  {
    "objectID": "lecs/w04/lec04.html#weight-sharing-1",
    "href": "lecs/w04/lec04.html#weight-sharing-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weight Sharing",
    "text": "Weight Sharing\nEach neuron on the higher layer is detecting the same feature, but in different locations on the lower layer\n\n\n\n“Detecting” = output (activation) is high if feature is present “Feature” = something in a part of the image, like an edge or shape"
  },
  {
    "objectID": "lecs/w04/lec04.html#sobel-filter---weights-to-detect-horizontal-edges",
    "href": "lecs/w04/lec04.html#sobel-filter---weights-to-detect-horizontal-edges",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Sobel Filter - Weights to Detect Horizontal Edges",
    "text": "Sobel Filter - Weights to Detect Horizontal Edges\n\n\n\n\n\nThe kernel we have seen in the previous example is commonly used to detect horizontal edges.\nIt is known as the Sobel Filter or Sobel Operator"
  },
  {
    "objectID": "lecs/w04/lec04.html#sobel-filter---weights-to-detect-vertical-edges",
    "href": "lecs/w04/lec04.html#sobel-filter---weights-to-detect-vertical-edges",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Sobel Filter - Weights to Detect Vertical Edges",
    "text": "Sobel Filter - Weights to Detect Vertical Edges\n\n\n\n\n\nThere is another kernel for vertical edges."
  },
  {
    "objectID": "lecs/w04/lec04.html#weights-to-detect-blobs",
    "href": "lecs/w04/lec04.html#weights-to-detect-blobs",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weights to Detect Blobs",
    "text": "Weights to Detect Blobs\n\nQ: What is the kernel size of this convolution?\n\n\nBlob detection is the task that aims at detecting regions of an image that differ from its surroundings.\nThis can involve e.g. brightness or color.\nHere, the kernel is of size \\(9\\times 9\\)"
  },
  {
    "objectID": "lecs/w04/lec04.html#example",
    "href": "lecs/w04/lec04.html#example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example:",
    "text": "Example:\nGreyscale input image: \\(7\\times 7\\)\nConvolution kernel: \\(3 \\times 3\\)\nQ: How many hidden units are in the output of this convolution?\n\n\nQ: How many trainable weights are there?\n\n\nThere are \\(3 \\times 3 + 1\\) trainable weights (\\(+ 1\\) for the bias)"
  },
  {
    "objectID": "lecs/w04/lec04.html#convolutions-in-practice",
    "href": "lecs/w04/lec04.html#convolutions-in-practice",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolutions in Practice",
    "text": "Convolutions in Practice\nWhat if we have a coloured image?\nWhat if we want to compute multiple features?"
  },
  {
    "objectID": "lecs/w04/lec04.html#convolution-in-rgb",
    "href": "lecs/w04/lec04.html#convolution-in-rgb",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolution in RGB",
    "text": "Convolution in RGB\n\nThe kernel becomes a 3-dimensional tensor!\nIn this example, the kernel has size 3 \\(\\times 3 \\times 3\\)"
  },
  {
    "objectID": "lecs/w04/lec04.html#convolutions-rgb-input",
    "href": "lecs/w04/lec04.html#convolutions-rgb-input",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolutions: RGB Input",
    "text": "Convolutions: RGB Input\nColour input image: 3 \\(\\times 7 \\times 7\\)\nConvolution kernel: 3 \\(\\times 3 \\times 3\\)\nQuestions:\n\nHow many units are in the output of this convolution?\nHow many trainable weights are there?"
  },
  {
    "objectID": "lecs/w04/lec04.html#terminology",
    "href": "lecs/w04/lec04.html#terminology",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Terminology",
    "text": "Terminology\nInput image: \\(3 \\times 32 \\times 32\\)\nConvolution kernel: 3 \\(\\times 3 \\times 3\\)\n\nThe number 3 is the number of input channels or input feature maps"
  },
  {
    "objectID": "lecs/w04/lec04.html#detecting-multiple-features",
    "href": "lecs/w04/lec04.html#detecting-multiple-features",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Detecting Multiple Features",
    "text": "Detecting Multiple Features\nQ: What if we want to detect many features of the input? (i.e. both horizontal edges and vertical edges, and maybe even other features?)\n\nA: Have many convolutional filters!"
  },
  {
    "objectID": "lecs/w04/lec04.html#many-convolutional-filters",
    "href": "lecs/w04/lec04.html#many-convolutional-filters",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Many Convolutional Filters",
    "text": "Many Convolutional Filters\nInput image: \\(3 \\times 7\\times 7\\)\nConvolution kernel: \\(3 \\times 3 \\times 3 \\times\\) 5\nQ:\n\nHow many units are in the output of this convolution?\nHow many trainable weights are there?"
  },
  {
    "objectID": "lecs/w04/lec04.html#more-terminology",
    "href": "lecs/w04/lec04.html#more-terminology",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "More Terminology",
    "text": "More Terminology\nInput image of size \\(3 \\times 32 \\times 32\\)\nConvolution kernel of 3 \\(\\times 3 \\times 3 \\times\\) 5\n\nThe number 3 is the number of input channels or input feature maps\nThe number 5 is the number of output channels or output feature maps"
  },
  {
    "objectID": "lecs/w04/lec04.html#example-1",
    "href": "lecs/w04/lec04.html#example-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example",
    "text": "Example\nInput features: \\(5 \\times 32 \\times 32\\)\nConvolution kernel: \\(5 \\times 3 \\times 3 \\times 10\\)\nQuestions:\n\nHow many input channels are there?\nHow many output channels are there?\nHow many units are in the higher layer?\nHow many trainable weights are there?\n\n\n\nModern deep learning frameworks have all of this implemented.\nStill important to know as you design your network."
  },
  {
    "objectID": "lecs/w04/lec04.html#consolidating-information",
    "href": "lecs/w04/lec04.html#consolidating-information",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Consolidating Information",
    "text": "Consolidating Information\nIn a neural network with fully-connected layers, we reduced the number of units in each hidden layer\nQ: Why?\n\n\nTo be able to consolidate information, and remove out information not useful for the current task\n\nQ: How can we consolidate information in a neural network with convolutional layers?\n\n\n\nmax pooling, average pooling, strided convolutions"
  },
  {
    "objectID": "lecs/w04/lec04.html#max-pooling",
    "href": "lecs/w04/lec04.html#max-pooling",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Max-Pooling",
    "text": "Max-Pooling\nIdea: take the maximum value in each \\(2 \\times 2\\) grid."
  },
  {
    "objectID": "lecs/w04/lec04.html#max-pooling-example",
    "href": "lecs/w04/lec04.html#max-pooling-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Max-Pooling Example",
    "text": "Max-Pooling Example\nWe can add a max-pooling layer after each convolutional layer"
  },
  {
    "objectID": "lecs/w04/lec04.html#average-pooling",
    "href": "lecs/w04/lec04.html#average-pooling",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Average Pooling",
    "text": "Average Pooling\n\nAverage pooling (compute the average activation of a region)\nMax pooling generally works better"
  },
  {
    "objectID": "lecs/w04/lec04.html#strided-convolution",
    "href": "lecs/w04/lec04.html#strided-convolution",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Strided Convolution",
    "text": "Strided Convolution\nMore recently people are doing away with pooling operations, using strided convolutions instead:\n\nShift the kernel by 2 (stride=2) when computing the next output feature.\n\nVisualization examples\n\nhttps://arxiv.org/pdf/1603.07285.pdf\nhttps://github.com/vdumoulin/conv_arithmetic"
  },
  {
    "objectID": "lecs/w04/lec04.html#how-do-we-train-a-conv-net",
    "href": "lecs/w04/lec04.html#how-do-we-train-a-conv-net",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "How do we train a conv net?",
    "text": "How do we train a conv net?\nWith backprop, of course!\n\nRecall what we need to do. Backprop is a message passing procedure, where each layer knows how to pass messages backwards through the computation graph. Let’s determine the updates for convolution layers."
  },
  {
    "objectID": "lecs/w04/lec04.html#how-do-we-train-a-conv-net-ii",
    "href": "lecs/w04/lec04.html#how-do-we-train-a-conv-net-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "How do we train a conv net? II",
    "text": "How do we train a conv net? II\n\nWe assume we are given the loss derivatives \\(\\overline{y_{i,t}}\\) with respect to the output units.\nWe need to compute the cost derivatives with respect to the input units and with respect to the weights.\n\n\nThe only new feature is: how do we do backprop with tied weights?\n\n\n\nThe derivatives with respect of the input units are not needed for the first layer\nBut they are needed for all subsequent layers as the inputs to these subsequent layers are a function of previous layers’ weights."
  },
  {
    "objectID": "lecs/w04/lec04.html#multivariate-chain-rule-inputs",
    "href": "lecs/w04/lec04.html#multivariate-chain-rule-inputs",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multivariate Chain Rule (inputs)",
    "text": "Multivariate Chain Rule (inputs)\nConsider the computation graph for the inputs:\n\n\nEach input unit influences all the output units that have it within their receptive fields. Using the multivariate Chain Rule, we need to sum together the derivative terms for all these edges\n\n\n\nThis is a 1d signal, e.g. think of a 1d image with a line camera\nHere we have “kernels” of size 3\nWe already applied multivariate chain rule for “tied inputs” in the past"
  },
  {
    "objectID": "lecs/w04/lec04.html#multivariate-chain-rule-weights",
    "href": "lecs/w04/lec04.html#multivariate-chain-rule-weights",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multivariate Chain Rule (weights)",
    "text": "Multivariate Chain Rule (weights)\nConsider the computation graph for the weights:\n\n\nEach of the weights affects all the output units for the corresponding input and output feature maps."
  },
  {
    "objectID": "lecs/w04/lec04.html#backpropagation-on-conv-layers",
    "href": "lecs/w04/lec04.html#backpropagation-on-conv-layers",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation on conv layers",
    "text": "Backpropagation on conv layers\nThe formula for the convolution layer for 1-D signals:\n\\[\ny_{i,t} = \\sum_{j=1}^{J} \\sum_{\\tau = -R}^{R} w_{i,j,\\tau} \\, x_{j, t + \\tau}.\n\\]\n\nWe compute the derivatives, which requires summing over all spatial locations:\n\\[\\begin{align*}\n\\overline{w_{i,j,\\tau}}\n    &= \\sum_{t} y_{i,t} \\frac{\\partial y_{i,t}}{\\partial w_{i,j,\\tau}} \\\\\n    &= \\sum_{t} y_{i,t} x_{j, t + \\tau}\n\\end{align*}\\]\n\n\n\nFocus on 1-D signals with \\(J\\) feature maps (e.g. colors) and kernel radius \\(R\\) (i.e. a kernal size \\(K=2R+1\\))\n\\(i\\) is the index of output feature map (of which we have \\(I\\)).\n\\(t\\) is the index of the output location (\\(t\\in \\{1, \\ldots, \\}\\))"
  },
  {
    "objectID": "lecs/w04/lec04.html#object-recognition",
    "href": "lecs/w04/lec04.html#object-recognition",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Object recognition",
    "text": "Object recognition\n\nObject recognition is the task of identifying which object category is present in an image.\nIt’s challenging because objects can differ widely in position, size, shape, appearance, etc., and we have to deal with occlusions, lighting changes, etc.\nWhy we care\n\nDirect applications to image search\nClosely related to object detection, the task of locating all instances of an object in an image"
  },
  {
    "objectID": "lecs/w04/lec04.html#datasets",
    "href": "lecs/w04/lec04.html#datasets",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Datasets",
    "text": "Datasets\n\nIn order to train and evaluate a machine learning system, we need to collect a dataset. The design of the dataset can have major implications.  \n\nSome questions to consider:\n\nWhich categories to include?\nWhere should the images come from?\nHow many images to collect?\nHow to normalize (preprocess) the images?"
  },
  {
    "objectID": "lecs/w04/lec04.html#mnist---handwritten-digits-dataset",
    "href": "lecs/w04/lec04.html#mnist---handwritten-digits-dataset",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "MNIST - Handwritten Digits Dataset",
    "text": "MNIST - Handwritten Digits Dataset\n\nCategories: 10 digit classes\nSource: Scans of handwritten zip codes from envelopes\nSize: 60,000 training images / 10,000 test images, Grayscale, 28 x 28 pixels\nNormalization: Centered within the image, scaled to a consistent size\n\n\n\nAssumption: Digit recognizer is part of a larger pipeline.\nIn 1998, Yann LeCun and colleagues built a conv net called LeNet\nIt was able to classify digits with 98.9% test accuracy.\nGood enough to be used in a system for automatically reading numbers on checks."
  },
  {
    "objectID": "lecs/w04/lec04.html#imagenet-i",
    "href": "lecs/w04/lec04.html#imagenet-i",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ImageNet I",
    "text": "ImageNet I"
  },
  {
    "objectID": "lecs/w04/lec04.html#imagenet-ii",
    "href": "lecs/w04/lec04.html#imagenet-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ImageNet II",
    "text": "ImageNet II\nUsed for: The ImageNet Large Scale Visual Recognition Challenge (ILSVRC), an annual benchmark competition for object recognition algorithms\n\nDesign Decisions\n\n\nCategories: Taken from a lexical database called WordNet\n\nWordNet consists of “synsets”\nAlmost 22,000 classes used\nThe 1,000 most common chosen for the ILSVRC\nThe categories are really specific, e.g., hundreds of kinds of dogs"
  },
  {
    "objectID": "lecs/w04/lec04.html#imagenet-iii",
    "href": "lecs/w04/lec04.html#imagenet-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ImageNet III",
    "text": "ImageNet III\n\nSize: 1.2 million full-sized images for the ILSVRC\nSource: Results from image search engines, hand-labeled by Mechanical Turkers\n\nLabeling such specific categories was challenging; annotators had to be given the WordNet hierarchy, Wikipedia, etc.\n\nNormalization: None, although the contestants are free to do preprocessing\n\n\n\nSynsets are sets of synonymous words"
  },
  {
    "objectID": "lecs/w04/lec04.html#imagenet-iv",
    "href": "lecs/w04/lec04.html#imagenet-iv",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ImageNet IV",
    "text": "ImageNet IV"
  },
  {
    "objectID": "lecs/w04/lec04.html#imagenet-v",
    "href": "lecs/w04/lec04.html#imagenet-v",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ImageNet V",
    "text": "ImageNet V"
  },
  {
    "objectID": "lecs/w04/lec04.html#imagenet-results",
    "href": "lecs/w04/lec04.html#imagenet-results",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ImageNet Results",
    "text": "ImageNet Results\n\n\n\nYear\nModel\nTop-5 error\n\n\n\n\n2010\nHand-designed descriptors + SVM\n28.2%\n\n\n2011\nCompressed Fisher Vectors + SVM\n25.8%\n\n\n2012\nAlexNet\n16.4%\n\n\n2013\na variant of AlexNet\n11.7%\n\n\n2014\nGoogLeNet\n6.6%\n\n\n2015\ndeep residual nets\n4.5%\n\n\n\n\n\nTop-5 error: True class among the 5 “best” results\nHuman-performance is around 5.1%.\nISVRC stopped running because the performance is already so good."
  },
  {
    "objectID": "lecs/w04/lec04.html#what-features-do-cnns-detect",
    "href": "lecs/w04/lec04.html#what-features-do-cnns-detect",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What features do CNN’s detect?",
    "text": "What features do CNN’s detect?"
  },
  {
    "objectID": "lecs/w04/lec04.html#size-of-a-convnet",
    "href": "lecs/w04/lec04.html#size-of-a-convnet",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Size of a convnet",
    "text": "Size of a convnet\n\nWays to measure the size of a network:\n\nNumber of units. The activations need to be stored in memory during training.\nNumber of weights. The weights need to be stored in memory / number of parameters determines overfitting.\nNumber of connections. There are approximately 3 add-multiply operations per connection (1 for the forward pass, 2 for the backward pass).\n\nFully connected layer with \\(M\\) inputs and \\(N\\) outputs has \\(MN\\) connections / weights.\n\n\n\nThe story for conv nets is more complicated."
  },
  {
    "objectID": "lecs/w04/lec04.html#size-of-a-convnet-ii",
    "href": "lecs/w04/lec04.html#size-of-a-convnet-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Size of a convnet II",
    "text": "Size of a convnet II\n\n\n\n\n\nOutput Units: We assume no change in dimensions. For conv nets, this requires padding.\nWeights Much fewer weights because \\(K^2 \\ll W^2H^2\\).\nConnections Not the same as weights anymore."
  },
  {
    "objectID": "lecs/w04/lec04.html#size-of-a-convnet-iii",
    "href": "lecs/w04/lec04.html#size-of-a-convnet-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Size of a convnet III",
    "text": "Size of a convnet III"
  },
  {
    "objectID": "lecs/w04/lec04.html#lenet-atchitecture",
    "href": "lecs/w04/lec04.html#lenet-atchitecture",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "LeNet Atchitecture",
    "text": "LeNet Atchitecture"
  },
  {
    "objectID": "lecs/w04/lec04.html#lenet-architecture-ii",
    "href": "lecs/w04/lec04.html#lenet-architecture-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "LeNet Architecture II",
    "text": "LeNet Architecture II\n\n\n\n\nInput: 32x32 pixel, greyscale image\nFirst convolution has 6 output features (5x5 convolution?)\nFirst subsampling is probably a max-pooling operation\nSecond convolution has 16 output features (5x5 convolution?)\n…\nSome number of fully-connected layers at the end"
  },
  {
    "objectID": "lecs/w04/lec04.html#resnet-architecture",
    "href": "lecs/w04/lec04.html#resnet-architecture",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ResNet Architecture",
    "text": "ResNet Architecture"
  },
  {
    "objectID": "lecs/w04/lec04.html#resnet-architecture-ii",
    "href": "lecs/w04/lec04.html#resnet-architecture-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ResNet Architecture II",
    "text": "ResNet Architecture II\n\nSuppose we add another layer. How can we ensure that the new set of represented functions contains the old set, before the layer was added?\nWhy do we need this? We’d like to get larger (nested) sets of functions as we add more layers and not just different (non-nested) sets."
  },
  {
    "objectID": "lecs/w04/lec04.html#resnet-blocks",
    "href": "lecs/w04/lec04.html#resnet-blocks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "ResNet Blocks",
    "text": "ResNet Blocks\n\n\n\n\nSide effect of adding identity \\(f(x) = x + g(x)\\): better gradient propagation\nSee https://d2l.ai/chapter_convolutional-modern/resnet.html"
  },
  {
    "objectID": "lecs/w04/lec04.html#densenet-blocks",
    "href": "lecs/w04/lec04.html#densenet-blocks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "DenseNet Blocks",
    "text": "DenseNet Blocks\n\n\n\nSame idea as ResNet blocks, but instead of addition \\(f(x) = x + g(x)\\) they use concatenation \\(f(x) = [x, g(x)]\\)."
  },
  {
    "objectID": "lecs/w04/lec04.html#densenet-architecture",
    "href": "lecs/w04/lec04.html#densenet-architecture",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "DenseNet Architecture",
    "text": "DenseNet Architecture\n\n\n\nSee https://d2l.ai/chapter_convolutional-modern/densenet.html"
  },
  {
    "objectID": "lecs/w04/lec04.html#summary-1",
    "href": "lecs/w04/lec04.html#summary-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Summary",
    "text": "Summary\n\nComputer Vision has been the main motivation for Conv Nets\nThey draw inspiration from biological vision systems\nKey ideas are: local connectivity and weight sharing\nConv Nets can be trained using backpropagation"
  },
  {
    "objectID": "lecs/w02/lec02.html#last-week",
    "href": "lecs/w02/lec02.html#last-week",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Last Week",
    "text": "Last Week\n\nReview of linear models\n\nlinear regression\nlinear classification (logistic regression)\n\nGradient descent to train these models"
  },
  {
    "objectID": "lecs/w02/lec02.html#this-week",
    "href": "lecs/w02/lec02.html#this-week",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "This Week",
    "text": "This Week\n\nBiological and Artifical Neurons\nLimitations of Linear Models for Classification\nMultilayer Perceptrons\nBackpropagation"
  },
  {
    "objectID": "lecs/w02/lec02.html#neuron",
    "href": "lecs/w02/lec02.html#neuron",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Neuron",
    "text": "Neuron\n\n\n\nA Neuron is a cell also known as nerve cell"
  },
  {
    "objectID": "lecs/w02/lec02.html#neuron-anatomy",
    "href": "lecs/w02/lec02.html#neuron-anatomy",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Neuron Anatomy",
    "text": "Neuron Anatomy\n\nThe dendrites, which are connected to other cells that provide information.\nThe cell body, which consolidates information from the dendrites.\nThe axon, which is an extension from the cell body that passes information to other cells.\nThe synapse, which is the area where the axon of one neuron and the dendrite of another connect."
  },
  {
    "objectID": "lecs/w02/lec02.html#what-does-a-neuron-do",
    "href": "lecs/w02/lec02.html#what-does-a-neuron-do",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What does a neuron do?",
    "text": "What does a neuron do?\n\n\n\n\n\nConsolidates “information” (voltage difference) from its dendrites\nIf the total activity in a neuron’s dendrite lowers the voltage difference enough, the entire cell depolarizes and the neuron fires"
  },
  {
    "objectID": "lecs/w02/lec02.html#what-does-a-neuron-do-1",
    "href": "lecs/w02/lec02.html#what-does-a-neuron-do-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What does a neuron do?",
    "text": "What does a neuron do?\n\n\n\n\n\nThe voltage signal spreads along the axon and to the synapse, then to the next neurons\nNeuron sends information to the next cell"
  },
  {
    "objectID": "lecs/w02/lec02.html#what-makes-a-neuron-fire",
    "href": "lecs/w02/lec02.html#what-makes-a-neuron-fire",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What makes a neuron fire?",
    "text": "What makes a neuron fire?\nNeurons can fire in response to…\n\nretinal cells\ncertain edges, lines, angles, movements\nhands and faces (in primates)\nspecific people (in humans)\n\nalthough the existence of these “grandmother cells” is contested\n\n\n\n\nGrandmother cells are also known as Jennifer Aniston neurons\nObserved single neuron firing when shown pictures of Jennifer Aniston by operating on patients with epileptic seizures."
  },
  {
    "objectID": "lecs/w02/lec02.html#modeling-individual-neurons",
    "href": "lecs/w02/lec02.html#modeling-individual-neurons",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Modeling Individual Neurons",
    "text": "Modeling Individual Neurons\n\n\n\n\n\\(x_{i}\\) are inputs to the neuron\n\\(w_{i}\\) are the neuron’s weights\n\\(b\\) is the neuron’s bias\n\n\n\nIn Learning with Artifical Neural Networks, we want to create a simplified model of neurons in the brain."
  },
  {
    "objectID": "lecs/w02/lec02.html#modeling-individual-neurons-ii",
    "href": "lecs/w02/lec02.html#modeling-individual-neurons-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Modeling Individual Neurons II",
    "text": "Modeling Individual Neurons II\n\n\n\n\n\\(f\\) is an activation function\n\\(f(\\sum_i x_i w_i + b)\\) is the neuron’s activation (output)"
  },
  {
    "objectID": "lecs/w02/lec02.html#linear-models-as-a-single-neuron",
    "href": "lecs/w02/lec02.html#linear-models-as-a-single-neuron",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Models as a Single Neuron",
    "text": "Linear Models as a Single Neuron\n\n\n\n\n\\(x_{i}\\) are the inputs\n\\(w_{i}\\) are components of the weight vector \\({\\bf w}\\)\n\\(b\\) is the bias\n\n\n\nA (univariate) linear model can be interpreted as one single neuron.\nUnivariate means that we model one single attribute."
  },
  {
    "objectID": "lecs/w02/lec02.html#linear-models-as-a-single-neuron-ii",
    "href": "lecs/w02/lec02.html#linear-models-as-a-single-neuron-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Models as a Single Neuron II",
    "text": "Linear Models as a Single Neuron II\n\n\n\n\n\\(f\\) is the identity function\n\\(y = \\sum_i x_i w_i + b = {\\bf w}^\\top {\\bf x} + b\\) is the output"
  },
  {
    "objectID": "lecs/w02/lec02.html#logistic-regression-models-for-binary-classification-as-a-single-neuron",
    "href": "lecs/w02/lec02.html#logistic-regression-models-for-binary-classification-as-a-single-neuron",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Logistic Regression Models (for Binary Classification) as a Single Neuron",
    "text": "Logistic Regression Models (for Binary Classification) as a Single Neuron\n\n\n\n\n\\(x_{i}\\) are the inputs\n\\(w_{i}\\) are components of the weight vector \\({\\bf w}\\)\n\\(b\\) is the bias"
  },
  {
    "objectID": "lecs/w02/lec02.html#logistic-regression-models-for-binary-classification-as-a-single-neuron-ii",
    "href": "lecs/w02/lec02.html#logistic-regression-models-for-binary-classification-as-a-single-neuron-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Logistic Regression Models (for Binary Classification) as a Single Neuron II",
    "text": "Logistic Regression Models (for Binary Classification) as a Single Neuron II\n\n\n\n\n\\(f = \\sigma\\)\n\\(y = \\sigma(\\sum_i x_i w_i + b) = \\sigma({\\bf w}^\\top {\\bf x} + b)\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#logistic-regression-models-for-multi-class-classification-as-a-neural-network",
    "href": "lecs/w02/lec02.html#logistic-regression-models-for-multi-class-classification-as-a-neural-network",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Logistic Regression Models (for Multi-Class Classification) as a Neural Network",
    "text": "Logistic Regression Models (for Multi-Class Classification) as a Neural Network\nWe use \\(K\\) neurons (one for each class):\n\n\\(x_{i}\\) are the inputs\n\\(w_{j,i}\\) are components of the weight matrix \\(W\\in \\mathbb{R}^{K\\times N}\\)\n\\(b_i\\) are components of the bias vector \\({\\bf b}\\)\n\\(f = \\text{softmax}\\) : applied to the entire vector of values\n\\({\\bf y} = \\text{softmax}(W{\\bf x} + {\\bf b})\\) : outputs of \\(K\\) neurons\n\n\n\nNow we have an entire weight matrix \\(W\\) with \\(K\\) rows and a \\(K\\)-dimensional bias vector. One for each class."
  },
  {
    "objectID": "lecs/w02/lec02.html#xor-example",
    "href": "lecs/w02/lec02.html#xor-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "XOR example",
    "text": "XOR example\n\nSingle neurons (linear classifiers) are very limited in expressive power\nXOR is a classic example of a function that’s not linearly separable, with an elegant proof using convexity"
  },
  {
    "objectID": "lecs/w02/lec02.html#convex-sets",
    "href": "lecs/w02/lec02.html#convex-sets",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convex Sets",
    "text": "Convex Sets\nA set \\(S\\subseteq\\mathbb{R}^n\\) is convex if any line segment connecting points in \\(S\\) lies in \\(S\\).\n\nMore formally, \\(S\\) is convex iff\n\\[{\\bf x_1}, {\\bf x_2} \\in S \\implies \\forall \\lambda \\in [0,\\, 1],\\, \\lambda {\\bf x_1} + (1 - \\lambda){\\bf x_2} \\in S.\\]\n\n\nA simple inductive argument shows that for \\({\\bf x_1}, \\dots, {\\bf x_N} \\in S\\), the weighted average or convex combination lies in the set:\n\\[\\lambda_1 {\\bf x_1} + \\dots + \\lambda_N {\\bf x_N} \\in S \\text{ for }\\lambda_1 + \\dots + \\lambda_N = 1\\ .\\]\n\n\n\niff stands for “if and only if”"
  },
  {
    "objectID": "lecs/w02/lec02.html#xor-not-linearly-separable",
    "href": "lecs/w02/lec02.html#xor-not-linearly-separable",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "XOR not linearly separable",
    "text": "XOR not linearly separable\nInitial Observations\n\nA binary linear classifier divides the euclidean space into two half-spaces\nHalf-spaces are convex"
  },
  {
    "objectID": "lecs/w02/lec02.html#xor-not-linearly-separable-ii",
    "href": "lecs/w02/lec02.html#xor-not-linearly-separable-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "XOR not linearly separable II",
    "text": "XOR not linearly separable II\n\n\nSuppose there were some feasible hypothesis. If the positive examples are in the positive half-space, then the green line segment must be as well.\nSimilarly, red line segment must lie within the negative half-space.\nBut the intersection can’t lie in both half-spaces. Contradiction!"
  },
  {
    "objectID": "lecs/w02/lec02.html#history-of-the-xor-example",
    "href": "lecs/w02/lec02.html#history-of-the-xor-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "History of the XOR Example",
    "text": "History of the XOR Example\n\n\nMinsky and Papert shown in their work Perceptrons that XOR cannot be learned by a Neuron.\nIts pessimistic outlook on perceptrons is assumed as one of the causes for the AI winter of the 70s / early 80s."
  },
  {
    "objectID": "lecs/w02/lec02.html#a-more-troubling-example",
    "href": "lecs/w02/lec02.html#a-more-troubling-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "A more troubling example",
    "text": "A more troubling example\n\n\nThese images represent 16-dimensional vectors. Want to distinguish patterns A and B in all possible translations (with wrap-around).\n\n\nSuppose there’s a feasible solution. The average of all translations of A is the vector (0.25, 0.25, . . . , 0.25). Therefore, this point must be classified as A. All translations of B have the same average. Contradiction!\n\n\n\nThe argument here is basically a convexity argument again\nThe average of all possible patterns A must be classified as A because of convexity. Same for B\nHowever, both patterns have the same average leading to a contradiction"
  },
  {
    "objectID": "lecs/w02/lec02.html#nonlinear-feature-maps",
    "href": "lecs/w02/lec02.html#nonlinear-feature-maps",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "(Nonlinear) Feature Maps",
    "text": "(Nonlinear) Feature Maps\nSometimes, we can overcome this limitation with nonlinear feature maps.\n\nNonlinear feature maps transform the original input features into a different (often higher dimensional) representation.\n\n\nConsider the XOR problem again and use the following feature map: \\[\\Psi({\\bf x}) = \\begin{pmatrix}x_1 \\\\ x_2 \\\\ x_1x_2 \\end{pmatrix}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#nonlinear-feature-maps-ii",
    "href": "lecs/w02/lec02.html#nonlinear-feature-maps-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "(Nonlinear) Feature Maps II",
    "text": "(Nonlinear) Feature Maps II\n\n\n\n\n\n\n\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(\\phi_1({\\bf x})\\)\n\\(\\phi_2({\\bf x})\\)\n\\(\\phi_3({\\bf x})\\)\nt\n\n\n\n\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n1\n0\n1\n\n\n1\n0\n1\n0\n0\n1\n\n\n1\n1\n1\n1\n1\n0\n\n\n\nThis is linearly separable (Try it!)\n\n… but generally, it can be hard to pick good basis functions.\n\n\nWe’ll use neural nets to learn nonlinear hypotheses directly"
  },
  {
    "objectID": "lecs/w02/lec02.html#an-artificial-neural-network-multilayer-perceptron",
    "href": "lecs/w02/lec02.html#an-artificial-neural-network-multilayer-perceptron",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "An Artificial Neural Network (Multilayer Perceptron)",
    "text": "An Artificial Neural Network (Multilayer Perceptron)\n\n\n\nIdea\n\nUse a simplified (mathematical) model of a neuron as building blocks\nConnect the neurons together accross multiple layers."
  },
  {
    "objectID": "lecs/w02/lec02.html#an-artificial-neural-network-multilayer-perceptron-ii",
    "href": "lecs/w02/lec02.html#an-artificial-neural-network-multilayer-perceptron-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "An Artificial Neural Network (Multilayer Perceptron) II",
    "text": "An Artificial Neural Network (Multilayer Perceptron) II\n\n\n\n\nAn input layer: feed in input features (e.g. like retinal cells in your eyes)\nA number of hidden layers: don’t have specific meaning\nAn output layer: interpret output like a “grandmother cell”"
  },
  {
    "objectID": "lecs/w02/lec02.html#but-what-do-these-neurons-mean",
    "href": "lecs/w02/lec02.html#but-what-do-these-neurons-mean",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "But what do these neurons mean?",
    "text": "But what do these neurons mean?\n\n\nUse \\(x_i\\) to encode the input\n\ne.g. pixels in an image\n\n\n\n\n\nUse \\(y\\) to encode the output (of a binary classification problem)\n\ne.g. cancer vs. not cancer\n\n\n\n\n\nUse \\(h_i^{(k)}\\) to denote a unit in the hidden layer\n\ndifficult to interpret\n\n\n\n\n\n\\(x_i\\) can be thought of as the neurons that are connected to the receptors in the eye\n\\(y\\) can be thought of as a grandmother cell."
  },
  {
    "objectID": "lecs/w02/lec02.html#example-mnist-digit-recognition",
    "href": "lecs/w02/lec02.html#example-mnist-digit-recognition",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: MNIST Digit Recognition",
    "text": "Example: MNIST Digit Recognition"
  },
  {
    "objectID": "lecs/w02/lec02.html#mnist-digit-recognition-ii",
    "href": "lecs/w02/lec02.html#mnist-digit-recognition-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "MNIST Digit Recognition II",
    "text": "MNIST Digit Recognition II\nWith a logistic regression model, we would have:\n\n\nInput: An 28x28 pixel grayscale image\n\n\\({\\bf x}\\) is a vector of length 784\n\n\n\n\n\nTarget: The digit represented in the image\n\n\\({\\bf t}\\) is a one-hot vector of length 10\n\n\n\n\n\nModel: \\({\\bf y} = \\text{softmax}(W{\\bf x} + {\\bf b})\\)\n\n\n\n\nEach \\(x_i\\) is a intensity value of the corresponding pixel, i.e. a value between 0 and 255.\nAs a reminder: the one-hot vector has 1 at the entry corresponding to the classified number and 0 everywhere elese."
  },
  {
    "objectID": "lecs/w02/lec02.html#adding-a-hidden-layer",
    "href": "lecs/w02/lec02.html#adding-a-hidden-layer",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Adding a Hidden Layer",
    "text": "Adding a Hidden Layer\nTwo layer neural network\n\n\n\n\nInput size: 784 (number of features)\nHidden size: 50 (we choose this number)\nOutput size: 10 (number of classes)\n\n\n\nAll the numbers that we can choose such as the number of hidden units are known as Hyperparameters."
  },
  {
    "objectID": "lecs/w02/lec02.html#side-note-about-machine-learning-models",
    "href": "lecs/w02/lec02.html#side-note-about-machine-learning-models",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Side note about machine learning models",
    "text": "Side note about machine learning models\nWhen discussing machine learning and deep learning models, we usually\n\nfirst talk about how to make predictions assume the weights are trained\nthen talk about how to train the weights\n\nOften the second step requires gradient descent or some other optimization method"
  },
  {
    "objectID": "lecs/w02/lec02.html#making-predictions-computing-the-hidden-layer",
    "href": "lecs/w02/lec02.html#making-predictions-computing-the-hidden-layer",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Making Predictions: computing the hidden layer",
    "text": "Making Predictions: computing the hidden layer\n\n\n\n\\[\\begin{align*}\nh_1 &= f\\left(\\sum_{i=1}^{784} w^{(1)}_{1,i} x_i + b^{(1)}_1\\right) \\\\\nh_2 &= f\\left(\\sum_{i=1}^{784} w^{(1)}_{2,i} x_i + b^{(1)}_2\\right) \\\\\n...\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#making-predictions-computing-the-output-pre-activation",
    "href": "lecs/w02/lec02.html#making-predictions-computing-the-output-pre-activation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Making Predictions: computing the output (pre-activation)",
    "text": "Making Predictions: computing the output (pre-activation)\n\n\n\n\\[\\begin{align*}\nz_1 &=  \\sum_{j=1}^{50} w^{(2)}_{1,j} h_j + b^{(2)}_1  \\\\\nz_2 &=  \\sum_{j=1}^{50} w^{(2)}_{2,j} h_j + b^{(2)}_2  \\\\\n...\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#making-predictions-applying-the-output-activation",
    "href": "lecs/w02/lec02.html#making-predictions-applying-the-output-activation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Making Predictions: applying the output activation",
    "text": "Making Predictions: applying the output activation\n\n\n\n\\[\\begin{align*}\n{\\bf z} &= \\begin{bmatrix}z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_{10}\\end{bmatrix},\\quad {\\bf y} = \\text{softmax}({\\bf z})\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#making-predictions-vectorized",
    "href": "lecs/w02/lec02.html#making-predictions-vectorized",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Making Predictions: Vectorized",
    "text": "Making Predictions: Vectorized\n\n\n\n\\[\\begin{align*}\n{\\bf h} &= f(W^{(1)}{\\bf x} + {\\bf b}^{(1)}) \\\\\n{\\bf z} &= W^{(2)}{\\bf h} + {\\bf b}^{(2)} \\\\\n{\\bf y} &= \\text{softmax}({\\bf z})\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#activation-functions-common-choices",
    "href": "lecs/w02/lec02.html#activation-functions-common-choices",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Activation Functions: common choices",
    "text": "Activation Functions: common choices\nCommon Choices:\n\nSigmoid activation\nTanh activation\nReLU activation\n\nRule of thumb: Start with ReLU activation. If necessary, try tanh."
  },
  {
    "objectID": "lecs/w02/lec02.html#activation-function-sigmoid",
    "href": "lecs/w02/lec02.html#activation-function-sigmoid",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Activation Function: Sigmoid",
    "text": "Activation Function: Sigmoid\n\n\n\n\nGradient vanishes at the extremes as the function converges to \\(0\\) and \\(1\\) respectively.\nAll activations are positive (see this blog post to learn why we don’t want this)\n\n\n\nHaving only positive activations can lead to undesired behavior during training because all weights entering a neuron can either be only increased or only decreased."
  },
  {
    "objectID": "lecs/w02/lec02.html#activation-function-tanh",
    "href": "lecs/w02/lec02.html#activation-function-tanh",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Activation Function: Tanh",
    "text": "Activation Function: Tanh\n\n\n\n\nscaled version of the sigmoid activation\nalso somewhat problematic due to gradient signal\nactivations can be positive or negative"
  },
  {
    "objectID": "lecs/w02/lec02.html#activation-function-relu",
    "href": "lecs/w02/lec02.html#activation-function-relu",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Activation Function: ReLU",
    "text": "Activation Function: ReLU\n\n\n\n\nmost often used nowadays\nall activations are positive\neasy to compute gradients\ncan be problematic if the bias is too large and negative, so the activations are always 0\n\n\n\nThe last point is known as the dying ReLU phenomenon if a neuron becomes inactive and never recovers."
  },
  {
    "objectID": "lecs/w02/lec02.html#feature-learning",
    "href": "lecs/w02/lec02.html#feature-learning",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature Learning",
    "text": "Feature Learning\nNeural nets can be viewed as a way of learning features:"
  },
  {
    "objectID": "lecs/w02/lec02.html#feature-learning-contd",
    "href": "lecs/w02/lec02.html#feature-learning-contd",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature Learning (cont’d)",
    "text": "Feature Learning (cont’d)\nThe goal is for these features to become linearly separable:"
  },
  {
    "objectID": "lecs/w02/lec02.html#computing-xor",
    "href": "lecs/w02/lec02.html#computing-xor",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing XOR",
    "text": "Computing XOR\nExercise: design a network to compute XOR\nUse a hard threshold activation function:\n\\[\\begin{align*}\n    f(x) = \\begin{cases}\n                1, & x \\geq 0 \\\\\n                0, & x &lt; 0\n            \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#computing-xor-demo",
    "href": "lecs/w02/lec02.html#computing-xor-demo",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing XOR Demo",
    "text": "Computing XOR Demo\nDemo: https://playground.tensorflow.org/"
  },
  {
    "objectID": "lecs/w02/lec02.html#expressive-power-linear-layers-no-activation-function",
    "href": "lecs/w02/lec02.html#expressive-power-linear-layers-no-activation-function",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Expressive Power: Linear Layers (No Activation Function)",
    "text": "Expressive Power: Linear Layers (No Activation Function)\n\nWe’ve seen that there are some functions that linear classifiers can’t represent. Are deep networks any better?\nAny sequence of layers (with no activation function) can be equivalently represented with a single linear layer.\n\n\n\\[\\begin{align*}\n{\\bf y} &= \\left(W^{(3)} W^{(2)} W^{(1)}\\right) {\\bf x} \\\\\n        &= W^\\prime {\\bf x}\n\\end{align*}\\]\n\n\nDeep linear networks are no more expressive than linear regression!"
  },
  {
    "objectID": "lecs/w02/lec02.html#expressive-power-mlp-nonlinear-activation",
    "href": "lecs/w02/lec02.html#expressive-power-mlp-nonlinear-activation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Expressive Power: MLP (nonlinear activation)",
    "text": "Expressive Power: MLP (nonlinear activation)\n\nMultilayer feed-forward neural nets with nonlinear activation functions are universal approximators: they can approximate any function arbitrarily well.\nThis has been shown for various activation functions (thresholds, logistic, ReLU, etc.)\n\nEven though ReLU is “almost” linear, it’s nonlinear enough!"
  },
  {
    "objectID": "lecs/w02/lec02.html#universality-for-binary-inputs-and-targets",
    "href": "lecs/w02/lec02.html#universality-for-binary-inputs-and-targets",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Universality for binary inputs and targets",
    "text": "Universality for binary inputs and targets\n\nHard threshold hidden units, linear output\nStrategy: \\(2^D\\) hidden units, each of which responds to one particular input configuration\n\nOnly requires one hidden layer, though it needs to be extremely wide!\n\n\n\nLimits of universality\n\nYou may need to represent an exponentially large network.\nIf you can learn any function, you might just overfit.\n\n\n\n\nAs an exercise: try to think how such units would look like."
  },
  {
    "objectID": "lecs/w02/lec02.html#training-neural-networks",
    "href": "lecs/w02/lec02.html#training-neural-networks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Training Neural Networks",
    "text": "Training Neural Networks\n\nHow do we find good weights for the neural network?\nWe can continue to use the loss functions:\n\ncross-entropy loss for classification\nsquare loss for regression\n\nThe neural network operations we used (weights, etc) are (almost everywhere) differentiable\n\n\nWe can use gradient descent!"
  },
  {
    "objectID": "lecs/w02/lec02.html#gradient-descent-recap",
    "href": "lecs/w02/lec02.html#gradient-descent-recap",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent Recap",
    "text": "Gradient Descent Recap\nGoal: Compute the minimum of a function \\(\\mathcal{E}({\\bf a})\\)\n\nStart with a set of parameters \\(\\mathbf{a}_0\\) (initialize to some value)\nCompute the gradient \\(\\frac{\\partial \\mathcal{E}}{\\partial \\mathbf{a}}\\).\nUpdate the parameters towards the negative direction of the gradient"
  },
  {
    "objectID": "lecs/w02/lec02.html#gradient-descent-for-neural-networks",
    "href": "lecs/w02/lec02.html#gradient-descent-for-neural-networks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent for Neural Networks",
    "text": "Gradient Descent for Neural Networks\nIdea: Use gradient descent for “learning” neural networks.\n\nWe have a lot of parameters\n\nHigh dimensional (all weights and biases are parameters)\nHard to visualize\nMany iterations (“steps”) needed\n\nIn Deep Learning \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\) is the average of \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) over multiple training examples\n\n\nChallenge: How to compute \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) effectively.\n\n\nSolution: Backpropagation!"
  },
  {
    "objectID": "lecs/w02/lec02.html#univariate-chain-rule",
    "href": "lecs/w02/lec02.html#univariate-chain-rule",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Univariate Chain Rule",
    "text": "Univariate Chain Rule\nRecall: if \\(f(x)\\) and \\(x(t)\\) are univariate functions, then\n\\[\\frac{d}{dt}f(x(t)) = \\frac{df}{dx}\\frac{dx}{dt}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#univariate-chain-rule-for-least-squares-with-a-logistic-model",
    "href": "lecs/w02/lec02.html#univariate-chain-rule-for-least-squares-with-a-logistic-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Univariate Chain Rule for Least Squares with a Logistic Model",
    "text": "Univariate Chain Rule for Least Squares with a Logistic Model\nRecall: Univariate logistic least squares model\n\\[\\begin{align*}\nz &= wx + b \\\\\ny &= \\sigma(z) \\\\\n\\mathcal{L} &= \\frac{1}{2}(y - t)^2\n\\end{align*}\\]\nLet’s compute the loss derivative"
  },
  {
    "objectID": "lecs/w02/lec02.html#univariate-chain-rule-computation-i",
    "href": "lecs/w02/lec02.html#univariate-chain-rule-computation-i",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Univariate Chain Rule Computation I",
    "text": "Univariate Chain Rule Computation I\nHow you would have done it in calculus class\n\\[\\begin{align*}\n\\mathcal{L} &= \\frac{1}{2} ( \\sigma(w x + b) - t)^2 \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w} &= \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{2} ( \\sigma(w x + b) - t)^2 \\right] \\\\\n&= \\frac{1}{2} \\frac{\\partial}{\\partial w} ( \\sigma(w x + b) - t)^2 \\\\\n&= (\\sigma(w x + b) - t) \\frac{\\partial}{\\partial w} (\\sigma(w x + b) - t) \\\\\n&\\ldots\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#univariate-chain-rule-computation-ii",
    "href": "lecs/w02/lec02.html#univariate-chain-rule-computation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Univariate Chain Rule Computation II",
    "text": "Univariate Chain Rule Computation II\nHow you would have done it in calculus class\n\\[\\begin{align*}\n\\ldots &= (\\sigma(w x + b) - t) \\frac{\\partial}{\\partial w} (\\sigma(w x + b) - t) \\\\\n&= (\\sigma(w x + b) - t) \\sigma^\\prime (w x + b) \\frac{\\partial}{\\partial w} (w x + b) \\\\\n&= (\\sigma(w x + b) - t) \\sigma^\\prime (w x + b) x\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#univariate-chain-rule-computation-iii",
    "href": "lecs/w02/lec02.html#univariate-chain-rule-computation-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Univariate Chain Rule Computation III",
    "text": "Univariate Chain Rule Computation III\nSimilarly for \\(\\frac{\\partial \\mathcal{L}}{\\partial b}\\)\n\\[\\begin{align*}\n\\mathcal{L} &= \\frac{1}{2} ( \\sigma(w x + b) - t)^2 \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left[ \\frac{1}{2} ( \\sigma(w x + b) - t)^2 \\right] \\\\\n        &= \\frac{1}{2} \\frac{\\partial}{\\partial b} ( \\sigma(w x + b) - t)^2 \\\\\n        &= (\\sigma(w x + b) - t) \\frac{\\partial}{\\partial b} (\\sigma(w x + b) - t) \\\\\n        &\\ldots\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#univariate-chain-rule-computation-iv",
    "href": "lecs/w02/lec02.html#univariate-chain-rule-computation-iv",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Univariate Chain Rule Computation IV",
    "text": "Univariate Chain Rule Computation IV\nSimilarly for \\(\\frac{\\partial \\mathcal{L}}{\\partial b}\\)\n\\[\\begin{align*}\n\\ldots &= (\\sigma(w x + b) - t) \\frac{\\partial}{\\partial b} (\\sigma(w x + b) - t) \\\\\n&= (\\sigma(w x + b) - t) \\sigma^\\prime (w x + b) \\frac{\\partial}{\\partial b} (w x + b) \\\\\n&= (\\sigma(w x + b) - t) \\sigma^\\prime (w x + b)\n\\end{align*}\\]\n\nQ: What are the disadvantages of this approach?"
  },
  {
    "objectID": "lecs/w02/lec02.html#a-more-structured-way-to-compute-the-derivatives",
    "href": "lecs/w02/lec02.html#a-more-structured-way-to-compute-the-derivatives",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "A More Structured Way to Compute the Derivatives",
    "text": "A More Structured Way to Compute the Derivatives\n\n\\[\\begin{align*}\nz &= wx + b \\\\\ny &= \\sigma(z) \\\\\n\\mathcal{L} &= \\frac{1}{2}(y - t)^2\n\\end{align*}\\] \nLess repeated work; easier to write a program to efficiently compute derivatives"
  },
  {
    "objectID": "lecs/w02/lec02.html#a-more-structured-way-to-compute-the-derivatives-ii",
    "href": "lecs/w02/lec02.html#a-more-structured-way-to-compute-the-derivatives-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "A More Structured Way to Compute the Derivatives II",
    "text": "A More Structured Way to Compute the Derivatives II\n\n\\[\\begin{align*}\n  \\frac{d \\mathcal{L}}{d y} &= y - t \\\\\n  \\frac{d \\mathcal{L}}{d z} &= \\frac{d \\mathcal{L}}{d y}\\sigma'(z) \\\\\n  \\frac{\\partial \\mathcal{L}}{\\partial w} &= \\frac{d \\mathcal{L}}{d z} \\, x \\\\\n  \\frac{\\partial \\mathcal{L}}{\\partial b} &= \\frac{d \\mathcal{L}}{d z}\n\\end{align*}\\] \nLess repeated work; easier to write a program to efficiently compute derivatives"
  },
  {
    "objectID": "lecs/w02/lec02.html#computation-graph",
    "href": "lecs/w02/lec02.html#computation-graph",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computation Graph",
    "text": "Computation Graph\nWe can diagram out the computations using a computation graph.\n\n\nThe nodes represent all the inputs and computed quantities.\nThe edges represent which nodes are computed directly as a function of which other nodes."
  },
  {
    "objectID": "lecs/w02/lec02.html#chain-rule-error-signal-notation",
    "href": "lecs/w02/lec02.html#chain-rule-error-signal-notation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Chain Rule (Error Signal) Notation",
    "text": "Chain Rule (Error Signal) Notation\n\nUse \\(\\overline{y}\\) to denote the derivative \\(\\frac{d \\mathcal{L}}{d y}\\)\n\nsometimes called the error signal\n\nThis notation emphasizes that the error signals are just values our program is computing (rather than a mathematical operation).\nThis is notation introduced by Prof. Roger Grosse, and not standard notation"
  },
  {
    "objectID": "lecs/w02/lec02.html#chain-rule-error-signal-notation-ii",
    "href": "lecs/w02/lec02.html#chain-rule-error-signal-notation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Chain Rule (Error Signal) Notation II",
    "text": "Chain Rule (Error Signal) Notation II\n\nComputing the loss:\n\\[\\begin{align*}\nz &= wx + b \\\\\ny &= \\sigma(z) \\\\\n\\mathcal{L} &= \\frac{1}{2}(y - t)^2\n\\end{align*}\\]\n\n\nComputing the derivatives:\n\\[\\begin{align*}\n\\overline{y} &= y - t \\\\\n\\overline{z} &= \\overline{y} \\sigma'(z) \\\\\n\\overline{w} &= \\overline{z} \\, x \\\\\n\\overline{b} &= \\overline{z}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#multiclass-logistic-regression-computation-graph",
    "href": "lecs/w02/lec02.html#multiclass-logistic-regression-computation-graph",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multiclass Logistic Regression Computation Graph",
    "text": "Multiclass Logistic Regression Computation Graph\nIn general, the computation graph fans out:"
  },
  {
    "objectID": "lecs/w02/lec02.html#multiclass-logistic-regression-computation-graph-ii",
    "href": "lecs/w02/lec02.html#multiclass-logistic-regression-computation-graph-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multiclass Logistic Regression Computation Graph II",
    "text": "Multiclass Logistic Regression Computation Graph II\n\n\n\n\\[\\begin{align*}\nz_l &= \\sum_j w_{lj} x_j + b_l \\\\\ny_k &= \\frac{e^{z_k}}{\\sum_l e^{z_l}} \\\\\n\\mathcal{L} &= -\\sum_k t_k \\log{y_k}\n\\end{align*}\\] \nThere are multiple paths for which a weight like \\(w_{11}\\) affects the loss \\(\\mathcal{L}\\)."
  },
  {
    "objectID": "lecs/w02/lec02.html#multivariate-chain-rule",
    "href": "lecs/w02/lec02.html#multivariate-chain-rule",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multivariate Chain Rule",
    "text": "Multivariate Chain Rule\nSuppose we have a function \\(f(x, y)\\) and functions \\(x(t)\\) and \\(y(t)\\) (all the variables here are scalar-valued). Then\n\\[\\frac{d}{dt}f(x(t), y(t)) = \\frac{\\partial f}{\\partial x}\\frac{dx}{dt} +  \\frac{\\partial f}{\\partial y}\\frac{dy}{dt}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#multivariate-chain-rule-example",
    "href": "lecs/w02/lec02.html#multivariate-chain-rule-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multivariate Chain Rule Example",
    "text": "Multivariate Chain Rule Example\nIf \\(f(x, y) = y + e^{xy}\\), \\(x(t) = \\cos t\\) and \\(y(t) = t^2\\)…\n\\[\\begin{align*}\n\\frac{d}{dt}f(x(t), y(t)) &= \\frac{\\partial f}{\\partial x}\\frac{dx}{dt} +  \\frac{\\partial f}{\\partial y}\\frac{dy}{dt} \\\\\n&= \\left( y e^{xy} \\right) \\cdot \\left( -\\sin (t) \\right) + \\left( 1 + xe^{xy} \\right) \\cdot 2t\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#multivariate-chain-rule-notation",
    "href": "lecs/w02/lec02.html#multivariate-chain-rule-notation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multivariate Chain Rule Notation",
    "text": "Multivariate Chain Rule Notation\n\n \n\nIn our notation\n\\[\\overline{t} = \\overline{x} \\frac{dx}{dt} +  \\overline{y} \\frac{dy}{dt}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-backpropagation-algorithm",
    "href": "lecs/w02/lec02.html#the-backpropagation-algorithm",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Backpropagation Algorithm",
    "text": "The Backpropagation Algorithm\n\nBackpropagation is an algorithm to compute gradients efficiency\n\nForward Pass: Compute predictions (and save intermediate values)\nBackwards Pass: Compute gradients\n\nThe idea behind backpropagation is very similar to dynamic programming\n\nUse chain rule, and be careful about the order in which we compute the derivatives"
  },
  {
    "objectID": "lecs/w02/lec02.html#backpropagation-example",
    "href": "lecs/w02/lec02.html#backpropagation-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation Example",
    "text": "Backpropagation Example"
  },
  {
    "objectID": "lecs/w02/lec02.html#backpropagation-for-a-mlp",
    "href": "lecs/w02/lec02.html#backpropagation-for-a-mlp",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation for a MLP",
    "text": "Backpropagation for a MLP\n\n\nForward pass: \\[\\begin{align*}\nz_i &= \\sum_j w_{ij}^{(1)} x_j + b_i^{(1)} \\\\\nh_i &= \\sigma(z_i) \\\\\ny_k &= \\sum_i w_{ki}^{(2)} h_i + b_k^{(2)} \\\\\n\\mathcal{L} &= \\frac{1}{2}\\sum_k (y_k - t_k)^2\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#backpropagation-for-a-mlp-ii",
    "href": "lecs/w02/lec02.html#backpropagation-for-a-mlp-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation for a MLP II",
    "text": "Backpropagation for a MLP II\nBackward pass:\n\n\\[\\begin{align*}\n\\overline{\\mathcal{L}} &= 1 \\\\\n\\overline{y_k}  &= \\overline{\\mathcal{L}}(y_k - t_k) \\\\\n\\overline{w_{ki}^{(2)}}  &= \\overline{y_k}h_i \\\\\n\\overline{b_{k}^{(2)}}  &= \\overline{y_k}\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n\\overline{h_i}  &= \\sum_k \\overline{y_k} w_{ki}^{(2)} \\\\\n\\overline{z_i} &= \\overline{h_i} \\sigma'(z_i) \\\\\n\\overline{w_{ij}^{(1)}} &= \\overline{z_i} x_j \\\\\n\\overline{b_{i}^{(1)}} &= \\overline{z_i}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#backpropagation-for-a-mlp-1",
    "href": "lecs/w02/lec02.html#backpropagation-for-a-mlp-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation for a MLP",
    "text": "Backpropagation for a MLP\n\n\n\nForward pass: \\[\\begin{align*}\n{\\bf z} &=  W^{(1)}{\\bf x} + {\\bf b}^{(1)} \\\\\n{\\bf h} &= \\sigma({\\bf z}) \\\\\n{\\bf y} &=  W^{(2)}{\\bf h} + {\\bf b}^{(2)} \\\\\n\\mathcal{L} &= \\frac{1}{2} || {\\bf y} - {\\bf t}||^2\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#backpropagation-for-a-mlp-2",
    "href": "lecs/w02/lec02.html#backpropagation-for-a-mlp-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation for a MLP",
    "text": "Backpropagation for a MLP\n\nBackward pass: \\[\\begin{align*}\n\\overline{\\mathcal{L}} &= 1 \\\\\n\\overline{{\\bf y}}  &= \\overline{\\mathcal{L}}({\\bf y} - {\\bf t}) \\\\\n\\overline{W^{(2)}}  &= \\overline{{\\bf y}}{\\bf h}^\\top \\\\\n\\overline{{\\bf b^{(2)}}}  &= \\overline{{\\bf y}} \\\\\n& \\ldots\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#backpropagation-for-a-mlp-3",
    "href": "lecs/w02/lec02.html#backpropagation-for-a-mlp-3",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation for a MLP",
    "text": "Backpropagation for a MLP\n\nBackward pass: \\[\\begin{align*}\n& \\ldots \\\\\n\\overline{{\\bf h}}  &= {W^{(2)}}^\\top\\overline{y} \\\\\n\\overline{{\\bf z}} &= \\overline{{\\bf h}} \\circ \\sigma'({\\bf z}) \\\\\n\\overline{W^{(1)}} &= \\overline{{\\bf z}} {\\bf x}^\\top \\\\\n\\overline{{\\bf b}^{(1)}} &= \\overline{{\\bf z}}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#implementing-backpropagation-i",
    "href": "lecs/w02/lec02.html#implementing-backpropagation-i",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Implementing Backpropagation I",
    "text": "Implementing Backpropagation I"
  },
  {
    "objectID": "lecs/w02/lec02.html#implementing-backpropagation-ii",
    "href": "lecs/w02/lec02.html#implementing-backpropagation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Implementing Backpropagation II",
    "text": "Implementing Backpropagation II\nForward pass: Each node…\n\nreceives messages (inputs) from its parents\nuses these messages to compute its own values\n\n\nBackward pass: Each node…\n\nreceives messages (error signals) from its children\nuses these messages to compute its own error signal\npasses message to its parents\n\n\n\nThis algorithm provides modularity!"
  },
  {
    "objectID": "lecs/w02/lec02.html#backpropagation-in-vectorized-form",
    "href": "lecs/w02/lec02.html#backpropagation-in-vectorized-form",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation in Vectorized Form",
    "text": "Backpropagation in Vectorized Form"
  },
  {
    "objectID": "lecs/w02/lec02.html#backpropagation-in-practice",
    "href": "lecs/w02/lec02.html#backpropagation-in-practice",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation in practice",
    "text": "Backpropagation in practice\n\n\nBackprop is used to train the overwhelming majority of neural nets today.\n\nEven optimization algorithms much fancier than gradient descent (e.g.~second-order methods) use backprop to compute the gradients."
  },
  {
    "objectID": "lecs/w02/lec02.html#backpropagation-in-practice-ii",
    "href": "lecs/w02/lec02.html#backpropagation-in-practice-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Backpropagation in practice II",
    "text": "Backpropagation in practice II\n\nDespite its practical success, backprop is believed to be neurally (biologically) implausible.\n\nNo evidence for biological signals analogous to error derivatives.\nAll the biologically plausible alternatives we know about learn much more slowly (on computers)."
  },
  {
    "objectID": "lecs/w02/lec02.html#summary",
    "href": "lecs/w02/lec02.html#summary",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Summary",
    "text": "Summary\n\nArtificial neurons draw inspiration from nerve cells / neurons in the brain\nOn its own, the expresiveness of a single neuron is limited\nStacking neurons and nonlinear activation functions allows for learning more complex functions\nBackpropagation can be used for this learning task"
  },
  {
    "objectID": "lecs/w02/lec02.html#what-to-do-this-week",
    "href": "lecs/w02/lec02.html#what-to-do-this-week",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What to do this week?",
    "text": "What to do this week?\n\nYou can already complete Assignment 1\n\nStart early so you can get help early!\n\nAttend tutorials this week!\nComplete the readings for this week.\nPreview next week’s materials"
  },
  {
    "objectID": "lecs/w10/lec10.html#announcements",
    "href": "lecs/w10/lec10.html#announcements",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Announcements",
    "text": "Announcements\n\nTest 2 is this week during tutorials\n\nWrite during the tutorial section that you are enrolled in unless you have made (or are making) alternative arrangements"
  },
  {
    "objectID": "lecs/w10/lec10.html#generating-images",
    "href": "lecs/w10/lec10.html#generating-images",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Generating Images",
    "text": "Generating Images\nHow to generate new data of certain types\n\ngenerate text that looks like our training data\n\n\n\ngenerate images that look like our training data"
  },
  {
    "objectID": "lecs/w10/lec10.html#generating-images-ii",
    "href": "lecs/w10/lec10.html#generating-images-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Generating Images II",
    "text": "Generating Images II\nModels:\n\nGenerative RNNs\n\n\n\nAutoencoder\n\n\n\n\nVariational Autoencoder (VAE)\n\n\n\n\nGenerative Adversarial Networks (GANs)\n\narxiv.org/abs/1406.2661\nuoft-csc413.github.io/2023/assets/slides/lec09.pdf"
  },
  {
    "objectID": "lecs/w10/lec10.html#generating-images-iii",
    "href": "lecs/w10/lec10.html#generating-images-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Generating Images III",
    "text": "Generating Images III\n\nDiffusion Models\n\nlilianweng.github.io/posts/2021-07-11-diffusion-models/\n\n\n\nWe’ll talk about autoencoders and VAEs today; GANs if we have time."
  },
  {
    "objectID": "lecs/w10/lec10.html#autoencoders",
    "href": "lecs/w10/lec10.html#autoencoders",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Autoencoders",
    "text": "Autoencoders\nThere are two ways of thinking of an image autoencoder:\n\na model that will eventually help us generate new images\n\n\n\na model that finds a low-dimensional representation of images\n\nBoth are considered unsupervised learning tasks, since no labels are involved.\nHowever, we do have a dataset of unlabelled images."
  },
  {
    "objectID": "lecs/w10/lec10.html#image-autoencoder",
    "href": "lecs/w10/lec10.html#image-autoencoder",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Image Autoencoder",
    "text": "Image Autoencoder\nIdea: In order to learn to generate images, we’ll learn to reconstruct images from a low-dimensional representation.\nAn image autoencoder has two components:\n\n\nAn encoder neural network that takes the image as input, and produces a low-dimensional embedding.\n\n\n\n\nA decoder neural network that takes the low-dimensional embedding as input, and reconstructs the image.\n\nA good, low-dimensional representation should allow us to reconstruct everything about the image."
  },
  {
    "objectID": "lecs/w10/lec10.html#the-components-of-an-autoencoder",
    "href": "lecs/w10/lec10.html#the-components-of-an-autoencoder",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Components of an Autoencoder",
    "text": "The Components of an Autoencoder\nEncoder:\n\nInput = image\nOutput = low-dimensional embedding\n\nDecoder:\n\nInput = low-dimensional embedding\nOutput = image"
  },
  {
    "objectID": "lecs/w10/lec10.html#why-autoencoders",
    "href": "lecs/w10/lec10.html#why-autoencoders",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Autoencoders?",
    "text": "Why Autoencoders?\n\nDimension reduction:\n\nfind a low dimensional representation of the image\n\n\n\n\nImage Generation:\n\ngenerate new images not in the training set\n(Any guesses on how we can do this?)"
  },
  {
    "objectID": "lecs/w10/lec10.html#image-encoder-architecture",
    "href": "lecs/w10/lec10.html#image-encoder-architecture",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Image Encoder Architecture",
    "text": "Image Encoder Architecture\nWhat would the architecture of the encoder look like?\n\n\nWe could use a MLP, but there are some issues (recall: what are these issues?)\n\n\n\n\nBut we can also use a convolutional neural network!\n\n\n\nWe can use downsampling to reduce the dimensionality of the data"
  },
  {
    "objectID": "lecs/w10/lec10.html#image-decoder-architecture",
    "href": "lecs/w10/lec10.html#image-decoder-architecture",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Image Decoder Architecture",
    "text": "Image Decoder Architecture\nWhat would the architecture of the decoder look like?\nWe need to be able to increase the image resolution.\nWe haven’t learned how to do this yet!"
  },
  {
    "objectID": "lecs/w10/lec10.html#transpose-convolution-1",
    "href": "lecs/w10/lec10.html#transpose-convolution-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Transpose Convolution",
    "text": "Transpose Convolution\nUsed to increase the resolution of a feature map.\nThis is useful for:\n\n\nimage generation problems\n\n\n\n\npixel-wise prediction problems"
  },
  {
    "objectID": "lecs/w10/lec10.html#pixel-wise-prediction",
    "href": "lecs/w10/lec10.html#pixel-wise-prediction",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Pixel-wise prediction",
    "text": "Pixel-wise prediction\nA prediction problem where we label the content of each pixel is known as a pixel-wise prediction problem\n\n\n\n\nhttp://deeplearning.net/tutorial/fcn_2D_segm.html\n\n\n\nQ: How do we generate pixel-wise predictions?"
  },
  {
    "objectID": "lecs/w10/lec10.html#what-we-need",
    "href": "lecs/w10/lec10.html#what-we-need",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What we need:",
    "text": "What we need:\nWe need to be able to up-sample features, i.e. to obtain high-resolution features from low-resolution features\n\n\nOpposite of max-pooling OR\n\n\n\n\nOpposite of a strided convolution\n\n\n\nWe need an inverse convolution – a.k.a a deconvolution or transpose convolution."
  },
  {
    "objectID": "lecs/w10/lec10.html#architectures-with-transpose-convolution",
    "href": "lecs/w10/lec10.html#architectures-with-transpose-convolution",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Architectures with Transpose Convolution",
    "text": "Architectures with Transpose Convolution"
  },
  {
    "objectID": "lecs/w10/lec10.html#architectures-with-transpose-convolution-2",
    "href": "lecs/w10/lec10.html#architectures-with-transpose-convolution-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Architectures with Transpose Convolution 2",
    "text": "Architectures with Transpose Convolution 2"
  },
  {
    "objectID": "lecs/w10/lec10.html#inverse-convolution",
    "href": "lecs/w10/lec10.html#inverse-convolution",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Inverse Convolution",
    "text": "Inverse Convolution\nimport torch\nx = torch.randn(2, 8, 64, 64)\nconv = torch.nn.Conv2d(in_channels=8,\n                       out_channels=8,\n                       kernel_size=5)\ny = conv(x)\ny.shape\n\n\n\ntorch.Size([2, 8, 60, 60])\n\n\n\n\nconvt = torch.nn.ConvTranspose2d(in_channels=8,\n                                 out_channels=8,\n                                 kernel_size=5)\nx = convt(y)\nx.shape\n\n\n\n\ntorch.Size([2, 8, 64, 64])\n\n\n\n\nshould get the same shape back!"
  },
  {
    "objectID": "lecs/w10/lec10.html#inverse-convolution-padding",
    "href": "lecs/w10/lec10.html#inverse-convolution-padding",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Inverse Convolution + Padding",
    "text": "Inverse Convolution + Padding\nx = torch.randn(2, 8, 64, 64)\nconv = torch.nn.Conv2d(in_channels=8,\n                       out_channels=8,\n                       kernel_size=5,\n                       padding=2)\ny = conv(x)\ny.shape\n\n\n\ntorch.Size([2, 8, 64, 64])\n\n\n\n\n&gt;&gt;&gt; convt = torch.nn.ConvTranspose2d(in_channels=8,\n                                     out_channels=8,\n                                     kernel_size=5,\n                                     padding=2)\n&gt;&gt;&gt; x = convt(y)\n&gt;&gt;&gt; x.shape\n\n\n\n\ntorch.Size([2, 8, 64, 64])\n\n\n\n\nshould get the same shape back!"
  },
  {
    "objectID": "lecs/w10/lec10.html#inverse-convolution-stride",
    "href": "lecs/w10/lec10.html#inverse-convolution-stride",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Inverse Convolution + Stride",
    "text": "Inverse Convolution + Stride\nx = torch.randn(2, 8, 64, 64)\nconv = torch.nn.Conv2d(in_channels=8,\n                       out_channels=8,\n                       kernel_size=5,\n                       stride=2)\ny = conv(x)\ny.shape\n\n\n\ntorch.Size([2, 8, 30, 30])\n\n\n\n\nconvt = torch.nn.ConvTranspose2d(in_channels=8,\n                                 out_channels=8,\n                                 kernel_size=5,\n                                 stride=2)\nx = convt(y)\nx.shape\n\n\n\n\ntorch.Size([2, 8, 63, 63])\n\n\n\n\n… almost the same shape …"
  },
  {
    "objectID": "lecs/w10/lec10.html#transpose-convolution-layer",
    "href": "lecs/w10/lec10.html#transpose-convolution-layer",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Transpose Convolution Layer",
    "text": "Transpose Convolution Layer\n\nhttps://www.mdpi.com/2072-4292/9/6/522/htmMore at https://github.com/vdumoulin/conv_arithmetic"
  },
  {
    "objectID": "lecs/w10/lec10.html#output-padding",
    "href": "lecs/w10/lec10.html#output-padding",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Output Padding",
    "text": "Output Padding\ntorch.nn.ConvTranspose2d(in_channels=8,\n                         out_channels=8,\n                         kernel_size=5,\n                         stride=2,\n                         output_padding=1) # +1 to output width/height"
  },
  {
    "objectID": "lecs/w10/lec10.html#lets-get-back-to-the-autoencoder",
    "href": "lecs/w10/lec10.html#lets-get-back-to-the-autoencoder",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Let’s get back to the Autoencoder",
    "text": "Let’s get back to the Autoencoder\nRecall that we want a model that generates images that looks like our training data\nIdea:\n\n\nIn order to learn to generate images, we’ll learn to reconstruct images from a low-dimensional representation.\n\n\n\n\nA good, low-dimensional representation should allow us to reconstruct everything about the image."
  },
  {
    "objectID": "lecs/w10/lec10.html#the-components-of-an-autoencoder-1",
    "href": "lecs/w10/lec10.html#the-components-of-an-autoencoder-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Components of an Autoencoder",
    "text": "The Components of an Autoencoder\n\n\n\nEncoder:\n\n\nInput = image\n\n\n\n\nOutput = low-dimensional embedding"
  },
  {
    "objectID": "lecs/w10/lec10.html#the-components-of-an-autoencoder-ii",
    "href": "lecs/w10/lec10.html#the-components-of-an-autoencoder-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Components of an Autoencoder II",
    "text": "The Components of an Autoencoder II\n\n\n\nDecoder:\n\n\nInput = low-dimensional embedding\n\n\n\n\nOutput = image"
  },
  {
    "objectID": "lecs/w10/lec10.html#why-autoencoders-1",
    "href": "lecs/w10/lec10.html#why-autoencoders-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Autoencoders?",
    "text": "Why Autoencoders?\n\nDimension reduction:\n\nfind a low dimensional representation of the image\n\n\n\n\nImage Generation:\n\ngenerate new images not in the training set\n\n\n\n\nAutoencoders are not used for supervised learning. The task is not to predict something about the image!\nAutoencoders are considered a generative model."
  },
  {
    "objectID": "lecs/w10/lec10.html#how-to-train-autoencoders",
    "href": "lecs/w10/lec10.html#how-to-train-autoencoders",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "How to Train Autoencoders?",
    "text": "How to Train Autoencoders?\n\nLoss function: How close were the reconstructed image from the original? Here are some ideas…\n\nMean Square Error (MSE): look at the mean square error across all pixels.\nMean Square-Gradient Error (MSGE): take the average of the differences of squared gradients (computed with something like the Sobel filter) across all pixels.\n\\(\\ldots\\)"
  },
  {
    "objectID": "lecs/w10/lec10.html#how-to-train-autoencoders-ii",
    "href": "lecs/w10/lec10.html#how-to-train-autoencoders-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "How to Train Autoencoders? II",
    "text": "How to Train Autoencoders? II\n\nLoss function: How close were the reconstructed image from the original? Here are some ideas…\n\n\\(\\ldots\\)\nCorner Detection: use computer vision to identify corners. Then across the image (or patches in a partition of the image), compare corner counts, corresponding positions, and/or nearest distances."
  },
  {
    "objectID": "lecs/w10/lec10.html#how-to-train-autoencoders-iii",
    "href": "lecs/w10/lec10.html#how-to-train-autoencoders-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "How to Train Autoencoders? III",
    "text": "How to Train Autoencoders? III\n\nOptimizer:\n\nJust like before!\nCommonly used for other network architectures too\n\n\n\n\nTraining loop:\n\nJust like before!"
  },
  {
    "objectID": "lecs/w10/lec10.html#structure-in-the-embedding-space",
    "href": "lecs/w10/lec10.html#structure-in-the-embedding-space",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Structure in the Embedding Space",
    "text": "Structure in the Embedding Space\nThe dimensionality reduction means that there will be structure in the embedding space.\nIf the dimensionality of the embedding space is not too large, similar images should map to similar locations."
  },
  {
    "objectID": "lecs/w10/lec10.html#interpolating-in-the-embedding-space",
    "href": "lecs/w10/lec10.html#interpolating-in-the-embedding-space",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Interpolating in the Embedding Space",
    "text": "Interpolating in the Embedding Space"
  },
  {
    "objectID": "lecs/w10/lec10.html#generating-new-images",
    "href": "lecs/w10/lec10.html#generating-new-images",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Generating New Images",
    "text": "Generating New Images\nQ: Can we pick a random point in the embedding space, and decode it to get an image of a digit?\n\nA: Unfortunately not necessarily. Can we figure out why not?"
  },
  {
    "objectID": "lecs/w10/lec10.html#autoencoder-overfitting",
    "href": "lecs/w10/lec10.html#autoencoder-overfitting",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Autoencoder Overfitting",
    "text": "Autoencoder Overfitting\nOverfitting can occur if the size of the embedding space is too large.\nIf the dimensionality of the embedding space is small, then the neural network needs to map similar images to similar locations.\nIf the dimensionality of the embedding space is too large, then the neural network can simply memorize the images!"
  },
  {
    "objectID": "lecs/w10/lec10.html#blurry-reconstructions",
    "href": "lecs/w10/lec10.html#blurry-reconstructions",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Blurry reconstructions",
    "text": "Blurry reconstructions\nQ: Why do autoencoders produce blurry images?\nHint: it has to do with the use of the MSELoss.\nRead more: https://ieeexplore.ieee.org/document/8461664"
  },
  {
    "objectID": "lecs/w07/lec07.html#example-cnn-alexnet",
    "href": "lecs/w07/lec07.html#example-cnn-alexnet",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example CNN: AlexNet",
    "text": "Example CNN: AlexNet\n\nimport torchvision.models\nalexNet = torchvision.models.alexnet(pretrained=False)"
  },
  {
    "objectID": "lecs/w07/lec07.html#convolutional-features",
    "href": "lecs/w07/lec07.html#convolutional-features",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Convolutional Features",
    "text": "Convolutional Features"
  },
  {
    "objectID": "lecs/w07/lec07.html#receptive-fields",
    "href": "lecs/w07/lec07.html#receptive-fields",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Receptive Fields",
    "text": "Receptive Fields\nBecause of downsampling (pooling and use of strides), higher-layer filters “cover” a larger region of the input than equal-sized filters in the lower layers."
  },
  {
    "objectID": "lecs/w07/lec07.html#transfer-learning",
    "href": "lecs/w07/lec07.html#transfer-learning",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nTransfer Learning is the idea of using weights/features trained on one task, and using it on another task.\nWe already saw the idea of transfer learning in project 2:\n\nTrain a model to predict the next work given the previous three\nUse the weights to determine word similarities"
  },
  {
    "objectID": "lecs/w07/lec07.html#transfer-learning-with-cnn",
    "href": "lecs/w07/lec07.html#transfer-learning-with-cnn",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Transfer Learning with CNN",
    "text": "Transfer Learning with CNN\nPractioners rarely train a CNN “from scratch”. Instead we could:\n\nTake a pre-trained CNN model (e.g. AlexNet), and use its features network to compute image features, which we then use to classify our own images\nInitialize our weights using the weights of a pre-trained CNN model (e.g. AlexNet)"
  },
  {
    "objectID": "lecs/w07/lec07.html#transfer-learning-with-cnn-1",
    "href": "lecs/w07/lec07.html#transfer-learning-with-cnn-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Transfer Learning with CNN",
    "text": "Transfer Learning with CNN\n\n\n\nWhat we want you to know:\n\nHow do we interpret CNN figures?\nHow were these architectures different from the previous?\nWhat new idea was introduced?"
  },
  {
    "objectID": "lecs/w07/lec07.html#alexnet-2012",
    "href": "lecs/w07/lec07.html#alexnet-2012",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "AlexNet (2012)",
    "text": "AlexNet (2012)\n\nimport torchvision.models\nalexNet = torchvision.models.alexnet(pretrained=False)"
  },
  {
    "objectID": "lecs/w07/lec07.html#vgg-2014",
    "href": "lecs/w07/lec07.html#vgg-2014",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VGG (2014)",
    "text": "VGG (2014)\n\n# There are many VGG versions\nvgg16 = torchvision.models.vgg.vgg16(pretrained=False)\nvgg19 = torchvision.models.vgg.vgg19(pretrained=False)"
  },
  {
    "objectID": "lecs/w07/lec07.html#what-is-new-in-vgg-compared-to-alexnet",
    "href": "lecs/w07/lec07.html#what-is-new-in-vgg-compared-to-alexnet",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What is new in VGG (compared to AlexNet)?",
    "text": "What is new in VGG (compared to AlexNet)?\n\n \n\n\n\nVGG uses very small receptive fields (\\(3 \\times 3\\) instead of \\(11 \\times 11\\))\nVGG incorporates \\(1 \\times 1\\) convolutional layers (why?)"
  },
  {
    "objectID": "lecs/w07/lec07.html#what-is-new-in-vgg-compared-to-alexnet-1",
    "href": "lecs/w07/lec07.html#what-is-new-in-vgg-compared-to-alexnet-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What is new in VGG (compared to AlexNet)?",
    "text": "What is new in VGG (compared to AlexNet)?\n\n \n\n\nFC layers can be expressed as CONV layers and vice versa\nE.g. FC layer with 4096 output units looking at an input volume of 7 x 7 x 512 is equivalent to a CONV layer with kernel size 7, stride 1, and 4096 filters.\nSee https://cs231n.github.io/convolutional-networks/."
  },
  {
    "objectID": "lecs/w07/lec07.html#one-more-idea",
    "href": "lecs/w07/lec07.html#one-more-idea",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "One More Idea…",
    "text": "One More Idea…\nMost of these networks have fully connected layers at the very end.\n\nPro: Fully connected layers computes features on the entire image\nCon: what if we wanted to work with images of various sizes?"
  },
  {
    "objectID": "lecs/w07/lec07.html#one-more-idea-1",
    "href": "lecs/w07/lec07.html#one-more-idea-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "One More Idea…",
    "text": "One More Idea…\nIdea: instead of fully connected layers, we could…\n\nUse a convolution layer with the same kernel size as hidden unit size and no padding\nUse global average-pooling\n\nThis is more frequently done on pixel-wise prediction problems, which we’ll see later in this course."
  },
  {
    "objectID": "lecs/w07/lec07.html#how-do-cnns-work",
    "href": "lecs/w07/lec07.html#how-do-cnns-work",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "How do CNNs Work?",
    "text": "How do CNNs Work?\nConvolutional neural networks are successful, but how do we know that the network has learned useful patterns from the training set?\nInterpretation of deep learning models is a challenge, but there are some tricks we can use to interpret CNN models"
  },
  {
    "objectID": "lecs/w07/lec07.html#weight-visualization",
    "href": "lecs/w07/lec07.html#weight-visualization",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Weight Visualization",
    "text": "Weight Visualization\nRecall: we can understand what first-layer features in a MLP are doing by visualizing the weight matrices (left)\n\n \n\nWe can do the same thing with convolutional networks (right)\nBut what about higher-level features?"
  },
  {
    "objectID": "lecs/w07/lec07.html#feature-visualization",
    "href": "lecs/w07/lec07.html#feature-visualization",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature Visualization",
    "text": "Feature Visualization\nOne approach: pick the images in the training set which activate a unit most strongly.\n\n\n(Compute forward pass for each image in the training set, track when a feature was most active, and look for the portion of the image that lead to that activation)\nHere is the visualization for layer 1:"
  },
  {
    "objectID": "lecs/w07/lec07.html#feature-visualization-layer-2",
    "href": "lecs/w07/lec07.html#feature-visualization-layer-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature Visualization: Layer 2",
    "text": "Feature Visualization: Layer 2"
  },
  {
    "objectID": "lecs/w07/lec07.html#feature-visualization-layer-3",
    "href": "lecs/w07/lec07.html#feature-visualization-layer-3",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature Visualization: Layer 3",
    "text": "Feature Visualization: Layer 3"
  },
  {
    "objectID": "lecs/w07/lec07.html#feature-visualization-layer-4",
    "href": "lecs/w07/lec07.html#feature-visualization-layer-4",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Feature Visualization: Layer 4",
    "text": "Feature Visualization: Layer 4"
  },
  {
    "objectID": "lecs/w07/lec07.html#the-issue-with-feature-visualizations",
    "href": "lecs/w07/lec07.html#the-issue-with-feature-visualizations",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The issue with feature visualizations",
    "text": "The issue with feature visualizations\nHigher layer seems to pick up more abstract, high-level information.\nProblem: Can’t tell what unit is actually responding in the image!\nMaybe we can use input gradients?"
  },
  {
    "objectID": "lecs/w07/lec07.html#input-gradients",
    "href": "lecs/w07/lec07.html#input-gradients",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Input Gradients",
    "text": "Input Gradients\nRecall this computation graph:\n\n\n\nFrom this graph, we could compute \\(\\frac{\\partial L}{\\partial x}\\) – the model’s sensitivity with respect to the input.\n(We’ve never done this because there hasn’t been a need to—until now!)"
  },
  {
    "objectID": "lecs/w07/lec07.html#the-problem-with-input-gradients",
    "href": "lecs/w07/lec07.html#the-problem-with-input-gradients",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Problem with Input Gradients",
    "text": "The Problem with Input Gradients\nInput gradients can be noisy and hard to interpret\nTake a good object recognition conv net and compute the gradient of \\(\\log\\left(p(y = \\text{\"deer\"}|{\\bf x})\\right)\\)"
  },
  {
    "objectID": "lecs/w07/lec07.html#smoothing-the-input-gradients",
    "href": "lecs/w07/lec07.html#smoothing-the-input-gradients",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Smoothing the Input Gradients",
    "text": "Smoothing the Input Gradients\nSeveral methods modify these gradients:\n\nGuided Backprop: accumulate only positive gradients when doing back propagation\nSmoothGrad: do the backward pass on a few noisy version of the input image, then average their input gradients"
  },
  {
    "objectID": "lecs/w07/lec07.html#cautionary-tale-of-image-gradients",
    "href": "lecs/w07/lec07.html#cautionary-tale-of-image-gradients",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cautionary Tale of Image Gradients",
    "text": "Cautionary Tale of Image Gradients\n\nFrom: https://proceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf"
  },
  {
    "objectID": "lecs/w07/lec07.html#optimizing-an-image-to-maximize-activations",
    "href": "lecs/w07/lec07.html#optimizing-an-image-to-maximize-activations",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Optimizing an Image to Maximize Activations",
    "text": "Optimizing an Image to Maximize Activations\nCan we use gradient ascent on an image to maximize the activation of a given neuron?\nRequires a few tricks to make this work; see https://distill.pub/2017/feature-visualization/"
  },
  {
    "objectID": "lecs/w07/lec07.html#section",
    "href": "lecs/w07/lec07.html#section",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": ".",
    "text": "."
  },
  {
    "objectID": "lecs/w07/lec07.html#deep-dream",
    "href": "lecs/w07/lec07.html#deep-dream",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Deep Dream",
    "text": "Deep Dream\nSimilar idea:\n\nStart with an image, and run a conv net on it.\nPick a layer in the network.\nChange the image such that units which were already highly activated get activated even more strongly. “Rich get richer.”\nRepeat.\n\nThis will accentuate whatever features of an image already kind of resemble the object (link)."
  },
  {
    "objectID": "lecs/w07/lec07.html#what-are-these-images-of",
    "href": "lecs/w07/lec07.html#what-are-these-images-of",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What are these Images of?",
    "text": "What are these Images of?\n\n\n\n\nProducing adversarial images: Given an image for one category (e.g. panda), compute the image gradient to maximize the network’s output unit for a different category (e.g. gibbon)"
  },
  {
    "objectID": "lecs/w07/lec07.html#non-targetted-adversarial-attack",
    "href": "lecs/w07/lec07.html#non-targetted-adversarial-attack",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Non-targetted Adversarial Attack",
    "text": "Non-targetted Adversarial Attack\nGoal: Choose a small perturbation \\(\\epsilon\\) on an image \\(x\\) so that a neural network \\(\\, f\\) misclassifies \\(\\, x + \\epsilon\\).\nApproach:\nUse the same optimization process to choose \\(\\epsilon\\) to minimize the probability that\n\\[f(x + \\epsilon) = \\text{correct class}\\]"
  },
  {
    "objectID": "lecs/w07/lec07.html#targeted-adversarial-attack",
    "href": "lecs/w07/lec07.html#targeted-adversarial-attack",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Targeted Adversarial Attack",
    "text": "Targeted Adversarial Attack\nTargeted attack\nMaximize the probability that \\(f(x + \\epsilon) =\\) target incorrect class\nNon-targeted attack\nMinimize the probability that \\(f(x + \\epsilon) =\\) correct class"
  },
  {
    "objectID": "lecs/w07/lec07.html#section-1",
    "href": "lecs/w07/lec07.html#section-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": ".",
    "text": ".\nDemo time!"
  },
  {
    "objectID": "lecs/w07/lec07.html#adversarial-attack",
    "href": "lecs/w07/lec07.html#adversarial-attack",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Adversarial Attack",
    "text": "Adversarial Attack\n\n2013: ha ha, how cute!\n\nThe paper which introduced adversarial examples was titled “Intriguing Properties of Neural Networks.”\n\n2018+: serious security threat\n\nNobody has found a reliable method yet to defend against them!\n7 of 8 proposed defenses accepted to ICLR 2018 were cracked within days."
  },
  {
    "objectID": "lecs/w07/lec07.html#white-box-vs-black-box-adversarial-attacks",
    "href": "lecs/w07/lec07.html#white-box-vs-black-box-adversarial-attacks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "White-box vs Black-box Adversarial Attacks",
    "text": "White-box vs Black-box Adversarial Attacks\nAdversarial examples transfer to different networks trained on a totally separate training set!\nWhite-box Adversarial Attack: Model architecture and weights are known, so we can compute gradients. (What we’ve been doing so far in the demos)"
  },
  {
    "objectID": "lecs/w07/lec07.html#white-box-vs-black-box-adversarial-attacks-1",
    "href": "lecs/w07/lec07.html#white-box-vs-black-box-adversarial-attacks-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "White-box vs Black-box Adversarial Attacks",
    "text": "White-box vs Black-box Adversarial Attacks\nBlack-box Adversarial Attack: Model architecture and weights are unknown.\n\nYou don’t need access to the original network!\nYou can train up a new network to match its predictions, and then construct adversarial examples for that.\n\nAttack carried out against proprietary classification networks accessed using prediction APIs (MetaMind, Amazon, Google)"
  },
  {
    "objectID": "lecs/w07/lec07.html#adversarial-examples-in-3d",
    "href": "lecs/w07/lec07.html#adversarial-examples-in-3d",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Adversarial Examples in 3D",
    "text": "Adversarial Examples in 3D\nIt is possible to have a 3D object that gets misclassified by a neural network from all angles.\nhttps://www.youtube.com/watch?v=piYnd_wYlT8"
  },
  {
    "objectID": "lecs/w07/lec07.html#printed-adversarial-examples",
    "href": "lecs/w07/lec07.html#printed-adversarial-examples",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Printed Adversarial Examples",
    "text": "Printed Adversarial Examples\nIt is possible for a printed image to cause object detection to fail.\nhttps://www.youtube.com/watch?v=MIbFvK2S9g8"
  },
  {
    "objectID": "lecs/w07/lec07.html#what-if-we-exclude-a-training-point",
    "href": "lecs/w07/lec07.html#what-if-we-exclude-a-training-point",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What if we exclude a training point?",
    "text": "What if we exclude a training point?\n\nLet’s suppose we have a training set \\(D=\\{(x_1,\\, y_1),\\, \\ldots,\\, (x_N,\\, y_N)\\}\\)\nWe typically solve the following problem on the training data:\n\n\\[\\begin{align*}\n    \\hat{\\theta} = \\text{argmin}_{\\theta} \\frac{1}{N} \\left[\\sum_{i=1}^{N} L\\left(x_i,\\, y_i;\\, \\theta\\right) \\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w07/lec07.html#what-if-we-exclude-a-training-point-1",
    "href": "lecs/w07/lec07.html#what-if-we-exclude-a-training-point-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What if we exclude a training point?",
    "text": "What if we exclude a training point?\n\nHow much does the optimum change if we weight point \\((x,\\, y) \\in D\\) by \\(1+\\epsilon\\)?\n\n\\[\\begin{align*}\n    \\hat{\\theta}({\\epsilon}) = \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] + \\epsilon L(x,y; \\theta)\n\\end{align*}\\]\n\nRemoving point \\((x,\\, y)\\) from \\(D\\) means computing \\(\\hat{\\theta}({\\epsilon})\\) for \\(\\epsilon=-1/N\\)"
  },
  {
    "objectID": "lecs/w07/lec07.html#do-we-need-to-compute-hatthetaepsilon-from-scratch",
    "href": "lecs/w07/lec07.html#do-we-need-to-compute-hatthetaepsilon-from-scratch",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Do We Need to Compute \\(\\hat{\\theta}({\\epsilon})\\) from Scratch?",
    "text": "Do We Need to Compute \\(\\hat{\\theta}({\\epsilon})\\) from Scratch?\n\nNot exactly. We can approximate it using Taylor expansion:\n\n\\[\\begin{align*}\n    \\hat{\\theta}({\\epsilon}) \\approx \\hat{\\theta} + \\epsilon\\frac{d\\hat{\\theta}(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0}\n\\end{align*}\\]\n\nThe derivative \\(\\frac{d\\hat{\\theta}(\\epsilon)}{d\\epsilon}{\\Bigr |}_{\\epsilon=0}\\) is called the influence of point \\((x,y)\\) on the optimum. We denote it as \\(\\mathcal{I}(x,y)\\).\nHow do we compute it?"
  },
  {
    "objectID": "lecs/w07/lec07.html#computing-mathcalix-y-the-influence-of-training-point-x-y",
    "href": "lecs/w07/lec07.html#computing-mathcalix-y-the-influence-of-training-point-x-y",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing \\(\\mathcal{I}(x,\\, y)\\), the influence of training point \\((x,\\, y)\\)",
    "text": "Computing \\(\\mathcal{I}(x,\\, y)\\), the influence of training point \\((x,\\, y)\\)\n\nLet’s denote \\(R(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N}L(x_i, y_i; \\theta)\\)\nSince \\(\\hat{\\theta}({\\epsilon}) = \\text{argmin}_{\\theta} \\left[ R(\\theta) + \\epsilon L(x,y; \\theta) \\right]\\) we have\n\n\\[\\begin{align*}\n    0 = \\nabla_{\\theta}R(\\theta) + \\epsilon \\nabla_{\\theta}L(x,y; \\theta) {\\Bigr |}_{\\theta=\\hat{\\theta}(\\epsilon)}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w07/lec07.html#computing-mathcalix-y-the-influence-of-training-point-x-y-1",
    "href": "lecs/w07/lec07.html#computing-mathcalix-y-the-influence-of-training-point-x-y-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing \\(\\mathcal{I}(x,\\, y)\\), the influence of training point \\((x,\\, y)\\)",
    "text": "Computing \\(\\mathcal{I}(x,\\, y)\\), the influence of training point \\((x,\\, y)\\)\n\nBut the RHS expression depends on \\(\\hat{\\theta}(\\epsilon)\\), whereas we’d like it to depend on \\(\\hat{\\theta}\\). Let’s apply Taylor series expansion on the RHS, in order to do that:\n\n\\[\\begin{multline*}\n0 \\approx \\nabla_{\\theta}R(\\hat{\\theta}) + \\epsilon \\nabla_{\\theta}L(x,y; \\hat{\\theta})\\\\ + \\left[ \\nabla_{\\theta}^2 R(\\hat{\\theta})\n  + \\epsilon \\nabla_{\\theta}^2 L(x,y; \\hat{\\theta}) \\right](\\hat{\\theta}(\\epsilon) - \\hat{\\theta})\n\\end{multline*}\\]\n\n\\(\\hat{\\theta}\\) is the minimizer of \\(R(\\theta)\\), so \\(\\nabla_{\\theta}R(\\hat{\\theta})=0\\)"
  },
  {
    "objectID": "lecs/w07/lec07.html#computing-mathcalix-y-the-influence-of-training-point-x-y-2",
    "href": "lecs/w07/lec07.html#computing-mathcalix-y-the-influence-of-training-point-x-y-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing \\(\\mathcal{I}(x,\\, y)\\), the influence of training point \\((x,\\, y)\\)",
    "text": "Computing \\(\\mathcal{I}(x,\\, y)\\), the influence of training point \\((x,\\, y)\\)\n\nSo, the RHS expression becomes:\n\n\\[\\begin{align*}\n    \\hat{\\theta}(\\epsilon) - \\hat{\\theta} \\approx -\\left[ \\nabla_{\\theta}^2 R(\\hat{\\theta}) + \\epsilon \\nabla_{\\theta}^2 L(x,y; \\hat{\\theta}) \\right]^{-1} \\nabla_{\\theta}L(x,y; \\hat{\\theta})\\epsilon\n\\end{align*}\\]\n\nOr equivalently:\n\n\\[\\begin{align*}\n    \\frac{\\hat{\\theta}(\\epsilon) - \\hat{\\theta}}{\\epsilon} \\approx -\\left[ \\nabla_{\\theta}^2 R(\\hat{\\theta}) + \\epsilon \\nabla_{\\theta}^2 L(x,y; \\hat{\\theta}) \\right]^{-1} \\nabla_{\\theta}L(x,y; \\hat{\\theta})\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w07/lec07.html#computing-mathcalix-y-the-influence-of-training-point-x-y-3",
    "href": "lecs/w07/lec07.html#computing-mathcalix-y-the-influence-of-training-point-x-y-3",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing \\(\\mathcal{I}(x,\\, y)\\), the Influence of Training Point \\((x,\\, y)\\)",
    "text": "Computing \\(\\mathcal{I}(x,\\, y)\\), the Influence of Training Point \\((x,\\, y)\\)\n\nAnd as \\(\\epsilon \\rightarrow 0\\):\n\n\\[\\begin{align*}\n    \\frac{d \\hat{\\theta}(\\epsilon)}{d \\epsilon} {\\Bigr |}_{\\epsilon=0} \\approx -\\left[ \\nabla_{\\theta}^2 R(\\hat{\\theta}) \\right]^{-1} \\nabla_{\\theta}L(x,y; \\hat{\\theta})\n\\end{align*}\\]\n\nSo, we have an approximate way to compute the influence \\(\\mathcal{I}(x,y)\\) as a function of \\(\\hat{\\theta}\\), without re-running the entire optimization process!"
  },
  {
    "objectID": "lecs/w07/lec07.html#why-is-this-important",
    "href": "lecs/w07/lec07.html#why-is-this-important",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why is this important?",
    "text": "Why is this important?\n\nBecause we can compute the sensitivity of the optimal weights to a training point\nWe can also compute the sensitivity of the test loss to a training point!\nConsider a test point \\((u,v)\\), a training point \\((x,y)\\), and the test loss \\(L(u,v; \\hat{\\theta})\\)\nHow sensitive is \\(L(u,v; \\hat{\\theta}(\\epsilon))\\) to a perturbation \\(\\hat{\\theta}(\\epsilon)\\) where we have upweighted the training point \\((x,y)\\)?"
  },
  {
    "objectID": "lecs/w07/lec07.html#why-is-this-important-1",
    "href": "lecs/w07/lec07.html#why-is-this-important-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why is this important?",
    "text": "Why is this important?\n\nHow sensitive is \\(L(u,v; \\hat{\\theta}(\\epsilon))\\) to a perturbation \\(\\hat{\\theta}(\\epsilon)\\) where we have upweighted the training point \\((x,y)\\)?\nUsing chain rule:\n\n\\[\\begin{align*}\n\\frac{d L(u,v; \\hat{\\theta}(\\epsilon))}{d \\epsilon} {\\Bigr |}_{\\epsilon=0} = \\frac{d L(u,v; \\theta)}{d \\theta}{\\Bigr |}_{\\theta=\\hat{\\theta}} \\frac{d \\hat{\\theta}(\\epsilon)}{d \\epsilon}{\\Bigr |}_{\\epsilon=0}\n\\end{align*}\\]\n\nWe do not have to re-run the optimization process here either!"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_translation.html",
    "href": "lecs/w09/seq2seq_transformer_translation.html",
    "title": "Seq2seq Transformers for Word Generation",
    "section": "",
    "text": "Seq2seq models are designed to learn the mapping between an input sequence and a target output sequence, making them ideal for tasks where structured input data is transformed into a related output sequence. These models capture complex relationships between sequential data, allowing them to generate coherent and contextually relevant sequences based on an input. Seq2seq models have been widely applied in natural language processing tasks such as language translation, where a model translates sentences from one language to another, text summarization, where it condenses long text into key points, and question-answering, where the model generates responses based on provided input questions.\nThe Transformer model, a breakthrough in seq2seq architectures, introduced a new approach to handling long-range dependencies within sequences using a mechanism called self-attention. Unlike traditional recurrent neural networks (RNNs) or long short-term memory (LSTM) models, which process sequences sequentially, the Transformer model processes all positions in the sequence in parallel, enabling it to capture global context more effectively and significantly reducing training time. This self-attention mechanism allows the model to focus on different parts of the input sequence with varying degrees of importance, dynamically adjusting to each input and providing a better understanding of context.\nWith these capabilities, Transformer-based seq2seq models have become the backbone of many modern applications. For instance, models like BERT, GPT, and T5, built on the Transformer architecture, have advanced the state of the art in machine translation, chatbots, summarization, and even code generation."
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_translation.html#load-opus-books-english---french-dataset",
    "href": "lecs/w09/seq2seq_transformer_translation.html#load-opus-books-english---french-dataset",
    "title": "Seq2seq Transformers for Word Generation",
    "section": "Load OPUS Books (English - French) dataset",
    "text": "Load OPUS Books (English - French) dataset\nStart by loading the English-French subset of the OPUS Books dataset:\n\nfrom datasets import load_dataset\n\nbooks = load_dataset(\"opus_books\", \"en-fr\")\n\n\nprint(books)\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 127085\n    })\n})\n\n\nThe dataset we downloaded only includes training data, with no separate test set provided. To address this, we used the train_test_split method to split the data into training and testing sets ourselves:\n\n# books = books[\"train\"].train_test_split(train_size = 0.8, test_size=0.2) # Train: 80% Test: 20%\n\n# Due to time constraints, we’re using just 10% of the data for training to speed up the process.\n# We allocate 10% of the data for training and 5% for testing, leaving the remaining samples unused.\nbooks = books[\"train\"].train_test_split(train_size = 0.1, test_size=0.05)\n\n\nprint(books)\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 12708\n    })\n    test: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 6355\n    })\n})\n\n\nThen take a look at an example:\n\nbooks[\"train\"][0]\n\n{'id': '62042',\n 'translation': {'en': 'From time to time, he followed a fine stone with his eye, and when it did good execution, he said, \"Hum!\"',\n  'fr': 'De temps en temps il suivait une belle pierre de l’œil, et, quand elle tuait bien, il disait : « Hun ! »'}}\n\n\ntranslation: an English and French translation of the text."
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_translation.html#preprocess",
    "href": "lecs/w09/seq2seq_transformer_translation.html#preprocess",
    "title": "Seq2seq Transformers for Word Generation",
    "section": "Preprocess",
    "text": "Preprocess\nThe next step is to load a T5 tokenizer to process the English-French language pairs. A tokenizer converts raw text into smaller units called tokens (like words or subwords) and encodes them into numerical representations that the model can understand.\n\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"t5-small\" # Use a pre-trained tokenizer\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nThe preprocessing function you want to create needs to:\n\nPrefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks.\nTokenize the input (English) and target (French) separately because you can’t tokenize French text with a tokenizer pretrained on an English vocabulary.\nTruncate sequences to be no longer than the maximum length set by the max_length parameter.\n\n\nsource_lang = \"en\"\ntarget_lang = \"fr\"\nprefix = \"translate English to French: \"\n\n\ndef preprocess_function(examples):\n    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n    targets = [example[target_lang] for example in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n    return model_inputs\n\nTo apply the preprocessing function over the entire dataset, use Datasets map method. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once:\n\ntokenized_books = books.map(preprocess_function, batched=True)\n\n\n\n\n\n\n\nNow create a batch of examples using DataCollatorForSeq2Seq. It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\nfrom transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_translation.html#train",
    "href": "lecs/w09/seq2seq_transformer_translation.html#train",
    "title": "Seq2seq Transformers for Word Generation",
    "section": "Train",
    "text": "Train\nOur goal is to fine-tune a pre-trained T5 model for English-to-French translation. Fine-tuning allows us to adapt the model’s existing language knowledge to our specific task, improving its translation quality on the dataset we’re using. We start by loading the pre-trained T5 model, which already has a strong foundation in language patterns, and then fine-tune it on our English-French language pairs to further enhance its performance for our needs.\n\nfrom transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\nTo fine-tune the model, just three steps remain:\n\nDefine training hyperparameters with Seq2SeqTrainingArguments. The only required parameter is output_dir, where your model will be saved.\nPass these training arguments to Seq2SeqTrainer, along with the model, dataset, tokenizer, data collator, and compute_metrics function.\nFinally, call train() to fine-tune your model.\n\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"CSC413_opus_books_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=1,\n    predict_with_generate=True,\n    fp16=True,\n    push_to_hub=False,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_books[\"train\"],\n    eval_dataset=tokenized_books[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\ntraining_results = trainer.train()\nprint(training_results)\n\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\n\n\n      \n      \n      [795/795 02:19, Epoch 1/1]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\n\n\n\n\n1\n2.110100\n1.829962\n\n\n\n\n\n\nTrainOutput(global_step=795, training_loss=2.0803384648928853, metrics={'train_runtime': 140.2121, 'train_samples_per_second': 90.634, 'train_steps_per_second': 5.67, 'total_flos': 312195471114240.0, 'train_loss': 2.0803384648928853, 'epoch': 1.0})"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_translation.html#inference",
    "href": "lecs/w09/seq2seq_transformer_translation.html#inference",
    "title": "Seq2seq Transformers for Word Generation",
    "section": "Inference",
    "text": "Inference\nGreat, now that we hve finetuned a T5 model for English-French translation!\nCome up with some text you’d like to translate to another language. For T5, you need to prefix your input depending on the task you’re working on. For translation from English to French, you should prefix your input as shown below:\n\n# text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\ntext = \"translate English to French: I'm a student at the University of Toronto and excited to dive into the amazing world of Deep Learning!\"\n\nTokenize the text and return the input_ids as PyTorch tensors:\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"CSC413_opus_books_model/checkpoint-{}\".format(training_results.global_step))\ninputs = tokenizer(text, return_tensors=\"pt\").input_ids\n\nUse the generate() method to create the translation.\n\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"CSC413_opus_books_model/checkpoint-{}\".format(training_results.global_step))\noutputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n\nDecode the generated token ids back into text:\n\ntokenizer.decode(outputs[0], skip_special_tokens=True)\n\n'Je suis une étudiante à l’Université de Toronto et exalté de plonger dans le monde merveilleux de Deep Learning.'"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html",
    "href": "lecs/w09/seq2seq_transformer_regression.html",
    "title": "Seq2Seq Transformers for Regression",
    "section": "",
    "text": "In this tutorial, we’ll explore how to use a Seq2Seq Transformer model for time series forecasting. We’ll generate synthetic sine wave data, train a Transformer model to predict future values based on past observations, and visualize the results."
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#introduction",
    "href": "lecs/w09/seq2seq_transformer_regression.html#introduction",
    "title": "Seq2Seq Transformers for Regression",
    "section": "Introduction",
    "text": "Introduction\nRegression forecasting involves predicting future values of a sequence based on its past observations. This is crucial in various domains like finance, weather prediction, stock market analysis, and more. Traditional models like ARIMA or LSTMs have been used for regression forecasting (e.g., for time series or continuous functions), but Transformers offer a new approach with their ability to handle long-range dependencies and parallel processing capabilities.\nIn this tutorial, we will:\n\nGenerate synthetic sine wave data for training and testing.\nImplement a Seq2Seq Transformer model using PyTorch.\nTrain the model to forecast future values of the sine wave.\nEvaluate the model’s performance by printing the MSE score per epoch.\nVisualize the predictions compared to the true values."
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#import-libraries",
    "href": "lecs/w09/seq2seq_transformer_regression.html#import-libraries",
    "title": "Seq2Seq Transformers for Regression",
    "section": "1. Import Libraries",
    "text": "1. Import Libraries\n\n# !pip install torch matplotlib numpy\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Ensure plots are displayed in the notebook\n%matplotlib inline"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#define-configuration-parameters",
    "href": "lecs/w09/seq2seq_transformer_regression.html#define-configuration-parameters",
    "title": "Seq2Seq Transformers for Regression",
    "section": "2. Define Configuration Parameters",
    "text": "2. Define Configuration Parameters\n\n# Learning parameters\nlearning_rate = 0.001  # Learning rate for the optimizer\ndecay = 0  # Weight decay (L2 regularization)\n\n# Input and output dimensions\nnum_input_features = 1  # Each time step has one feature (the sine value)\nnum_output_features = 1  # Predicting one feature at each time step\n\n# Training parameters\nbatch_size = 512\nsteps_per_epoch = 200\nepochs = 20\n\n# Sequence lengths\ninput_sequence_length = 15  # Number of past time steps provided to the model\ntarget_sequence_length = 15  # Number of future time steps the model will predict\n\n# Data parameters\nnum_signals = 2  # Number of sine waves combined to create a more complex signal\n\n# Device configuration\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nUsing device: cuda"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#generate-data",
    "href": "lecs/w09/seq2seq_transformer_regression.html#generate-data",
    "title": "Seq2Seq Transformers for Regression",
    "section": "3. Generate Data",
    "text": "3. Generate Data\nWe need to generate a synthetic continuous function. We’ll create a function that generates batches of sine waves with random frequencies, amplitudes, and phases.\nFor training, the decoder input is the shifted version of the target sequence, starting with the last value from the encoder input.\n\ndef random_sine(batch_size, input_sequence_length, target_sequence_length,\n                min_frequency, max_frequency, min_amplitude, max_amplitude,\n                min_offset, max_offset, num_signals, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    total_length = input_sequence_length + target_sequence_length\n    t = np.arange(0, total_length).reshape(1, -1)\n    while True:\n        # Generate random parameters for the sine waves\n        freqs = np.random.uniform(min_frequency, max_frequency, size=(batch_size, num_signals))\n        amps = np.random.uniform(min_amplitude, max_amplitude, size=(batch_size, num_signals))\n        phases = np.random.uniform(0, 2 * np.pi, size=(batch_size, num_signals))\n        offsets = np.random.uniform(min_offset, max_offset, size=(batch_size, num_signals))\n\n        # Generate sine waves and combine them\n        y = np.zeros((batch_size, total_length))\n        for i in range(num_signals):\n            y += (amps[:, i][:, np.newaxis] *\n                  np.sin(2 * np.pi * freqs[:, i][:, np.newaxis] * t / total_length + phases[:, i][:, np.newaxis]) +\n                  offsets[:, i][:, np.newaxis])\n\n        # Normalize the data\n        y = (y - y.mean(axis=1, keepdims=True)) / y.std(axis=1, keepdims=True)\n\n        # Split into input and target sequences\n        x_batch = y[:, :input_sequence_length]\n        y_batch = y[:, input_sequence_length:]\n\n        x_batch = x_batch.reshape(batch_size, input_sequence_length, num_input_features)\n        y_batch = y_batch.reshape(batch_size, target_sequence_length, num_output_features)\n\n        # Prepare decoder input by shifting target sequence and adding last encoder value\n        decoder_input = np.zeros((batch_size, target_sequence_length, num_output_features))\n        decoder_input[:, 0, :] = x_batch[:, -1, :]  # Start with last value of encoder input\n        decoder_input[:, 1:, :] = y_batch[:, :-1, :]  # Shifted target sequence\n\n        yield (x_batch, decoder_input), y_batch\n\nLet’s visualize a sample from the data generator.\n\n# Create a data generator instance\ndata_gen = random_sine(\n    batch_size=1,\n    input_sequence_length=input_sequence_length,\n    target_sequence_length=target_sequence_length,\n    min_frequency=0.1,\n    max_frequency=3.0,\n    min_amplitude=0.1,\n    max_amplitude=1.0,\n    min_offset=-0.5,\n    max_offset=0.5,\n    num_signals=num_signals,\n    seed=42\n)\n\n# Get one batch\n(encoder_input_sample, decoder_input_sample), decoder_output_sample = next(data_gen)\n\n# Plot the sample\nplt.figure(figsize=(12, 4))\nx = encoder_input_sample[0].flatten()\ny_true = decoder_output_sample[0].flatten()\n\nplt.plot(range(input_sequence_length), x, \"o--b\", label=\"Input Sequence\")\nplt.plot(range(input_sequence_length, input_sequence_length + target_sequence_length),\n         y_true, \"x--r\", label=\"Target Sequence\")\nplt.legend(loc='best')\nplt.title(\"Sample Input and Target Sequences\")\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Normalized Value\")\nplt.show()"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#positional-encoding",
    "href": "lecs/w09/seq2seq_transformer_regression.html#positional-encoding",
    "title": "Seq2Seq Transformers for Regression",
    "section": "4. Positional Encoding",
    "text": "4. Positional Encoding\nTransformers lack inherent positional information about the sequence data. To address this, we’ll implement positional encoding to provide the model with information about the position of each element in the sequence.\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        # Create position encodings\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        # Add positional encoding to input\n        x = x + self.pe[:, :x.size(1)].to(x.device)\n        return x"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#build-the-transformer-model",
    "href": "lecs/w09/seq2seq_transformer_regression.html#build-the-transformer-model",
    "title": "Seq2Seq Transformers for Regression",
    "section": "5. Build the Transformer Model",
    "text": "5. Build the Transformer Model\nWe’ll define the Transformer model using PyTorch’s built-in Transformer modules.\nComponents of the transformer: - embedding: Transforms input features into a higher-dimensional space. - transformer_encoder: Processes the input sequence to create memory representations. - transformer_decoder: Generates output sequences using the encoder’s memory and the target sequence. - output_layer: Maps the decoder outputs to the desired output dimension.\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_dim, output_dim, d_model=128, n_heads=4, num_layers=2, d_ff=512, dropout=0.1):\n        super(TransformerModel, self).__init__()\n        self.d_model = d_model\n\n        # Input embedding layer\n        self.embedding = nn.Linear(input_dim, d_model)\n\n        # Positional encoding\n        self.pos_encoder = PositionalEncoding(d_model)\n\n        # Encoder layers\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads,\n                                                   dim_feedforward=d_ff, dropout=dropout,\n                                                   batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # Decoder layers\n        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads,\n                                                   dim_feedforward=d_ff, dropout=dropout,\n                                                   batch_first=True)\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n\n        # Output layer\n        self.output_layer = nn.Linear(d_model, output_dim)\n\n    def forward(self, src, tgt, tgt_mask=None):\n        # Encode the source sequence\n        src = self.embedding(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        memory = self.transformer_encoder(src)\n\n        # Encode the target sequence\n        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n        tgt = self.pos_encoder(tgt)\n        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n\n        # Generate predictions\n        output = self.output_layer(output)\n        return output"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#training-the-model",
    "href": "lecs/w09/seq2seq_transformer_regression.html#training-the-model",
    "title": "Seq2Seq Transformers for Regression",
    "section": "6. Training the Model",
    "text": "6. Training the Model\nInstantiate the model.\n\n# Instantiate the transformer model\nmodel = TransformerModel(\n    input_dim=num_input_features,\n    output_dim=num_output_features,\n    d_model=128,\n    n_heads=4,\n    num_layers=2,\n    d_ff=512,\n    dropout=0.1\n).to(device)\n\nDefine the optimizer and loss function. In this case, we will use mean squared error (MSE).\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\nloss_fn = nn.MSELoss()\n\nGenerate the target mask - we need to create a mask to prevent the model from “seeing” future positions in the target sequence during training.\n\ndef generate_square_subsequent_mask(sz):\n    mask = torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n    return mask\n\nTo monitor the model’s performance during training, we’ll create a validation data generator that provides a fixed validation dataset.\n\n# Create a validation data generator\nvalid_data_generator = random_sine(\n    batch_size=512,\n    input_sequence_length=input_sequence_length,\n    target_sequence_length=target_sequence_length,\n    min_frequency=0.1,\n    max_frequency=1.0,\n    min_amplitude=0.1,\n    max_amplitude=1.0,\n    min_offset=-0.5,\n    max_offset=0.5,\n    num_signals=num_signals,\n    seed=2021  # Fixed seed for reproducibility\n)\n\n# Generate validation data (one epoch's worth)\n(valid_encoder_inputs, valid_decoder_inputs), valid_decoder_outputs = next(valid_data_generator)\nvalid_encoder_inputs = torch.tensor(valid_encoder_inputs, dtype=torch.float32).to(device)\nvalid_decoder_inputs = torch.tensor(valid_decoder_inputs, dtype=torch.float32).to(device)\nvalid_decoder_outputs = torch.tensor(valid_decoder_outputs, dtype=torch.float32).to(device)\n\nNext, we create a training function to compute and print the MSE score for each epoch using the validation dataset. We set the model to evaluation mode (model.eval()) during validation to disable dropout layers.\n\ndef train(model, data_generator, optimizer, loss_fn, epochs, steps_per_epoch, valid_data):\n    model.train()\n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        for step in range(steps_per_epoch):\n            (encoder_input, decoder_input), decoder_output = next(data_generator)\n            encoder_input = torch.tensor(encoder_input, dtype=torch.float32).to(device)\n            decoder_input = torch.tensor(decoder_input, dtype=torch.float32).to(device)\n            decoder_output = torch.tensor(decoder_output, dtype=torch.float32).to(device)\n\n            # Generate target mask\n            tgt_mask = generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n\n            # Forward pass\n            optimizer.zero_grad()\n            output = model(encoder_input, decoder_input, tgt_mask)\n            loss = loss_fn(output, decoder_output)\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            print(f\"Epoch {epoch+1}/{epochs}, Step {step+1}/{steps_per_epoch}, Loss: {loss.item():.4f}\", end='\\r')\n\n        avg_epoch_loss = epoch_loss / steps_per_epoch\n\n        # Evaluate on validation data\n        model.eval()\n        with torch.no_grad():\n            tgt_mask = generate_square_subsequent_mask(valid_decoder_inputs.size(1)).to(device)\n            valid_output = model(valid_encoder_inputs, valid_decoder_inputs, tgt_mask)\n            valid_loss = loss_fn(valid_output, valid_decoder_outputs).item()\n        model.train()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_epoch_loss:.4f}, Validation MSE: {valid_loss:.4f}\")\n\nNow, let’s train the model.\n\n# Create the training data generator\ntrain_data_generator = random_sine(\n    batch_size=batch_size,\n    input_sequence_length=input_sequence_length,\n    target_sequence_length=target_sequence_length,\n    min_frequency=0.1, max_frequency=1.0,  # Reduced frequency range for simplicity\n    min_amplitude=0.1, max_amplitude=1.0,\n    min_offset=-0.5, max_offset=0.5,\n    num_signals=num_signals,\n    seed=1969\n)\n\n# Start training with validation\ntrain(model, train_data_generator, optimizer, loss_fn, epochs, steps_per_epoch,\n      valid_data=(valid_encoder_inputs, valid_decoder_inputs, valid_decoder_outputs))\n\nEpoch 1/20, Training Loss: 0.0981, Validation MSE: 0.0043\nEpoch 2/20, Training Loss: 0.0063, Validation MSE: 0.0023\nEpoch 3/20, Training Loss: 0.0038, Validation MSE: 0.0014\nEpoch 4/20, Training Loss: 0.0028, Validation MSE: 0.0008\nEpoch 5/20, Training Loss: 0.0023, Validation MSE: 0.0007\nEpoch 6/20, Training Loss: 0.0020, Validation MSE: 0.0021\nEpoch 7/20, Training Loss: 0.0018, Validation MSE: 0.0008\nEpoch 8/20, Training Loss: 0.0017, Validation MSE: 0.0004\nEpoch 9/20, Training Loss: 0.0015, Validation MSE: 0.0007\nEpoch 10/20, Training Loss: 0.0014, Validation MSE: 0.0008\nEpoch 11/20, Training Loss: 0.0013, Validation MSE: 0.0005\nEpoch 12/20, Training Loss: 0.0012, Validation MSE: 0.0004\nEpoch 13/20, Training Loss: 0.0012, Validation MSE: 0.0010\nEpoch 14/20, Training Loss: 0.0010, Validation MSE: 0.0010\nEpoch 15/20, Training Loss: 0.0010, Validation MSE: 0.0006\nEpoch 16/20, Training Loss: 0.0010, Validation MSE: 0.0006\nEpoch 17/20, Training Loss: 0.0009, Validation MSE: 0.0002\nEpoch 18/20, Training Loss: 0.0009, Validation MSE: 0.0003\nEpoch 19/20, Training Loss: 0.0008, Validation MSE: 0.0004\nEpoch 20/20, Training Loss: 0.0008, Validation MSE: 0.0007"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#evaluating-the-model",
    "href": "lecs/w09/seq2seq_transformer_regression.html#evaluating-the-model",
    "title": "Seq2Seq Transformers for Regression",
    "section": "7. Evaluating the Model",
    "text": "7. Evaluating the Model\nAfter training, we’ll evaluate the model on test data to see how well it generalizes.\n\n# Generate test data\ntest_data_generator = random_sine(\n    batch_size=1000,\n    input_sequence_length=input_sequence_length,\n    target_sequence_length=target_sequence_length,\n    min_frequency=0.1, max_frequency=1.0,  # Consistent frequency range\n    min_amplitude=0.1, max_amplitude=1.0,\n    min_offset=-0.5, max_offset=0.5,\n    num_signals=num_signals,\n    seed=2000  # Different seed for test data\n)\n\n# Get a batch of test data\n(x_test, _), y_test = next(test_data_generator)"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#make-predictions",
    "href": "lecs/w09/seq2seq_transformer_regression.html#make-predictions",
    "title": "Seq2Seq Transformers for Regression",
    "section": "8. Make Predictions",
    "text": "8. Make Predictions\nWe’ll use the trained model to make predictions on the test data. At each time step, the model predicts the next value and appends it to the decoder input for the next prediction.\n\n# Convert test data to tensors\nx_encoder_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n\n# Initialize decoder input with the last value from the encoder input\ndecoder_input = x_encoder_test[:, -1:, :]  # Shape: [batch_size, 1, num_output_features]\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Make predictions step by step\ny_test_predicted = []\nwith torch.no_grad():\n    for t in range(target_sequence_length):\n        tgt_mask = generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n        output = model(x_encoder_test, decoder_input, tgt_mask)\n        next_value = output[:, -1:, :]  # Get the last time step prediction\n        y_test_predicted.append(next_value.cpu().numpy())\n        # Append the next_value to decoder_input\n        decoder_input = torch.cat((decoder_input, next_value.to(device)), dim=1)\n\n# Concatenate predictions along the time dimension\ny_test_predicted = np.concatenate(y_test_predicted, axis=1)"
  },
  {
    "objectID": "lecs/w09/seq2seq_transformer_regression.html#visualize-predictions",
    "href": "lecs/w09/seq2seq_transformer_regression.html#visualize-predictions",
    "title": "Seq2Seq Transformers for Regression",
    "section": "9. Visualize Predictions",
    "text": "9. Visualize Predictions\nLet’s visualize the model’s predictions compared to the true future values.\n\n# Select a few random examples to plot\nindices = np.random.choice(range(x_test.shape[0]), replace=False, size=5)\n\nfor index in indices:\n    plt.figure(figsize=(12, 4))\n    x = x_test[index, :, :]\n    y_true = y_test[index, :, :]\n    y_pred = y_test_predicted[index, :, :]\n\n    # Plot the past values\n    plt.plot(range(input_sequence_length), x.flatten(), \"o--b\", label=\"Input Sequence\")\n\n    # Plot the true future values\n    plt.plot(range(input_sequence_length, input_sequence_length + target_sequence_length),\n             y_true.flatten(), \"x--r\", label=\"True Future Values\")\n\n    # Plot the predicted future values\n    plt.plot(range(input_sequence_length, input_sequence_length + target_sequence_length),\n             y_pred.flatten(), \"o--g\", label=\"Predicted Future Values\")\n\n    plt.legend(loc='best')\n    plt.title(\"Model Predictions vs. True Values\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Normalized Value\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext Steps:\n\nTry changing the number of sine waves, sequence lengths, or model parameters to see how the model’s performance is affected.\nApply the model to real-world time series datasets, such as stock prices or weather data.\nImplement techniques like teacher forcing during training to improve performance."
  },
  {
    "objectID": "lecs/w01/lec01.html#introduction-instructors",
    "href": "lecs/w01/lec01.html#introduction-instructors",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Introduction: Instructors",
    "text": "Introduction: Instructors\nIgor Gilitschenski (LEC0101)\n\ni.gilitschenski@utoronto.ca\nOffice Hours: Tue 7-8pm DH3070\n\nFlorian Shkurti (LEC0102)\n\nflorian@cs.toronto.edu\nOffice Hours: Wed 10-11am DH3066\n\nPlease use Piazza for course-related questions"
  },
  {
    "objectID": "lecs/w01/lec01.html#introduction---you-why-you-are-here",
    "href": "lecs/w01/lec01.html#introduction---you-why-you-are-here",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Introduction - You! Why you are here",
    "text": "Introduction - You! Why you are here\n\n\nI hope to apply [deep learning models] in my future career\nTo gain a better understanding of AI and potentially open up a career path in this direction\n\n\n\n\nI am kinda leaning towards doing machine learning in grad school\nMy [ROP] supervisor suggested that I take this course."
  },
  {
    "objectID": "lecs/w01/lec01.html#introduction---you-why-you-are-here-ii",
    "href": "lecs/w01/lec01.html#introduction---you-why-you-are-here-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Introduction - You! Why you are here II",
    "text": "Introduction - You! Why you are here II\n\n\nI am excited by the prospects of neural networks in the near future in fields such as finance, health, entertainment, etc.\nFields such as genetics are currently teeming with opportunities for deep learning, and I’d like to be a part of that advancement in the future.\n\n\n\n\nIt’s fascinating what tasks NN can do and that’s why i want to learn that.\nInterested in learning about neural network after csc311"
  },
  {
    "objectID": "lecs/w01/lec01.html#computational-resources",
    "href": "lecs/w01/lec01.html#computational-resources",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computational Resources?",
    "text": "Computational Resources?\n\nGoogle Colab: 41%\nOwn machine: 59%\nUTM machine: 0%"
  },
  {
    "objectID": "lecs/w01/lec01.html#flipped-classroom",
    "href": "lecs/w01/lec01.html#flipped-classroom",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Flipped Classroom",
    "text": "Flipped Classroom\n\nWe turned tutorials into labs (practical, for credit)\nWe are trying active learning:\n\nwe provide a lecture recording before class\nyou watch it carefully before class\nyou come to class to ask questions and do group exercises (practical, not for credit)\nmaximizing interaction and discussion, minimizing one-way lecture"
  },
  {
    "objectID": "lecs/w01/lec01.html#will-it-work-better-than-typical-lectures",
    "href": "lecs/w01/lec01.html#will-it-work-better-than-typical-lectures",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Will it Work Better than Typical Lectures?",
    "text": "Will it Work Better than Typical Lectures?\n\nWe don’t know, but it’s a useful experiment to try\nMeasuring success: students do better on average than previous years"
  },
  {
    "objectID": "lecs/w01/lec01.html#what-is-the-difference-between",
    "href": "lecs/w01/lec01.html#what-is-the-difference-between",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What is the Difference Between…",
    "text": "What is the Difference Between…\n\nArtificial Intelligence\nMachine Learning\nDeep Learning"
  },
  {
    "objectID": "lecs/w01/lec01.html#ai-vs-ml-vs-dl",
    "href": "lecs/w01/lec01.html#ai-vs-ml-vs-dl",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "AI vs ML vs DL",
    "text": "AI vs ML vs DL\nArtificial Intelligence: Create intelligent machines that perceive, reason, and act like humans. (CSC384)\nMachine Learning: Find an algorithm that automatically learns from example data. (CSC311)\nDeep Learning: Using deep neural networks to automatically learn from example data. (CSC413)"
  },
  {
    "objectID": "lecs/w01/lec01.html#relationship",
    "href": "lecs/w01/lec01.html#relationship",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Relationship",
    "text": "Relationship"
  },
  {
    "objectID": "lecs/w01/lec01.html#why-machine-learning",
    "href": "lecs/w01/lec01.html#why-machine-learning",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Machine Learning?",
    "text": "Why Machine Learning?\nFor many problems, it is difficult to program the correct behaviour by hand.\nMachine learning approach: program an algorithm to automatically learn from data."
  },
  {
    "objectID": "lecs/w01/lec01.html#types-of-machine-learning-problems",
    "href": "lecs/w01/lec01.html#types-of-machine-learning-problems",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Types of Machine Learning Problems",
    "text": "Types of Machine Learning Problems\n\nSupervised Learning: have labeled examples of the correct behavior, i.e. ground truth input/output response\n\nRegression (e.g. height prediction)\nClassification (e.g. sentiment classification)\n\nUnsupervised Learning: no labeled examples; instead, looking for interesting patterns in the data (e.g. clustering)\nReinforcement Learning: learning system receives a reward signal, tries to learn to maximize the reward signal (e.g. playing StarCraft)"
  },
  {
    "objectID": "lecs/w01/lec01.html#the-machine-learning-approach",
    "href": "lecs/w01/lec01.html#the-machine-learning-approach",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Machine Learning Approach",
    "text": "The Machine Learning Approach\nReframe learning problems into optimization problems by:\n\nChoosing a model (with parameters to be tuned)\nChoosing a loss/cost function to measure the “badness” of a choice of parameters\nChoosing an optimizer to minimize the cost function\n\nDifferent machine learning approaches differ in the model, loss, and optimizer choice.\nThis is why it is important to have a strong foundation in math, specifically calculus, linear algebra, and probability"
  },
  {
    "objectID": "lecs/w01/lec01.html#what-are-neural-networks",
    "href": "lecs/w01/lec01.html#what-are-neural-networks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What are Neural Networks?",
    "text": "What are Neural Networks?\nNeural networks are a class of models originally inspired by the brain.\n\nMost of the biological details aren’t essential, so we use vastly simplified models of neurons.\nNowadays we mostly think about math, statistics, etc\n\n\\[\ny = \\phi \\left(\\bf{w}^\\top \\bf{x} + b\\right)\n\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#what-are-neural-networks-ii",
    "href": "lecs/w01/lec01.html#what-are-neural-networks-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What are Neural Networks? II",
    "text": "What are Neural Networks? II\n\nNowadays we mostly think about math, statistics, etc\n\n\\[\ny = \\phi \\left(\\bf{w}^\\top \\bf{x} + b\\right)\n\\]\n\nNeural networks are collections of thousands (or millions) of these simple processing units that together perform useful computations"
  },
  {
    "objectID": "lecs/w01/lec01.html#why-neural-networks",
    "href": "lecs/w01/lec01.html#why-neural-networks",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Neural Networks?",
    "text": "Why Neural Networks?\n\nVery effective across a range of applications (vision, text, speech, medicine, robotics, etc.)\nWidely used in both academia and the tech industry\nPowerful software frameworks (PyTorch, TensorFlow, JAX, etc.) let us quickly implement sophisticated algorithms"
  },
  {
    "objectID": "lecs/w01/lec01.html#what-is-deep-learning",
    "href": "lecs/w01/lec01.html#what-is-deep-learning",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?\nA “deep” neural network contains many “layers”.\nLater layers use the output of earlier layers as input.\nThe term deep learning emphasizes that the neural network algorithms often involve hierarchies with many stages of processing."
  },
  {
    "objectID": "lecs/w01/lec01.html#deep-learning-caveats-interpretability",
    "href": "lecs/w01/lec01.html#deep-learning-caveats-interpretability",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Deep Learning Caveats: Interpretability",
    "text": "Deep Learning Caveats: Interpretability\n\n\n\nfrom https://xkcd.com/1838/"
  },
  {
    "objectID": "lecs/w01/lec01.html#deep-learning-caveats-adversarial-examples",
    "href": "lecs/w01/lec01.html#deep-learning-caveats-adversarial-examples",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Deep Learning Caveats: Adversarial Examples",
    "text": "Deep Learning Caveats: Adversarial Examples"
  },
  {
    "objectID": "lecs/w01/lec01.html#deep-learning-caveats-fairness",
    "href": "lecs/w01/lec01.html#deep-learning-caveats-fairness",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Deep Learning Caveats: Fairness",
    "text": "Deep Learning Caveats: Fairness"
  },
  {
    "objectID": "lecs/w01/lec01.html#fairness-in-machine-learning",
    "href": "lecs/w01/lec01.html#fairness-in-machine-learning",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Fairness in Machine Learning",
    "text": "Fairness in Machine Learning"
  },
  {
    "objectID": "lecs/w01/lec01.html#course-coverage",
    "href": "lecs/w01/lec01.html#course-coverage",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Course Coverage",
    "text": "Course Coverage\n\nMostly supervised learning\n\nMulti-layer perceptions (week 2-5)\nConvolutional Neural Networks for images (weeks 6-7)\nRecurrent Neural Networks for sequences (weeks 8-9)\n\nSome unsupervised learning\n\nAutoencoders (week 10)\nGenerative Adversarial Networks (week 11)\n\nSpecial topic TBA (week 12)"
  },
  {
    "objectID": "lecs/w01/lec01.html#course-syllabus-scavenger-hunt-breakout-group",
    "href": "lecs/w01/lec01.html#course-syllabus-scavenger-hunt-breakout-group",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Course Syllabus Scavenger Hunt: Breakout Group",
    "text": "Course Syllabus Scavenger Hunt: Breakout Group\n\nWhat textbook (if any) are we using for this course?\nHow much are the assignments worth and what time are they due?\nCan you do complete the assignments in a group?\nWhat happens during tutorials? Where are they held?\nWhat software will we use for this course?"
  },
  {
    "objectID": "lecs/w01/lec01.html#course-syllabus-scavenger-hunt-breakout-group-ii",
    "href": "lecs/w01/lec01.html#course-syllabus-scavenger-hunt-breakout-group-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Course Syllabus Scavenger Hunt: Breakout Group II",
    "text": "Course Syllabus Scavenger Hunt: Breakout Group II\n\nWhat is the late policy in this course? How do grace tokens work?\nWhen is are the tests? How much are they worth?\nIs there an exam? Is there a final project? How does it work?\nWhat is plagiarism and how can you avoid it?"
  },
  {
    "objectID": "lecs/w01/lec01.html#supervised-learning-with-linear-models",
    "href": "lecs/w01/lec01.html#supervised-learning-with-linear-models",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Supervised Learning with Linear Models",
    "text": "Supervised Learning with Linear Models\nOne of the fundamental building blocks in deep learning are linear models, where you decide based on a linear function of the input vector.\nCommon supervised learning problems;\n\nRegression: predict a scalar-valued target (e.g. stock price)\nBinary classification: predict a binary label (e.g. spam vs. non-spam email)\nMulti-class classification: predict a discrete label (e.g. object category, from a list)"
  },
  {
    "objectID": "lecs/w01/lec01.html#problem-setup",
    "href": "lecs/w01/lec01.html#problem-setup",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Problem Setup",
    "text": "Problem Setup\nInput: Represented using the vector \\(\\textbf{x}\\)\n\nExample: \\(\\textbf{x}\\) represents assignment grades (0-100)\nTo start, let’s assume that \\(x\\) is a scalar, and that we only have the cumulative assignment grade\n\nOutput: Represented using the scalar \\(t\\)\n\nExample: \\(t\\) represents the grade on an exam (0-100)\nWe’ll use the scalar \\(y\\) to denote a prediction of the value of \\(t\\)"
  },
  {
    "objectID": "lecs/w01/lec01.html#example-exam-grade-prediction",
    "href": "lecs/w01/lec01.html#example-exam-grade-prediction",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: Exam Grade Prediction",
    "text": "Example: Exam Grade Prediction\n\n\nData: \\((x^{(1)}, t^{(1)}),\\, (x^{(2)}, t^{(2)}),\\, \\ldots , (x^{(N)}, t^{(N)})\\)\nThe \\(x^{(i)}\\) are called inputs\nThe \\(t^{(i)}\\) are called targets"
  },
  {
    "objectID": "lecs/w01/lec01.html#linear-regression-model",
    "href": "lecs/w01/lec01.html#linear-regression-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nA model is a set of assumptions about the underlying nature of the data we wish to learn about. The model, or architecture defines the set of allowed family of hypotheses.\nIn linear regression, our model will look like this\n\\[y = \\sum_j w_j x_j + b\\]\nWhere \\(y\\) is a prediction for \\(t\\), and the \\(w_j\\) and \\(b\\) are parameters of the model, to be determined based on the data."
  },
  {
    "objectID": "lecs/w01/lec01.html#linear-regression-for-exam-grade-prediction",
    "href": "lecs/w01/lec01.html#linear-regression-for-exam-grade-prediction",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Regression for Exam Grade Prediction",
    "text": "Linear Regression for Exam Grade Prediction\nFor the exam prediction problem, we only have a single feature, so we can simplify our model to:\n\\[y = w x + b\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#linear-regression-for-exam-grade-prediction-ii",
    "href": "lecs/w01/lec01.html#linear-regression-for-exam-grade-prediction-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Regression for Exam Grade Prediction II",
    "text": "Linear Regression for Exam Grade Prediction II\nOur hypothesis space includes all functions of the form \\(y = w x + b\\). Here are some examples:\n\n\\(y = 0.4 x + 0.2\\)\n\\(y = 0.9 x + 0.2\\)\n\\(y = 0.1 x + 0.7\\)\n\\(y = - x - 1\\)\n…"
  },
  {
    "objectID": "lecs/w01/lec01.html#linear-regression-for-exam-grade-prediction-iii",
    "href": "lecs/w01/lec01.html#linear-regression-for-exam-grade-prediction-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Regression for Exam Grade Prediction III",
    "text": "Linear Regression for Exam Grade Prediction III\nThe variables \\(w\\) and \\(b\\) are called weights or parameters of our model. (Sometimes \\(w\\) and \\(b\\) are referred to as coefficients and intercept, respectively.)"
  },
  {
    "objectID": "lecs/w01/lec01.html#which-hypothesis-is-better-suited-to-the-data",
    "href": "lecs/w01/lec01.html#which-hypothesis-is-better-suited-to-the-data",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Which Hypothesis is Better Suited to the Data?",
    "text": "Which Hypothesis is Better Suited to the Data?"
  },
  {
    "objectID": "lecs/w01/lec01.html#hypothesis-space",
    "href": "lecs/w01/lec01.html#hypothesis-space",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Hypothesis Space",
    "text": "Hypothesis Space\nWe can visualize the hypothesis space or weight space:\n\nEach point in the weight space represents a hypothesis."
  },
  {
    "objectID": "lecs/w01/lec01.html#cost-function-loss-function",
    "href": "lecs/w01/lec01.html#cost-function-loss-function",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cost Function (Loss Function)",
    "text": "Cost Function (Loss Function)\nThe “badness” of an entire hypothesis is the average badness across our labeled data.\n\\[\\begin{align*}\n\\mathcal{E}(w, b) &= \\frac{1}{N} \\sum_i \\mathcal{L}\\left(y^{(i)}, t^{(i)}\\right) \\\\\n    &= \\frac{1}{2N} \\sum_i \\left(y^{(i)} - t^{(i)}\\right)^2 \\\\\n    &= \\frac{1}{2N} \\sum_i \\left(\\left(w x^{(i)} + b\\right) - t^{(i)}\\right)^2\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#cost-function-loss-function-ii",
    "href": "lecs/w01/lec01.html#cost-function-loss-function-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cost Function (Loss Function) II",
    "text": "Cost Function (Loss Function) II\nThis is called the cost of a particular hypothesis (in practice, “loss” and “cost” functions are used inter-changeably).\nSince the loss depends on the choice of \\(w\\) and \\(b\\), we call \\(\\mathcal{E}(w, b)\\) the cost function."
  },
  {
    "objectID": "lecs/w01/lec01.html#minimize-cost-direct-solution",
    "href": "lecs/w01/lec01.html#minimize-cost-direct-solution",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Minimize Cost: Direct Solution",
    "text": "Minimize Cost: Direct Solution\nFind a critical point by setting \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w} = 0 \\quad \\text{and} \\quad \\frac{\\partial \\mathcal{E}}{\\partial b} = 0\n\\]\nPossible for our hypothesis space, and covered in the notes.\nHowever, let’s use a technique that can also be applied to more general models."
  },
  {
    "objectID": "lecs/w01/lec01.html#minimize-cost-gradient-descent",
    "href": "lecs/w01/lec01.html#minimize-cost-gradient-descent",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Minimize Cost: Gradient Descent",
    "text": "Minimize Cost: Gradient Descent\nWe can use gradient descent to minimize the cost function.\n\\[\n\\textbf{w} \\leftarrow \\textbf{w} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial \\textbf{w}}, \\quad \\text{where }\\,\n\\frac{\\partial \\mathcal{E}}{\\partial \\textbf{w}} = \\begin{bmatrix}\n    \\frac{\\partial \\mathcal{E}}{\\partial w_1} \\\\\n    \\vdots \\\\\n    \\frac{\\partial \\mathcal{E}}{\\partial w_D} \\\\\n\\end{bmatrix}\n\\]\nThe \\(\\alpha\\) is the learning rate, which we choose."
  },
  {
    "objectID": "lecs/w01/lec01.html#gradient-descent-for-grade-prediction",
    "href": "lecs/w01/lec01.html#gradient-descent-for-grade-prediction",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent for Grade Prediction",
    "text": "Gradient Descent for Grade Prediction\n\n\n\nWe’ll initialize \\(w = 0\\) and \\(b = 0\\) (arbitrary choice)\nWe’ll also choose \\(\\alpha = 0.5\\)"
  },
  {
    "objectID": "lecs/w01/lec01.html#gradient-descent-step-0",
    "href": "lecs/w01/lec01.html#gradient-descent-step-0",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent: Step 0",
    "text": "Gradient Descent: Step 0"
  },
  {
    "objectID": "lecs/w01/lec01.html#gradient-descent-step-1",
    "href": "lecs/w01/lec01.html#gradient-descent-step-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent: Step 1",
    "text": "Gradient Descent: Step 1"
  },
  {
    "objectID": "lecs/w01/lec01.html#gradient-descent-step-2",
    "href": "lecs/w01/lec01.html#gradient-descent-step-2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent: Step 2",
    "text": "Gradient Descent: Step 2"
  },
  {
    "objectID": "lecs/w01/lec01.html#gradient-descent-step-3",
    "href": "lecs/w01/lec01.html#gradient-descent-step-3",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent: Step 3",
    "text": "Gradient Descent: Step 3"
  },
  {
    "objectID": "lecs/w01/lec01.html#gradient-descent-step-4",
    "href": "lecs/w01/lec01.html#gradient-descent-step-4",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent: Step 4",
    "text": "Gradient Descent: Step 4"
  },
  {
    "objectID": "lecs/w01/lec01.html#gradient-descent-when-to-stop",
    "href": "lecs/w01/lec01.html#gradient-descent-when-to-stop",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent: When to stop?",
    "text": "Gradient Descent: When to stop?\nIn theory:\n\nStop when \\(w\\) and \\(b\\) stop changing (convergence)\n\nIn practice:\n\nStop when \\(\\mathcal{E}\\) almost stops changing (another notion of convergence)\nStop until we’re tired of waiting"
  },
  {
    "objectID": "lecs/w01/lec01.html#gradient-descent-how-to-choose-the-learning-rate",
    "href": "lecs/w01/lec01.html#gradient-descent-how-to-choose-the-learning-rate",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Descent: how to Choose the Learning Rate?",
    "text": "Gradient Descent: how to Choose the Learning Rate?\n\nIf \\(\\alpha\\) is too small, then training will be slow\n\nTake a long time to converge\n\nIf \\(\\alpha\\) is too large, then we can have divergence!\n\nTake a long time to converge"
  },
  {
    "objectID": "lecs/w01/lec01.html#computing-the-gradient",
    "href": "lecs/w01/lec01.html#computing-the-gradient",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing the Gradient",
    "text": "Computing the Gradient\nTo compute the gradient \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\)\n\\[\n\\frac{\\partial \\mathcal{E}}{\\partial w} = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{\\partial \\mathcal{L}(y^{(i)},  t^{(i)})}{\\partial w}\n\\]\nBut this computation can be expensive if \\(N\\) is large!\n\nSolution: estimate \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\) using a subset of the data"
  },
  {
    "objectID": "lecs/w01/lec01.html#stochastic-gradient-descent",
    "href": "lecs/w01/lec01.html#stochastic-gradient-descent",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nFull batch gradient descent:\n\\[\n\\frac{1}{N}\\sum_{i=1}^{N} \\frac{\\partial \\mathcal{L}(y^{(i)},  t^{(i)})}{\\partial w}\n\\]\nStochastic Gradient Descent:\nEstimate the above quantity by computing the average of \\(\\frac{\\partial \\mathcal{L}(y^{(i)},  t^{(i)})}{\\partial w}\\) across a small number of \\(i\\)’s"
  },
  {
    "objectID": "lecs/w01/lec01.html#stochastic-gradient-descent-ii",
    "href": "lecs/w01/lec01.html#stochastic-gradient-descent-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Stochastic Gradient Descent II",
    "text": "Stochastic Gradient Descent II\nStochastic Gradient Descent:\nEstimate the above quantity by computing the average of \\(\\frac{\\partial \\mathcal{L}(y^{(i)},  t^{(i)})}{\\partial w}\\) across a small number of \\(i\\)’s\nThe set of examples that we use to estimate the gradient is called a mini-batch.\nThe number of examples in each mini-batch is called the mini-batch size or just the batch size"
  },
  {
    "objectID": "lecs/w01/lec01.html#stochastic-gradient-descent-algorithm",
    "href": "lecs/w01/lec01.html#stochastic-gradient-descent-algorithm",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Stochastic Gradient Descent Algorithm",
    "text": "Stochastic Gradient Descent Algorithm\nIn theory, any way of sampling a mini-batch is okay.\nIn practice, SGD is almost always implemented like this:\n# repeat until convergence:\n    # randomly split the data into mini-batches of size k\n    # for each mini-batch:\n        # estimate the gradient using the mini-batch\n        # update the parameters based on the estimate\n\n\nEach pass of the inner loop is called an iteration.\n\nOne iteration = one update for each weight\n\nEach pass of the outer loop is called an epoch.\n\nOne epoch = one pass over the data set"
  },
  {
    "objectID": "lecs/w01/lec01.html#iterations-epochs-and-batch-size",
    "href": "lecs/w01/lec01.html#iterations-epochs-and-batch-size",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Iterations, Epochs, and Batch Size",
    "text": "Iterations, Epochs, and Batch Size\nSuppose we have 1000 examples in our training set.\nQ: How many iterations are in one epoch if our batch size is 10?\n\nQ: How many iterations are in one epoch if our batch size is 50?"
  },
  {
    "objectID": "lecs/w01/lec01.html#batch-size-choice",
    "href": "lecs/w01/lec01.html#batch-size-choice",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Batch Size Choice",
    "text": "Batch Size Choice\nQ: What happens if the batch size is too large?\nQ: What happens if the batch size is too small?"
  },
  {
    "objectID": "lecs/w01/lec01.html#linear-regression-summary",
    "href": "lecs/w01/lec01.html#linear-regression-summary",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Regression Summary",
    "text": "Linear Regression Summary\n\n\n\n\n\n\n\nModel\n\\[y = {\\bf w}^T{\\bf x} + b\\]\n\n\nLoss Function\n\\[\\mathcal{L}(y, t) = (y- t)^2\\]\n\n\nOptimization Method\n\\[\\min_{{\\bf w},\\, b}\\left(\\left\\{\\mathcal{E}({\\bf w}, b)\\right\\}\\right) \\text{ via Gradient Descent}\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#linear-regression-summary-ii",
    "href": "lecs/w01/lec01.html#linear-regression-summary-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Regression Summary II",
    "text": "Linear Regression Summary II\nUpdating rules:\n\\[\n{\\bf w} \\leftarrow {\\bf w} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial {\\bf w}}, \\quad b \\leftarrow b  - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial b}\n\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#linear-regression-vectorization",
    "href": "lecs/w01/lec01.html#linear-regression-vectorization",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Regression Vectorization",
    "text": "Linear Regression Vectorization\nUse vectors rather than writing\n\\[\n\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_{i = 1}^N \\left(\\left({\\bf w} {\\bf x}^{(i)} + b\\right) - t^{(i)}\\right)^2\n\\]\nSo we have: \\[\n\\textbf{y} = \\textbf{X}\\textbf{w} + b{\\bf 1}\n\\]\nwhere…"
  },
  {
    "objectID": "lecs/w01/lec01.html#linear-regression-vectorization-ii",
    "href": "lecs/w01/lec01.html#linear-regression-vectorization-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Linear Regression Vectorization II",
    "text": "Linear Regression Vectorization II\n\\[\\begin{align*}\n\\textbf{X} &=\n\\begin{bmatrix}\n    x^{(1)}_1 & ... & x^{(1)}_D \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    x^{(N)}_1 & ... & x^{(N)}_D\n\\end{bmatrix},\n\\,\n\\textbf{w} =\n\\begin{bmatrix}\nw_1 \\\\\n\\vdots \\\\\nw_D \\\\\n\\end{bmatrix}, \\,\n\\textbf{y} =\n\\begin{bmatrix}\n    y^{(1)} \\\\\n    \\vdots \\\\\n    y^{(N)} \\\\\n\\end{bmatrix},\n\\,\n\\textbf{t} =\n\\begin{bmatrix}\n    t^{(1)} \\\\\n    \\vdots \\\\\n    t^{(N)} \\\\\n\\end{bmatrix}\n\\end{align*}\\]\n(You can also fold the bias \\(b\\) into the weight , but we won’t.)"
  },
  {
    "objectID": "lecs/w01/lec01.html#vectorized-loss-function",
    "href": "lecs/w01/lec01.html#vectorized-loss-function",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Vectorized Loss Function",
    "text": "Vectorized Loss Function\nAfter vectorization, the loss function becomes:\n\\[\n\\mathcal{E}(\\textbf{w}) = \\frac{1}{2N}(\\textbf{y} - \\textbf{t})^\\top(\\textbf{y} - \\textbf{t})\n\\]\nor\n\\[\n\\mathcal{E}(\\textbf{w}) = \\frac{1}{2N}({\\bf Xw} + b{\\bf 1}- {\\bf t})^\\top({\\bf Xw} + b{\\bf 1}- {\\bf t})\n\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#vectorized-gradient-descent",
    "href": "lecs/w01/lec01.html#vectorized-gradient-descent",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Vectorized Gradient Descent",
    "text": "Vectorized Gradient Descent\n\\[\n{\\bf w} \\leftarrow {\\bf w} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial {\\bf w}}, \\quad b \\leftarrow b  - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial b}\n\\] Where \\(\\frac{\\partial \\mathcal{E}}{\\partial \\textbf{w}}\\) is the vector of partial derivatives: \\[\\begin{align*}\n\\frac{\\partial \\mathcal{E}}{\\partial \\textbf{w}} = \\begin{bmatrix}\n    \\frac{\\partial \\mathcal{E}}{\\partial w_1} \\\\\n    ... \\\\\n    \\frac{\\partial \\mathcal{E}}{\\partial w_D} \\\\\n\\end{bmatrix}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#why-vectorize",
    "href": "lecs/w01/lec01.html#why-vectorize",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Vectorize?",
    "text": "Why Vectorize?\nVectorization is not just for mathematical elegance.\nWhen using Python with numpy/PyTorch/Tensorflow/JAX, code that performs vector computations is faster than code that loops.\nSame holds for many other high level languages and software."
  },
  {
    "objectID": "lecs/w01/lec01.html#classification-setup",
    "href": "lecs/w01/lec01.html#classification-setup",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Classification Setup",
    "text": "Classification Setup\n\nData: \\((x^{(1)}, t^{(1)})\\), \\((x^{(2)}, t^{(2)})\\), … \\((x^{(N)}, t^{(N)})\\)\nThe \\(x^{(i)}\\) are called inputs\nThe \\(t^{(i)}\\) are called targets\n\nIn classification, the \\(t^{(i)}\\) are discrete.\nIn binary classification, we’ll use the labels \\(t \\in \\{0, 1\\}\\). Training examples with\n\n\\(t = 1\\) is called a positive example\n\\(t = 0\\) is called a negative example"
  },
  {
    "objectID": "lecs/w01/lec01.html#classification-example",
    "href": "lecs/w01/lec01.html#classification-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Classification Example",
    "text": "Classification Example\n\n\\(x^{(i)}\\) represents a person’s assignment grade\n\\(t^{(i)}\\) represents whether that person had a “high” exam grade (arbitrary cutoff)"
  },
  {
    "objectID": "lecs/w01/lec01.html#q-why-not-use-regression",
    "href": "lecs/w01/lec01.html#q-why-not-use-regression",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Q: Why not Use Regression?",
    "text": "Q: Why not Use Regression?\nWhy can’t we set up this problem as a regression problem?\nUse the model:\n\\[\ny = wx + b\n\\]\nOur prediction for \\(t\\) would be \\(1\\) if \\(y \\geq 0.5\\), and \\(0\\) otherwise.\n\nWith the loss function\n\\[\\mathcal{L}(y, t) = \\frac{1}{2}(y - t)^2\\]\nAnd minimize the cost function via gradient descent?"
  },
  {
    "objectID": "lecs/w01/lec01.html#classification-as-regression-problem",
    "href": "lecs/w01/lec01.html#classification-as-regression-problem",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Classification as Regression: Problem",
    "text": "Classification as Regression: Problem\n\n\n\nIf we have \\(\\mathcal{L}(y, t) = \\frac{1}{2}(y - t)^2\\), then points that are correctly classified will still have high loss!\n(blue dotted line above = decision boundary)"
  },
  {
    "objectID": "lecs/w01/lec01.html#the-problem-continued",
    "href": "lecs/w01/lec01.html#the-problem-continued",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Problem (continued)",
    "text": "The Problem (continued)\n\n\n\n\nExample: a point on the top right\n\nModel makes the correct prediction for point on top right\nHowever, \\((y - t)^2\\) is large\nSo we are penalizing our model, even though it is making the right prediction!"
  },
  {
    "objectID": "lecs/w01/lec01.html#q-why-not-use-classification-error",
    "href": "lecs/w01/lec01.html#q-why-not-use-classification-error",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Q: Why not Use Classification Error?",
    "text": "Q: Why not Use Classification Error?\nWhy not still use the model:\n\\[\ny = \\begin{cases} 1, & \\text{ if } \\mathbf{w}^\\top\\mathbf{x} + b &gt; 0 \\\\ 0, & \\text{ otherwise }\\end{cases}\n\\]\nBut use this loss function instead:\n\\[\n\\mathcal{L}(y, t) = \\begin{cases} 0, & \\text{ if } y = t \\\\ 1, & \\text{ otherwise }\\end{cases}\n\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#first-attempt-at-a-loss-function-the-0-1-loss",
    "href": "lecs/w01/lec01.html#first-attempt-at-a-loss-function-the-0-1-loss",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "First Attempt at a Loss Function: The 0-1 Loss",
    "text": "First Attempt at a Loss Function: The 0-1 Loss\n\\[\n\\mathcal{L}(y, t) = \\begin{cases} 0, & \\text{ if } y = t \\\\ 1, & \\text{ otherwise }\\end{cases}\n\\]\nThe gradient of this function is 0 almost everywhere!\nSo gradient descent will not change the weights! We need to define a surrogate loss function that is better behaved."
  },
  {
    "objectID": "lecs/w01/lec01.html#logistic-regression-model",
    "href": "lecs/w01/lec01.html#logistic-regression-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\nApply a non-linearity or activation function to the linear model \\(z\\):\n\\[\\begin{align*}\nz &= wx + b \\quad \\quad \\text{also called the logit}\\\\\ny &= \\sigma(z) \\quad \\quad \\text{also called a log-linear model}\n\\end{align*}\\]\nwhere\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\nis called the logistic or sigmoid function. Using the model \\(y\\) for solving a classification problem is called logistic regression."
  },
  {
    "objectID": "lecs/w01/lec01.html#the-sigmoid-function",
    "href": "lecs/w01/lec01.html#the-sigmoid-function",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Sigmoid Function",
    "text": "The Sigmoid Function\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\n\n\n\n\nProperties:\n\n\\(\\sigma(z) \\in [0;\\; 1]\\)\n\\(\\sigma(0) = \\frac{1}{2}\\)"
  },
  {
    "objectID": "lecs/w01/lec01.html#logistic-regression-example",
    "href": "lecs/w01/lec01.html#logistic-regression-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Logistic Regression Example",
    "text": "Logistic Regression Example\nA logistic regression model will have this shape:\n\n\n\nBut how do we train this model?"
  },
  {
    "objectID": "lecs/w01/lec01.html#logistic-regression-square-loss",
    "href": "lecs/w01/lec01.html#logistic-regression-square-loss",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Logistic Regression: Square Loss?",
    "text": "Logistic Regression: Square Loss?\nSuppose we define the model like this:\n\\[\\begin{align*}\nz &= wx + b \\\\\ny &= \\sigma(z) \\\\\n\\mathcal{L}_{SE}(y, t) &= \\frac{1}{2}(y - t)^2\n\\end{align*}\\]\nThe gradient of \\(\\mathcal{L}\\) with respect to \\(w\\) is (homework):\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial w} =\n    \\frac{\\partial \\mathcal{L}}{\\partial y}\n    \\frac{dy}{dz}\n    \\frac{\\partial z}{\\partial w} = (y - t) y (1 - y) x\n\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#the-problem-with-square-loss",
    "href": "lecs/w01/lec01.html#the-problem-with-square-loss",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Problem with Square Loss",
    "text": "The Problem with Square Loss\nSuppose we have a positive example (\\(t = 1\\)) that our model classifies extremely wrongly (\\(z = -5\\)):\nThen we have \\(y = \\sigma(z) \\approx 0.0067\\)\n\nIdeally, the gradient should give us strong signals regarding how to update \\(w\\) to do better.\nBut… \\(\\frac{\\partial \\mathcal{L}}{\\partial w} = (y - t) y (1- y) x\\) is small!\n\n\nWhich means that the update \\(w \\leftarrow w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}\\) won’t change \\(w\\) much!"
  },
  {
    "objectID": "lecs/w01/lec01.html#gradient-signal",
    "href": "lecs/w01/lec01.html#gradient-signal",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Gradient Signal",
    "text": "Gradient Signal\nThe problem with using sigmoid activation with square loss is that we get poor gradient signal.\n\n\n\n\n\nThe loss for a very wrong prediction (\\(y=0.0001\\)) vs a wrong prediction (\\(y=0.01\\)) are similar\nThis is a problem, because the gradients in the region would be close to 0\n\n\nWe need a loss function that distinguishes between a wrong prediction and a very wrong prediction."
  },
  {
    "objectID": "lecs/w01/lec01.html#the-cross-entropy-loss",
    "href": "lecs/w01/lec01.html#the-cross-entropy-loss",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Cross Entropy Loss",
    "text": "The Cross Entropy Loss\nThe cross entropy loss provides the desired behaviour:\n\\[\n\\mathcal{L}(y, t) = \\begin{cases} -\\log(y), & \\text{if } t = 1  \\\\ -\\log(1 - y), & \\text{ if } t = 0\\end{cases}\n\\]\nWe can write the loss as:\n\\[\\mathcal{L}(y, t) = - t \\log(y) - (1-t) \\log(1-y)\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#logistic-regression-summary",
    "href": "lecs/w01/lec01.html#logistic-regression-summary",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Logistic Regression Summary",
    "text": "Logistic Regression Summary\n\n\n\n\n\n\n\nModel\n\\[y = \\sigma({\\bf w}^T{\\bf x} + b)\\]\n\n\nLoss Function\n\\[\\mathcal{L}(y, t) = -t \\log(y) - (1-t) \\log(1-y)\\]\n\n\nOptimization Method\n\\[\\min_{{\\bf w},\\, b}\\mathcal{E}({\\bf w}, b) \\text{ via Gradient Descent}\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#logistic-regression-summary-ii",
    "href": "lecs/w01/lec01.html#logistic-regression-summary-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Logistic Regression Summary II",
    "text": "Logistic Regression Summary II\nUpdating rules:\n\\[\n{\\bf w} \\leftarrow {\\bf w} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial {\\bf w}}, \\quad b \\leftarrow b  - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial b}\n\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#grade-classification-example",
    "href": "lecs/w01/lec01.html#grade-classification-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Grade Classification Example",
    "text": "Grade Classification Example\nAfter running gradient descent, we’ll get a model that looks something like:"
  },
  {
    "objectID": "lecs/w01/lec01.html#multi-class-classification-1",
    "href": "lecs/w01/lec01.html#multi-class-classification-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multi-class Classification",
    "text": "Multi-class Classification\nInstead of there being two targets (pass/fail, cancer/not cancer, before/after 2000), we have \\(K &gt; 2\\) targets.\nExample:\n\nBeatles (\\(K\\) = 4):\n\nJohn Lennon, Paul McCartney, George Harrison, Ringo Starr\n\nPets (\\(K\\) = something large):\n\ncat, dog, hamster, parrot, python, …"
  },
  {
    "objectID": "lecs/w01/lec01.html#representing-the-targets",
    "href": "lecs/w01/lec01.html#representing-the-targets",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Representing the Targets",
    "text": "Representing the Targets\nWe use a one-hot vector to represent the target:\n\\[{\\bf t} = (0,\\, 0,\\, \\ldots ,\\, 1,\\, \\ldots ,\\, 0)\\]\nThis vector contains \\(K-1\\) zeros, and a single 1 somewhere.\nEach index (column) in the vector represents one of the classes."
  },
  {
    "objectID": "lecs/w01/lec01.html#representing-the-prediction",
    "href": "lecs/w01/lec01.html#representing-the-prediction",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Representing the Prediction",
    "text": "Representing the Prediction\nThe prediction \\({\\bf y}\\) will also be a vector. Like in logistic regression there will be a linear part, and an activation function.\nLinear part: \\({\\bf z} = {\\bf W}^\\top{\\bf x} + {\\bf b}\\)\nSo far, this is like having \\(K\\) separate logistic regression models, one for each element of the one-hot vector.\nQ: What are the shapes of \\({\\bf z}\\), \\({\\bf W}\\), \\({\\bf x}\\) and \\({\\bf b}\\)?"
  },
  {
    "objectID": "lecs/w01/lec01.html#activation-function",
    "href": "lecs/w01/lec01.html#activation-function",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Activation Function",
    "text": "Activation Function\nInstead of using a sigmoid function, we instead use a softmax activation function:\n\\[y_k = \\text{softmax}(z_1,...,z_K)_k = \\frac{e^{z_k}}{\\sum_{m=1}^K e^{z_m}}\\]\nThe vector of predictions \\(y_k\\) is now a probability distribution over the classes!"
  },
  {
    "objectID": "lecs/w01/lec01.html#why-softmax",
    "href": "lecs/w01/lec01.html#why-softmax",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Why Softmax?",
    "text": "Why Softmax?\n\nSoftmax is like the multi-class equivalent of sigmoid \nSoftmax is a continuous analog of the “argmax” function\nIf one of the \\(z_k\\) is much larger than the other, then the softmax will be approximately the argmax, in the one-hot encoding"
  },
  {
    "objectID": "lecs/w01/lec01.html#cross-entropy-loss",
    "href": "lecs/w01/lec01.html#cross-entropy-loss",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Cross-Entropy Loss",
    "text": "Cross-Entropy Loss\nThe cross-entropy loss naturally generalizes to the multi-class case:\n\\[\\begin{align*}\n\\mathcal{L}({\\bf y}, {\\bf t}) &= -\\sum_{k=1}^K t_k \\log (y_k) \\\\\n&= - {\\bf t}^\\top \\log({\\bf y})\n\\end{align*}\\]\nRecall that only one of the \\(t_k\\) is going to be 1, and the rest are 0."
  },
  {
    "objectID": "lecs/w01/lec01.html#multi-class-classification-summary",
    "href": "lecs/w01/lec01.html#multi-class-classification-summary",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multi-class Classification Summary",
    "text": "Multi-class Classification Summary\n\n\n\n\n\n\n\nModel\n\\[{\\bf y} = \\text{softmax}({\\bf W}^T{\\bf x} + {\\bf b})\\]\n\n\nLoss Function\n\\[\\mathcal{L}({\\bf y}, {\\bf t}) =- {\\bf t}^T \\log({\\bf y})\\]\n\n\nOptimization Method\n\\[\\min_{{\\bf w},\\, b}\\mathcal{E}({\\bf w}, b) \\text{ via Gradient Descent}\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#multi-class-classification-summary-ii",
    "href": "lecs/w01/lec01.html#multi-class-classification-summary-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Multi-class Classification Summary II",
    "text": "Multi-class Classification Summary II\nUpdating rules:\n\\[\n{\\bf W} \\leftarrow {\\bf W} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial {\\bf W}}, \\quad {\\bf b} \\leftarrow {\\bf b}  - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial {\\bf b}}\n\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#example-beatle-recognition",
    "href": "lecs/w01/lec01.html#example-beatle-recognition",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Example: Beatle Recognition",
    "text": "Example: Beatle Recognition\nGiven a \\(100 \\times 100\\) pixel colour image of a face of a Beatle, identify the Beatle\n\nFour possible labels:\n\nJohn Lennon\nPaul McCartney\nGeorge Harrison\nRingo Starr"
  },
  {
    "objectID": "lecs/w01/lec01.html#aside-representing-an-image",
    "href": "lecs/w01/lec01.html#aside-representing-an-image",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Aside: Representing an Image",
    "text": "Aside: Representing an Image\nThis is what John Lennon looks like to a computer:"
  },
  {
    "objectID": "lecs/w01/lec01.html#image-as-a-vector-of-features",
    "href": "lecs/w01/lec01.html#image-as-a-vector-of-features",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Image as a Vector of Features",
    "text": "Image as a Vector of Features"
  },
  {
    "objectID": "lecs/w01/lec01.html#features-and-targets",
    "href": "lecs/w01/lec01.html#features-and-targets",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Features and Targets",
    "text": "Features and Targets\nEach of our input images are \\(100 \\times 100\\) pixels\n\\({\\bf y} = \\text{softmax}\\left({\\bf W}^\\top{\\bf x} + {\\bf b}\\right)\\)\nQ: What will be the length of our input (feature) vectors \\({\\bf x}\\)?\nQ: What will be the length of our one-hot targets \\({\\bf t}\\)?\nQ: What are the shapes of \\({\\bf W}\\) and \\({\\bf b}\\)?\nQ: How many (scalar) parameters are in our model, in total?"
  },
  {
    "objectID": "lecs/w12/lec12.html#announcements",
    "href": "lecs/w12/lec12.html#announcements",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Announcements",
    "text": "Announcements\n\nBook a meeting to discuss your project with an instructor if you haven’t already!\n\n\n\nYou should have an overfit version of the model already\n\n\n\n\nThis is your first step, and should let you know the time/memory requirements of your model"
  },
  {
    "objectID": "lecs/w12/lec12.html#review-autoencoders",
    "href": "lecs/w12/lec12.html#review-autoencoders",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Review: Autoencoders",
    "text": "Review: Autoencoders\nA couple of weeks ago, we discussed the autoencoders\n\nEncoder: maps \\(x\\) to a low-dimensional embedding \\(z\\)\n\n\n\nDecoder: uses the low-dimensional embedding \\(z\\) to reconstructs \\(x\\)\n\n\n\n\n\n\n\nLet’s see how much you remember!"
  },
  {
    "objectID": "lecs/w12/lec12.html#review-q1",
    "href": "lecs/w12/lec12.html#review-q1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Review Q1",
    "text": "Review Q1\nWhat was the objective that we used to train the autoencoder?"
  },
  {
    "objectID": "lecs/w12/lec12.html#review-q2",
    "href": "lecs/w12/lec12.html#review-q2",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Review Q2",
    "text": "Review Q2\nIf we train an autoencoder, what tasks can we accomplish with just the encoder portion of the autoencoder?"
  },
  {
    "objectID": "lecs/w12/lec12.html#review-q3",
    "href": "lecs/w12/lec12.html#review-q3",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Review Q3",
    "text": "Review Q3\nIf we train an autoencoder, what tasks can we accomplish with mainly the decoder portion of the autoencoder?"
  },
  {
    "objectID": "lecs/w12/lec12.html#review-q4",
    "href": "lecs/w12/lec12.html#review-q4",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Review Q4",
    "text": "Review Q4\nWhat are some limitations of the autoencoder?"
  },
  {
    "objectID": "lecs/w12/lec12.html#autoencoder-limitations",
    "href": "lecs/w12/lec12.html#autoencoder-limitations",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Autoencoder Limitations",
    "text": "Autoencoder Limitations\n\nImages are blurry due to the use of MSE loss\n\n\n\nIt’s not certain what good values of embeddings \\(z\\) would be\n\nWhich part of the embedding space does the encoder maps data to?\nThis uncertainty means that we can’t generate images without referring back to the encoder\n\n\n\n\n\nIt’s not clear what the dimension of the embedding \\(z\\) should be"
  },
  {
    "objectID": "lecs/w12/lec12.html#autoencoder-limitations-ii",
    "href": "lecs/w12/lec12.html#autoencoder-limitations-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Autoencoder Limitations II",
    "text": "Autoencoder Limitations II\nCould we resolve (but not all) some of the issues with autoencoder, if we use a more theoretically grounded approach?\nIs there a probabilistic version of the autoencoder model?"
  },
  {
    "objectID": "lecs/w12/lec12.html#generative-model",
    "href": "lecs/w12/lec12.html#generative-model",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Generative Model",
    "text": "Generative Model\nIn CSC311, we learned about generative models that describe the distribution that the data comes from\n\n\ni.e. describe the distribution \\({\\bf x} \\sim p({\\bf x})\\), where \\({\\bf x}\\) is a single data point\n\n\n\nFor example, in the Naive Bayes model for data \\({\\bf x}\\) (e.g. bag-of-word encoding of an email, which could be spam or not spam) with \\({\\bf x} \\sim p({\\bf x})\\), we assumed that \\(p({\\bf x}) = \\sum_c p({\\bf x}|c)p(c)\\), where \\(c\\) is either spam or not spam. We made further assumptions about \\(p({\\bf x}|c)\\), e.g. that each \\(x_i\\) is an independent Bernoulli."
  },
  {
    "objectID": "lecs/w12/lec12.html#mathematical-notation-and-assumptions",
    "href": "lecs/w12/lec12.html#mathematical-notation-and-assumptions",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Mathematical Notation and Assumptions",
    "text": "Mathematical Notation and Assumptions\nData \\(x_i \\in \\mathbb{R}^d\\) are:\n\nindependent, identically distributed (i.i.d)\n\n\n\ngenerated from the following joint distribution (with the true parameter \\(\\theta^{*}\\) unknown)\n\n\\[p_{\\theta^{*}}(\\textbf{z}, \\textbf{x}) = p_{\\theta^{*}}(\\textbf{z})p_{\\theta^{*}}(\\textbf{x} | \\textbf{z})\\]\nWhere \\({\\bf z}\\) is a low-dimensional vector (latent embedding)"
  },
  {
    "objectID": "lecs/w12/lec12.html#mathematical-notation-and-assumptions-ii",
    "href": "lecs/w12/lec12.html#mathematical-notation-and-assumptions-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Mathematical Notation and Assumptions II",
    "text": "Mathematical Notation and Assumptions II\n\nExample \\({\\bf x}\\) could be an MNIST digit\n\n\n\nThink of \\({\\bf z}\\) as encoding digit features like digit shape, tilt, line thickness, font style, etc…\n\n\n\n\nTo generate an image, we first sample from the prior distribution \\(p_{\\theta^{*}}(\\textbf{z})\\) to decide on these digit features, and use \\(p_{\\theta^{*}}(\\textbf{x} | \\textbf{z})\\) to generate an image given those features"
  },
  {
    "objectID": "lecs/w12/lec12.html#intractability",
    "href": "lecs/w12/lec12.html#intractability",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Intractability",
    "text": "Intractability\nOur data set is large, and so the following are intractable\n\nevidence \\(p_{\\theta^{*}}(\\textbf{x})\\)\n\n\n\nposterior distributions \\(p_{\\theta^{*}}(\\textbf{z} | \\textbf{x})\\)\n\nIn other words, exactly computing the distribution of \\(p(\\textbf{x})\\) and \\(p(\\textbf{z} | \\textbf{x})\\) using our dataset has high runtime complexity."
  },
  {
    "objectID": "lecs/w12/lec12.html#the-decoder-and-encoder",
    "href": "lecs/w12/lec12.html#the-decoder-and-encoder",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Decoder and Encoder",
    "text": "The Decoder and Encoder\n\n\n\n\n\n\nWith this assumption, we can think of the autoencoder as doing the following:\n\nDecoder: A point approximation of the true distribution \\(p_{\\theta^{*}}(\\textbf{x}|\\textbf{z})\\)\nEncoder: Making a point prediction for the value of the latent vector \\(z\\) that generated the image \\(x\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#the-decoder-and-encoder-ii",
    "href": "lecs/w12/lec12.html#the-decoder-and-encoder-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Decoder and Encoder II",
    "text": "The Decoder and Encoder II\nAlternative:\n\nwhat if, instead, we try to infer the distribution \\(p_{\\theta^{*}}(\\textbf{z}|\\textbf{x})\\)?"
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-setup-so-far",
    "href": "lecs/w12/lec12.html#vae-setup-so-far",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE Setup so far",
    "text": "VAE Setup so far\nDecoder: An approximation of the true distribution \\(p_{\\theta^{*}}(\\textbf{x}|\\textbf{z})\\)\nEncoder: An approximation of the true distribution \\(p_{\\theta^{*}}(\\textbf{z}|\\textbf{x})\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#computing-the-encoding-distribution-p_thetatextbfztextbfx",
    "href": "lecs/w12/lec12.html#computing-the-encoding-distribution-p_thetatextbfztextbfx",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing the encoding distribution \\(p_{\\theta^{*}}(\\textbf{z}|\\textbf{x})\\)",
    "text": "Computing the encoding distribution \\(p_{\\theta^{*}}(\\textbf{z}|\\textbf{x})\\)\nUnfortunately, the true distribution \\(p_{\\theta^{*}}({\\bf z}|{\\bf x})\\) is complex (e.g. can be multi-modal).\nBut can we approximate this distribution with a simpler distribution?\nLet’s restrict our estimate \\(q_\\phi({\\bf z}|{\\bf x}) = \\mathcal{N}({\\bf z}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) to be a multivariate Gaussian distribution with \\(\\phi = (\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#computing-the-encoding-distribution-p_thetatextbfztextbfx-ii",
    "href": "lecs/w12/lec12.html#computing-the-encoding-distribution-p_thetatextbfztextbfx-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing the encoding distribution \\(p_{\\theta^{*}}(\\textbf{z}|\\textbf{x})\\) II",
    "text": "Computing the encoding distribution \\(p_{\\theta^{*}}(\\textbf{z}|\\textbf{x})\\) II\n\nIt suffices to estimate the mean \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\) of \\(q_\\phi({\\bf z}|{\\bf x})\\)\nLet’s make it simpler and assume that the covariance matrix is diagonal, \\(\\boldsymbol{\\Sigma}=\\sigma^2 \\textbf{I}_{d \\times d}\\)\n\n(Note: we don’t have to make this assumption, but it will make computation easier later on)"
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-setup-so-far-1",
    "href": "lecs/w12/lec12.html#vae-setup-so-far-1",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE Setup so far",
    "text": "VAE Setup so far\nDecoder: An approximation of the true distribution \\(p_{\\theta^{*}}(\\textbf{x}|\\textbf{z})\\)\nEncoder: Predicts the mean and standard deviations of a distribution \\(q_\\phi({\\bf z}|{\\bf x})\\), so that the distribution is close to the true distribution \\(p_{\\theta^{*}}(\\textbf{z}|\\textbf{x})\\)\nWe want our estimate distribution to be close to the true distribution. How do we measure the difference between distributions?"
  },
  {
    "objectID": "lecs/w12/lec12.html#discrete-entropy",
    "href": "lecs/w12/lec12.html#discrete-entropy",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "(Discrete) Entropy",
    "text": "(Discrete) Entropy\n\\[H[X] = \\sum_x p(X = x) \\log \\left(\\frac{1}{p(X = x)}\\right) = \\text{E}\\left[\\log \\frac{1}{p(X)}\\right]\\]\nMany ways to think about this quantity:\n\nThe expected number of yes/no questions you would need to ask to correctly predict the next symbol sampled from distribution \\(p(X)\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#discrete-entropy-ii",
    "href": "lecs/w12/lec12.html#discrete-entropy-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "(Discrete) Entropy II",
    "text": "(Discrete) Entropy II\n\nThe expected “surprise” or “information” in the possible outcomes of random variable \\(X\\)\n\n\n\nThe minimum number of bits required to compress a symbol \\(x\\) sampled from distribution \\(p(X)\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#discrete-entropy-of-a-coin-flip",
    "href": "lecs/w12/lec12.html#discrete-entropy-of-a-coin-flip",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "(Discrete) Entropy of a Coin Flip",
    "text": "(Discrete) Entropy of a Coin Flip\n\n\n\n\n\n\n\nEntropy of a fair coin flip is \\(0.5\\log(2) + 0.5\\log(2) = \\log(2) = 1\\) bits\nEntropy of a fair dice is \\(\\log(6) = 2.58\\) bits\n\n\n\n\nEntropy of characters in English words is about 2.62 bits\n\n\n\n\nEntropy of characters from the English alphabet selected uniformly at random is \\(\\log(26) = 4.7\\) bits"
  },
  {
    "objectID": "lecs/w12/lec12.html#kullback-leibler-divergence",
    "href": "lecs/w12/lec12.html#kullback-leibler-divergence",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Kullback-Leibler Divergence",
    "text": "Kullback-Leibler Divergence\nAlso called: KL Divergence, Relative Entropy\nFor discrete probability distributions:\n\\[D_\\text{KL}(q(z) ~||~ p(z)) = \\sum_z q(z) \\log \\left(\\frac{q(z)}{p(z)}\\right)\\]\nFor continuous probability distributions:\n\\[D_\\text{KL}(q(z) ~||~ p(z)) = \\int q(z) \\log \\left(\\frac{q(z)}{p(z)}\\right)\\, dz\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-example-computation",
    "href": "lecs/w12/lec12.html#kl-divergence-example-computation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence Example Computation",
    "text": "KL Divergence Example Computation\nApproximating an unfair coin with a fair coin.\n\n\\(p(z = 1) = 0.7\\) and \\(p(z = 0) = 0.3\\)\n\n\n\n\\(q(z = 1) = q(z = 0) = 0.5\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-example-computation-ii",
    "href": "lecs/w12/lec12.html#kl-divergence-example-computation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence Example Computation II",
    "text": "KL Divergence Example Computation II\n\\[\\begin{align*}\nD_\\text{KL}(q(z) ~||~ p(z)) &= \\sum_z q(z) \\log \\left(\\frac{q(z)}{p(z)}\\right) \\\\\n&= q(0) \\log \\left(\\frac{q(0)}{p(0)}\\right) +  q(1) \\log \\left(\\frac{q(1)}{p(1)}\\right) \\\\\n&= 0.5 \\log \\left(\\frac{0.5}{0.3}\\right) +  0.5 \\log \\left(\\frac{0.5}{0.7}\\right) \\\\\n&= 0.872\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-is-not-symmetric",
    "href": "lecs/w12/lec12.html#kl-divergence-is-not-symmetric",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence is not Symmetric!",
    "text": "KL Divergence is not Symmetric!\nApproximating a fair coin with an unfair coin.\n\n\\(p(z = 1) = 0.7\\) and \\(p(z = 0) = 0.3\\)\n\n\n\n\\(q(z = 1) = q(z = 0) = 0.5\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-is-not-symmetric-ii",
    "href": "lecs/w12/lec12.html#kl-divergence-is-not-symmetric-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence is not Symmetric! II",
    "text": "KL Divergence is not Symmetric! II\n\\[\\begin{align*}\nD_\\text{KL}(p(z) ~||~ q(z)) &= \\sum_z p(z) \\log \\left(\\frac{p(z)}{q(z)}\\right) \\\\\n&= p(0) \\log \\left(\\frac{p(0)}{q(0)}\\right) +  p(1) \\log \\left(\\frac{p(1)}{q(1)}\\right) \\\\\n&= 0.3 \\log \\left(\\frac{0.3}{0.5}\\right) +  0.7 \\log \\left(\\frac{0.7}{0.5}\\right) \\\\\n&= 0.823 \\\\\n&\\neq D_\\text{KL}(q(z) ~||~ p(z))\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#minimizing-kl-divergence",
    "href": "lecs/w12/lec12.html#minimizing-kl-divergence",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Minimizing KL Divergence",
    "text": "Minimizing KL Divergence"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-properties",
    "href": "lecs/w12/lec12.html#kl-divergence-properties",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence Properties",
    "text": "KL Divergence Properties\nThe KL divergence is a measure of the difference between probability distributions.\nKL divergence is an asymmetric, nonnegative measure, not a norm. It doesn’t obey the triangle inequality.\nKL divergence is always positive. Hint: you can show this using the inequality \\(\\ln(x) \\leq x - 1\\) for \\(x &gt; 0\\)."
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-continuous-example",
    "href": "lecs/w12/lec12.html#kl-divergence-continuous-example",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence: Continuous Example",
    "text": "KL Divergence: Continuous Example\nSuppose we have two Gaussian distributions \\(p(x) \\sim N\\left(\\mu_1, \\sigma_1^2\\right)\\) and \\(q(x) \\sim N\\left(\\mu_2, \\sigma_2^2\\right)\\).\nWhat is the KL divergence \\(D_\\text{KL}(p(z) ~||~ q(z))\\)?\nRecall:\n\\[p\\left(z; \\mu_1, \\sigma_1^2\\right) = \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} e^{-\\frac{(z - \\mu_1)^2}{2\\sigma_1^2}}\\]\n\\[\\log \\left(p\\left(z; \\mu_1, \\sigma_1^2\\right)\\right) = - \\log \\sqrt{2 \\pi \\sigma_1^2} - \\frac{(z - \\mu_1)^2}{2\\sigma_1^2}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-entropy-and-cross-entropy",
    "href": "lecs/w12/lec12.html#kl-divergence-entropy-and-cross-entropy",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence: Entropy and Cross-Entropy",
    "text": "KL Divergence: Entropy and Cross-Entropy\nWe can split the KL divergence into two terms, which we can compute separately:\n\\[\\begin{align*}\nD_\\text{KL}(p(z) ~||~ q(z)) &= \\int p(z) \\log \\frac{p(z)}{q(z)} dz \\\\\n    &= \\int p(z) (\\log p(z) - \\log q(z)) dz \\\\\n    &= \\int p(z) \\log p(z) dz - \\int p(z) \\log q(z) dz \\\\\n    &= -\\text{entropy} - \\text{cross-entropy}\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-continuous-example-entropy-computation",
    "href": "lecs/w12/lec12.html#kl-divergence-continuous-example-entropy-computation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence: Continuous Example, Entropy Computation",
    "text": "KL Divergence: Continuous Example, Entropy Computation\n\\[\\begin{align*}\n\\int p(z) \\log\\left(p(z)\\right)\\, dz \\\\\n&\\hspace{-24pt}= \\int p(z) \\left(-\\log\\left(\\sqrt{2 \\pi \\sigma_1^2}\\right) - \\frac{(z - \\mu_1)^2}{2\\sigma_1^2}\\right)\\, dz \\\\\n&\\hspace{-24pt}=  - \\int p(z) \\frac{1}{2}\\log\\left(2 \\pi \\sigma_1^2\\right)\\, dz - \\int p(z) \\frac{(z - \\mu_1)^2}{2\\sigma_1^2}\\, dz \\\\\n&\\hspace{-24pt}= \\ldots\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-continuous-example-entropy-computation-ii",
    "href": "lecs/w12/lec12.html#kl-divergence-continuous-example-entropy-computation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence: Continuous Example, Entropy Computation II",
    "text": "KL Divergence: Continuous Example, Entropy Computation II\n\\[\\begin{align*}\n\\ldots &= -\\frac{1}{2}\\log\\left(2 \\pi \\sigma_1^2\\right) \\int p(z)\\, dz - \\frac{1}{2\\sigma_1^2}\\int p(z) (z - \\mu_1)^2\\, dz \\\\\n&= -\\frac{1}{2}\\log\\left(2 \\pi \\sigma_1^2\\right) - \\frac{1}{2} \\\\\n&= -\\frac{1}{2}\\log\\left(\\sigma_1^2\\right) - \\frac{1}{2}\\log (2 \\pi) - \\frac{1}{2}\n\\end{align*}\\]\nSince \\(\\displaystyle \\int p(z)\\, dz = 1\\) and \\(\\displaystyle\\int p(z) (z - \\mu_1)^2\\, dz = \\sigma_1^2\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-continuous-example-cross-entropy-computation",
    "href": "lecs/w12/lec12.html#kl-divergence-continuous-example-cross-entropy-computation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence: Continuous Example, Cross-Entropy Computation",
    "text": "KL Divergence: Continuous Example, Cross-Entropy Computation\n\\[\\begin{align*}\n\\int p(z) \\log\\left(q(z)\\right)\\, dz \\\\\n&\\hspace{-36pt}= \\int p(z) \\left(-\\log\\left(\\sqrt{2 \\pi \\sigma_2^2}\\right) - \\frac{(z - \\mu_2)^2}{2\\sigma_2^2}\\right)\\, dz \\\\\n&\\hspace{-36pt}= -\\int p(z) \\frac{1}{2}\\log (2 \\pi \\sigma_2^2)\\, dz - \\int p(z) \\frac{(z - \\mu_2)^2}{2\\sigma_2^2}\\, dz \\\\\n&\\hspace{-36pt}= -\\frac{1}{2}\\log (2 \\pi \\sigma_2^2) - \\frac{1}{2\\sigma_2^2}\\int p(z) (z - \\mu_2)^2\\, dz =  \\ldots\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#kl-divergence-continuous-example-cross-entropy-computation-ii",
    "href": "lecs/w12/lec12.html#kl-divergence-continuous-example-cross-entropy-computation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "KL Divergence: Continuous Example, Cross-Entropy Computation II",
    "text": "KL Divergence: Continuous Example, Cross-Entropy Computation II\n\\[\\ldots = - \\frac{1}{2}\\log (2 \\pi \\sigma_2^2) - \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#back-to-autoencoders-summary-so-far",
    "href": "lecs/w12/lec12.html#back-to-autoencoders-summary-so-far",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Back to Autoencoders: Summary so far",
    "text": "Back to Autoencoders: Summary so far\nAutoencoder:\n\nDecoder: point estimate of \\(p_{\\theta^{*}}(\\textbf{x} | \\textbf{z})\\)\n\n\n\nEncoder: point estimate of the value of \\(\\textbf{z}\\) that generated the image \\(\\textbf{x}\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#back-to-autoencoders-summary-so-far-ii",
    "href": "lecs/w12/lec12.html#back-to-autoencoders-summary-so-far-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Back to Autoencoders: Summary so far II",
    "text": "Back to Autoencoders: Summary so far II\nVAE:\n\nDecoder: probabilistic estimate of \\(p_{\\theta^{*}}(\\textbf{x} | \\textbf{z})\\)\n\n\n\nEncoder: probabilistic estimate of a Gaussian distribution \\(q_{\\phi}(\\textbf{z} | \\textbf{x})\\) that approximates the distribution \\(p_{\\theta^{*}}(\\textbf{z} | \\textbf{x})\\)\n\nIn particular, our encoder will be a neural network that predicts the mean and standard deviation of \\(q_{\\phi}(\\textbf{z} | \\textbf{x})\\)\nWe can then sample \\({\\bf z}\\) from this distribution!"
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-objective",
    "href": "lecs/w12/lec12.html#vae-objective",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE Objective",
    "text": "VAE Objective\nBut how do we train a VAE?\nWe want to maximize the likelihood of our data:\n\\[\\displaystyle \\log(p(x)) = \\log\\left(\\int p(x|z)p(x)\\, dz\\right)\\]\nAnd we want to make sure that the distributions \\(q(z|x)\\) and \\(p(z|x)\\) are close:\n\nWe want to minimize \\(D_\\text{KL}(q({\\bf z}|{\\bf x}) ~||~ p({\\bf z} | {\\bf x}))\\)\n\n\n\nThis is a measure of encoder quality"
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-objective-ii",
    "href": "lecs/w12/lec12.html#vae-objective-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE Objective II",
    "text": "VAE Objective II\nIn other words, we want to maximize\n\\[-D_\\text{KL}(q({\\bf z}|{\\bf x}) ~||~ p({\\bf z} | {\\bf x})) + \\log(p(x))\\]\nHow can we optimize this quantity in a tractable way?"
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-evidence-lower-bound",
    "href": "lecs/w12/lec12.html#vae-evidence-lower-bound",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE: Evidence Lower-Bound",
    "text": "VAE: Evidence Lower-Bound\n\\[\\begin{align*}\nD_\\text{KL}(q({\\bf z}|{\\bf x}) ~||~ p({\\bf z} | {\\bf x})) &= \\int q({\\bf z}|{\\bf x}) \\log\\left(\\frac{q({\\bf z}|{\\bf x})}{p({\\bf z}|{\\bf x})}\\right)\\, dz \\\\\n&= \\text{E}_q\\left(\\log\\left(\\frac{q({\\bf z}|{\\bf x})}{p({\\bf z}|{\\bf x})}\\right)\\right) \\\\\n&= \\text{E}_q (\\log (q({\\bf z}|{\\bf x}))) - \\text{E}_q(\\log(p({\\bf z}|{\\bf x}))) \\\\\n&= \\text{E}_q(\\log(q({\\bf z}|{\\bf x}))) - \\text{E}_q(\\log(p({\\bf z},{\\bf x}))) \\\\\n&\\hspace{12pt} + \\text{E}_q(\\log(p({\\bf x}))) \\\\\n&= \\text{E}_q(\\log(q({\\bf z}|{\\bf x}))) - \\text{E}_q(\\log(p({\\bf z},{\\bf x}))) \\\\\n&\\hspace{12pt} + \\log p({\\bf x})\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-evidence-lower-bound-ii",
    "href": "lecs/w12/lec12.html#vae-evidence-lower-bound-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE: Evidence Lower-Bound II",
    "text": "VAE: Evidence Lower-Bound II\nWe’ll define the evidence lower-bound: \\[\\text{ELBO}_q({\\bf x}) = \\text{E}_q(\\log(p({\\bf z},{\\bf x})) - \\log(q({\\bf z}|{\\bf x})))\\]\nSo we have \\[\\log(p({\\bf x})) - D_\\text{KL}(q({\\bf z}|{\\bf x}) ~||~ p({\\bf z} | {\\bf x})) = \\text{ELBO}_q({\\bf x})\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#optimizing-the-elbo",
    "href": "lecs/w12/lec12.html#optimizing-the-elbo",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Optimizing the ELBO",
    "text": "Optimizing the ELBO\nThe ELBO gives us a way to estimate the gradients of \\(\\log(p({\\bf x})) - D_\\text{KL}(q({\\bf z}|{\\bf x}) ~||~ p({\\bf z} | {\\bf x}))\\)\nHow?\n\\[\\text{ELBO}_q({\\bf x}) = \\text{E}_q(\\log(p({\\bf z},{\\bf x})) - \\log(q({\\bf z}|{\\bf x})))\\]\n\nThe right hand side of this expression is an expectation over \\(z \\sim q(z|x)\\)\n\n\n\nTo estimate the ELBO, we can sample from the distribution \\(z \\sim q(z|x)\\), and compute the terms inside."
  },
  {
    "objectID": "lecs/w12/lec12.html#optimizing-the-elbo-ii",
    "href": "lecs/w12/lec12.html#optimizing-the-elbo-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Optimizing the ELBO II",
    "text": "Optimizing the ELBO II\n\nWe can estimate gradients in the same way—this is called a Monte-Carlo gradient estimator"
  },
  {
    "objectID": "lecs/w12/lec12.html#monte-carlo-estimation",
    "href": "lecs/w12/lec12.html#monte-carlo-estimation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Monte Carlo Estimation",
    "text": "Monte Carlo Estimation\n(This notation is unrelated to other slides: \\(p(z)\\) is just a univariate Gaussian distribution, and \\(f_\\phi(z)\\) is a function parameterized by \\(\\phi\\))\nSuppose we want to optimize an objective \\(\\mathcal{L}(\\phi) = \\text{E}_{z \\sim p(z)}(f_\\phi(z))\\) where \\(p(z)\\) is a normal distribution.\nWe can estimate \\(\\mathcal{L}(\\phi)\\) by sampling \\(z_i \\sim p(z)\\) and computing\n\\[\\mathcal{L}(\\phi) = \\text{E}_{z \\sim p(z)}(f_\\phi(z)) = \\int_z p(z)f_\\phi(z)\\, dz \\approx \\frac{1}{N} \\sum_{i=1}^N f_\\phi(z_i)\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#monte-carlo-gradient-estimation",
    "href": "lecs/w12/lec12.html#monte-carlo-gradient-estimation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Monte Carlo Gradient Estimation",
    "text": "Monte Carlo Gradient Estimation\nLikewise, if we want to estimate \\(\\nabla_\\phi \\mathcal{L}\\), we can sample \\(z_i \\sim p(z)\\) and compute\n\\[\\begin{align*}\n\\nabla_\\phi \\mathcal{L} &= \\nabla_\\phi \\text{E}_{z \\sim p(z)}(f_\\phi(z)) \\\\\n                        &= \\nabla_\\phi \\int_z p(z)f_\\phi(z)\\, dz \\\\\n                        &\\approx \\nabla_\\phi  \\frac{1}{N} \\sum_{i=1}^N f_\\phi(z_i) \\\\\n                        &= \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\phi  f_\\phi(z_i) \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#the-reparamaterization-trick",
    "href": "lecs/w12/lec12.html#the-reparamaterization-trick",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Reparamaterization Trick",
    "text": "The Reparamaterization Trick\n\\(\\text{ELBO}_{\\theta,\\phi}(\\textbf{x}) = \\text{E}_{q_{\\phi}}(\\log(p_{\\theta}(\\textbf{z}, \\textbf{x})) - \\log(q_{\\phi}(\\textbf{z}|\\textbf{x})))\\)\nProblem: typical Monte-Carlo gradient estimator with samples \\(\\textbf{z} \\sim q_{\\phi}(\\textbf{z}|\\textbf{x})\\) has very high variance.\nReparameterization trick: instead of sampling \\(\\textbf{z} \\sim q_{\\phi}(\\textbf{z}|\\textbf{x})\\) express \\(\\textbf{z}=g_{\\phi}(\\epsilon, \\textbf{x})\\) where \\(g\\) is deterministic and only \\(\\epsilon\\) is stochastic."
  },
  {
    "objectID": "lecs/w12/lec12.html#the-reparamaterization-trick-ii",
    "href": "lecs/w12/lec12.html#the-reparamaterization-trick-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "The Reparamaterization Trick II",
    "text": "The Reparamaterization Trick II\nIn practise, the reparameterization trick is what makes the VAE encoder deterministic. When running a VAE forward pass:\n\nWe get the means and standard deviations from the VAE\nWe sample from \\(\\mathcal{N}({\\bf 0}, {\\bf I})\\)\nWe use the samples from step 2 to get a sample from \\(q(z)\\) obtained from step 1"
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-summary-so-far",
    "href": "lecs/w12/lec12.html#vae-summary-so-far",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE: Summary so far",
    "text": "VAE: Summary so far\nDecoder: estimate of \\(p_{\\theta^{*}}(\\textbf{x} | \\textbf{z})\\).\nEncoder: estimate of a Gaussian distribution \\(q_{\\phi}(\\textbf{z} | \\textbf{x})\\) that approximates the distribution \\(p_{\\theta^{*}}(\\textbf{z} | \\textbf{x})\\).\n\nEncoder is a NN that predicts the mean and standard deviation of \\(q_{\\phi}(\\textbf{z} | \\textbf{x})\\)\n\n\n\nUse the reparameterization trick to sample from this distribution"
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-summary-so-far-ii",
    "href": "lecs/w12/lec12.html#vae-summary-so-far-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE: Summary so far II",
    "text": "VAE: Summary so far II\nThe VAE objective is equal to the evidence lower-bound:\n\\[\\log(p({\\bf x})) - D_\\text{KL}(q({\\bf z}|{\\bf x}) ~||~ p({\\bf z} | {\\bf x})) = \\text{ELBO}_q({\\bf x})\\]\nWhich we can estimate using Monte Carlo\n\\[\\text{ELBO}_q({\\bf x}) = \\text{E}_q (\\log(p({\\bf z},{\\bf x})) - \\log(q({\\bf z}|{\\bf x})))\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-summary-so-far-iii",
    "href": "lecs/w12/lec12.html#vae-summary-so-far-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE: Summary so far III",
    "text": "VAE: Summary so far III\nBut given a value \\(z \\sim q(z|x)\\), how can we compute\n\\[\\log p({\\bf z},{\\bf x}) - \\log q({\\bf z}|{\\bf x})\\]\n…or its derivative with respect to the neural network parameters?\nWe need to do some more math to write this quantity in a form that is easier to estimate."
  },
  {
    "objectID": "lecs/w12/lec12.html#vae-a-simpler-form",
    "href": "lecs/w12/lec12.html#vae-a-simpler-form",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "VAE: a Simpler Form",
    "text": "VAE: a Simpler Form\n\\[\\begin{aligned}\n    \\text{ELBO}_{\\theta,\\phi}(\\textbf{x}) &= \\text{E}_{q_{\\phi}}(\\log(p_{\\theta}(\\textbf{z}, \\textbf{x})) - \\log(q_{\\phi}(\\textbf{z}|\\textbf{x}))) \\\\\n    &= \\text{E}_{q_{\\phi}}(\\log(p_{\\theta}(\\textbf{x} | \\textbf{z})) + \\log(p_{\\theta}(\\textbf{z})) - \\log(q_{\\phi}(\\textbf{z}|\\textbf{x}))) \\\\\n    &= \\text{E}_{q_{\\phi}}(\\log(p_{\\theta}(\\textbf{x} | \\textbf{z}))) - \\text{E}_{q_{\\phi}}(\\log(p_{\\theta}(\\textbf{z})) + \\log(q_{\\phi}(\\textbf{z}|\\textbf{x}))) \\\\\n    &= \\text{E}_{q_{\\phi}}(\\log(p_{\\theta}(\\textbf{x} | \\textbf{z}))) - D_\\text{KL}(q_{\\phi}(\\textbf{z}|\\textbf{x}) ~||~ p_{\\theta}(\\textbf{z})) \\\\\n    &= \\text{decoding quality} - \\text{encoding regularization}\n\\end{aligned}\\]\nBoth terms can be computed easily if we make some simplifying assumptions\nLet’s see how…"
  },
  {
    "objectID": "lecs/w12/lec12.html#computing-decoding-quality",
    "href": "lecs/w12/lec12.html#computing-decoding-quality",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing Decoding Quality",
    "text": "Computing Decoding Quality\nIn order to estimate this quantity\n\\[\\text{E}_{q_{\\phi}}(\\log(p_{\\theta}(\\textbf{x} | \\textbf{z})))\\]\n…we need to make some assumptions about the distribution \\(p_{\\theta}(\\textbf{x} | \\textbf{z})\\)."
  },
  {
    "objectID": "lecs/w12/lec12.html#computing-decoding-quality-ii",
    "href": "lecs/w12/lec12.html#computing-decoding-quality-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing Decoding Quality II",
    "text": "Computing Decoding Quality II\nIf we make the assumption that \\(p_{\\theta}(\\textbf{x} | \\textbf{z})\\) is a normal distribution centered around some pixel intensity, then optimizing \\(p_{\\theta}(\\textbf{x} | \\textbf{z})\\) is equivalent to optimizing the square loss!\nThat is, \\(p_{\\theta}(\\textbf{x} | \\textbf{z})\\) tells us how intense a pixel could be, but that pixel could be a bit darker/lighter, following a normal distribution.\nBonus: A traditional autoencoder is optimizing this same quantity!"
  },
  {
    "objectID": "lecs/w12/lec12.html#computing-encoding-quality",
    "href": "lecs/w12/lec12.html#computing-encoding-quality",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing Encoding Quality",
    "text": "Computing Encoding Quality\nThis KL divergence computes the difference in distribution between two distributions:\n\\[D_\\text{KL}(q_{\\phi}(\\textbf{z}|\\textbf{x})~||~p_{\\theta}(\\textbf{z}))\\]\n\n\\(q_{\\phi}(\\textbf{z}|\\textbf{x})\\) is a normal distribution that approximates \\(p_\\theta(\\textbf{z}|\\textbf{x})\\)\n\n\n\n\\(p_{\\theta}(\\textbf{z})\\) is the prior distribution on \\({\\bf z}\\)\n\ndistribution of \\(z\\) when we don’t know anything about \\({\\bf x}\\) or any other quantity"
  },
  {
    "objectID": "lecs/w12/lec12.html#computing-encoding-quality-ii",
    "href": "lecs/w12/lec12.html#computing-encoding-quality-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Computing Encoding Quality II",
    "text": "Computing Encoding Quality II\nSince \\({\\bf z}\\) is a latent variable, not actually observed in the real word, we can choose \\(p_{\\theta}(\\textbf{z})\\)\n\nwe choose \\(p_\\theta (z) = \\mathcal{N}({\\bf 0}, {\\bf I})\\)\n\n…and we know how to compute the KL divergence of two Gaussian distributions!"
  },
  {
    "objectID": "lecs/w12/lec12.html#interpretation",
    "href": "lecs/w12/lec12.html#interpretation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Interpretation",
    "text": "Interpretation\nThe VAE objective\n\\[\\text{E}_{q_{\\phi}}(\\log(p_{\\theta}(\\textbf{x} | \\textbf{z}))) - D_\\text{KL}(q_{\\phi}(\\textbf{z}|\\textbf{x}) ~||~ p_{\\theta}(\\textbf{z}))\\]\nhas an extra regularization term that the traditional autoencoder does not.\nThis extra regularization term pushes the values of \\({\\bf z}\\) to be closer to \\(0\\)."
  },
  {
    "objectID": "lecs/w12/lec12.html#mnist-results",
    "href": "lecs/w12/lec12.html#mnist-results",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "MNIST results",
    "text": "MNIST results"
  },
  {
    "objectID": "lecs/w12/lec12.html#frey-faces-results",
    "href": "lecs/w12/lec12.html#frey-faces-results",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Frey Faces results",
    "text": "Frey Faces results"
  },
  {
    "objectID": "lecs/w12/lec12.html#dimension-of-latent-variables",
    "href": "lecs/w12/lec12.html#dimension-of-latent-variables",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Dimension of latent variables",
    "text": "Dimension of latent variables"
  },
  {
    "objectID": "lecs/w12/lec12.html#variational-inference",
    "href": "lecs/w12/lec12.html#variational-inference",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Variational Inference",
    "text": "Variational Inference\nVariational inference is used in other areas… (TODO)"
  },
  {
    "objectID": "lecs/w12/lec12.html#an-example-data-from-mixture-of-gaussians",
    "href": "lecs/w12/lec12.html#an-example-data-from-mixture-of-gaussians",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "An example: Data from Mixture of Gaussians",
    "text": "An example: Data from Mixture of Gaussians\n\nK mixture components, corresponding to normal distributions\n\n\n\nMeans \\(\\boldsymbol{\\mu}=\\{\\mu_1,...,\\mu_K\\} \\sim \\mathcal{N}(0, \\sigma^2\\boldsymbol{I})\\)\n\n\n\n\nMixture selection variable \\(c_i \\sim \\text{Categorical}(1/K, ..., 1/K)\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#an-example-data-from-mixture-of-gaussians-ii",
    "href": "lecs/w12/lec12.html#an-example-data-from-mixture-of-gaussians-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "An example: Data from Mixture of Gaussians II",
    "text": "An example: Data from Mixture of Gaussians II\n\nJoint model \\(\\displaystyle p(\\boldsymbol{\\mu}, c_{1:n}, x_{1:n})=p(\\boldsymbol{\\mu}) \\prod_{i=1}^n p(c_i)p(x_i|c_i,\\boldsymbol{\\mu})\\)\n\n\n\n\n\n\n\n\n\nEach \\(c_i\\) has K options, and we have \\(n\\) data points, so \\(O(K^n)\\) to evaluate \\(\\displaystyle p(x_{1:n}) = \\int p(\\boldsymbol{\\mu}, c_{1:n}, x_{1:n})\\, d\\boldsymbol{\\mu}dc_{1:n}\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#an-example-data-from-mixture-of-gaussians-iii",
    "href": "lecs/w12/lec12.html#an-example-data-from-mixture-of-gaussians-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "An example: Data from Mixture of Gaussians III",
    "text": "An example: Data from Mixture of Gaussians III\n\nEach \\(c_i\\) has K options, and we have \\(n\\) data points, so \\(O(K^n)\\) to evaluate \\(\\displaystyle p(x_{1:n}) = \\int p(\\boldsymbol{\\mu}, c_{1:n}, x_{1:n})\\, d\\boldsymbol{\\mu}dc_{1:n}\\)\n\n\n\nTakeaway message: can’t use direct estimation of the evidence \\(p(x_{1:n})\\)\n\n\n\n\nIn this particular example we can use EM, but in general it assumes that you know \\(p(\\textbf{z}|\\textbf{x})\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#evidence-lower-bound-elbo",
    "href": "lecs/w12/lec12.html#evidence-lower-bound-elbo",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Evidence Lower Bound (ELBO)",
    "text": "Evidence Lower Bound (ELBO)\n\\[\\begin{aligned}\n    D_\\text{KL}(q(\\textbf{z})~||~p(\\textbf{z}|\\textbf{x})) &= \\text{E}_{q}\\left(\\log\\left(\\frac{q(\\textbf{z})}{p(\\textbf{z} | \\textbf{x})}\\right)\\right) \\\\\n    &= \\text{E}_{q}(\\log(q(\\textbf{z}))) - \\text{E}_{q}(\\log(p(\\textbf{z} | \\textbf{x}))) \\\\\n    &= \\text{E}_{q}(\\log(q(\\textbf{z}))) - \\text{E}_{q}(\\log(p(\\textbf{z},\\textbf{x}))) \\\\\n    &\\hspace{12pt} + \\text{E}_{q}(\\log(p(\\textbf{x}))) \\\\\n    &= \\text{E}_{q}(\\log(q(\\textbf{z}))) - \\text{E}_{q}(\\log(p(\\textbf{z},\\textbf{x}))) \\\\\n    &\\hspace{12pt} + \\log(p(\\textbf{x})) \\\\\n    &= -\\text{ELBO}_{q}(\\textbf{x}) + \\log(p(\\textbf{x}))\n\\end{aligned}\\]"
  },
  {
    "objectID": "lecs/w12/lec12.html#evidence-lower-bound-elbo-ii",
    "href": "lecs/w12/lec12.html#evidence-lower-bound-elbo-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Evidence Lower Bound (ELBO) II",
    "text": "Evidence Lower Bound (ELBO) II\nLog-evidence:\n\\[\\log(p(\\textbf{x})) = D_\\text{KL}(q(\\textbf{z}) ~||~ p(\\textbf{z} | \\textbf{x})) + \\text{ELBO}_q(\\textbf{x})\\]\nVariational Inference \\(\\rightarrow\\) find \\(q(\\textbf{z})\\) that maximizes \\(\\text{ELBO}_q\\)"
  },
  {
    "objectID": "lecs/w12/lec12.html#mean-field-approximation",
    "href": "lecs/w12/lec12.html#mean-field-approximation",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Mean-Field Approximation",
    "text": "Mean-Field Approximation\n\nSimplification for posterior approximator \\(q(\\textbf{z})\\):\n\n\n\n\\(\\displaystyle q(\\textbf{z}) = \\prod_j q_j(z_j)\\)\n\n\n\n\nAll latent variables \\(z_j\\) are mutually independent\n\n\n\n\nEach is governed by its own distribution \\(q_j\\)\n\n\n\n\nWHY? It makes the optimization easier (analytical gradients)"
  },
  {
    "objectID": "lecs/w12/lec12.html#mean-field-approximation-ii",
    "href": "lecs/w12/lec12.html#mean-field-approximation-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Mean-Field Approximation II",
    "text": "Mean-Field Approximation II\n\nWHY NOT? It fails to model correlations among latent variables, and underestimates variance"
  },
  {
    "objectID": "lecs/w12/lec12.html#optimization-algorithms",
    "href": "lecs/w12/lec12.html#optimization-algorithms",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Optimization algorithms",
    "text": "Optimization algorithms\n\nAlgo \\(\\#1\\): coordinate ascent along each latent variable of ELBO\n\n\n\nMain problem is that it evaluates ELBO on the entire dataset (not great for big data)"
  },
  {
    "objectID": "lecs/w12/lec12.html#optimization-algorithms-ii",
    "href": "lecs/w12/lec12.html#optimization-algorithms-ii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Optimization algorithms II",
    "text": "Optimization algorithms II\n\nAlso susceptible to local minima"
  },
  {
    "objectID": "lecs/w12/lec12.html#optimization-algorithms-iii",
    "href": "lecs/w12/lec12.html#optimization-algorithms-iii",
    "title": "CSC413 Neural Networks and Deep Learning",
    "section": "Optimization algorithms III",
    "text": "Optimization algorithms III\n\nAlgo \\(\\#2\\): stochastic optimization over all latent variables\n\n\n\nUses the natural gradient to account for manifold on which distributions live\n\n\n\n\nEvaluates ELBO on single data points, or minibatches"
  }
]