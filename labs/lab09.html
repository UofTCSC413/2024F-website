<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>lab09 – CSC413 - Fall 2024</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-87aedce6c5c32840e93824ffc3714634.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-69b08db278f499bc7d235ce342f73d67.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-87aedce6c5c32840e93824ffc3714634.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="CSC413 - Fall 2024">
<meta property="og:description" content="Homepage for CSC413 - Neural Networks and Deep Learning, Fall 2024.">
<meta property="og:site_name" content="CSC413 - Fall 2024">
<meta name="twitter:title" content="CSC413 - Fall 2024">
<meta name="twitter:description" content="Homepage for CSC413 - Neural Networks and Deep Learning, Fall 2024.">
<meta name="twitter:image" content="https://uoftcsc413.github.io/2024F-website/labs/images/twitter-card.png">
<meta name="twitter:creator" content="@minebocek">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#csc413-lab-9-gradcam-and-input-gradients" id="toc-csc413-lab-9-gradcam-and-input-gradients" class="nav-link active" data-scroll-target="#csc413-lab-9-gradcam-and-input-gradients">CSC413 Lab 9: GradCAM and Input Gradients</a>
  <ul class="collapse">
  <li><a href="#submission" id="toc-submission" class="nav-link" data-scroll-target="#submission">Submission</a></li>
  <li><a href="#part-1.-class-activation-maps-cam" id="toc-part-1.-class-activation-maps-cam" class="nav-link" data-scroll-target="#part-1.-class-activation-maps-cam">Part 1. Class Activation Maps (CAM)</a></li>
  <li><a href="#part-2.-grad-cam" id="toc-part-2.-grad-cam" class="nav-link" data-scroll-target="#part-2.-grad-cam">Part 2. Grad-CAM</a></li>
  <li><a href="#just-for-fun" id="toc-just-for-fun" class="nav-link" data-scroll-target="#just-for-fun">Just For Fun</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/UofTCSC413/2024F-website/edit/main/labs/lab09.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/UofTCSC413/2024F-website/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>





<section id="csc413-lab-9-gradcam-and-input-gradients" class="level1">
<h1>CSC413 Lab 9: GradCAM and Input Gradients</h1>
<p>We have seen that convolutional neural networks (CNN) are successful in many computer vision tasks, including classification, object detection and others. However, it is not immediately clear how CNNs work, and how one can explain the predictions made by CNNs. A deeper understanding of how CNNs work can also help us identify reasons why CNNs may fail to produce correct predictions for some samples.</p>
<p>A line of work started to visualize and interpret computed features of convolutional neural networks. CAM and Grad-CAM are two influential and fundamental works to find which parts of the input have the most impact on the final output of the models by analyzing the model’s extracted feature maps.</p>
<p>By the end of this lab, you will be able to:</p>
<ol type="1">
<li>Explain how visualizing the regions of an image that contribute to the CNN model’s prediction can help explain how the model works.</li>
<li>Implement CAM on a convolutional neural network with GAP + a single fc layer.</li>
<li>Explain the limitations of CAM, and how Grad-CAM overcomes this limitation.</li>
<li>Implement Grad-CAM on a convolutional neural network.</li>
</ol>
<p>Acknowledgements: 2. We have borrowed some codes from <a href="https://github.com/zhoubolei/CAM">CAM Official Repo</a>. 3. We have borrowd texts, figures and formulas from main papers of <a href="https://arxiv.org/pdf/1512.04150v1.pdf">CAM</a> and <a href="https://arxiv.org/pdf/1610.02391.pdf">Grad-CAM</a>.</p>
<section id="submission" class="level2">
<h2 class="anchored" data-anchor-id="submission">Submission</h2>
<p>If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.</p>
<p>Submit the ipynb file <code>lab09.ipynb</code> on Markus <strong>containing all your solutions to the Graded Task</strong>s. Your notebook file must contain your code <strong>and outputs</strong> where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.</p>
<p>For this lab, you should submit the following:</p>
<ul>
<li>Part 1. Your implementation of the <code>predict</code> function (1 point)</li>
<li>Part 1. Your implementation of the <code>get_resnet_features</code> function (1 point)</li>
<li>Part 1. Your implementation of the <code>compute_cam</code> function (3 point)</li>
<li>Part 1. Your interpretation of the grad cam outputs (2 point)</li>
<li>Part 2. Your implementation of the <code>compute_gradcam</code> function (3 point)</li>
</ul>
</section>
<section id="part-1.-class-activation-maps-cam" class="level2">
<h2 class="anchored" data-anchor-id="part-1.-class-activation-maps-cam">Part 1. Class Activation Maps (CAM)</h2>
<p>CAM and its extension Grad-CAM takes the approach of identifying the regions of the an image that contributes most to the model’s prediction. This information can be visualized as a heat map, and provides a way to interpret a model’s prediction: did the model predict that the image is that of a “boat” because of the shape of the ears, or because of the water in the background?</p>
<p>We discussed, during lecture, that convolutional layers preserve the geometry of the image, and that these convolutional layers actually behave as feature/ object detectors of various complexity. Since the geometry of the output of a CNN layer corresponds to the geometry of input image, it is straightforward to locate the region of the image that corresponds to a particularly high activation value. This is because the computations that we use in a CNN (convolutions, max pooling, activations) are all geometry preserving (equivariant).</p>
<p>However, fully-connected layers are typically used for classification in the final layers of a CNN. These fully-connected layers are <strong>not</strong> geometry preserving, thus information about the locations of discriminating features are lost when fully-connected layers are used for classification.</p>
<p>The idea behind CAM is to avoid using these fully-connected layers for classification, so that we can reconstruct location information in a straightforward way. Instead of fully-connected layers, we use:</p>
<ol type="1">
<li>A global average pooling (GAP) layer. This layer will take as input the output of a CNN layer (e.g., of shape <code>H x W x C</code>) and perform an average operation for each <em>channel</em> along the entire activation height/width (producing an output vector of shape <code>C</code>).</li>
<li>A single linear layer to map this vector (of length <code>C</code>) into the output space.</li>
</ol>
<p>Since both the pooling and linear layers have straightforward computation, it is possible to assign credit for a output score for a class back to specific activation values of the CNN output.</p>
<p>The framework of the Class Activation Mapping is as below (from https://github.com/zhoubolei/CAM):</p>
<p><img src="https://camo.githubusercontent.com/ef86405fc14b4af391c891c93a50ce92c8bfc3f8fae6e7c04f5f7185b21c3eca/687474703a2f2f636e6e6c6f63616c697a6174696f6e2e637361696c2e6d69742e6564752f6672616d65776f726b2e6a7067" width="500px"></p>
<p>In this part of the lab, we will implement CAM to produce a heatmap of the contribution to locations in the image to a predicted class. We will use the pre-trained convolutional neural network <strong>ResNet</strong>, chosen because this model’s architecture uses global average pooling (GAP). ResNet is trained on the ImageNet data set.</p>
<pre><code>from torchvision import models, transforms
import torch.nn.functional as F
import torch
import json
import numpy as np
import cv2
import matplotlib.pyplot as plt

# Download resnet pretrained weights
resnet = models.resnet18(pretrained=True)

# CAM can only be used when the models are in "evaluation phase".
resnet.eval()

# Print model architecture
# We will use the CNN activations computed after layer 4, and before GAP.
resnet</code></pre>
<p>The ImageNet labels are a bit challenging to read. We will download a list of human-readable labels from here:</p>
<pre><code>!wget https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json</code></pre>
<pre><code># Load the imagenet category list
with open('imagenet-simple-labels.json') as f:
    classes = json.load(f)</code></pre>
<p>To remind ourselves of how ResNet works, let’s predict what class these images belongs to:</p>
<p><img src="https://www.cs.toronto.edu/~lczhang/413/cat.jpg" width="300px"> <img src="https://www.cs.toronto.edu/~lczhang/413/boat.jpg" width="300px"></p>
<pre><code>!wget https://www.cs.toronto.edu/~lczhang/413/cat.jpg
!wget https://www.cs.toronto.edu/~lczhang/413/boat.jpg</code></pre>
<pre><code>from PIL import Image

def process_input(image_file):
    # open the image
    img = Image.open(image_file)

    # transform the images by resizing and normalizing
    preprocess = transforms.Compose([
       transforms.Resize((224,224)),
       transforms.ToTensor(),
       transforms.Normalize(
          mean=[0.485, 0.456, 0.406],
          std=[0.229, 0.224, 0.225])])

    return preprocess(img).unsqueeze(0)</code></pre>
<p><strong>Graded Task</strong>: Write a function that takes a model and an image file and produces a list of the top 5 predictions with the corresponding probability score.</p>
<pre><code>def predict(model, image_file):
    """
    Return the top 5 class predictions with the corresponding probability score.

    Parameters:
        `model`      - nn.Module
        `image_file` - file path to the image

    Returns: a list of 5 (string, int, float) pairs: 
        the string is the predicted ResNet class name (see classes above),
        the int is the predicted ResNet class id,
        and the float is the prediction probability. The list should be ordered
        so that the highest probabilty score appears first.

    Example:
        &gt;&gt;&gt; predict(resnet, 'cat.jpg')
        [('prison', 743, 0.23517875373363495),
         ('shopping cart', 791, 0.07393667101860046),
         ('rocking chair', 765, 0.06884343922138214),
         ('wheelbarrow', 428, 0.06603048741817474),
         ('ring-tailed lemur', 383, 0.0434008426964283)]
    """
    x = process_input(image_file)

    result = None # TODO

    scores = model(x) # SOLUTION
    y = [float(s) for s in torch.softmax(scores, axis=1)[0]] # SOLUTION
    result = sorted(zip(classes, range(1000),  y), key=lambda x: x[2], reverse=True)[:5] # SOLUTION
    return result</code></pre>
<p>Please include the output of the below cell in your submission.</p>
<pre><code>predict(resnet, 'cat.jpg')</code></pre>
<p>Now that we can use ResNet to make predictions, we need two additional pieces of information for CAM.</p>
<p>First, given an image, we need to be able to compute the <strong>features/activations of the last convolutional layer</strong>. This feature map is the input to the GAP layer. Although this information is computed in a forward pass, we will need to write some code to extract this information.</p>
<p>Second, we will need the weights of the final fully-connected layer in ResNet.</p>
<p><strong>Graded Task</strong>: Complete the following function that takes an image file and produces the weights of the finally fully-connected layer in Resnet. You may find the <code>named_children()</code> method of resnet helpful, as it produces a sequence of (named) layers. We would like the feature map directly before the global average pooling layer.</p>
<pre><code>for (name, model) in resnet.named_children():
    print(name)</code></pre>
<pre><code>def get_resnet_features(image_file):
    """
    Return the final CNN layer (layer4) feature map in resnet

    Parameters:
        `image_file` - file path to the image

    Returns: PyTorch tensor of shape [1, 512, 7, 7]
    """

    x = process_input(image_file)

    result = None # TODO
    result = x # SOLUTION
    for (name, model) in resnet.named_children():
        # TODO -- update result
        result = model.forward(result) # SOLUTION
        if name == 'layer4':
            break
    return result</code></pre>
<pre><code>fets = get_resnet_features('cat.jpg')
print(fets.shape) # should be [1, 512, 7, 7]</code></pre>
<p><strong>Task</strong>: Assign the variable <code>fc_weight</code> to the weights of the final fully-connected layer in resnet.</p>
<pre><code>weights = None # TODO
weights = resnet.fc.weight # SOLUTIONS
print(weights.shape) # should be [1000, 512]</code></pre>
<p><strong>Graded Task</strong>: Complete the function <code>compute_cam</code>, which takes the CNN feature map (from the <code>get_resnet_features</code> function), the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the following approach.</p>
<p>We will use the notation <span class="math inline">\({\bf X}\)</span> to denote the CNN feature map (the input to the GAP), with <span class="math inline">\(X_{i,j,c}\)</span> being the activation at location <span class="math inline">\((i, j)\)</span> and channel <span class="math inline">\(c\)</span>. Here, <span class="math inline">\({\bf X}\)</span> is a tensor with shape <span class="math inline">\(H \times W \times C\)</span>, where <span class="math inline">\(H \times W\)</span> is the height and width of the feature map and <span class="math inline">\(C\)</span> is the number of channels. We will use the vector <span class="math inline">\({\bf h}\)</span> to denote the output of the GAP, so that <span class="math inline">\(h_c = \frac{1}{HW} \sum_{i=1}^{H} \sum_{j=1}^{W} X_{i,j,c}\)</span>. Finally, we will use <span class="math inline">\({\bf W}\)</span> to denote the finally fully connected layer weights, and <span class="math inline">\({\bf z}\)</span> to the denote the prediction score, so that <span class="math inline">\({\bf z} = {\bf W}{\bf h}\)</span>.</p>
<p>Now, we would like to relate the features <span class="math inline">\(X_{i,j,c}\)</span> to the scores <span class="math inline">\(z_k\)</span>, so that we can compute the contribution of the features at position <span class="math inline">\((i,j)\)</span> to the score <span class="math inline">\(k\)</span>.</p>
<p>For an output class <span class="math inline">\(k\)</span>, we have:</p>
<p><span class="math display">\[z_k  = \sum_{c=1}^{C} w_{k,c} h_c\]</span></p>
<p>Substituing <span class="math inline">\(h_c\)</span> for its definition, we have:</p>
<p><span class="math display">\[z_k  = \sum_{c=1}^{C} w_{k,c} \frac{1}{HW} \sum_{i=1}^H \sum_{j=1}^W X_{i,j,c}\]</span></p>
<p>Rearranging the sums, we have:</p>
<p><span class="math display">\[z_k  = \frac{1}{HW} \sum_{i=1}^H \sum_{j=1}^W \sum_{c=1}^C w_{k,c} X_{i,j,c}\]</span></p>
<p>The inner term <span class="math inline">\(\sum_c w_{k,c} X_{i,j,c}\)</span> is exactly what we are looking for: this term indicates how much the value in location <span class="math inline">\((i, j)\)</span> of the feature map <span class="math inline">\({\bf X}\)</span> attributes to class <span class="math inline">\(k\)</span>.</p>
<pre><code>def compute_cam(features, label):
    """
    Computes the contribution of each location in `features` towards 
    `label` using CAM.

    Parameters:
        `features`: PyTorch Tensor of shape [1, 512, 7, 7] representing
                    final layer feature map in ResNet (e.g., from calling
                    `get_resnet_features`)
        `label`   : resnet label, integer between 0-999

    Returns: PyTorch Tensor of shape [7, 7]
    """
    features = features.squeeze(0) # remove the first dimension
    result = None # TODO 
    weight = resnet.fc.weight[label]   # SOLUTIONS
    result = torch.mul(weight, features.permute((1, 2, 0))).sum(dim=2)  # SOLUTIONS
    return result</code></pre>
<p><strong>Task</strong>: Run the below code, which superimposes the result of the <code>compute_cam</code> operation on the image.</p>
<pre><code>def visualize_cam(image_file, label):
    # open the image
    img = Image.open(image_file)

    # compute CAM features
    fets = get_resnet_features('cat.jpg')
    m = compute_cam(fets, label)

    # normalize "m"
    m = m - m.min()
    m = m / m.max()
    # convert "m" into pixel intensities
    m = np.uint8(255 * m.detach().numpy())
    # apply a color map
    m = cv2.resize(m, img.size)
    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)

    plt.figure(figsize=(6, 6))
    plt.title("%s %s" % (image_file, classes[label]))
    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img
    plt.show()</code></pre>
<pre><code>visualize_cam('cat.jpg', 743)
visualize_cam('cat.jpg', 383)</code></pre>
<p><strong>Graded Task</strong> Compare the above two outputs, and explain what conclusion you may be able to draw about the contribution of the pixel locations to those two classes. Why do you think the model misclassified the image?</p>
<pre><code># TODO: your explanation goes here</code></pre>
</section>
<section id="part-2.-grad-cam" class="level2">
<h2 class="anchored" data-anchor-id="part-2.-grad-cam">Part 2. Grad-CAM</h2>
<p>Although CAM was an important step toward understanding convolutional neural networks, the technique is only applicable to convolutional networks with GAP and a single fully-connected layer. Recall that it leveraged the following relationship between the output of the GAP layer <span class="math inline">\({\bf h}\)</span> and the score for the output class <span class="math inline">\(z_k\)</span>:</p>
<p><span class="math display">\[z_k  = \sum_c w_{k,c} h_c\]</span></p>
<p>Where <span class="math inline">\(w_{k,c}\)</span> is the fully connected layer weight that describes the strength of the connection between <span class="math inline">\(h_c\)</span> and <span class="math inline">\(z_k\)</span>. In other words, <span class="math inline">\(w_{k,c}\)</span> describes the following gradient:</p>
<p><span class="math display">\[\frac{\partial z_k}{\partial h_c}\]</span></p>
<p>With this in mind, you may be able to see how CAM may be generalized so that <span class="math inline">\({\bf z}\)</span> may be a more complex function of <span class="math inline">\({\bf h}\)</span>—e.g., a MLP or even an RNN!</p>
<p>Gradient-weighted Class Activation Mapping (Grad-CAM) is a generalized form of CAM, and can be applied to any convolutional neural network. In Grad-CAM, we use the gradient <span class="math inline">\(\frac{\partial z_k}{\partial h_c}\)</span> in place of <span class="math inline">\(w_{k,c}\)</span> when attributing class scores to locations <span class="math inline">\((i, j)\)</span>. In other words, the below term indicates how much the value in location <span class="math inline">\((i, j)\)</span> of the feature map <span class="math inline">\({\bf X}\)</span> attributes to class <span class="math inline">\(k\)</span>.</p>
<p><span class="math display">\[ReLU(\sum_c \frac{\partial z_k}{\partial h_c} X_{i,j,c})\]</span></p>
<p>The addition of the ReLU activation only allows positive contributions to be visualized.</p>
<p>Sidenote: To generalize this result even further, we can replace <span class="math inline">\(z_k\)</span> with any target we would like! Grad-CAM has been used on neural networks that performs image caption generation: a model with a CNN <em>encoder</em> and an RNN <em>decoder</em>. We can use use the gradients of any target concept (say “dog” in a classification network or a sequence of words in a captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Taking a look at <a href="https://www.youtube.com/watch?v=COjUB9Izk6E">this video</a> helps you to understand the power of Grad-CAM.</p>
<p>Let’s explore GradCAM with the VGG network:</p>
<pre><code>vgg19 = models.vgg19(pretrained=True)
vgg19.eval()
vgg19</code></pre>
<pre><code>predict(vgg19, 'cat.jpg')</code></pre>
<p><strong>Task</strong> Just like with CAM, we will need to extract the feature map obtained from the last convolutional layer. This step is actually very straightforward with VGG since <code>vgg19</code> splits the network into a <code>features</code> network and a <code>classifier</code> network.</p>
<pre><code>def get_vgg_features(image_file):
    """
    Return the output of `vgg19.features` network for the image

    Parameters:
        `image_file` - file path to the image

    Returns: PyTorch tensor of shape [1, 512, 7, 7]
    """

    x = process_input(image_file)
    result = None # TODO
    result = vgg19.features(x) # SOLUTIONS
    return result</code></pre>
<pre><code>get_vgg_features('cat.jpg').shape</code></pre>
<p><strong>Task</strong>: Read the forward method of the VGG model here. https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py What other steps are remaining in the forward pass?</p>
<pre><code># TODO: Explain the remaining steps here</code></pre>
<p><strong>Graded Task</strong>: Complete the function <code>compute_gradcam</code>, which takes an image file path, the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the GradCAM approach described at the beginning of Part 2.</p>
<pre><code>def compute_gradcam(image_file, label):
    """
    Computes the contribution of each location in `features` towards 
    `label` using GradCAM.

    Parameters:
        `image_file` - file path to the image
        `label`   : resnet label, integer between 0-999

    Returns: PyTorch Tensor of shape [7, 7]
    """
    # obtain the image input features
    x = process_input(image_file)

    # obtain the output of the features network in the CNN
    fets = vgg19.features(x)

    # tell PyTorch to compute the gradients with respect
    # to "fets"
    fets.retain_grad()

    # TODO: compute the rest of the vgg19 forward pass from `fets`
    out = None # should be the output of the classifier network
    out = vgg19.avgpool(fets)     # SOLUTION
    out = torch.flatten(out, 1)  # SOLUTION
    out = vgg19.classifier(out)  # SOLUTION

    z_k = out.squeeze(0)[label] # identify the target output class
    z_k.backward()              # backpropagation to compute gradients

    features_grad = fets.grad   # identify the gradient of z_k with respect to fets

    # account for the pooling operation, so that "pooled_grad"
    # aligns with the notation used
    n, c, h, w = features_grad.shape
    features_grad = torch.reshape(features_grad, (c, h*w))
    pooled_grad = features_grad.sum(dim=1)

    # rearrange "fets" so that "X" aligns with the notation
    # used above
    X = fets.squeeze(0).permute((1, 2, 0))

    # TODO: Compute the heatmap using the gradcam
    m = None
    m = torch.matmul(X, pooled_grad) # SOLUTION
    m = F.relu(m) # apply the ReLU operation
    return m</code></pre>
<p><strong>Task</strong>: Run the below code, which superimposes the result of the <code>compute_gradcam</code> operation on the image.</p>
<pre><code>def visualize_gradcam(image_file, label):
    # open the image
    img = Image.open(image_file)

    # compute CAM features
    m = compute_gradcam(image_file, label)

    # normalize "m"
    m = m - m.min()
    m = m / m.max()
    # convert "m" into pixel intensities
    m = np.uint8(255 * m.detach().numpy())
    # apply a color map
    m = cv2.resize(m, img.size)
    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)

    plt.figure(figsize=(6, 6))
    plt.title("%s %s" % (image_file, classes[label]))
    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img
    plt.show()</code></pre>
<pre><code>visualize_cam('cat.jpg', 743)</code></pre>
<pre><code>visualize_cam('cat.jpg', 383)</code></pre>
<pre><code>visualize_cam('boat.jpg', 536)</code></pre>
</section>
<section id="just-for-fun" class="level2">
<h2 class="anchored" data-anchor-id="just-for-fun">Just For Fun</h2>
<p>As you might have seen in the <a href="https://www.youtube.com/watch?v=COjUB9Izk6E">video</a>, Grad-CAM can be applied to text-generating models. For example, in image-captioning tasks, a text is generated describing the given image. Some methods first feed the image to a convolutional neural network to extract features, and then feed the extracted features to an RNN, to generate the text. Neuraltalk2 was one of the earliest models using this approach. Similar to the classification task, it is enough to compute the gradient of the score (what is the score in an image-captioning task?) with respect to the last convolutional layer.</p>
<p>If you are interested in how neuraltalk2 functions you can check <a href="https://cs.stanford.edu/people/karpathy/deepimagesent/">this project</a>. Moreover, if you are looking for more hands-on experience, <a href="https://github.com/ruotianluo/ImageCaptioning.pytorch">this repo</a> has implemented many image-captioning methods, and you can easily apply Grad-CAM on them (especially show and tell).</p>
<p>Hint: There is a file which re-implements the forward pipeline of ResNet101, where you can store the features.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/uoftcsc413\.github\.io\/2024F-website\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Florian Shkurti</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/UofTCSC413/2024F-website/edit/main/labs/lab09.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/UofTCSC413/2024F-website/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>