<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lab 01 – CSC413 - Fall 2024</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-87aedce6c5c32840e93824ffc3714634.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-69b08db278f499bc7d235ce342f73d67.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-87aedce6c5c32840e93824ffc3714634.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Lab 01 – CSC413 - Fall 2024">
<meta property="og:description" content="Homepage for CSC413 - Neural Networks and Deep Learning, Fall 2024.">
<meta property="og:site_name" content="CSC413 - Fall 2024">
<meta name="twitter:title" content="Lab 01 – CSC413 - Fall 2024">
<meta name="twitter:description" content="Homepage for CSC413 - Neural Networks and Deep Learning, Fall 2024.">
<meta name="twitter:image" content="https://uoftcsc413.github.io/2024F-website/labs/images/twitter-card.png">
<meta name="twitter:creator" content="@minebocek">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#csc413-lab-1-linear-models" id="toc-csc413-lab-1-linear-models" class="nav-link active" data-scroll-target="#csc413-lab-1-linear-models">CSC413 Lab 1: Linear Models</a>
  <ul class="collapse">
  <li><a href="#submission" id="toc-submission" class="nav-link" data-scroll-target="#submission">Submission</a></li>
  <li><a href="#google-colab-setup" id="toc-google-colab-setup" class="nav-link" data-scroll-target="#google-colab-setup">Google Colab Setup</a></li>
  <li><a href="#part-1.-data" id="toc-part-1.-data" class="nav-link" data-scroll-target="#part-1.-data">Part 1. Data</a></li>
  <li><a href="#part-2.-a-linear-model-in-pytorch" id="toc-part-2.-a-linear-model-in-pytorch" class="nav-link" data-scroll-target="#part-2.-a-linear-model-in-pytorch">Part 2. A Linear Model in PyTorch</a></li>
  <li><a href="#part-3.-cross-entropy-loss-and-automatic-gradient-computation" id="toc-part-3.-cross-entropy-loss-and-automatic-gradient-computation" class="nav-link" data-scroll-target="#part-3.-cross-entropy-loss-and-automatic-gradient-computation">Part 3. Cross Entropy Loss and Automatic Gradient Computation</a></li>
  <li><a href="#part-4.-neural-network-training-via-stochastic-gradient-descent." id="toc-part-4.-neural-network-training-via-stochastic-gradient-descent." class="nav-link" data-scroll-target="#part-4.-neural-network-training-via-stochastic-gradient-descent.">Part 4. Neural Network Training via Stochastic Gradient Descent.</a></li>
  <li><a href="#part-5.-hyperparameter-tuning" id="toc-part-5.-hyperparameter-tuning" class="nav-link" data-scroll-target="#part-5.-hyperparameter-tuning">Part 5. Hyperparameter Tuning</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/UofTCSC413/2024F-website/edit/main/labs/lab01.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/UofTCSC413/2024F-website/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lab 01</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="csc413-lab-1-linear-models" class="level1">
<h1>CSC413 Lab 1: Linear Models</h1>
<p>In this lab, we will review <strong>linear models</strong> for classification, which was discussed in depth in CSC311. We will use this as an opportunity to review key ideas, including the splitting of the dataset into train/validation/test sets, optimization methods using Stochastic Gradient Descent, and so on. This lab reviews Python libraries that you have used in CSC311, including <code>numpy</code>, <code>matplotlib</code> and others.</p>
<p>But the main aim of this lab is to introduce a new Python library that we will be using throughout this course: PyTorch. PyTorch provides automatic differentiation capabilities and other neural network tools. This means that <strong>we do not need to compute gradients ourselves</strong>! Instead, we rely on PyTorch to build the computation graph and compute gradients. PyTorch can do this because it knows how to compute gradients for simple operations like addition, multiplication, ReLU activation, and common functions like exponentials, logarithms, and so on. The neural networks we build require computation that are combinations of these simple operations.</p>
<p>For now, we will we solve a multi-class classification problem in two ways: first with <code>numpy</code>, and then with PyTorch.</p>
<p>By the end of this lab, you will be able to:</p>
<ol type="1">
<li>Implement and train a multi-class logistic regression model using PyTorch.</li>
<li>Compute the accuracy metric for a machine learning model.</li>
<li>Compute numerically, via numpy, the gradient of a linear model.</li>
<li>Check that PyTorch correctly computes gradients for a linear model via automatic differentiation.</li>
<li>Identify and explain the elements of the training loop in a PyTorch implementation.</li>
<li>Identify optimization parameters and hyperparameters, and explain how hyperparameter choices impact training, underfitting and overfitting (i.e.&nbsp;bias/variance decomposition).</li>
</ol>
<p>Please work in groups of 1-2 during the lab, but submit your own solution individually.</p>
<section id="submission" class="level2">
<h2 class="anchored" data-anchor-id="submission">Submission</h2>
<p>If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.</p>
<p>Submit the ipynb file <code>lab01.ipynb</code> on Markus <strong>containing all your solutions to the Graded Task</strong>s. Your notebook file must contain your code <strong>and outputs</strong> where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.</p>
<p>For this lab, you should submit the following:</p>
<ul>
<li>Part 1. Your explanation of the purpose of the training and validation sets. (1 point)</li>
<li>Part 2. Your implementation of <code>accuracy_basic</code>. (2 point)</li>
<li>Part 2. Your implementation of <code>accuracy_vectorized</code>. (1 point)</li>
<li>Part 2. Your implementation of <code>accuracy</code>. (1 point)</li>
<li>Part 3. Your computation of <code>model_bias_grad</code>. (2 points)</li>
<li>Part 4. Your completion of <code>train_model</code>. (1 point)</li>
<li>Part 5. Your list of hyperparameters. (1 point)</li>
<li>Part 5. Your explanation of what happens if the learning rate is too large. (1 point)</li>
</ul>
</section>
<section id="google-colab-setup" class="level2">
<h2 class="anchored" data-anchor-id="google-colab-setup">Google Colab Setup</h2>
<p>We will use Google Colab to open IPython Notebook (ipynb) file. This tool allows us to write and execute Python code through our browser, without any environmental setup.</p>
<p>Here are the steps to open ipynb file on Google Colab.</p>
<ol type="1">
<li>Download <code>lab01.ipynb</code>, available from the Quercus course website.</li>
<li>Click on the following link to open Google Colab: https://colab.research.google.com/</li>
<li>Click “Upload”, then choose the file which has been downloaded in step 1.</li>
</ol>
<p>And that’s it! Now we can start writing the codes, creating the new code or text cell, etc.</p>
<p>Here are some basic functionalities and features that you might find useful.</p>
<ol type="1">
<li><p>Running a cell<br>
Click the run button on the left side of the code cell (looks like a “play” button with a triangle in a circle)<br>
or<br>
press SHIFT + ENTER.</p></li>
<li><p>Installing libraries using Bash Commands<br>
Although most of the commonly used libraries (e.g.&nbsp;NumPy, Pandas, Matplotlib) are pre-installed, we may occasionally ask you to install new libraries or run other bash commands. Bash commands can be run by prefixing instructions in a code cell with ‘!’ in Google Colab (One exception: ‘cd’ command can be run by prefixing with ‘%’), e.g.&nbsp;<code>!pip install [package name]</code></p></li>
<li><p>Mounting Google Drive<br>
You may optionally mount Google Drive. Click the files button on the left pane, then click on ‘mount drive’ button (looks like a file icon with a google drive logo).<br>
or<br>
Run the following code snippet: <code>from google.colab import drive     drive.mount('/content/drive')</code> By mounting the drive, we can use any files or folders in our drive by using the path as follows: <code>/content/drive/MyDrive/[folder name]</code> For example, we can read the csv file uploaded in the drive using Pandas library as follows: <code>pd.read_csv('/content/drive/MyDrive/myfolder/myfile.csv')</code></p></li>
</ol>
<p>Now, we are ready to import the necessary packages and begin our lab.</p>
<pre><code>import torch
import matplotlib.pyplot as plt
import numpy as np</code></pre>
</section>
<section id="part-1.-data" class="level2">
<h2 class="anchored" data-anchor-id="part-1.-data">Part 1. Data</h2>
<p>We will use the MNIST data set, which consists of hand-written digits. This dataset is available within the <code>torchvision.datasets</code> library. The dataset creators divided the MNIST imgages into a training and test set, so that different researchers report test accuracy on a consistent set of images. (Recall that the test set is to be set aside and <strong>not</strong> used during training or to make any model decisions, and that it is used to estimate <em>how well your models generalize</em> to new data that it has never seen before.)</p>
<pre><code>from torchvision.datasets import MNIST

mnist_train = MNIST(root=".",      # where on the disk to store the data
                    download=True, # download the data if it does not already exist
                    train=True)    # use the training set (rather than the test set)</code></pre>
<p><strong>Task:</strong> If different practitioners are exploring machine learning models for the same task and data set, why is it important that they use these practitioners report their test performance (e.g., accuracy) on the same test set?</p>
<pre><code># TODO: Write your answer here</code></pre>
<p>It is always a good idea to visually inspect our data before working with it. First, let’s take a look at the first element of the training set:</p>
<pre><code>print(mnist_train[0]) # a tuple consisting of the image, and the label (5)</code></pre>
<p>The image can be displayed on Google Colab using matplotlib:</p>
<pre><code>plt.imshow(mnist_train[0][0], cmap='gray') # display the image</code></pre>
<p>It is important to note that images are represented using numbers on your machine. Converting this image into a numpy array shows a representation of the image using 28x28 numbers, each representing a pixel value.</p>
<pre><code>np.array(mnist_train[0][0])</code></pre>
<p><strong>Task:</strong> What does the numerical value 0 (smallest possible value) mean in the image? What about the largest possible value, 255?</p>
<pre><code># TODO: Write your answer here</code></pre>
<p>For our purposes, we will only use the first 5000 elements of the training set. This is to make training faster.</p>
<p>PyTorch also makes it easy to apply pre-processing transformations to the data, for example to normalize the data prior to using for training. We will use the standard preprocessing functions to <em>transform the images into tensors</em> for PyTorch to be able to use. This transformation also changes the values to be floating-point numbers between 0 and 1. Performing this transformation to PyTorch tensors now makes it easier to use PyTorch functionalities to help us create minibatches with this data.</p>
<pre><code>import torchvision.transforms as transforms

mnist_data = MNIST(root=".",      # where on the disk to store the data
                   download=True, # download the data if it does not already exist
                   train=True,    # use the canonical training set (rather than the test set)
                   transform=transforms.ToTensor()) # transforms the images into PyTorch tensors
mnist_data = list(mnist_data)[:5000]

print(mnist_data[0]) # a tuple consisting of a PyTorch tensor of shape (1, 28, 28) and an integer target label</code></pre>
<p>We will split the data set into 3000 for training, 1000 for validation, and 1000 for test:</p>
<pre><code>train_data = mnist_data[:3000]
val_data   = mnist_data[3000:4000]
test_data  = mnist_data[4000:]</code></pre>
<p><strong>Graded Task</strong>: We described, above, that the purpose of the <strong>test set</strong> is to estimate how well our models would generalize to new data that it has never seen before. What are the purposes of the training and validation sets?</p>
<pre><code># TODO: Write your answer here</code></pre>
</section>
<section id="part-2.-a-linear-model-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="part-2.-a-linear-model-in-pytorch">Part 2. A Linear Model in PyTorch</h2>
<p>You may recall from CSC311 that in machine learning, we often describe a model by first describing how to make predictions with a model (e.g., multi-class classification), and <strong>then</strong> describe how to find appropriate weights (e.g., an optimization method like gradient descent). We will follow that process here.</p>
<p>Recall that, mathematically, the multi-class classification model can be written as follows:</p>
<p><span class="math display">\[{\bf y} = \textrm{softmax}({\bf W} {\bf x} + {\bf b})\]</span></p>
<p>Where the <span class="math display">\[{\bf x}\]</span> vector represents the input (i.e., the vector representation of the MNIST image), and the <span class="math display">\[{\bf y}\]</span> vector contains the predicted probably of the image being in each class (i.e., predicted probability of the image being of each digit 0-9).</p>
<p><strong>Task</strong>: In the MNIST image classification tas, what are the shapes of the quantities <span class="math display">\[{\bf x}\]</span>, <span class="math display">\[{\bf W}\]</span>, <span class="math display">\[{\bf b}\]</span> and <span class="math display">\[{\bf y}\]</span>?</p>
<pre><code># TODO: Write your answer here</code></pre>
<p>The matrix <span class="math display">\[{\bf W}\]</span> and <span class="math display">\[{\bf b}\]</span> are the parameters of the model. The matrix <span class="math display">\[{\bf W}\]</span> is sometimes called the <em>weight matrix</em> and the vector <span class="math display">\[{\bf b}\]</span> the <em>bias vector</em>, but these parameters taken together are often referred to collectively as the <strong>weights</strong>. “Good” values of the parameters <span class="math display">\[{\bf W}\]</span> and <span class="math display">\[{\bf b}\]</span> are those that would produce values of the vector <span class="math display">\[{\bf y}\]</span> that more accurately match the actual target label. We will need to discuss what “good” means and how to measure “good”ness and optimize it. For now, let’s begin by applying and analyzing a “bad” model: a model where the parameters <span class="math display">\[{\bf W}\]</span> and <span class="math display">\[{\bf b}\]</span> are chosen randomly.</p>
<p>Notice that there are two parts to the computation <span class="math display">\[\textrm{softmax}({\bf W} {\bf x} + {\bf b})\]</span>: there is a <em>linear transformation</em> on <span class="math display">\[{\bf X}\]</span>, and then there is a softmax operation. PyTorch models these two components separately.</p>
<p>The <strong>linear transformation</strong> is modeled as a Python class in PyTorch. Since the parameters <span class="math display">\[{\bf W}\]</span> and <span class="math display">\[{\bf b}\]</span> are parameters of the linear transformation portion of the computation, the weights and biases are class attributes. The output of the linear transformation step is typically denoted using the symbol <span class="math display">\[{\bf z}\]</span>, and is called the <strong>logit</strong> (or unnormalized logits).</p>
<pre><code>import torch.nn as nn

model = nn.Linear(in_features=784,
                  out_features=10)

print(model.weight) # the weight parameter, initialized to random values
print(model.bias)   # the bias parameter, initialized to random values</code></pre>
<p>This <code>model</code> object can be called like a function to perform the computation <span class="math display">\[{\bf W}{\bf x} + {\bf b}\]</span>:</p>
<pre><code># reshape the input image into the shape [1, 784]
# PyTorch always expects inputs of shape [batch_size, num_features]
x = train_data[0][0].reshape(1, 784)

# Computes z = Wx + b, also called the *logit*
z = model(x) 
print(z)</code></pre>
<p>The <strong>softmax operation</strong> has no trainable parameters (i.e., no numbers that we tune that would effect the predictions of our model). Torch ahs a function <code>torch.softmax</code> that performs this operation, which normalizes the prediction so that the prediction represents a probability distribution. The <code>dim</code> parameter to the function tells PyTorch which dimension represent the different label classes (as opposed to, say, the dimension that represents different images in a batch).</p>
<pre><code>y = torch.softmax(z, dim=1)
print(y) # notice that this represents a probability distribution!</code></pre>
<p>If we are looking for a discrete/point prediction rather than a probability distribution, we will typically choose the label that the model believes to be <strong>most probable</strong>:</p>
<pre><code>pred = torch.argmax(y, axis=1)
print(pred) # a prediction of which class/digit the image belong to</code></pre>
<p>Note that if all we wanted was a discrete prediction, we need not compute the softmax! (Why is that? How can we prove this property mathematically, using the definition of the softmax operation?)</p>
<pre><code>pred = torch.argmax(z, axis=1)
print(pred)</code></pre>
<p><strong>Graded Task</strong>: Complete the function below, which computes the <strong>accuracy</strong> of a PyTorch model over a dataset. The accuracy metric is the proportion of predictions made that is correct, or:</p>
<p><span class="math display">\[\frac{\textrm{the number of correct predictions}}{\textrm{total number of predictions made}}\]</span></p>
<pre><code>def accuracy_basic(model, dataset):
    """
    Compute the accuracy of `model` over the `dataset`.
    We will take the **most probable class**
    as the class predicted by the model.

    Parameters:
        `model` - A torch.nn model. We will only be passing `nn.Linear` models.
                  However, to make your code more generally useful, do not access
                  `model.weight` and `model.bias` parameters directly. These
                  class attributes may not exist for other kinds of models.
        `dataset` - A list of 2-tuples of the form (x, t), where `x` is a PyTorch
                  tensor of shape [1, 28, 28] representing an MNIST image,
                  and `t` is the corresponding target label

    Returns: a floating-point value between 0 and 1.
    """
    total = 0      # count the total number of predictions made
    correct = 0    # count the number of correct predictions made
    for img, t in dataset:
        x = None # TODO - what should the input be? Recall that 
        x = img.reshape(1, 784) # SOLUTION
        z = None # TODO - how can we compute z = Wx + b using `model`?
        z = model(x) # SOLUTION
        pred = None # TODO - how can we obtain a point prediction?
        pred = int(torch.argmax(z, axis=1)) # SOLUTION
        if t == pred:
            correct += 1
        total += 1
    return correct / total


print("Accuracy over the training set:")
print(accuracy_basic(model, train_data))</code></pre>
<p><strong>Task</strong>: Explain why we would expect the training accuracy above to be poor.</p>
<pre><code># TODO: Your explanation goes here.</code></pre>
<p>One other nice thing about our <code>nn.Linear</code> model is that PyTorch <strong>vectorizes</strong> computations for us: we can make predictions for <em>many</em> images at the same time. Here is a rudimentary example where we make predictions for the first three images of our training set.</p>
<pre><code>x1 = train_data[0][0].reshape(1, 784)
x2 = train_data[1][0].reshape(1, 784)
x3 = train_data[2][0].reshape(1, 784)

X = torch.cat([x1, x2, x3]).reshape(-1, 784) # note the -1 value here, explained below
print(X.shape) # Pytorch figures out that the shape of this tensor needs to be [3, 784]</code></pre>
<p>The above code uses a features of PyTorch’s <code>reshape()</code> method that allows you to use the size <code>-1</code> as a placeholder value and let PyTorch figure out what the correct size should be.</p>
<p>Now, we can make predictions for all 3 images simultaneously</p>
<pre><code>z = model(X)
y = torch.softmax(z, dim=1)
print(y)</code></pre>
<p><strong>Task</strong> Complete the function <code>accuracy_vectorized</code> that outputs the same result as <code>accuracy_basic</code>, but uses vectorization to compute predictions for <strong>all</strong> inputs in the <code>dataset</code> simultaneously.</p>
<pre><code>def accuracy_vectorized(model, dataset):
    """
    Same signature as the `accuracy_basic()` function, but the call to model() is vectorized
    """
    X = torch.concat([x for x, t in dataset])
    t = torch.Tensor([t for x, t in dataset])
    z = None # TODO: use a single call to model() to compute the prediction for all inputs
    z = model(X.reshape(-1, 784)) # SOLUTION
    pred = None # TODO: `pred` should have the same shape as `t`
    pred = torch.argmax(z, axis=1)  # SOLUTION
    correct = int(torch.sum(t == pred)) # count the number of correct predictions
    total = t.shape[0]                  # count the total number of predictions made
    return correct / total</code></pre>
<p><strong>Task</strong>: Compare the runtime of <code>accuracy_basic</code> and <code>accuracy_vectorized</code> by running the two cells below. The line <code>%%time</code> prints the amount of time that Colab takes to run the code in the cell. The function call is repeated 100 times so that we can more clearly see the difference in runtime. Using what you learned from CSC311, explain why <code>accuracy_vectorized</code> is faster.</p>
<pre><code>import time

start_time = time.time()
for i in range(100):
    accuracy_basic(model, train_data)
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Elapsed time: {elapsed_time:.4f} seconds")</code></pre>
<pre><code>import time

start_time = time.time()
for i in range(100):
    accuracy_vectorized(model, train_data)
end_time = time.time()
elapsed_time = end_time - start_time

print(f"Elapsed time: {elapsed_time:.4f} seconds")</code></pre>
<pre><code># TODO: Your explanation goes here</code></pre>
<p>If our data set is large, feeding all inputs into the model at the same time may result in an out of memory error. Thus, we may find PyTorch’s <code>DataLoader</code> useful. This class takes our data set and the desired batch size, and splits the data into mini-batches of that size. We can use a loop to iterate over the minibatches:</p>
<pre><code>train_loader = torch.utils.data.DataLoader(train_data, batch_size=100)
for X, t in train_loader:
    print(X.shape)
    print(t.shape)
    break</code></pre>
<p><strong>Task</strong>: Complete the definition of the <code>accuracy</code> function below:</p>
<pre><code>def accuracy(model, dataset):
    """
    Same signature as the `accuracy_basic()` function, but we will use a DataLoader and process
    100 images at a time
    """
    correct, total = 0, 0
    loader = torch.utils.data.DataLoader(dataset, batch_size=100)
    for X, t in loader:
        z = None # TODO: use a single call to `model()` here as before
        z = model(X.reshape(-1, 784)) # SOLUTION
        pred = None # TODO: `pred` should have the same shape as `t`
        pred = torch.argmax(z, axis=1)  # SOLUTION
        # TODO: update `correct` and `total`
        correct += int(torch.sum(t == pred))  # SOLUTION
        total   += t.shape[0]  # SOLUTION
    return correct / total

accuracy(model, train_data)</code></pre>
</section>
<section id="part-3.-cross-entropy-loss-and-automatic-gradient-computation" class="level2">
<h2 class="anchored" data-anchor-id="part-3.-cross-entropy-loss-and-automatic-gradient-computation">Part 3. Cross Entropy Loss and Automatic Gradient Computation</h2>
<p>Now that we understand how a linear model makes predictions, we can explore how to modify its parameters to produce a model that makes “better” predictions. To do this, we will define a measure of “good”ness (or rather, “bad”ness) of a model that is <strong>differentiable with respect to the parameters</strong>. Differentiability is important, because it allows us to compute derivatives with respect to these parameters, which tells us how to tune the parameters to decrease “bad”ness.</p>
<p>This “badness” metric is called a <strong>loss function</strong>. A loss function compares the model prediction against the ground-truth target and produces a value representing how different the prediction is from the target. Like in CSC311, we will use the <strong>Cross-Entropy Loss</strong> for multi-class classification. In statistics courses you may learn about the theoretical reasons why the cross-entropy loss is appropriate for the multi-class classification task.</p>
<p><span class="math display">\[\mathcal{L}(y, t) = - t \log(y) - (1-t) \log(1-y)\]</span></p>
<p>You can read more about PyTorch’s implementation of the Cross-entropy loss here: <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" class="uri">https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html</a> One key thing to note is that the cross entropy loss takes as input the <strong>unnormalized logits</strong> (the z’s) and <em>not</em> the post-softmax, normalized probabilities (the y’s).</p>
<pre><code>criterion = nn.CrossEntropyLoss()</code></pre>
<p>Before we go further, let’s get some more intuition about the cross-entropy loss. We first demonstrate the cross entropy loss in action by computing hte “badness”</p>
<pre><code>x = train_data[0][0].reshape(1, 784)        # input
t = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index
z = model(x.reshape(-1, 784))               # prediction logit

loss = criterion(z, t)

print(loss)</code></pre>
<p><strong>Graded Task</strong>: Consider the code below. What value of <code>label</code> would produce the lowest cross-entropy loss? Why? To answer this question, start by exploring the arguments passed to the call to <code>criterion</code>, and form an understanding of what those arugments represent. (Why are there 3 possible values of <code>label</code>? What do the 3 floating-point values in the first argument to <code>criterion</code> represent?)</p>
<pre><code>label = None # 0, 1, or 2?
label = 2 # SOLUTION

loss = criterion(torch.Tensor([[1.5, 2.2, 3.2]]),
                 torch.Tensor([label]).long())
print(loss)</code></pre>
<pre><code># TODO: Explain why the label choice produces the lowest loss.</code></pre>
<p>Now that we have an understanding of the cross-entropy loss, we can begin to understand how PyTorch computes gradients. Notice that when we print the variable <code>loss</code> above, the value printed is not only a numerical value, but also has other information attached. These information help PyTorch compute gradients, and these gradients can be propagated backwards using the <code>loss.backward()</code> method.</p>
<p>Under the hood, PyTorch <em>performs backpropagation</em>, which we discussed in CSC311 and will review again in the coming weeks. After <code>loss.backward()</code> is computed, tensors like <code>model.bias</code> and <code>model.weights</code> will have gradients.</p>
<pre><code># recreate the model to clean up some hidden variables
model = nn.Linear(in_features=784, out_features=10)

x = train_data[0][0].reshape(1, 784)        # input
t = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index
z = model(x.reshape(-1, 784))               # prediction logit

loss = criterion(z, t)
print(loss)

loss.backward() # propagate the gradients with respect to the loss</code></pre>
<pre><code>print(model.bias.grad) # the gradient of the loss with respect to the bias vector</code></pre>
<pre><code>print(model.weight.grad) # the gradient of the loss with respect to the weight matrix</code></pre>
<p><strong>Graded Task</strong>: Verify that <code>model.bias.grad</code> is correct by computing this gradient explicitly in PyTorch. You may wish to begin by reviewing your past CSC311 notes, or by using calculus to find an expression to represent this quantity.</p>
<pre><code>t_onehot = torch.eye(10)[t] # You may find this useful. (Why? What does this quantity represent)
model_bias_grad = None      # TODO
model_bias_grad = (torch.softmax(z, dim=1) - t_onehot)   # SOLUTION

print(model_bias_grad) # should be the same as model.bias.grad</code></pre>
<p>The gradient computation works in a vectorized setting as well.</p>
<pre><code># create a data set with 3 inputs, 3 targets

x1 = train_data[0][0].reshape(1, 784)
x2 = train_data[1][0].reshape(1, 784)
x3 = train_data[1][0].reshape(1, 784)
X = torch.cat([x1, x2, x3]).reshape(-1, 784)
t = torch.Tensor([train_data[0][1], train_data[1][1], train_data[2][1]]).long()
                 
print(X.shape)
print(t)</code></pre>
<p>The average loss (mean) across the data points are shown:</p>
<pre><code>model = nn.Linear(in_features=784, out_features=10)
z = model(X)
loss = criterion(z, t)
print(loss)

loss.backward() # propagate the gradients with respect to the loss</code></pre>
<p><strong>Graded Task</strong>: Verify that <code>model.bias.grad</code> is correct when using vectorized input by computing this gradient explicitly in PyTorch. Again, we recommend working this out by hand first. After you have done so, you may find the function <code>torch.sum()</code> or <code>torch.mean()</code> helpful.</p>
<pre><code>t_onehot = torch.eye(10)[t] # TODO: understand the shape of this quantity before using it
model_bias_grad = None      # TODO
model_bias_grad = torch.mean((torch.softmax(z, dim=1) - t_onehot), axis=0) # SOLUTION

print(model_bias_grad)
print(model.bias.grad) # should be the same as above</code></pre>
</section>
<section id="part-4.-neural-network-training-via-stochastic-gradient-descent." class="level2">
<h2 class="anchored" data-anchor-id="part-4.-neural-network-training-via-stochastic-gradient-descent.">Part 4. Neural Network Training via Stochastic Gradient Descent.</h2>
<p>We are almost ready to use PyTorch to train the model. One last piece that we need is an optimizer that updates the model parameter based on the gradient. This update can be done in different ways, and the most basic approach discussed in CSC311 was using <strong>gradient descent</strong>.</p>
<p><span class="math display">\[{\bf W} \leftarrow {\bf W} - \lambda \frac{\partial \mathcal{L}}{\partial {\bf W}}\]</span></p>
<p>TODO: description of gradient descent here!!</p>
<p>When we use the entire training data set to compute the gradient of the mean loss (with respect to each parameter), we call the approach <strong>full batch gradient descent</strong>. However, this approach is expensive if we have a large training set. Typically, we approximate this mean loss using a <strong>minibatch</strong>, or a small sample of the training set. Using gradient descent with this approximate gradient is called <strong>stochastic gradient descent</strong>.</p>
<p>PyTorch has built-in classes inside the package <code>torch.optim</code> to perform these gradient update steps. We will use the <code>SGD</code> class. Initializing this class requires a few things, including the list of model parameters that we want to optimize. For us, the list of parameters to optimize include <code>model.weight</code> and <code>model.bias</code>, which we can obtain by calling <code>model.parameters()</code>.</p>
<pre><code>import torch.optim as optim

optimizer = optim.SGD(model.parameters(), # the parameters to optimize
                      lr=0.005)           # the learning rate</code></pre>
<p>There are two important optimizer methods that we will use. First is the <code>optimizer.zero_grad()</code> method, which clears the <code>.grad</code> attribute of the parameters. Let’s see how it works:</p>
<pre><code>print(model.bias.grad) # should be a nonzero value from above

optimizer.zero_grad()

print(model.bias.grad) # should be cleared</code></pre>
<p>The other method is the <code>step()</code> method, which performs the actual gradient descent update.</p>
<pre><code>model = nn.Linear(in_features=784, out_features=10)
optimizer = optim.SGD(model.parameters(), lr=0.005)

print(model.bias)

z = model(X)
loss = criterion(z, t)
loss.backward()

print(model.bias.grad) # gradient
print(model.bias - 0.005 * model.bias.grad) # this should be the updated value of model.bias

optimizer.step()

print(model.bias) # should be different compared to above</code></pre>
<p>Now that we have everything in place, we are ready to write the training loop:</p>
<p><strong>Graded Task</strong>: Complete the function below, which trains the model.</p>
<pre><code>def train_model(model,
                train_data,
                val_data,
                learning_rate=0.005,
                batch_size=100,
                num_epochs=10,
                plot_every=10):
    """
    Train the PyTorch model `model` using the training data `train_data` and the
    corresponding hyperparameters. Report training loss, training accuracy, and
    validation accuracy every `plot_every` iterations.
    """
    train_loader = torch.utils.data.DataLoader(train_data,
                                               batch_size=batch_size,
                                               shuffle=True) # reshuffle minibatches every epoch
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)

    # these lists will be used to track the training progress
    # and to plot the training curve
    iters, train_loss, train_acc, val_acc = [], [], [], []
    iter_count = 0 # count the number of iterations that has passed

    try:
        for e in range(num_epochs):
            for i, (images, labels) in enumerate(train_loader):
                z = None # TODO
                z = model(images.reshape(-1, 784)) # SOLUTION

                loss = None # TODO
                loss = criterion(z, labels) # SOLUTION

                loss.backward() # propagate the gradients
                optimizer.step() # update the parameters
                optimizer.zero_grad() # clean up accumualted gradients

                iter_count += 1
                if iter_count % plot_every == 0:
                    iters.append(iter_count)
                    train_loss.append(float(loss))
                    train_acc.append(accuracy(model, train_data))
                    val_acc.append(accuracy(model, val_data))
    finally:
        # This try/finally block is to display the training curve
        # even if training is interrupted
        plt.figure()
        plt.plot(iters[:len(train_loss)], train_loss)
        plt.title("Loss over iterations")
        plt.xlabel("Iterations")
        plt.ylabel("Loss")

        plt.figure()
        plt.plot(iters[:len(train_acc)], train_acc)
        plt.plot(iters[:len(val_acc)], val_acc)
        plt.title("Accuracy over iterations")
        plt.xlabel("Iterations")
        plt.ylabel("Accuracy")
        plt.legend(["Train", "Validation"])

model = nn.Linear(784, 10)
train_model(model, train_data, val_data)</code></pre>
</section>
<section id="part-5.-hyperparameter-tuning" class="level2">
<h2 class="anchored" data-anchor-id="part-5.-hyperparameter-tuning">Part 5. Hyperparameter Tuning</h2>
<p>Our training process is not yet complete. In general, the performance of machine learning models depend heavily on the hyperparameter settings used. Hyperparameters are settings that cannot be tuned via gradient descent in a straightforward way. These settings can affect the model architecture, but may also affect the optimization process.</p>
<p><strong>Graded Task</strong>: What are some examples of hyperparameters that affect the model architecture of a model? You may consider some hyperparameters that you learned about in CSC311. List at least 3 examples.</p>
<pre><code># TODO: List at least 3 examples</code></pre>
<p><strong>Task</strong>: What are some examples of hyperparameters that affect the optimization process?</p>
<pre><code># TODO: List at least 2 examples</code></pre>
<p>Model architecture related hyperparameters are important to tune and should <em>not</em> be neglected in practical application. However, since we are working with a linear model right now, we are limited in this lab to exploring the optimization hyperparameters.</p>
<p><strong>Task</strong>: What happens if the learning rate is too small? Provide an example training curve by calling <code>train_model</code> with a low learning rate, and describe the features of the training curve that you see.</p>
<pre><code># TODO</code></pre>
<p><strong>Graded Task</strong>: What happens if the learning rate is too large? Provide an example training curve by calling <code>train_model</code> with a large learning rate, and describe the features of the training curve that you see.</p>
<pre><code># TODO</code></pre>
<p>Hyperparameter choices interact with one another. Thus, practitioners use a strategy called <strong>grid search</strong> to try all variations of hyperparameters from a set of hyperparameters. We will not do that here since linear models don’t yet have many hyperparameters to work with.</p>
<p><strong>Task</strong>: Choose the best model that you have trained. Typically we make this choice using the validation accuracy. To understand how well the model you choose would generalize to unseen data, we use the <em>test data</em>. Compute the test accuracy for this model by calling the function <code>accuracy()</code> on the model and the test data.</p>
<pre><code># TODO: Compute the test accuracy</code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/uoftcsc413\.github\.io\/2024F-website\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Florian Shkurti</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/UofTCSC413/2024F-website/edit/main/labs/lab01.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/UofTCSC413/2024F-website/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>