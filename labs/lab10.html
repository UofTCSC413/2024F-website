<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>lab10 – CSC413 - Fall 2024</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-87aedce6c5c32840e93824ffc3714634.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-69b08db278f499bc7d235ce342f73d67.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-87aedce6c5c32840e93824ffc3714634.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="CSC413 - Fall 2024">
<meta property="og:description" content="Homepage for CSC413 - Neural Networks and Deep Learning, Fall 2024.">
<meta property="og:site_name" content="CSC413 - Fall 2024">
<meta name="twitter:title" content="CSC413 - Fall 2024">
<meta name="twitter:description" content="Homepage for CSC413 - Neural Networks and Deep Learning, Fall 2024.">
<meta name="twitter:image" content="https://uoftcsc413.github.io/2024F-website/labs/images/twitter-card.png">
<meta name="twitter:creator" content="@minebocek">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#csc413-lab-8-text-classification-using-rnns" id="toc-csc413-lab-8-text-classification-using-rnns" class="nav-link active" data-scroll-target="#csc413-lab-8-text-classification-using-rnns">CSC413 Lab 8: Text Classification using RNNs</a>
  <ul class="collapse">
  <li><a href="#submission" id="toc-submission" class="nav-link" data-scroll-target="#submission">Submission</a></li>
  <li><a href="#part-1.-data" id="toc-part-1.-data" class="nav-link" data-scroll-target="#part-1.-data">Part 1. Data</a></li>
  <li><a href="#part-2.-model" id="toc-part-2.-model" class="nav-link" data-scroll-target="#part-2.-model">Part 2. Model</a></li>
  <li><a href="#part-3.-training" id="toc-part-3.-training" class="nav-link" data-scroll-target="#part-3.-training">Part 3. Training</a></li>
  <li><a href="#part-4.-pretrained-embeddings" id="toc-part-4.-pretrained-embeddings" class="nav-link" data-scroll-target="#part-4.-pretrained-embeddings">Part 4. Pretrained Embeddings</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/UofTCSC413/2024F-website/edit/main/labs/lab10.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/UofTCSC413/2024F-website/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>





<section id="csc413-lab-8-text-classification-using-rnns" class="level1">
<h1>CSC413 Lab 8: Text Classification using RNNs</h1>
<p><strong>Sentiment Analysis</strong> is the problem of identifying the writer’s sentiment given a piece of text. Sentiment Analysis can be applied to movie reviews, feedback of other forms, emails, tweets, course evaluations, and much more.</p>
<p>In this lab, we will build an RNN to classify positive vs negative tweets We use the Sentiment140 data set, which contains tweets with either a positive or negative emoticon. Our goal is to determine whether which type of emoticon the tweet (with the emoticon removed) contained. The dataset was actually collected by a group of students, much like you, who are doing their first machine learning projects.</p>
<p>By the end of this lab, you will be able to:</p>
<ul>
<li>Use PyTorch to train an RNN model</li>
<li>Apply and analyze the components of an RNN model</li>
<li>Explain how batching is done on sequence data, where the training data in a batch may have different lengths</li>
<li>Use pre-trained word embeddings as part of a transfer learning strategy for text classification</li>
<li>Understand the bias that exists in word embeddings and language models.</li>
</ul>
<p>Acknowledgements:</p>
<ul>
<li>Data is sampled from http://help.sentiment140.com/for-students</li>
</ul>
<p>Please work in groups of 1-2 during the lab.</p>
<section id="submission" class="level2">
<h2 class="anchored" data-anchor-id="submission">Submission</h2>
<p>If you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.</p>
<p>Submit the ipynb file <code>lab10.ipynb</code> on Markus <strong>containing all your solutions to the Graded Task</strong>s. Your notebook file must contain your code <strong>and outputs</strong> where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.</p>
<p>For this lab, you should submit the following:</p>
<ul>
<li>Part 1. Your output showing several positive tweets. (1 point)</li>
<li>Part 2. Your explanation of the shapes of <code>wordemb</code>. (1 point)</li>
<li>Part 2. Your explanation of the shapes of <code>h</code> and <code>out</code>. (2 points)</li>
<li>Part 2. Your explanation of why computing the mean and max of hidden states across <em>all</em> time steps is likely more informative than using the final output state. (1 point)</li>
<li>Part 3. Your demonstration of the model’s ability to “overfit” on a data set. (1 point)</li>
<li>Part 3. Your output from training the model on the full data set. (1 point)</li>
<li>Part 4. Your explanation of why <code>MyGloveRNN</code> requires fewer iteration to obtain “good” accuracy. (1 point)</li>
<li>Part 4. Your comparison of <code>MyGloveRNN</code> and <code>MyRNN</code> in low data settings.. (1 point)</li>
<li>Part 4. Your explanation of where the biases in embeddings come from, and whether our model will have the same sorts of baises.. (1 point)</li>
</ul>
</section>
<section id="part-1.-data" class="level2">
<h2 class="anchored" data-anchor-id="part-1.-data">Part 1. Data</h2>
<p>Start by running these two lines of code to download the data on to Google Colab.</p>
<pre><code># Download tutorial data files.
!wget https://www.cs.toronto.edu/~lczhang/413/sample_tweets.csv</code></pre>
<p>As always, we start by understanding what our data looks like. Notice that the test set has been set aside for us. Both the training and test set files follow the same format. Each line in the csv file contains the tweet text, the string label “4” (positive) or “0” (negative), and some additional information about the tweet.</p>
<pre><code>import csv
datafile = "sample_tweets.csv"

# Training/Validation set
data = csv.reader(open(datafile))
for i, line in enumerate(data):
    print(line)
    if i &gt; 10:
        break</code></pre>
<p><strong>Task</strong>: How many positive and negative tweets are in this file?</p>
<pre><code># TODO
from collections import Counter # SOLUTION
print(Counter(x[0] for x in csv.reader(open(datafile))))</code></pre>
<p><strong>Graded Task</strong>: We have printed several negative tweets above. Print 10 positive tweets.</p>
<pre><code># TODO: Please make sure to include both your code and the
# printed output</code></pre>
<p>We will now split the dataset into training, validation, and test sets:</p>
<pre><code># read the data; convert labels into integers
data = [(review, int(label=='4'))  # label 1 = positive, 0 = negative
        for label, _, _, _, _, review in csv.reader(open(datafile))]

# shuffle the data, since the file stores all negative tweets first
import random
random.seed(42)
random.shuffle(data)

train_data = data[:50000] 
val_data = data[50000:60000] 
test_data = data[60000:]</code></pre>
<p>In order to be able to use neural networks to make predictions about these tweets, we need to begin by convert these tweets into sequences of numbers, each representing a words. This is akin to a one-hot encoding: each word will be converted into an a number representing the unique <em>index</em> of that word.</p>
<p>Although we could do this conversion by writing our own python code, torch has a package called <strong>torchtext</strong> that has utilities useful for text classification and generation tasks. In particular, the <code>Vocab</code> class and <code>build_vocab_from_iterator</code> will be useful for us for building the mapping from words to indices.</p>
<pre><code>import torchtext

from torchtext.data.utils import get_tokenizer
from torchtext.vocab import Vocab, build_vocab_from_iterator

# we will *tokenize* each word by using a tokenzier from 
# https://pytorch.org/text/stable/data_utils.html#get-tokenizer

tokenizer = get_tokenizer("basic_english")
train_data_words = [tokenizer(x) for x, t in train_data]

# build the vocabulary object. the parameters to this function
# is described below
vocab = build_vocab_from_iterator(train_data_words,
                                  specials=['&lt;bos&gt;', '&lt;eos&gt;', '&lt;unk&gt;', '&lt;pad&gt;'],
                                  min_freq=10)

# set the index of a word not in the vocabulary
vocab.set_default_index(2) # this is the index of the `&lt;unk&gt;` keyword</code></pre>
<p>Now, <code>vocab</code> is an object of class <code>Vocab</code> (see more here <a href="https://pytorch.org/text/stable/vocab.html">https://pytorch.org/text/stable/vocab.html</a> ) that provides functionalities for converting words into their indices. In addition to words appearing in the training set, ther are four special tokens that we use, akin to placeholder words:</p>
<ul>
<li><code>&lt;bos&gt;</code>, to indicate the beginning of a sequence.</li>
<li><code>&lt;eos&gt;</code>, to indicate the end of a sequence.</li>
<li><code>&lt;unk&gt;</code>, to indicate a word that is <em>not</em> in the vocabulary. This includes words that appear too infrequently to be included in the vocabulary, and any other words in the validation/test sets that are not see in training.</li>
<li><code>&lt;pad&gt;</code>, used for padding shorter sequences in a batch: since each tweet may have different length, the shorter tweets in each batch will be padded with the <code>&lt;pad&gt;</code> token so that each sequence (tweet) in a batch has the same length.</li>
</ul>
<p>The <code>min_freq</code> parameter identifies the minimum number of times a word must appear in the training set in order to be included in the vocabulary.</p>
<p>Here you can see the <code>vocab</code> object in action:</p>
<pre><code># Print the number of words in the vocabulary
print(len(vocab))

# Convert a tweet into a sequence of word indices.
tweet = 'The movie Pneumonoultramicroscopicsilicovolcanoconiosis is a good movie, it is very funny'
tokens = tokenizer(f'&lt;bos&gt; {tweet} &lt;eos&gt;')
print(tokens)
indices = vocab.forward(tokens)
print(indices)</code></pre>
<p><strong>Task</strong>: What is the index of the <code>&lt;pad&gt;</code> token?</p>
<pre><code># TODO: write code to identify the index of the `&lt;pad&gt;` token
vocab.forward(['&lt;pad&gt;']) # SOLUTION: 3</code></pre>
<p>Now let’s apply this transformation to the entire set of training, validation, and test data.</p>
<pre><code>
def convert_indices(data, vocab):
    """Convert data of form [(tweet, label)...] where tweet is a string
    into an equivalent list, but where the tweets represented as a list
    of word indices.
    """
    return [(vocab.forward(tokenizer(f'&lt;bos&gt; {text} &lt;eos&gt;')), label)
            for (text, label) in data]

train_data_indices = convert_indices(train_data, vocab)
val_data_indices = convert_indices(val_data, vocab)
test_data_indices = convert_indices(test_data, vocab)</code></pre>
<p>We have seen that PyTorch’s <code>DataLoader</code> provides an easy way to form minibatches when we worked with image data. However, text and sequence data is more challenging to work with since the sequences may not be the same length.</p>
<p>Although we can (and will!) continue to use <code>DataLoader</code> for our text data, we need to provide a function that merges sequences of various lengths into two PyTorch tensors correspondingg to the inputs and targets for that batch.</p>
<p><strong>Task</strong>: Following the instructions below, complete the <code>collate_batch</code> function, which creates the input and target tensors for a batch of data.</p>
<pre><code>import torch
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

def collate_batch(batch):
    """
    Returns the input and target tensors for a batch of data

    Parameters:
        `batch` - An iterable data structure of tuples (indices, label),
                  where `indices` is a sequence of word indices, and 
                  `label` is either 1 or 0.

    Returns: a tuple `(X, t)`, where 
        - `X` is a PyTorch tensor of shape (batch_size, sequence_length)
        - `t` is a PyTorch tensor of shape (batch_size)
    where `sequence_length` is the length of the longest sequence in the batch
    """

    text_list = []  # collect each sample's sequence of word indices
    label_list = [] # collect each sample's target labels
    for (text_indices, label) in batch:
        text_list.append(torch.tensor(text_indices))
        # TODO: what do we need to do with `label`?
        label_list.append(label) # SOLUTION

    X = pad_sequence(text_list, padding_value=3).transpose(0, 1)
    t = None # TODO
    t = torch.tensor(label_list) # SOLUTION
    return X, t


train_dataloader = DataLoader(train_data_indices, batch_size=10, shuffle=True,
                              collate_fn=collate_batch)</code></pre>
<p>With the above code in mind, we should be able to extract batches from <code>train_dataloader</code>. Notice that <code>X.shape</code> is different in each batch. You should also see that the index <code>3</code> is used to pad shorter sequences in in a batch.</p>
<pre><code>for i, (X, t) in enumerate(train_dataloader):
    print(X.shape, t.shape)
    if i &gt;= 10:
        break

print(X)</code></pre>
<p><strong>Task</strong>: Why does each sequence begin with the token <code>0</code>, and end with the token <code>1</code> (ignoring the paddings).</p>
<pre><code># TODO: Your explanation goes here</code></pre>
</section>
<section id="part-2.-model" class="level2">
<h2 class="anchored" data-anchor-id="part-2.-model">Part 2. Model</h2>
<p>We will use a recurrent neural network model to classify positive vs negative sentiments. Our RNN model will have three components that are typical in a sequence classification model:</p>
<ul>
<li>An <em>embedding layer</em>, which will map each word index (akin to a one-hot embedding) into a low-dimensional vector. This layer as having the same functionality as the weights <span class="math inline">\(W^{(word)}\)</span> from lab 2.</li>
<li>A <em>recurrent layer</em>, which performs the recurrent neural network computation. The input to this layer is the low-dimensional embedding vectors for each word in the sequence.</li>
<li>A <em>fully connected layer</em>, which computes the final binary classification using features computed from the recurrent layer. In our case, we concatenate the <em>max</em> and <em>mean</em> of the hidden units across the time steps (i.e.&nbsp;across each word).</li>
</ul>
<p>Let’s define the model that we will use, and then explore it step by step.</p>
<pre><code>import torch.nn as nn

class MyRNN(nn.Module):
    def __init__(self, vocab_size, emb_size, hidden_size, num_classes):
        super(MyRNN, self).__init__()
        self.vocab_size = vocab_size
        self.emb_size = emb_size
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        self.emb = nn.Embedding(vocab_size, emb_size)
        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, X):
        # Look up the embedding
        wordemb = self.emb(X)
        # Forward propagate the RNN
        h, out = self.rnn(wordemb)
        # combine the hidden features computed from *each* time step of
        # the RNN. we do this by 
        features = torch.cat([torch.amax(h, dim=1),
                              torch.mean(h, dim=1)], axis=-1)
        # Compute the final prediction
        z = self.fc(features)
        return z

model = MyRNN(len(vocab), 128, 64, 2)</code></pre>
<p>To explore exactly what this model is doing, let’s grab one batch of data from the data loader we created. We will observe, step-by-step, what computation will be performed on the input <code>X</code> to obtain the final prediction. We do this by emulating the <code>forward</code> method of the <code>MyRNN</code> function.</p>
<pre><code>X, t = next(iter(train_dataloader))

print(X.shape)</code></pre>
<p><strong>Graded Task</strong>: Run the code below to check the shape of <code>wordemb</code>. What shape does this tensor have? Explain what each dimension in this shape means.</p>
<pre><code>wordemb = model.emb(X)

print(wordemb.shape)

# TODO: Include your explanation here</code></pre>
<p><strong>Graded Task</strong>: Run the code below, which computes the RNN forward pass, with <code>wordemb</code> as input. What shape do the tensors <code>h</code> and <code>out</code> have? Explain what these tensors correspond to. (See the RNN reference <a href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html">https://pytorch.org/docs/stable/generated/torch.nn.RNN.html</a> on the PyTorch documentation page.)</p>
<pre><code>h, out = model.rnn(wordemb)

print(h.shape)
print(out.shape)

# The tensors `h` and `out` are related. To see the relation,
# choose an index in the batch and compare the following two
# vectors in `h` and `out`.
index = 2 # choose an index to iterate through the batch
print(h[index, -1, :])
print(out[0, index, :])

# TODO: Include your explanation here</code></pre>
<p><strong>Graded Task</strong>: There is a step in the MyRNN forward pass that combines the features from <em>each</em> time step of the RNN by computing:</p>
<ol type="1">
<li>the <em>maximum</em> value of each position in the hidden vector.</li>
<li>the <em>mean</em> value of each position in the hidden vector.</li>
<li>concatenating the resulting two vectors.</li>
</ol>
<p>(Note that in the demo below, we are working with a minibatch. Thus, each of <code>out1</code>, <code>out2</code>, and <code>features</code> below are <em>matrices</em> containing the vectors from each minibatch)</p>
<p>This method typically performs better than, say, taking the hidden state at the last time step (the value <code>out</code> from above). Explain, intuitively, why you might expect this performance to be the case for a sentiment analysis task.</p>
<pre><code>out1 = torch.amax(h, dim=1)
out2 = torch.mean(h, dim=1)
features = torch.cat([out1, out2], axis=-1)

# Compare, for a single input in the batch, the connection between
# `h`, `out1`, `out2` and `features`:
print(h[index, :, :])
print(out1[index, :])
print(out2[index, :])
print(features[index, :])

# TODO: Include your explanation here</code></pre>
<p><strong>Task</strong>: Finally, the model uses the <code>features</code> tensor to compute the prediction for each element in the batch. Run the code below to complete this step.</p>
<pre><code>print(model.fc(features))</code></pre>
<p>There is one more thing we need to do before training the model, which is to write a function to estimate the accuracy of the model. This is done for you below.</p>
<pre><code>def accuracy(model, dataset, max=1000):
    """
    Estimate the accuracy of `model` over the `dataset`.
    We will take the **most probable class**
    as the class predicted by the model.

    Parameters:
        `model`   - An object of class nn.Module
        `dataset` - A dataset of the same type as `train_data`.
        `max`     - The max number of samples to use to estimate 
                    model accuracy

    Returns: a floating-point value between 0 and 1.
    """

    correct, total = 0, 0
    dataloader = DataLoader(dataset,
                            batch_size=1,  # use batch size 1 to prevent padding
                            collate_fn=collate_batch)
    for i, (x, t) in enumerate(dataloader):
        z = model(x)
        y = torch.argmax(z, axis=1)
        correct += int(torch.sum(t == y))
        total   += 1
        if i &gt;= max:
            break
    return correct / total

accuracy(model, train_data_indices) # should be close to half</code></pre>
</section>
<section id="part-3.-training" class="level2">
<h2 class="anchored" data-anchor-id="part-3.-training">Part 3. Training</h2>
<p>In this section, we will train the <code>MyRNN</code> model to classify tweets. As the models that we are building begin to increase in complexity, it is important to use good debugging techniques. In this section, we will introduce the technique of checking whether the model and training code is able to overfit on a small training set. This is a way to check for bugs in the implementation.</p>
<p><strong>Task</strong>: Complete the training code below</p>
<pre><code>import torch.optim as optim 
import matplotlib.pyplot as plt

def train_model(model,                # an instance of MLPModel
                train_data,           # training data
                val_data,             # validation data
                learning_rate=0.001,
                batch_size=100,
                num_epochs=10,
                plot_every=50,        # how often (in # iterations) to track metrics
                plot=True):           # whether to plot the training curve
    train_loader = torch.utils.data.DataLoader(train_data,
                                               batch_size=batch_size,
                                               collate_fn=collate_batch,
                                               shuffle=True) # reshuffle minibatches every epoch
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # these lists will be used to track the training progress
    # and to plot the training curve
    iters, train_loss, train_acc, val_acc = [], [], [], []
    iter_count = 0 # count the number of iterations that has passed

    try:
        for e in range(num_epochs):
            for i, (texts, labels) in enumerate(train_loader):
                z = None # TODO
                z = model(texts) # SOLUTION

                loss = None # TODO
                loss = criterion(z, labels) # SOLUTION

                loss.backward() # propagate the gradients
                optimizer.step() # update the parameters
                optimizer.zero_grad() # clean up accumualted gradients

                iter_count += 1
                if iter_count % plot_every == 0:
                    iters.append(iter_count)
                    ta = accuracy(model, train_data)
                    va = accuracy(model, val_data)
                    train_loss.append(float(loss))
                    train_acc.append(ta)
                    val_acc.append(va)
                    print(iter_count, "Loss:", float(loss), "Train Acc:", ta, "Val Acc:", va)
    finally:
        # This try/finally block is to display the training curve
        # even if training is interrupted
        if plot:
            plt.figure()
            plt.plot(iters[:len(train_loss)], train_loss)
            plt.title("Loss over iterations")
            plt.xlabel("Iterations")
            plt.ylabel("Loss")

            plt.figure()
            plt.plot(iters[:len(train_acc)], train_acc)
            plt.plot(iters[:len(val_acc)], val_acc)
            plt.title("Accuracy over iterations")
            plt.xlabel("Iterations")
            plt.ylabel("Loss")
            plt.legend(["Train", "Validation"])</code></pre>
<p><strong>Graded Task</strong>: As a way to check the model and training code, check if your model can obtain a 100% training accuracy relatively quickly (e.g.&nbsp;within less than a minute of training time), when training on only the first 20 element of the training data.</p>
<pre><code>model = MyRNN(vocab_size=len(vocab),
              emb_size=300,
              hidden_size=64,
              num_classes=2)
# TODO: Include your code and output 
train_model(model, train_data_indices[:20], val_data_indices[:20], # SOLUTION
            batch_size=10, num_epochs=100, plot_every=1,    # SOLUTION
            learning_rate=0.001)                            # SOLUTION</code></pre>
<p><strong>Task</strong>: Will this model that you trained above have a high accuracy over the validation set? Explain why or why not.</p>
<pre><code># TODO: Your explanation goes here</code></pre>
<p><strong>Graded Task</strong>: Train your model on the full data set. What validation accuracy can you achieve?</p>
<pre><code># TODO: Include your code here. Try a few hyperparameter choices until you
# are satisfied that your model performance is reasonable (i.e. no obviously
# poor hyperparameter choices)
model = MyRNN(vocab_size=len(vocab), emb_size=300, hidden_size=64, num_classes=2) # SOLUTION
train_model(model, train_data_indices, val_data_indices, batch_size=100, num_epochs=20, learning_rate=0.001) # SOLUTION</code></pre>
<p>Instead of a (vanilla) RNN model, PyTorch also makes available <code>nn.LSTM</code> and <code>nn.GRU</code> units. They can be used in place of <code>nn.RNN</code> without further changes to the <code>MyRNN</code> code.</p>
<p>In general, gated units like LSTM’s are much more frequently used than vanilla RNNs, although transformers are much more popular now as well.</p>
</section>
<section id="part-4.-pretrained-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="part-4.-pretrained-embeddings">Part 4. Pretrained Embeddings</h2>
<p>As we saw in the previous lab on images, <strong>transfer learning</strong> is a useful technique in practical machine learning, especially in low-data settings: instead of training an entire neural network from scratch, we use (part of) a model that is pretrained on large amounts of similar data. We use the intermediate state of this pretrained model as features to our model—i.e.&nbsp;we use the pretrained models to compute <em>features</em>.</p>
<p>Just like with images, using a pretrained model is an important strategy for working with text. Large language models is an excellent demonstration of how generalizable pretrained features can be.</p>
<p>In this part of the lab, we will use a slightly older idea of using pretrained <em>word embeddings</em>. In particular, instead of training our own <code>nn.Embedding</code> layer, we will use GloVe embeddings (2014) <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a> trained on a large data set containing all of Wikipedia and other webpages.</p>
<p>Nowadays, large language model (LLMs), including those with APIs provided by various organizations, can also be used to map words/sentences into embeddings. However, the basic idea of using pretrained models in low-data settings remains similar. We will also identify some bias issues with pretrained word embeddings. There is evidence that these types of bias issues continues to persist in LLMs as well.</p>
<pre><code>from torchtext.vocab import GloVe

glove = torchtext.vocab.GloVe(name="6B", dim=300)</code></pre>
<p><strong>Task</strong>: Run the below code to print the GloVe word embedding for the word “cat”.</p>
<pre><code>print(glove['cat'])</code></pre>
<p>Unfortunately, it is not straightforward to add the <code>&lt;pad&gt;</code>, <code>&lt;unk&gt;</code>, <code>&lt;bos&gt;</code> and <code>&lt;eos&gt;</code> tokens. So we will do without them.</p>
<p><strong>Task</strong>: Run the below code to look up GloVe word indices for the training, validation, and test sets.</p>
<pre><code>def convert_indices_glove(data, default=len(glove)-1):
    result = []
    for text, label in data:
        words = tokenizer(text) # for simplicity, we wont use &lt;bos&gt; and &lt;eos&gt;
        indices = []
        for w in words:
            if w in glove.stoi:
                indices.append(glove.stoi[w])
            else:
                # this is a bit of a hack, but we will repurpose *last* word
                # (least common word) appearing in the GloVe vocabluary as our
                # '&lt;pad&gt;' token
                indices.append(default)
        result.append((indices, label),)
    return result

train_data_glove = convert_indices_glove(train_data)
val_data_glove = convert_indices_glove(val_data)
test_data_glove = convert_indices_glove(test_data)</code></pre>
<p>Now, we will modify the <code>MyRNN</code> to use the pretrained GloVe vectors:</p>
<pre><code>class MyGloveRNN(nn.Module):
    def __init__(self,  hidden_size, num_classes):
        super(MyGloveRNN, self).__init__()
        self.vocab_size, self.emb_size = glove.vectors.shape
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        self.emb = nn.Embedding.from_pretrained(glove.vectors)
        self.emb.requires_grad=False # do *not* update the glove embeddings
        self.rnn = nn.RNN(self.emb_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, X):
        # Look up the embedding
        wordemb = self.emb(X)
        # Forward propagate the RNN
        h, out = self.rnn(wordemb)
        # combine the hidden features computed from *each* time step of
        # the RNN. we do this by 
        features = torch.cat([torch.amax(h, dim=1),
                              torch.mean(h, dim=1)], axis=-1)
        # Compute the final prediction
        z = self.fc(features)
        return z

    def parameters(self):
        # do not return the parameters of self.emb 
        # so the optimizer will not update the parameters of self.emb
        return (p for p in super(MyGloveRNN, self).parameters() if p.requires_grad)


model = MyGloveRNN(64, 2)</code></pre>
<p><strong>Task</strong> Train this model. Use comparable hyperparameters so that you can compare your result against <code>MyRNN</code>.</p>
<pre><code># TODO: Train your model here, and include the output
model = MyGloveRNN(100, 2)                           # SOLUTION
train_model(model, train_data_glove, val_data_glove, # SOLUTION
              batch_size=100, # SOLUTION
              num_epochs=200, # SOLUTION
              plot_every=100, # SOLUTION
              learning_rate=0.001) # SOLUTION</code></pre>
<p><strong>Graded Task</strong>: You might notice that a <em>very</em> smaller number of iterations will be required to train this model to a reasonable performance (e.g.&nbsp;&gt;70% validation accuracy). Why might this be?</p>
<pre><code># TODO: Include your explanation here</code></pre>
<p><strong>Graded Task</strong>: Train both MyGloveRNN and MyRNN models using the corresponding embeddings (pretrained vs.&nbsp;not), <strong>but only with the first 200 data points in the training set</strong>. How do the validation accuracies compare between these two models?</p>
<pre><code># TODO: Training code for MyGloveRNN.
# Include outputs and training curves in your submission
glove_model = MyGloveRNN(100, 2)                                  # SOLUTION
train_model(glove_model, train_data_glove[:200], val_data_glove, # SOLUTION
              batch_size=200, # SOLUTION
              num_epochs=200, # SOLUTION
              plot_every=100, # SOLUTION
              learning_rate=0.001) # SOLUTION</code></pre>
<pre><code># TODO: Training code for MyRNN
# Include outputs and training curves in your submission
rnn_model = MyRNN(len(vocab), 300, 100, 2)                                  # SOLUTION
train_model(rnn_model, train_data_indices[:200], val_data_indices, # SOLUTION
              batch_size=200, # SOLUTION
              num_epochs=200, # SOLUTION
              plot_every=100, # SOLUTION
              learning_rate=0.001) # SOLUTION</code></pre>
<pre><code># TODO: Compare the validation accuaries here</code></pre>
<p>Machine learning models have an air of “fairness” about them, since models make decisions without human intervention. However, models can and do learn whatever bias is present in the training data. GloVe vectors seems innocuous enough: they are just representations of words in some embedding space. Even so, we will show that the structure of the GloVe vectors encodes the everyday biases present in the texts that they are trained on.</p>
<p>We start with an example analogy to demonstrate the power of GloVe embeddings that allows us to complete analogies by applying arithmetic operations to the word vectors.</p>
<p><span class="math display">\[doctor - man + woman \approx ??\]</span></p>
<p>To find the answers to the above analogy, we will compute the following vector, and then find the word whose vector representation is <em>closest</em> to it.</p>
<pre><code>v = glove['doctor'] - glove['man'] + glove['woman']</code></pre>
<p><strong>Task</strong>: Run the code below to find the closets word. You should see the word “nurse” fairly high up in that list.</p>
<pre><code>def print_closest_words(vec, n=5):
    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words
    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance
    for idx, difference in lst[1:n+1]:                         # take the top n
        print(glove.itos[idx], difference)

print_closest_words(v)</code></pre>
<p><strong>Task</strong>: To compare, use a similar method to find the answer to this analogy: <span class="math display">\[doctor - woman + man \approx ??\]</span></p>
<p>In other words, we go the opposite direction in the “gender” axis to check if similarly concerning analogies exist.</p>
<pre><code>print_closest_words(glove['doctor'] - glove['woman'] + glove['man'])</code></pre>
<p><strong>Task</strong>: Compare the following two outputs.</p>
<pre><code>print_closest_words(glove['programmer'] - glove['man'] + glove['woman'])</code></pre>
<pre><code>print_closest_words(glove['programmer'] - glove['woman'] + glove['man'])</code></pre>
<p><strong>Task</strong>: Compare the following two outputs.</p>
<pre><code>print_closest_words(glove['professor'] - glove['man'] + glove['woman'])</code></pre>
<pre><code>print_closest_words(glove['professor'] - glove['woman'] + glove['man'])</code></pre>
<p><strong>Task</strong>: Compare the following two outputs.</p>
<pre><code>print_closest_words(glove['engineer'] - glove['man'] + glove['woman'])</code></pre>
<pre><code>print_closest_words(glove['engineer'] - glove['woman'] + glove['man'])</code></pre>
<p><strong>Graded Task</strong>: Explain where the bias in these embeddings come from. Would you expect our word embeddings (trained on tweets) to be similarly problematic? Why or why not?</p>
<pre><code># TODO: Your explanation goes here</code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/uoftcsc413\.github\.io\/2024F-website\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Florian Shkurti</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/UofTCSC413/2024F-website/edit/main/labs/lab10.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/UofTCSC413/2024F-website/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>