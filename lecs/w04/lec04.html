<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <title>CSC413 - Fall 2024, UTM – CSC413 Neural Networks and Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta property="og:description" content="Lecture 4">
<meta property="og:site_name" content="CSC413 - Fall 2024, UTM">
<meta name="twitter:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta name="twitter:description" content="Lecture 4">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC413 Neural Networks and Deep Learning</h1>
  <p class="subtitle">Lecture 4</p>

<div class="quarto-title-authors">
</div>

</section>
<section>
<section id="lecture-overview" class="title-slide slide level1 center">
<h1>Lecture Overview</h1>

</section>
<section id="last-week" class="slide level2">
<h2>Last Week</h2>
<ul>
<li class="fragment">Automatic differentiation</li>
<li class="fragment">Distributed representations</li>
<li class="fragment">GloVe embeddings</li>
</ul>
</section>
<section id="this-week" class="slide level2">
<h2>This week</h2>
<ul>
<li class="fragment">Computer Vision</li>
<li class="fragment">Convolutional Layers</li>
<li class="fragment">Downsampling</li>
<li class="fragment">Training a ConvNet</li>
<li class="fragment">Examples and Applications</li>
</ul>
<!-- Commented out the GloVe section below as it is from a previous lecture. -->
<!-- 01vision -->
</section></section>
<section>
<section id="computer-vision" class="title-slide slide level1 center">
<h1>Computer Vision</h1>

</section>
<section id="computer-vision-is-hard" class="slide level2">
<h2>Computer vision is hard</h2>
<center>
<img data-src="imgs/bumbly.png" height="400"> <img data-src="imgs/bumbly2.png" height="400">
</center>
<aside class="notes">
<ul>
<li>In 2012, when Deep Learning had its most recent comeback it was because of computer vision.</li>
<li>Object change in pose, size, viewpoint, background, illumination</li>
<li>Some objects are hidden behind others: <em>occlusion</em></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="computer-vision-is-really-hard" class="slide level2">
<h2>Computer vision is really hard</h2>
<center>
<img data-src="imgs/cheesecat.png" height="400">
</center>
<p>How can you “hard code” an algorithm that still recognizes that this is a cat?</p>
</section>
<section id="working-with-small-images" class="slide level2">
<h2>Working with Small Images</h2>
<p>In the week 3 tutorial, we worked with small, MNIST images, which are <span class="math inline">\(28 \times 28\)</span> pixels, black and white.</p>
<center>
<img data-src="imgs/mnist.png" height="350">
</center>
<p>How do our models work?</p>
</section>
<section id="notebook-demo---logistic-regression-weights" class="slide level2">
<h2>Notebook Demo - Logistic Regression Weights</h2>
<center>
<img data-src="imgs/logistic_weights.png" height="500">
</center>
</section>
<section id="notebook-demo---mlp-weights-first-layer" class="slide level2">
<h2>Notebook Demo - MLP Weights (first layer)</h2>
<center>
<img data-src="imgs/mlp_weights.png" height="500">
</center>
</section>
<section id="working-with-large-images" class="slide level2">
<h2>Working with Large Images</h2>
<ul>
<li>Suppose you have an image that is 200 pixels x 200 pixels</li>
<li>There are 500 units in the first hidden layer</li>
</ul>
<p>Q: How many <strong>parameters</strong> will there be in the first layer?</p>
<div class="fragment">
<p>A: <span class="math inline">\(200 \times 200 \times 500 + 500 =\)</span> over 20 million!</p>
</div>
</section>
<section id="working-with-large-images-ii" class="slide level2">
<h2>Working with Large Images II</h2>
<p>Q: Why might using a fully connected layer be problematic?</p>
<ul>
<li class="fragment">computing predictions (forward pass) will take a long time</li>
<li class="fragment">large number of weights requires a lot of training data to avoid overfitting</li>
<li class="fragment">small shift in image can result in large change in prediction</li>
</ul>
<!-- 02 Convolutional Layers -->
</section></section>
<section>
<section id="convolutional-layers" class="title-slide slide level1 center">
<h1>Convolutional Layers</h1>

</section>
<section id="biological-influence" class="slide level2">
<h2>Biological Influence</h2>
<center>
<img data-src="imgs/biology.png" height="300">
</center>
<p>There is evidence that biological neurons in the visual cortex have <em>locally-connected</em> connections</p>
<p>See <a href="https://www.youtube.com/watch?v=IOHayh06LJ4">Hubel and Wiesel Cat Experiment</a> (Note: there is an anesthetised cat in the video that some may find disturbing).</p>
<aside class="notes">
<ul>
<li>Harvard neurophysiologists David H. Hubel / Torsten Wiesel</li>
<li>Inserted a microelectrode into primary visual cortex of a cat.</li>
<li>Projected patterns of light and dark on a screen in front of the cat</li>
<li>Found that some neurons fired rapidly when presented with lines at one angle, while others responded best to another angle.</li>
<li>More on this <a href="https://historyofinformation.com/detail.php?entryid=4726">here</a></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolutional-neural-network" class="slide level2">
<h2>Convolutional Neural Network</h2>
<ul>
<li class="fragment"><strong>Locally-connected layers</strong>: compute <em>local</em> features based on small regions of the image
<ul>
<li class="fragment">Examples of <em>features</em>:
<ul>
<li class="fragment">a horizontal edge in an area</li>
<li class="fragment">a vertical edge in an area</li>
<li class="fragment">a blob (no edges) in the area</li>
<li class="fragment">a circular shape in the area</li>
</ul></li>
</ul></li>
<li class="fragment"><strong>Weight-sharing</strong>: detect the <em>same</em> local features across the entire image</li>
</ul>
</section>
<section id="locally-connected-layers" class="slide level2">
<h2>Locally Connected Layers</h2>
<center>
<img data-src="imgs/excel/conv1_connections.png" height="400">
</center>
<p>Each hidden unit connects to a small region of the input (in this case a <span class="math inline">\(3 \times 3\)</span> region)</p>
</section>
<section id="locally-connected-layers-1" class="slide level2">
<h2>Locally Connected Layers</h2>
<center>
<img data-src="imgs/excel/conv1.png" height="400"> (Remove lines for readability)
</center>
</section>
<section id="locally-connected-layers-2" class="slide level2">
<h2>Locally Connected Layers</h2>
<center>
<img data-src="imgs/excel/conv2.png" height="400">
</center>
<p>Hidden unit geometry has a 2D geometry consistent with the input.</p>
</section>
<section id="locally-connected-layers-3" class="slide level2">
<h2>Locally Connected Layers</h2>
<center>
<img data-src="imgs/excel/conv3.png" height="400">
</center>
</section>
<section id="locally-connected-layers-4" class="slide level2">
<h2>Locally Connected Layers</h2>
<center>
<img data-src="imgs/excel/conv4.png" height="400">
</center>
</section>
<section id="locally-connected-layers-5" class="slide level2">
<h2>Locally Connected Layers</h2>
<center>
<img data-src="imgs/excel/conv5.png" height="400">
</center>
</section>
<section id="locally-connected-layers-6" class="slide level2">
<h2>Locally Connected Layers</h2>
<center>
<img data-src="imgs/excel/conv6.png" height="400">
</center>
</section>
<section id="locally-connected-layers-7" class="slide level2">
<h2>Locally Connected Layers</h2>
<center>
<img data-src="imgs/excel/convq.png" height="400">
</center>
<p>Q: Which region of the input is this hidden unit connected to?</p>
</section>
<section id="locally-connected-layers-8" class="slide level2">
<h2>Locally Connected Layers</h2>
<center>
<img data-src="imgs/excel/conv7.png" height="400">
</center>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<p>Fully-connected layers:</p>
<center>
<img data-src="imgs/fc.png" height="200">
</center>
<div class="fragment">
<p>Locally connected layers:</p>
<center>
<img data-src="imgs/lc.png" height="200">
</center>
</div>
</section>
<section id="weight-sharing" class="slide level2">
<h2>Weight Sharing</h2>
<div class="column" style="width:50%;">
<center>
<p><strong>Locally connected layers</strong></p>
<p><img data-src="imgs/lc.png" height="200"></p>
</center>
</div><div class="column" style="width:50%;">
<div class="fragment">
<center>
<p><strong>Convolutional layers</strong></p>
<p><img data-src="imgs/cn.png" height="200"></p>
</center>
</div>
</div><div class="fragment">
<p>Use the <em>same weights</em> across each region (each colour represents the same weight)</p>
</div>
</section>
<section id="convolution-computation" class="slide level2">
<h2>Convolution Computation</h2>
<center>
<img data-src="imgs/excel/comp1.png" height="300">
</center>
<p><span class="math display">\[\begin{align*}
300 = &amp; 100 \times 1 + 100 \times 2  + 100 \times 1 +  \\
      &amp; 100 \times 0 + 100 \times 0 + 100 \times 0 + \\
      &amp; 100 \times (-1) + 0 \times (-2) + 0 \times (-1)
\end{align*}\]</span></p>
</section>
<section id="convolution-computation-ii" class="slide level2">
<h2>Convolution Computation II</h2>
<p><span class="math display">\[\begin{align*}
300 = &amp; 100 \times 1 + 100 \times 2  + 100 \times 1 +  \\
      &amp; 100 \times 0 + 100 \times 0 + 100 \times 0 + \\
      &amp; 100 \times (-1) + 0 \times (-2) + 0 \times (-1)
\end{align*}\]</span></p>
<ul>
<li>The <strong>kernel</strong> or <strong>filter</strong> (middle) contains the trainable weights</li>
<li>In our example, the <strong>kernel size</strong> is <span class="math inline">\(3\times3\)</span></li>
<li>The “<strong>convolved features</strong>” is another term for the output hidden activation</li>
</ul>
</section>
<section id="convolution-computation-1" class="slide level2">
<h2>Convolution Computation</h2>
<center>
<img data-src="imgs/excel/comp2.png" height="300">
</center>
<p><span class="math display">\[\begin{align*}
300 = &amp;100 \times 1 + 100 \times 2  + 100 \times 1 +  \\
      &amp;100 \times 0 + 100 \times 0 + 100 \times 0 + \\
      &amp;0 \times (-1) + 0 \times (-2) + 100 \times (-1)
\end{align*}\]</span></p>
</section>
<section id="convolution-computation-2" class="slide level2">
<h2>Convolution Computation</h2>
<center>
<img data-src="imgs/excel/comp3.png" height="300">
</center>
<p>Q: What is the value of the highlighted hidden activation?</p>
<aside class="notes">
<ul>
<li>Pause here and try to solve this for yourself.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolution-computation-3" class="slide level2">
<h2>Convolution Computation</h2>
<center>
<img data-src="imgs/excel/comp4.png" height="300">
</center>
<p><span class="math display">\[\begin{align*}
100 = &amp;100 \times 1 + 100 \times 2  + 100 \times 1 +  \\
      &amp;100 \times 0 + 100 \times 0 + 100 \times 0 + \\
      &amp;0 \times (-1) + 100 \times (-2) + 100 \times (-1)
\end{align*}\]</span></p>
</section>
<section id="convolution-computation-4" class="slide level2">
<h2>Convolution Computation</h2>
<center>
<img data-src="imgs/excel/comp_all.png" height="400">
</center>
</section>
<section id="weight-sharing-1" class="slide level2">
<h2>Weight Sharing</h2>
<p>Each neuron on the higher layer is detecting the same feature, but in different locations on the lower layer</p>
<center>
<img data-src="imgs/conv.png" height="300">
</center>
<p>“Detecting” = output (activation) is high if feature is present “Feature” = something in a part of the image, like an edge or shape</p>
</section>
<section id="sobel-filter---weights-to-detect-horizontal-edges" class="slide level2">
<h2>Sobel Filter - Weights to Detect Horizontal Edges</h2>
<center>
<img data-src="imgs/sobel2.png" height="500">
</center>
<aside class="notes">
<ul>
<li>The kernel we have seen in the previous example is commonly used to detect horizontal edges.</li>
<li>It is known as the <em>Sobel Filter</em> or <em>Sobel Operator</em></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="sobel-filter---weights-to-detect-vertical-edges" class="slide level2">
<h2>Sobel Filter - Weights to Detect Vertical Edges</h2>
<center>
<img data-src="imgs/sobel1.png">
</center>
<aside class="notes">
<ul>
<li>There is another kernel for vertical edges.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="weights-to-detect-blobs" class="slide level2">
<h2>Weights to Detect Blobs</h2>

<img data-src="imgs/blob.png" class="r-stretch"><p>Q: What is the <em>kernel size</em> of this convolution?</p>
<aside class="notes">
<ul>
<li><em>Blob detection</em> is the task that aims at detecting regions of an image that differ from its surroundings.</li>
<li>This can involve e.g.&nbsp;brightness or color.</li>
<li>Here, the kernel is of size <span class="math inline">\(9\times 9\)</span></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="example" class="slide level2">
<h2>Example:</h2>
<p>Greyscale input image: <span class="math inline">\(7\times 7\)</span></p>
<p>Convolution <strong>kernel</strong>: <span class="math inline">\(3 \times 3\)</span></p>
<p>Q: How many hidden units are in the output of this convolution?</p>
<div class="fragment">
<p><img data-src="imgs/stride1.png" style="height:70.0%"></p>
<p>Q: How many trainable weights are there?</p>
</div>
<div class="fragment">
<p>There are <span class="math inline">\(3 \times 3 + 1\)</span> trainable weights (<span class="math inline">\(+ 1\)</span> for the bias)</p>
</div>
</section>
<section id="convolutions-in-practice" class="slide level2">
<h2>Convolutions in Practice</h2>
<p>What if we have a coloured image?</p>
<p>What if we want to compute <em>multiple</em> features?</p>
<!-- ![](imgs/vadar.jpg){ height=70% } -->
</section>
<section id="convolution-in-rgb" class="slide level2">
<h2>Convolution in RGB</h2>
<p><img data-src="imgs/conv_colour.png" style="height:70.0%"></p>
<p>The kernel becomes a 3-dimensional tensor!</p>
<p>In this example, the kernel has size <strong>3</strong> <span class="math inline">\(\times 3 \times 3\)</span></p>
</section>
<section id="convolutions-rgb-input" class="slide level2">
<h2>Convolutions: RGB Input</h2>
<p>Colour input image: <strong>3</strong> <span class="math inline">\(\times 7 \times 7\)</span></p>
<p>Convolution kernel: <strong>3</strong> <span class="math inline">\(\times 3 \times 3\)</span></p>
<p>Questions:</p>
<ul>
<li>How many units are in the output of this convolution?</li>
<li>How many trainable weights are there?</li>
</ul>
</section>
<section id="terminology" class="slide level2">
<h2>Terminology</h2>
<p>Input image: <span class="math inline">\(3 \times 32 \times 32\)</span></p>
<p>Convolution kernel: <strong>3</strong> <span class="math inline">\(\times 3 \times 3\)</span></p>
<ul>
<li>The number 3 is the number of <strong>input channels</strong> or <strong>input feature maps</strong></li>
</ul>
</section>
<section id="detecting-multiple-features" class="slide level2">
<h2>Detecting Multiple Features</h2>
<p>Q: What if we want to detect many features of the input? (i.e.&nbsp;<strong>both</strong> horizontal edges and vertical edges, and maybe even other features?)</p>
<div class="fragment">
<p>A: Have many convolutional filters!</p>
</div>
<div class="fragment">
<p><img data-src="imgs/depthcol.jpeg" style="width:50.0%"></p>
</div>
</section>
<section id="many-convolutional-filters" class="slide level2">
<h2>Many Convolutional Filters</h2>
<p>Input image: <span class="math inline">\(3 \times 7\times 7\)</span></p>
<p>Convolution kernel: <span class="math inline">\(3 \times 3 \times 3 \times\)</span> <strong>5</strong></p>
<p>Q:</p>
<ul>
<li>How many units are in the output of this convolution?</li>
<li>How many trainable weights are there?</li>
</ul>
</section>
<section id="more-terminology" class="slide level2">
<h2>More Terminology</h2>
<p>Input image of size <span class="math inline">\(3 \times 32 \times 32\)</span></p>
<p>Convolution kernel of <strong>3</strong> <span class="math inline">\(\times 3 \times 3 \times\)</span> <strong>5</strong></p>
<ul>
<li>The number 3 is the number of <strong>input channels</strong> or <strong>input feature maps</strong></li>
<li>The number 5 is the number of <strong>output channels</strong> or <strong>output feature maps</strong></li>
</ul>
</section>
<section id="example-1" class="slide level2">
<h2>Example</h2>
<p>Input features: <span class="math inline">\(5 \times 32 \times 32\)</span></p>
<p>Convolution kernel: <span class="math inline">\(5 \times 3 \times 3 \times 10\)</span></p>
<p>Questions:</p>
<ul>
<li>How many input channels are there?</li>
<li>How many output channels are there?</li>
<li>How many units are in the higher layer?</li>
<li>How many trainable weights are there?</li>
</ul>
<aside class="notes">
<ul>
<li>Modern deep learning frameworks have all of this implemented.</li>
<li>Still important to know as you design your network.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- Downsampling -->
</section></section>
<section>
<section id="downsampling" class="title-slide slide level1 center">
<h1>Downsampling</h1>

</section>
<section id="consolidating-information" class="slide level2">
<h2>Consolidating Information</h2>
<p>In a neural network with fully-connected layers, we reduced the number of units in each hidden layer</p>
<p>Q: Why?</p>
<div class="fragment">
<ul>
<li>To be able to consolidate information, and remove out information not useful for the current task</li>
</ul>
<p>Q: How can we consolidate information in a neural network with convolutional layers?</p>
</div>
<div class="fragment">
<ul>
<li>max pooling, average pooling, strided convolutions</li>
</ul>
</div>
</section>
<section id="max-pooling" class="slide level2">
<h2>Max-Pooling</h2>
<p>Idea: take the <strong>maximum value</strong> in each <span class="math inline">\(2 \times 2\)</span> grid.</p>

<img data-src="imgs/maxpool.jpeg" class="r-stretch"></section>
<section id="max-pooling-example" class="slide level2">
<h2>Max-Pooling Example</h2>
<p>We can add a max-pooling layer <em>after</em> each convolutional layer</p>

<img data-src="imgs/pooling.png" class="r-stretch"></section>
<section id="average-pooling" class="slide level2">
<h2>Average Pooling</h2>
<ul>
<li>Average pooling (compute the average activation of a region)</li>
<li>Max pooling generally works better</li>
</ul>
</section>
<section id="strided-convolution" class="slide level2">
<h2>Strided Convolution</h2>
<p>More recently people are doing away with pooling operations, using <strong>strided</strong> convolutions instead:</p>

<img data-src="imgs/stride2.png" class="r-stretch"><p>Shift the kernel by <strong>2</strong> (stride=2) when computing the next output feature.</p>
<aside class="notes">
<p>Visualization examples</p>
<ul>
<li><a href="https://arxiv.org/pdf/1603.07285.pdf" class="uri">https://arxiv.org/pdf/1603.07285.pdf</a></li>
<li><a href="https://github.com/vdumoulin/conv_arithmetic" class="uri">https://github.com/vdumoulin/conv_arithmetic</a></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- 04train -->
</section></section>
<section>
<section id="training-a-conv-net" class="title-slide slide level1 center">
<h1>Training a Conv Net</h1>

</section>
<section id="how-do-we-train-a-conv-net" class="slide level2">
<h2>How do we train a conv net?</h2>
<p>With backprop, of course!</p>
<div class="fragment">
<p>Recall what we need to do. Backprop is a message passing procedure, where each layer knows how to pass messages backwards through the computation graph. Let’s determine the updates for convolution layers.</p>
</div>
</section>
<section id="how-do-we-train-a-conv-net-ii" class="slide level2">
<h2>How do we train a conv net? II</h2>
<ul>
<li class="fragment">We assume we are given the loss derivatives <span class="math inline">\(\overline{y_{i,t}}\)</span> with respect to the output units.</li>
<li class="fragment">We need to compute the cost derivatives with respect to the input units and with respect to the weights.</li>
</ul>
<div class="fragment">
<p>The only new feature is: how do we do backprop with tied weights?</p>
</div>
<aside class="notes">
<ul>
<li>The derivatives with respect of the input units are not needed for the first layer</li>
<li>But they are needed for all subsequent layers as the inputs to these subsequent layers are a function of previous layers’ weights.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="multivariate-chain-rule-inputs" class="slide level2">
<h2>Multivariate Chain Rule (inputs)</h2>
<p>Consider the computation graph for the inputs:</p>
<p><img data-src="imgs/comp_graph.png" style="height:70.0%"></p>
<div class="fragment">
<p>Each input unit influences all the output units that have it within their receptive fields. Using the <strong>multivariate Chain Rule</strong>, we need to sum together the derivative terms for all these edges</p>
</div>
<aside class="notes">
<ul>
<li>This is a 1d signal, e.g.&nbsp;think of a 1d image with a line camera</li>
<li>Here we have “kernels” of size 3</li>
<li>We already applied multivariate chain rule for “tied inputs” in the past</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="multivariate-chain-rule-weights" class="slide level2">
<h2>Multivariate Chain Rule (weights)</h2>
<p>Consider the computation graph for the weights:</p>
<p><img data-src="imgs/comp_graph_weights.png" style="height:70.0%"></p>
<div class="fragment">
<p>Each of the weights affects all the output units for the corresponding input and output feature maps.</p>
</div>
</section>
<section id="backpropagation-on-conv-layers" class="slide level2">
<h2>Backpropagation on conv layers</h2>
<p>The formula for the convolution layer for 1-D signals:</p>
<p><span class="math display">\[
y_{i,t} = \sum_{j=1}^{J} \sum_{\tau = -R}^{R} w_{i,j,\tau} \, x_{j, t + \tau}.
\]</span></p>
<div class="fragment">
<p>We compute the derivatives, which requires summing over all spatial locations:</p>
<p><span class="math display">\[\begin{align*}
\overline{w_{i,j,\tau}}
    &amp;= \sum_{t} y_{i,t} \frac{\partial y_{i,t}}{\partial w_{i,j,\tau}} \\
    &amp;= \sum_{t} y_{i,t} x_{j, t + \tau}
\end{align*}\]</span></p>
</div>
<aside class="notes">
<ul>
<li>Focus on 1-D signals with <span class="math inline">\(J\)</span> feature maps (e.g.&nbsp;colors) and kernel <em>radius</em> <span class="math inline">\(R\)</span> (i.e.&nbsp;a kernal size <span class="math inline">\(K=2R+1\)</span>)</li>
<li><span class="math inline">\(i\)</span> is the index of output feature map (of which we have <span class="math inline">\(I\)</span>).</li>
<li><span class="math inline">\(t\)</span> is the index of the output location (<span class="math inline">\(t\in \{1, \ldots, \}\)</span>)</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- 05 popular architecture examples -->
</section></section>
<section>
<section id="examples-and-applications" class="title-slide slide level1 center">
<h1>Examples and Applications</h1>

</section>
<section id="object-recognition" class="slide level2">
<h2>Object recognition</h2>
<ul>
<li class="fragment"><p>Object recognition is the task of identifying which object category is present in an image.</p></li>
<li class="fragment"><p>It’s challenging because <em>objects can differ widely</em> in position, size, shape, appearance, etc., and we have to deal with occlusions, lighting changes, etc.</p></li>
<li class="fragment"><p>Why we care</p>
<ul>
<li class="fragment">Direct applications to image search</li>
<li class="fragment">Closely related to object detection, the task of locating all instances of an object in an image</li>
</ul></li>
</ul>
</section>
<section id="datasets" class="slide level2">
<h2>Datasets</h2>
<ul>
<li class="fragment">In order to train and evaluate a machine learning system, we need to collect a dataset. The design of the dataset can have major implications. &nbsp;<br>
</li>
<li class="fragment">Some questions to consider:
<ul>
<li class="fragment">Which categories to include?</li>
<li class="fragment">Where should the images come from?</li>
<li class="fragment">How many images to collect?</li>
<li class="fragment">How to normalize (preprocess) the images?</li>
</ul></li>
</ul>
</section>
<section id="mnist---handwritten-digits-dataset" class="slide level2">
<h2>MNIST - Handwritten Digits Dataset</h2>
<ul>
<li class="fragment"><strong>Categories:</strong> 10 digit classes</li>
<li class="fragment"><strong>Source:</strong> Scans of handwritten zip codes from envelopes</li>
<li class="fragment"><strong>Size:</strong> 60,000 training images / 10,000 test images, Grayscale, 28 x 28 pixels</li>
<li class="fragment"><strong>Normalization:</strong> Centered within the image, scaled to a consistent size</li>
</ul>
<aside class="notes">
<ul>
<li>Assumption: Digit recognizer is part of a larger pipeline.</li>
<li>In 1998, Yann LeCun and colleagues built a conv net called LeNet</li>
<li>It was able to classify digits with 98.9% test accuracy.</li>
<li>Good enough to be used in a system for automatically reading numbers on checks.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="imagenet-i" class="slide level2">
<h2>ImageNet I</h2>
<center>
<img data-src="imgs/w05_5.png" height="600">
</center>
</section>
<section id="imagenet-ii" class="slide level2">
<h2>ImageNet II</h2>
<p><strong>Used for:</strong> The ImageNet Large Scale Visual Recognition Challenge (ILSVRC), an annual benchmark competition for object recognition algorithms</p>
<div class="fragment">
<p><strong>Design Decisions</strong></p>
</div>
<ul>
<li class="fragment"><strong>Categories:</strong> Taken from a lexical database called WordNet
<ul>
<li class="fragment">WordNet consists of “synsets”</li>
<li class="fragment">Almost 22,000 classes used</li>
<li class="fragment">The 1,000 most common chosen for the ILSVRC</li>
<li class="fragment">The categories are really specific, e.g., hundreds of kinds of dogs</li>
</ul></li>
</ul>
</section>
<section id="imagenet-iii" class="slide level2">
<h2>ImageNet III</h2>
<ul>
<li><p><strong>Size:</strong> 1.2 million full-sized images for the ILSVRC</p></li>
<li><p><strong>Source:</strong> Results from image search engines, hand-labeled by Mechanical Turkers</p>
<ul>
<li>Labeling such specific categories was challenging; annotators had to be given the WordNet hierarchy, Wikipedia, etc.</li>
</ul></li>
<li><p><strong>Normalization:</strong> None, although the contestants are free to do preprocessing</p></li>
</ul>
<aside class="notes">
<ul>
<li>Synsets are sets of synonymous words</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="imagenet-iv" class="slide level2">
<h2>ImageNet IV</h2>
<center>
<img data-src="imgs/w05_7.png" height="600">
</center>
</section>
<section id="imagenet-v" class="slide level2">
<h2>ImageNet V</h2>
<center>
<img data-src="imgs/w05_8.png" height="600">
</center>
</section>
<section id="imagenet-results" class="slide level2">
<h2>ImageNet Results</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Year</th>
<th>Model</th>
<th>Top-5 error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2010</td>
<td>Hand-designed descriptors + SVM</td>
<td>28.2%</td>
</tr>
<tr class="even">
<td>2011</td>
<td>Compressed Fisher Vectors + SVM</td>
<td>25.8%</td>
</tr>
<tr class="odd">
<td>2012</td>
<td>AlexNet</td>
<td>16.4%</td>
</tr>
<tr class="even">
<td>2013</td>
<td>a variant of AlexNet</td>
<td>11.7%</td>
</tr>
<tr class="odd">
<td>2014</td>
<td>GoogLeNet</td>
<td>6.6%</td>
</tr>
<tr class="even">
<td>2015</td>
<td>deep residual nets</td>
<td>4.5%</td>
</tr>
</tbody>
</table>
<aside class="notes">
<ul>
<li>Top-5 error: True class among the 5 “best” results</li>
<li>Human-performance is around 5.1%.</li>
<li>ISVRC stopped running because the performance is already so good.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-features-do-cnns-detect" class="slide level2">
<h2>What features do CNN’s detect?</h2>
<center>
<img data-src="imgs/features.png" height="600">
</center>
</section>
<section id="size-of-a-convnet" class="slide level2">
<h2>Size of a convnet</h2>
<ul>
<li class="fragment">Ways to measure the size of a network:
<ul>
<li class="fragment"><strong>Number of units.</strong> The activations need to be stored in memory during training.</li>
<li class="fragment"><strong>Number of weights.</strong> The weights need to be stored in memory / number of parameters determines overfitting.</li>
<li class="fragment"><strong>Number of connections.</strong> There are approximately 3 add-multiply operations per connection (1 for the forward pass, 2 for the backward pass).</li>
</ul></li>
<li class="fragment">Fully connected layer with <span class="math inline">\(M\)</span> inputs and <span class="math inline">\(N\)</span> outputs has <span class="math inline">\(MN\)</span> connections / weights.</li>
</ul>
<aside class="notes">
<ul>
<li>The story for conv nets is more complicated.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="size-of-a-convnet-ii" class="slide level2">
<h2>Size of a convnet II</h2>
<center>
<img data-src="imgs/w05_11.png" height="600">
</center>
<aside class="notes">
<ul>
<li><strong>Output Units:</strong> We assume no change in dimensions. For conv nets, this requires padding.</li>
<li><strong>Weights</strong> Much fewer weights because <span class="math inline">\(K^2 \ll W^2H^2\)</span>.</li>
<li><strong>Connections</strong> Not the same as weights anymore.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="size-of-a-convnet-iii" class="slide level2">
<h2>Size of a convnet III</h2>
<center>
<img data-src="imgs/w05_12.png" height="600">
</center>
</section>
<section id="lenet-atchitecture" class="slide level2">
<h2>LeNet Atchitecture</h2>
<center>
<img data-src="imgs/w05_9.png" height="600">
</center>
<!-- Moved from Downsampling section -->
</section>
<section id="lenet-architecture-ii" class="slide level2">
<h2>LeNet Architecture II</h2>
<center>
<img data-src="imgs/lenet.png">
</center>
<ul>
<li>Input: 32x32 pixel, greyscale image</li>
<li>First convolution has 6 output features (5x5 convolution?)</li>
<li>First subsampling is probably a max-pooling operation</li>
<li>Second convolution has 16 output features (5x5 convolution?)</li>
<li>…</li>
<li>Some number of fully-connected layers at the end</li>
</ul>
</section>
<section id="resnet-architecture" class="slide level2">
<h2>ResNet Architecture</h2>
<p><img data-src="imgs/resnet.png" height="600"> <img data-src="imgs/nested_function_classes.png" height="300"></p>
</section>
<section id="resnet-architecture-ii" class="slide level2">
<h2>ResNet Architecture II</h2>
<ul>
<li class="fragment">Suppose we add another layer. How can we ensure that the new set of represented functions contains the old set, before the layer was added?</li>
<li class="fragment">Why do we need this? We’d like to get larger (nested) sets of functions as we add more layers and not just different (non-nested) sets.</li>
</ul>
</section>
<section id="resnet-blocks" class="slide level2">
<h2>ResNet Blocks</h2>
<center>
<img data-src="imgs/resnet_block.png" height="300">
</center>
<ul>
<li>Side effect of adding identity <span class="math inline">\(f(x) = x + g(x)\)</span>: better gradient propagation</li>
<li>See <a href="https://d2l.ai/chapter_convolutional-modern/resnet.html" class="uri">https://d2l.ai/chapter_convolutional-modern/resnet.html</a></li>
</ul>
</section>
<section id="densenet-blocks" class="slide level2">
<h2>DenseNet Blocks</h2>
<center>
<img data-src="imgs/dense_blocks.png" height="400">
</center>
<p>Same idea as ResNet blocks, but instead of addition <span class="math inline">\(f(x) = x + g(x)\)</span> they use concatenation <span class="math inline">\(f(x) = [x, g(x)]\)</span>.</p>
</section>
<section id="densenet-architecture" class="slide level2">
<h2>DenseNet Architecture</h2>
<center>
<img data-src="imgs/densenets_1.png" style="height:70.0%">
</center>
<p>See <a href="https://d2l.ai/chapter_convolutional-modern/densenet.html" class="uri">https://d2l.ai/chapter_convolutional-modern/densenet.html</a></p>
</section></section>
<section>
<section id="wrap-up" class="title-slide slide level1 center">
<h1>Wrap Up</h1>

</section>
<section id="summary-1" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">Computer Vision has been the main motivation for Conv Nets</li>
<li class="fragment">They draw inspiration from biological vision systems</li>
<li class="fragment">Key ideas are: <em>local connectivity</em> and <em>weight sharing</em></li>
<li class="fragment">Conv Nets can be trained using backpropagation</li>
</ul>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://utm-csc413.github.io/2024F-website/" target="_blank">↩︎ Back to Course Website</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"src":"chalkboard.json","boardmarkerWidth":2,"chalkWidth":2,"chalkEffect":1},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/utm-csc413\.github\.io\/2024F-website\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>