<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <title>CSC413 - Fall 2024, UTM – CSC413 Neural Networks and Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta property="og:description" content="Lecture 9">
<meta property="og:site_name" content="CSC413 - Fall 2024, UTM">
<meta name="twitter:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta name="twitter:description" content="Lecture 9">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC413 Neural Networks and Deep Learning</h1>
  <p class="subtitle">Lecture 9</p>

<div class="quarto-title-authors">
</div>

</section>
<section id="announcements" class="slide level2">
<h2>Announcements</h2>
<ul>
<li>The project plan is due early next week (March 21)</li>
</ul>
<!-- 00game -->
</section>
<section>
<section id="rnn-review-game" class="title-slide slide level1 center">
<h1>RNN Review Game</h1>

</section>
<section id="in-what-situation-would-you-use-an-rnn" class="slide level2">
<h2>In what Situation would you use an RNN?</h2>
<ol type="a">
<li>When we want to make predictions about a sequence</li>
<li>When we want to make predictions about an image</li>
<li>When we want to generate a sequence</li>
<li>When we want to generate an image</li>
</ol>
</section>
<section id="using-glove-embeddings-of-words-as-input-features-to-an-rnn-is-an-example-of" class="slide level2">
<h2>Using GloVe Embeddings of Words as Input Features to an RNN is an Example of…</h2>
<ol type="a">
<li>LSTM</li>
<li>GRU</li>
<li>MLP</li>
<li>Transfer Learning</li>
</ol>
</section>
<section id="what-is-a-disadvantage-of-using-a-vanilla-rnn-without-gating" class="slide level2">
<h2>What is a Disadvantage of using a Vanilla RNN (without gating)?</h2>
<ol type="a">
<li>It is challenging to learn long-term dependencies</li>
<li>The gradient might vanish</li>
<li>The gradient might explode</li>
<li>All of the above</li>
<li>None of the above</li>
</ol>
</section>
<section id="what-is-a-cliff" class="slide level2">
<h2>What is a Cliff?</h2>
<p>A part of the loss landscape where the gradient of the loss with respect to a parameter…</p>
<ol type="a">
<li>… is large</li>
<li>… is close to 0</li>
<li>… is close to 0 in most regions, and very large in some places</li>
</ol>
</section>
<section id="when-deep-learning-practitioners-talk-about-rnns-they-are-usually-referring-to" class="slide level2">
<h2>When Deep Learning Practitioners talk about RNNs, they are usually referring to…</h2>
<ol type="a">
<li>vanilla RNN (without gating)</li>
<li>RNN with LSTM units, or sometimes GRU units</li>
</ol>
<!-- 01gen -->
</section>
<section id="text-generation-with-rnn" class="slide level2">
<h2>Text Generation with RNN</h2>
<p>RNN For Prediction:</p>
<ul>
<li>Process tokens one at a time</li>
</ul>
<div class="fragment">
<ul>
<li>Hidden state is a representation of <strong>all the tokens read thus far</strong></li>
</ul>
</div>
</section>
<section id="text-generation-with-rnn-ii" class="slide level2">
<h2>Text Generation with RNN II</h2>
<p>RNN For Generation:</p>
<ul>
<li>Generate tokens one at a time</li>
</ul>
<div class="fragment">
<ul>
<li>Hidden state is a representation of <strong>all the tokens to be generated</strong></li>
</ul>
</div>
</section>
<section id="rnn-hidden-state-updates" class="slide level2">
<h2>RNN Hidden State Updates</h2>
<p>RNN For Prediction:</p>
<ul>
<li>Update hidden state with new input (token)</li>
</ul>
<div class="fragment">
<ul>
<li>Get prediction (e.g.&nbsp;distribution over possible labels)</li>
</ul>
</div>
<div class="fragment">
<p>RNN For Generation:</p>
<ul>
<li>Get prediction distribution of next token</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Generate a token from the distribution</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Update the hidden state with new token</li>
</ul>
</div>
</section>
<section id="text-generation-diagram" class="slide level2">
<h2>Text Generation Diagram</h2>
<center>
<img data-src="imgs/rnn_gen_figure.png" height="250">
</center>
<ul>
<li>Get prediction distribution of next token</li>
</ul>
<div class="fragment">
<ul>
<li>Generate a token from the distribution</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Update the hidden state with new token:</li>
</ul>
</div>
</section>
<section id="test-time-behaviour-of-generative-rnn" class="slide level2">
<h2>Test Time Behaviour of Generative RNN</h2>
<p>Unlike other models we discussed so far, the training time behaviour of Generative RNNs will be <strong>different</strong> from the test time behaviour</p>
<p>Test time behaviour:</p>
<ul>
<li>At each time step:
<ul>
<li>Obtain a <strong>distribution</strong> over possible next tokens</li>
<li>Sample a token from that distribution</li>
<li>Update the hidden state based on the sample token</li>
</ul></li>
</ul>
</section>
<section id="training-time-behaviour-of-generative-rnn" class="slide level2">
<h2>Training Time Behaviour of Generative RNN</h2>
<p>During training, we try to get the RNN to generate one particular sequence in the training set:</p>
<ul>
<li>At each time step:
<ul>
<li>Obtain a <strong>distribution</strong> over possible next tokens</li>
<li>Compare this with the <em>actual</em> next token</li>
</ul></li>
</ul>
<p>Q1: What kind of a problem is this? (regression or classification?)</p>
<p>Q2: What loss function should we use during training?</p>
</section>
<section id="text-generation-step-1" class="slide level2">
<h2>Text Generation: Step 1</h2>
<center>
<img data-src="imgs/rnn_gen_figure.png" height="200">
</center>
<p>First classification problem:</p>
<ul>
<li>Start with an initial hidden state</li>
</ul>
<div class="fragment">
<ul>
<li>Update the hidden state with a “&lt;BOS&gt;” (beginning of string) token, so that the hidden state becomes meaningful (not just zeros)</li>
</ul>
</div>
</section>
<section id="text-generation-step-1-ii" class="slide level2">
<h2>Text Generation: Step 1 II</h2>
<center>
<img data-src="imgs/rnn_gen_figure.png" height="200">
</center>
<p>First classification problem:</p>
<ul>
<li>Get the distribution over the first character</li>
</ul>
<div class="fragment">
<ul>
<li>Compute the cross-entropy loss against the ground truth (R)</li>
</ul>
</div>
</section>
<section id="text-generation-with-teacher-forcing" class="slide level2">
<h2>Text Generation with Teacher Forcing</h2>
<center>
<img data-src="imgs/rnn_gen_figure.png" height="200">
</center>
<p>Second classification problem:</p>
<ul>
<li>Update the hidden state with the <strong>ground truth</strong> token (R) regardless of the prediction from the previous step
<ul>
<li>This technique is called <strong>teacher forcing</strong></li>
</ul></li>
</ul>
</section>
<section id="text-generation-with-teacher-forcing-ii" class="slide level2">
<h2>Text Generation with Teacher Forcing II</h2>
<center>
<img data-src="imgs/rnn_gen_figure.png" height="200">
</center>
<p>Second classification problem:</p>
<ul>
<li>Get the distribution over the second character</li>
</ul>
<div class="fragment">
<ul>
<li>Compute the cross-entropy loss against the ground truth (I)</li>
</ul>
</div>
</section>
<section id="text-generation-later-steps" class="slide level2">
<h2>Text Generation: Later Steps</h2>

<img data-src="imgs/rnn_gen_figure.png" class="r-stretch"><p>Continue until we get to the “&lt;EOS&gt;” (end of string) token</p>
</section>
<section id="some-remaining-challenges" class="slide level2">
<h2>Some Remaining Challenges</h2>
<ul>
<li>Vocabularies can be very large once you include people, places, etc.</li>
</ul>
<div class="fragment">
<ul>
<li>It’s computationally difficult to predict distributions over millions of words.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>How do we deal with words we haven’t seen before?</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>In some languages (e.g.&nbsp;German), it’s hard to define what should be considered a word.</li>
</ul>
</div>
</section>
<section id="character-vs-word-level" class="slide level2">
<h2>Character vs Word-level</h2>
<p>Another approach is to model text <em>one character at a time</em></p>
<p>This solves the problem of what to do about previously unseen words.</p>
<p>Note that long-term memory is essential at the character level!</p>
<!-- 02attn -->
</section></section>
<section>
<section id="recurrent-neural-networks-with-attention" class="title-slide slide level1 center">
<h1>Recurrent Neural Networks with Attention</h1>

</section>
<section id="recurrent-neural-networks" class="slide level2">
<h2>Recurrent Neural Networks</h2>
<p>In lecture 8, we showed a <strong>discriminative RNN</strong> that makes a <em>prediction</em> based on a sequence (sequence as an input).</p>
<p>In the week 11 tutorial, we will build a <strong>generator RNN</strong> to generate sequences (sequence as an output)</p>
</section>
<section id="sequence-to-sequence-tasks" class="slide level2">
<h2>Sequence-to-sequence Tasks</h2>
<p>Another common example of a sequence-to-sequence task (seq2seq) is <strong>machine translation</strong>.</p>
<p><img data-src="imgs/nmt.png" style="height:30.0%"></p>
<p>The network first reads and memorizes the sentences. When it sees the “end token”, it starts outputting the translation. The “encoder” and “decoder” are two different networks with different weights.</p>
</section>
<section id="how-seq2seq-works" class="slide level2">
<h2>How Seq2Seq Works</h2>
<p>The <strong>encoder network</strong> reads an input sentence and stores all the information in its hidden units.</p>
<p>The <strong>decoder network</strong> then generates the output sentence one word at a time.</p>
<p><img data-src="imgs/ilya_lstm.png" style="height:35.0%"></p>
</section>
<section id="how-seq2seq-works-ii" class="slide level2">
<h2>How Seq2Seq Works II</h2>
<p><img data-src="imgs/ilya_lstm.png" style="height:35.0%"></p>
<p>But some sentences can be really long. Can we really store all the information in a vector of hidden units?</p>
<p>Human translators <strong>refer back to the input</strong>.</p>
</section>
<section id="attention-based-machine-translation" class="slide level2">
<h2>Attention-Based Machine Translation</h2>
<p>We’ll look at the translation model from the classic paper:</p>
<blockquote>
<p>Bahdanau et al., Neural machine translation by jointly learning to align and translate. ICLR, 2015.</p>
</blockquote>
<p>Basic idea: each <strong>output word</strong> comes from <strong>one input word</strong>, or a handful of input words. Maybe we can learn to attend to only the relevant ones as we produce the output.</p>
<p>We’ll use the opportunity to look at architectural changes we can make to RNN models to make it even more performant.</p>
</section>
<section id="encoder-decoder-architectures" class="slide level2">
<h2>Encoder &amp; Decoder Architectures</h2>
<p>The encoder computes an <strong>annotation</strong> (hidden state) of each word in the input.</p>
<ul>
<li>The encoder is a <strong>bidirectional RNN</strong></li>
</ul>
<div class="fragment">
<p>The decoder network is also an RNN, and makes predictions one word at a time.</p>
<ul>
<li>The decoder uses an <strong>attention</strong> mechanism (RNN with attention)</li>
</ul>
</div>
</section>
<section id="encoder-bidirectional-rnn" class="slide level2">
<h2>Encoder: Bidirectional RNN</h2>
<p>The encoder is a <strong>bidirectional RNN</strong>. We have two RNNs: one that runs forward and one that runs backwards. These RNNs can be LSTMs or GRUs.</p>
<p>The annotation of a word is the concatenation of the forward and backward hidden vectors.</p>
<center>
<img data-src="imgs/bidirectional_rnn.png" height="300">
</center>
</section>
<section id="decoder-rnn-with-attention" class="slide level2">
<h2>Decoder: RNN with Attention</h2>
<p>The decoder network is also an RNN, and makes predictions one word at a time.</p>
<div class="columns">
<div class="column" style="width:60%;">
<center>
<img data-src="imgs/decoder_network.png">
</center>
</div><div class="column" style="width:40%;">
<p>The difference is that it also derives a <strong>context vector</strong> <span class="math inline">\({\bf c}^{(t)}\)</span> at each time step, computed by <strong>attending to the inputs</strong></p>
</div></div>
</section>
<section id="intuition-behind-attending-to-the-input" class="slide level2">
<h2>Intuition Behind “Attending to the Input”</h2>
<blockquote>
<p>“My language model tells me the next word should be an adjective. Find me an adjective in the input”</p>
</blockquote>
<p>We would like to <em>refer back</em> to one (or a few) of the input words to help with the translation task (e.g.&nbsp;find the adjective)</p>
<p>If you were programming a translator, you might…</p>
</section>
<section id="intuition-behind-aattending-to-the-input-ii" class="slide level2">
<h2>Intuition Behind Aattending to the Input” II</h2>
<p>If you were programming a translator, you might</p>
<ol type="1">
<li>find an input word that is most likely an adjective (the attention function)</li>
<li>look up the input word that is the adjective (the weighted average)</li>
<li>translate the input word, e.g.&nbsp;using the input word, and a dictionary (the projection MLP)</li>
</ol>
<p>An attentional decoder is like a <em>continuous</em> form of these last three steps.</p>
</section>
<section id="the-math-behind-attending-to-the-input" class="slide level2">
<h2>The Math Behind “Attending to the Input”</h2>
<ul>
<li>“My language model tells me the next word should be an adjective. Find me an adjective in the input”</li>
</ul>
<p>The context vector is computed as a <strong>weighted average</strong> of the encoder’s annotations:</p>
<p><span class="math display">\[{\bf c}^{(i)} = \sum_j \alpha_{ij} {\bf h}^{(j)}\]</span></p>
</section>
<section id="the-math-behind-attending-to-the-input-ii" class="slide level2">
<h2>The Math Behind “Attending to the Input” II</h2>
<p>The attention weights are computed as a softmax, where the input depends on the <em>annotation</em> <span class="math inline">\({\bf h}^{(j)}\)</span> and the <em>decoder states</em> <span class="math inline">\({\bf s}^{(t)}\)</span>:</p>
<p><span class="math display">\[
e_{ij} = a({\bf s}^{(i-1)}, {\bf h}^{(j)}), \qquad \alpha_{ij} = \frac{ \exp(e_{ij}) }{\sum_{j^\prime} exp(e_{ij^\prime})}
\]</span></p>
<p>The attention function depends on the <strong>annotation vector</strong>, rather than the <strong>position in the sentence</strong>. It is a form of content-based addressing.</p>
</section>
<section id="example-how-to-obtain-a-context-vector" class="slide level2">
<h2>Example: How to Obtain a Context Vector?</h2>
<p><span class="math display">\[\begin{align*}
{\bf h}^{(1)} &amp;= \begin{bmatrix}1 &amp; 3 &amp; 9\end{bmatrix}^\top \\
{\bf h}^{(2)} &amp;= \begin{bmatrix}0 &amp; 0 &amp; 1\end{bmatrix}^\top \\
{\bf h}^{(3)} &amp;= \begin{bmatrix}5 &amp; -1 &amp; 2\end{bmatrix}^\top \\
\\
{\bf c}^{(t)} &amp;= \begin{bmatrix}? &amp; ? &amp; ?\end{bmatrix}^\top \\
\end{align*}\]</span></p>
</section>
<section id="example-average-pooling-is-context-independent" class="slide level2">
<h2>Example: Average Pooling is Context Independent</h2>
<p><span class="math display">\[\begin{align*}
{\bf h}^{(1)} &amp;= \begin{bmatrix}1 &amp; 3 &amp; 9\end{bmatrix}^\top \\
{\bf h}^{(2)} &amp;= \begin{bmatrix}0 &amp; 0 &amp; 1\end{bmatrix}^\top \\
{\bf h}^{(3)} &amp;= \begin{bmatrix}5 &amp; -1 &amp; 2\end{bmatrix}^\top \\
\\
{\bf c}^{(t)} &amp;= \text{average}({\bf h}^{(1)} , {\bf h}^{(2)}, {\bf h}^{(3)})\\
&amp;= \begin{bmatrix}2 &amp; 0.6 &amp; 1\end{bmatrix}^\top \\
\end{align*}\]</span></p>
</section>
<section id="example-attention" class="slide level2">
<h2>Example: Attention</h2>
<p><span class="math display">\[\begin{align*}
{\bf h}^{(1)} &amp;= \begin{bmatrix}1 &amp; 3 &amp; 9\end{bmatrix}^\top \\
{\bf h}^{(2)} &amp;= \begin{bmatrix}0 &amp; 0 &amp; 1\end{bmatrix}^\top \\
{\bf h}^{(3)} &amp;= \begin{bmatrix}5 &amp; -1 &amp; 2\end{bmatrix}^\top \\
\\
{\bf s}^{(t-1)} &amp;= \begin{bmatrix}0 &amp; 1 &amp; 1\end{bmatrix}^\top \\
\alpha_t &amp;= \text{softmax}\left(\begin{bmatrix}
f({\bf s}^{(t-1)}, {\bf h}^{(1)}) \\
f({\bf s}^{(t-1)}, {\bf h}^{(2)}) \\
f({\bf s}^{(t-1)}, {\bf h}^{(3)}) \\
\end{bmatrix}\right) = \begin{bmatrix}\alpha_{t1} \\ \alpha_{t2} \\ \alpha_{t3}\end{bmatrix}
\end{align*}\]</span></p>
</section>
<section id="example-dot-product-attention" class="slide level2">
<h2>Example: Dot-Product Attention</h2>
<p><span class="math display">\[\begin{align*}
{\bf h}^{(1)} &amp;= \begin{bmatrix}1 &amp; 3 &amp; 9\end{bmatrix}^\top \\
{\bf h}^{(2)} &amp;= \begin{bmatrix}0 &amp; 0 &amp; 1\end{bmatrix}^\top \\
{\bf h}^{(3)} &amp;= \begin{bmatrix}5 &amp; -1 &amp; 2\end{bmatrix}^\top \\
\\
{\bf s}^{(t-1)} &amp;= \begin{bmatrix}0 &amp; 1 &amp; 1\end{bmatrix}^\top \\
\alpha_t &amp;= \text{softmax}\left(\begin{bmatrix}
               {\bf s}^{(t-1)} \cdot {\bf h}^{(1)} \\
               {\bf s}^{(t-1)} \cdot {\bf h}^{(2)} \\
               {\bf s}^{(t-1)} \cdot {\bf h}^{(3)} \\
               \end{bmatrix}\right) = \begin{bmatrix}\alpha_{t1} \\ \alpha_{t2} \\ \alpha_{t3}\end{bmatrix}
\end{align*}\]</span></p>
</section>
<section id="example-dot-product-attention-ii" class="slide level2">
<h2>Example: Dot-Product Attention II</h2>
<p><span class="math display">\[\begin{align*}
{\bf h}^{(1)} &amp;= \begin{bmatrix}1 &amp; 3 &amp; 9\end{bmatrix}^\top \\
{\bf h}^{(2)} &amp;= \begin{bmatrix}0 &amp; 0 &amp; 1\end{bmatrix}^\top \qquad \quad {\bf s}^{(t-1)} = \begin{bmatrix}0 &amp; 1 &amp; 1\end{bmatrix}^\top \\
{\bf h}^{(3)} &amp;= \begin{bmatrix}5 &amp; -1 &amp; 2\end{bmatrix}^\top \\
\\
\alpha_t &amp;= \text{softmax}\left(\begin{bmatrix}1 &amp; 0 &amp; 5 \\ 3 &amp; 0 &amp; -1 \\ 0 &amp; 1 &amp;2 \end{bmatrix}^\top
                    \begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix}\right) \\
{\bf c}^{(t)} &amp;= \alpha_{t1} {\bf h}^{(1)} + \alpha_{t2} {\bf h}^{(2)} + \alpha_{t2} {\bf h}^{(3)}\\
\end{align*}\]</span></p>
</section>
<section id="attention-demo-video" class="slide level2">
<h2>Attention Demo Video</h2>
<p><a href="https://play.library.utoronto.ca/watch/9ed8b3c497f82b510e9ecf441c5eef4f" class="uri">https://play.library.utoronto.ca/watch/9ed8b3c497f82b510e9ecf441c5eef4f</a></p>
</section>
<section id="visualization-of-attention" class="slide level2">
<h2>Visualization of Attention</h2>
<p>Visualization of the attention map (the <span class="math inline">\(\alpha_{ij}\)</span>s at each time step)</p>
<center>
<img data-src="imgs/alignments.png" height="350">
</center>
<p>Nothing forces the model to go (roughly) linearly through the input sentences, but somehow it learns to do it!</p>
</section>
<section id="attention-performance" class="slide level2">
<h2>Attention Performance</h2>
<p>The attention-based translation model does much better than the encoder/decoder model on long sentences.</p>
<center>
<img data-src="imgs/long_sentences.png" height="400">
</center>
</section>
<section id="attention-based-caption-generation" class="slide level2">
<h2>Attention-Based Caption Generation</h2>
<p>Caption Generation Task:</p>
<ul>
<li>Input: Image</li>
</ul>
<div class="fragment">
<ul>
<li>Output: Caption (sequence of words or characters)</li>
</ul>
</div>
</section>
<section id="attention-based-caption-generation-ii" class="slide level2">
<h2>Attention-Based Caption Generation II</h2>
<p>Attention can also be used to understand images.</p>
<ul>
<li>We humans can’t process a whole visual scene at once.</li>
</ul>
<div class="fragment">
<ul>
<li>The fovea of the eye gives us high-acuity vision in only a tiny region of our field of view.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Instead, we must integrate information from a series of glimpses.</li>
</ul>
</div>
</section>
<section id="attention-based-caption-generation-iii" class="slide level2">
<h2>Attention-Based Caption Generation III</h2>
<p>The next few slides are based on this paper from the UofT machine learning group:</p>
<blockquote>
<p>Xu et al.&nbsp;Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention. ICML, 2015.</p>
</blockquote>
</section>
<section id="attention-for-caption-generation" class="slide level2">
<h2>Attention for Caption Generation</h2>
<p>The caption generation task: take an image as input, and produce a sentence describing the image.</p>
<ul>
<li><strong>Encoder</strong>: a classification conv net like VGG. This computes a bunch of feature maps over the image.</li>
</ul>
</section>
<section id="attention-for-caption-generation-ii" class="slide level2">
<h2>Attention for Caption Generation II</h2>
<ul>
<li><strong>Decoder</strong>: an attention-based RNN, analogous to the decoder in the translation model
<ul>
<li>In each time step, the decoder computes an attention map over the entire image, effectively deciding which regions to focus on.</li>
<li>It receives a context vector, which is the weighted average of the convnet features.</li>
</ul></li>
</ul>
</section>
<section id="attention-for-caption-generation-iii" class="slide level2">
<h2>Attention for Caption Generation III</h2>
<p>Similar math as before: difference is that <span class="math inline">\(j\)</span> is a pixel location</p>
<p><span class="math display">\[\begin{align*}
e_{ij} &amp;= a({\bf s}^{(i-1)}, {\bf h}^{(j)}) \\
\alpha_{ij} &amp;= \frac{ \exp(e_{ij}) }{\sum_{j^\prime} exp(e_{ij^\prime})}
\end{align*}\]</span></p>
</section>
<section id="what-attention-tells-us" class="slide level2">
<h2>What Attention tells us</h2>
<p>This lets us understand where the network is looking as it generates a sentence.</p>
<center>
<p><img data-src="imgs/bird.png"></p>
<p><img data-src="imgs/locations_for_words.png" height="300"></p>
</center>
</section>
<section id="what-attention-tells-us-about-mistakes" class="slide level2">
<h2>What Attention tells us about mistakes</h2>
<p>This can also help us understand the network’s mistakes.</p>
<center>
<img data-src="imgs/caption_mistakes.png">
</center>
</section>
<section id="multi-layer-rnns" class="slide level2">
<h2>Multi-Layer RNNs</h2>
<p>Finally, to get more capacity/performance out of RNNs, you can <strong>stack</strong> multiple RNN’s together!</p>
<p>The hidden state of your first RNN becomes the input to your second layer RNN.</p>
<center>
<img data-src="imgs/notation_rnn.png" height="350">
</center>
</section>
<section id="rnn-disadvantage" class="slide level2">
<h2>RNN Disadvantage</h2>
<p>One disadvantage of RNNS (and especially multi-layer RNNs) is that they require a long time to train, and are more difficult to parallelize. (Need the previous hidden state <span class="math inline">\(h^{(t)}\)</span> to be able to compute <span class="math inline">\(h^{(t+1)}\)</span>)</p>
<!--

## Computational Cost and Parallelism

There are a few things we should consider when designing an RNN.

Computational cost:

- Number of connections. How many add-multiply operations for the forward and backward pass.
- Number of time steps. How many copies of hidden units to store for
- Backpropgation Through Time.  Number of sequential operations. The computations cannot be parallelized. (The part of the model that requires a for loop).

Maximum path length across time: the shortest path length
between the first encoder input and the last decoder output.

- It tells us how easy it is for the RNN to remember / retreive
information from the input sequence.

## Computational Cost Example

If we have a $d$ layer RNN with $k$ hidden units, training on a sequence of length $t$...

![](imgs/notation_rnn.png){ height=40% }

- There are $k^2$ connections for each hidden-to-hidden connections: total $tk^2d$ connections
- We need to store $tkd$ hidden units during training
- Only $kd$ hidden units need  to be stored at test time

## Parallelism

![](imgs/rnn_parallel.png){ height=40% }

- Both the input embeddings and outputs of an RNN can be computed in parallel
- The blue hidden units are independent given the red
- The number of sequential operations is still proportional to $t$

## Backprop

During backprop, in the standard encoder-decoder RNN, the maximum
path length across time is the number of time steps.

Attention-based RNNs have a constant path length between the
encoder inputs and the decoder hidden states.
(Learning becomes easier. Why?)

![](imgs/bptt_atten.png){ height=40% }

## Backprop

Let's not get into this. I don't think our students will appreciate this...

-->
<!-- 03transformer -->
</section></section>
<section>
<section id="transformer" class="title-slide slide level1 center">
<h1>Transformer</h1>

</section>
<section id="chatgpt" class="slide level2">
<h2>ChatGPT</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/ChatGPT_logo.svg" height="450"></p>
<figcaption>Logo of OpenAI’s ChatGPT</figcaption>
</figure>
</div>
</center>
</section>
<section id="chatgpt-ii" class="slide level2">
<h2>ChatGPT II</h2>
<p>What is ChatGPT? We’ll let it speak for itself:</p>
<blockquote>
<p>I am ChatGPT, a large language model developed by OpenAI. I use machine learning algorithms to generate responses to questions and statements posed to me by users. I am designed to understand and generate natural language responses in a variety of domains and topics, from general knowledge to specific technical fields. My purpose is to assist users in generating accurate and informative responses to their queries and to provide helpful insights and suggestions.</p>
</blockquote>
</section>
<section id="chatgpt-iii" class="slide level2">
<h2>ChatGPT III</h2>
<p>ChatGPT is based on OpenAI’s GPT-3, which itself is based on the <strong>transformer</strong> architecture.</p>
</section>
<section id="transformer-1" class="slide level2">
<h2>Transformer</h2>
<p>Idea: Do away with recurrent networks altogether; instead exclusively use attention to obtain the history at the hidden layers</p>
<center>
<img data-src="imgs/transformer_motivation.png" height="250">
</center>
<blockquote>
<p>Vaswani, Ashish, et al.&nbsp;”Attention is all you need.” Advances in Neural Information Processing Systems. 2017.</p>
</blockquote>
</section>
<section id="better-language-models" class="slide level2">
<h2>Better Language Models</h2>
<p><a href="https://openai.com/blog/better-language-models/" class="uri">https://openai.com/blog/better-language-models/</a></p>
<ul>
<li>Input: Human-Written Prompt (small paragraph)</li>
</ul>
<div class="fragment">
<ul>
<li>Output: Article about the topic (many paragraphs)</li>
</ul>
</div>
</section>
<section id="transformer-tutorial" class="slide level2">
<h2>Transformer Tutorial</h2>
<p><a href="https://www.youtube.com/watch?v=XSSTuhyAmnI&amp;ab_channel=AriSeff">www.youtube.com/watch?v=XSSTuhyAmnI&amp;ab_channel=AriSeff</a></p>
</section>
<section id="vaswani-ashish-et-al.-attention-is-all-you-need." class="slide level2">
<h2>Vaswani, Ashish, et al.&nbsp;Attention is all you need.</h2>
<div class="columns">
<div class="column" style="width:60%;">
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/attention.png" height="400"></p>
<figcaption>typical encoder-decoder blocks of a transformer</figcaption>
</figure>
</div>
</center>
</div><div class="column" style="width:40%;">
<p>Transformer has a encoder-decoder architecture similar to the previous sequence-to-sequence RNN models, except all the recurrent connections are replaced by the attention modules.</p>
</div></div>
</section>
<section id="attention-mapping" class="slide level2">
<h2>Attention Mapping</h2>
<p>In general, attention mapping can be described as a function of a query and a set of key-value pairs. Transformer uses a “scaled dot-product attention” to obtain the context vector:</p>
<p><span class="math display">\[\begin{align*}
{\bf c}^{(t)} = \text{attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_K}}\right) V
\end{align*}\]</span></p>
<p>This is very similar to the attetion mechanism we saw eariler, but we scale the pre-softmax values (the logits) down by the square root of the key dimension <span class="math inline">\(d_K\)</span>.</p>
</section>
<section id="attention-mapping-in-the-decoder" class="slide level2">
<h2>Attention Mapping in the Decoder</h2>
<p>When training the decoder (e.g.&nbsp;to generate a sequence), we desired output so that have to be careful to <strong>mask out</strong> the desired output so that we preserve the autoregressive property.</p>
<center>
<img data-src="imgs/rnn_gen_figure.png" height="300">
</center>
</section>
<section id="recall-dot-product-attention" class="slide level2">
<h2>Recall: Dot-Product Attention</h2>
<p><span class="math display">\[\begin{align*}
{\bf h}^{(1)} &amp;= \begin{bmatrix}1 &amp; 3 &amp; 9\end{bmatrix}^\top \\
{\bf h}^{(2)} &amp;= \begin{bmatrix}0 &amp; 0 &amp; 1\end{bmatrix}^\top \qquad \quad {\bf s}^{(t-1)} = \begin{bmatrix}0 &amp; 1 &amp; 1\end{bmatrix}^\top \\
{\bf h}^{(3)} &amp;= \begin{bmatrix}5 &amp; -1 &amp; 2\end{bmatrix}^\top \\
\\
\alpha_t &amp;= \text{softmax}\left(\begin{bmatrix}1 &amp; 0 &amp; 5 \\ 3 &amp; 0 &amp; -1 \\ 0 &amp; 1 &amp;2 \end{bmatrix}^\top
                    \begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix}\right) \\
{\bf c}^{(t)} &amp;= \alpha_{t1} {\bf h}^{(1)} + \alpha_{t2} {\bf h}^{(2)} + \alpha_{t2} {\bf h}^{(3)}\\
\end{align*}\]</span></p>
</section>
<section id="scaled-dot-product-attention" class="slide level2">
<h2>Scaled Dot-Product Attention</h2>
<p><span class="math display">\[\begin{align*}
{\bf h}^{(1)} &amp;= \begin{bmatrix}1 &amp; 3 &amp; 9\end{bmatrix}^\top \qquad \quad {\bf h}^{(2)} = \begin{bmatrix}0 &amp; 0 &amp; 1\end{bmatrix}^\top \\
{\bf h}^{(3)} &amp;= \begin{bmatrix}5 &amp; -1 &amp; 2\end{bmatrix}^\top \qquad \quad {\bf s}^{(t-1)} = \begin{bmatrix}0 &amp; 1 &amp; 1\end{bmatrix}^\top \\
\alpha_t &amp;= \text{softmax}\left(\frac{1}{\sqrt{3}} \begin{bmatrix}1 &amp; 0 &amp; 5 \\ 3 &amp; 0 &amp; -1 \\ 0 &amp; 1 &amp;2 \end{bmatrix}^\top
                    \begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix}\right) \\
{\bf c}^{(t)} &amp;= \alpha_{t1} {\bf h}^{(1)} + \alpha_{t2} {\bf h}^{(2)} + \alpha_{t2} {\bf h}^{(3)}\\
\end{align*}\]</span> Q: Which values represent the Q, K, and V?</p>
</section>
<section id="attending-to-the-input-encoder" class="slide level2">
<h2>Attending to the Input (Encoder)</h2>
<p>Transformer models attend to both the encoder annotations and its previous hidden layers.</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/encoder_attn.png" height="350"></p>
<figcaption>how the encoder contributes <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span></figcaption>
</figure>
</div>
</center>
</section>
<section id="attending-to-the-input-encoder-ii" class="slide level2">
<h2>Attending to the Input (Encoder) II</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/encoder_attn.png" height="350"></p>
<figcaption>how the encoder contributes <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span></figcaption>
</figure>
</div>
</center>
<p>When attending to the encoder annotations, the model computes the key-value pairs using linearly transformed the encoder outputs.</p>
</section>
<section id="self-attention" class="slide level2">
<h2>Self-attention</h2>
<p>Transformer models also use “self-attention” on its previous hidden layers. When applying attention to the previous hidden layers, the causal structure is preserved:</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/self_attn.png" height="350"></p>
<figcaption>feeding to deeper layers and future hidden states</figcaption>
</figure>
</div>
</center>
</section>
<section id="multi-headed-attention" class="slide level2">
<h2>Multi-headed Attention</h2>
<p>The Scaled Dot-Product Attention attends to one or few entries in the input key-value pairs.</p>
<p>But humans can attend to many things simultaneously</p>
<p><strong>Idea:</strong> apply scaled dot-product attention <em>multiple times</em> on the linearly transformed inputs:</p>
<p><span class="math display">\[\begin{align*}
\mathbf c_i &amp;= \text{attention}\left(QW_i^Q, KW_i^K, VW_i^V\right) \\
\text{MultiHead}(Q, K, V) &amp;= \text{concat}({\bf c_1}, \dots, {\mathbf c_h})W^O
\end{align*}\]</span></p>
</section>
<section id="input-sequence-order" class="slide level2">
<h2>Input Sequence Order</h2>
<p><span class="math display">\[\begin{align*}
\mathbf c_i &amp;= \text{attention}\left(QW_i^Q, KW_i^K, VW_i^V\right) \\
\text{MultiHead}(Q, K, V) &amp;= \text{concat}({\bf c_1}, \dots, {\mathbf c_h})W^O
\end{align*}\]</span></p>
<p>Unlike RNNs and CNN encoders, the attention encoder output do <em>not</em> depend on the order of the inputs. Can you see why?</p>
<p>However, the order of the sequence convey important information for the machine translation task, language modeling, and other tasks.</p>
</section>
<section id="positional-encoding" class="slide level2">
<h2>Positional Encoding</h2>
<p><strong>Idea:</strong> Add positional information of each input token in the sequence into the input embedding vectors.</p>
<p><span class="math display">\[\begin{align*}
PE_{\text{pos}, 2i} &amp;= \sin\left(\text{pos}/10000^{2i/d_{emb}}\right) \\
PE_{\text{pos}, 2i+1} &amp;= \cos\left(\text{pos}/10000^{2i/d_{emb}}\right)
\end{align*}\]</span></p>
</section>
<section id="positional-encoding-ii" class="slide level2">
<h2>Positional Encoding II</h2>
<p>The final input embeddings are the concatenation of the learnable embeddings and the positional encoding.</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/sinoidual.png"></p>
<figcaption>sinusoidal position encodings visualized</figcaption>
</figure>
</div>
</center>
</section>
<section id="transformer-machine-translation" class="slide level2">
<h2>Transformer Machine Translation</h2>
<div class="columns">
<div class="column" style="width:30%;">
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/tmt.png"></p>
<figcaption>typical encoder-decoder blocks of a transformer</figcaption>
</figure>
</div>
</center>
</div><div class="column" style="width:70%;">
<ul>
<li>Encoder-Decoder architecture like RNN, but use attention modules instead of recurrent modules.</li>
<li>Use N stacked self-attention layers.</li>
<li>Skip-connections help preserve the positional and identity information from the input sequences.</li>
</ul>
</div></div>
</section>
<section id="visualizing-attention" class="slide level2">
<h2>Visualizing Attention</h2>
<p>Self-attention layer learns that “it” could refer to different entities in different contexts.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/transformer_it.png" style="height:40.0%"></p>
<figcaption>visualized attention on a sentence</figcaption>
</figure>
</div>
<p>See <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></p>
</section>
<section id="backprop" class="slide level2">
<h2>Backprop</h2>
<p>During backprop, in the standard encoder-decoder RNN, the maximum path length across time is the number of time steps.</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/bptt_atten.png" height="350"></p>
<figcaption>backprop paths in RNNs vs transformers</figcaption>
</figure>
</div>
</center>
</section>
<section id="backprop-ii" class="slide level2">
<h2>Backprop II</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/bptt_atten.png" height="350"></p>
<figcaption>backprop paths in RNNs vs transformers</figcaption>
</figure>
</div>
</center>
<p>Attention-based RNNs have a constant path length between the encoder inputs and the decoder hidden states. (Learning becomes easier. Why?)</p>
</section>
<section id="transformer-language-pre-training" class="slide level2">
<h2>Transformer Language Pre-Training</h2>
<p>Similar to pre-training computer vision models on ImageNet, we can pre-train a language model for NLP tasks.</p>
<div class="columns">
<div class="column" style="width:60%;">
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/gen_pretrain.png" height="250"></p>
<figcaption>Radford, Alec, et al.&nbsp;”Improving Language Understanding by Generative Pre-Training.” 2018</figcaption>
</figure>
</div>
</center>
</div><div class="column" style="width:40%;">
<p>The pre-trained model is then fine-tuned on textual entailment, question answering, semantic similarity assessment, and document classification</p>
</div></div>
</section>
<section id="gpt-3-generating-an-article" class="slide level2">
<h2>GPT-3: Generating an Article</h2>
<p>Title: United Methodists Agree to Historic Split</p>
<p>Subtitle: Those who oppose gay marriage will form their own denomination</p>
</section>
<section id="gpt-3-generating-an-article-ii" class="slide level2">
<h2>GPT-3: Generating an Article II</h2>
<p>Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be “theologically and socially conservative,” according to The Washington Post. The majority of delegates attending the church’s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will “discipline” clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination.</p>
</section>
<section id="gpt-3-generating-an-article-iii" class="slide level2">
<h2>GPT-3: Generating an Article III</h2>
<p>The Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the “largest Protestant denomination in the U.S.,” but that it has been shrinking in recent decades. The new split will be the second in the church’s history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church.</p>
</section>
<section id="gpt-3-generating-an-article-iv" class="slide level2">
<h2>GPT-3: Generating an Article IV</h2>
<p>The Post notes that the proposed split “comes at a critical time for the church, which has been losing members for years,” which has been “pushed toward the brink of a schism over the role of LGBTQ people in the church.” Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban the</p>
<p><a href="https://arxiv.org/pdf/2005.14165.pdf">Brown et al.&nbsp;2020, Language Models are Few-Shot Learners</a></p>
</section>
<section id="gpt-3-correcting-english-grammar" class="slide level2">
<h2>GPT-3: Correcting English Grammar</h2>
<p>Poor English input: The patient was died.</p>
<p>Good English output: The patient died.</p>
<p>Poor English input: We think that Leslie likes ourselves.</p>
<p>Good English output: We think that Leslie likes us.</p>
<p>Poor English input: Janet broke Bill on the finger.</p>
<p>Good English output: Janet broke Bill’s finger.</p>
<p>Poor English input: Mary arranged for, in St.&nbsp;Louis, John to rent a house cheap.</p>
<p>Good English output: Mary arranged for John to rent a house in St.&nbsp;Louis.</p>
</section>
<section id="gpt-3-correcting-english-grammar-ii" class="slide level2">
<h2>GPT-3: Correcting English Grammar II</h2>
<p>Poor English input: Today I have went to the store to to buys some many bottle of water.</p>
<p>Good English output: Today I went to the store to buy some bottles of water.</p>
<p>Poor English input: I have tried to hit ball with bat, but my swing is has miss.</p>
<p>Good English output: I tried to hit the ball with the bat, but my swing missed.</p>
<p><a href="https://arxiv.org/pdf/2005.14165.pdf">Brown et al.&nbsp;2020, Language Models are Few-Shot Learners</a></p>
</section>
<section id="large-language-models-llms" class="slide level2">
<h2>Large Language Models (LLMs)</h2>
<p>Many Transformer-based models have been developed since 2017.</p>
<ul>
<li>Generative Pre-Trained Transformer (GPT-1/2/3)
<ul>
<li>Decoder-only 12-layer/12-headed Transformer.</li>
<li>Inspired GPT-Neo, GPT-J, GPT-NeoX, ChatGPT.</li>
<li>GPT-4 coming out soon.</li>
</ul></li>
</ul>
</section>
<section id="large-language-models-llms-ii" class="slide level2">
<h2>Large Language Models (LLMs) II</h2>
<ul>
<li>Bidirectional Encoder Representations from Transformers (BERT)
<ul>
<li>Encoder-only 12-layer/12-headed Transformer.</li>
<li>Inspired ALBERT, RoBERTa, SBERT, DeBERTa, etc.</li>
</ul></li>
</ul>
</section>
<section id="large-language-models-llms-iii" class="slide level2">
<h2>Large Language Models (LLMs) III</h2>
<ul>
<li>And many, many more: <a href="https://en.wikipedia.org/wiki/Large_language_model">en.wikipedia.org/wiki/Large_language_model</a></li>
</ul>
<p>Many benchmarks have been developed such as GLUE and SQuAD.</p>
<p>Big players in the LLM space include Google (Brain, DeepMind), Meta (formerly Facebook, FAIR), Microsoft, Amazon, EleutherAI, OpenAI, Cohere, Hugging Face.</p>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/utm-csc413\.github\.io\/2024F-website\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>