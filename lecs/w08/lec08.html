<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <title>CSC413 - Fall 2024, UTM – CSC413 Neural Networks and Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta property="og:description" content="Lecture 8">
<meta property="og:site_name" content="CSC413 - Fall 2024, UTM">
<meta name="twitter:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta name="twitter:description" content="Lecture 8">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC413 Neural Networks and Deep Learning</h1>
  <p class="subtitle">Lecture 8</p>

<div class="quarto-title-authors">
</div>

</section>
<section>
<section id="lecture-overview" class="title-slide slide level1 center">
<h1>Lecture Overview</h1>

</section>
<section id="last-week" class="slide level2">
<h2>Last Week</h2>
<ul>
<li class="fragment">CNN Feature Visualization and Interpretation</li>
<li class="fragment">Transfer Learning</li>
<li class="fragment">Adversarial Examples</li>
<li class="fragment">Influence Functions</li>
</ul>
</section>
<section id="this-week" class="slide level2">
<h2>This week</h2>
<ul>
<li class="fragment">Recurrent Neural Networks</li>
<li class="fragment">Sentiment Analysis with Recurrent Neural Networks</li>
<li class="fragment">Gradient Explosion and Vanishing</li>
<li class="fragment">Text Generation with RNN</li>
<li class="fragment">Sequence-to-Sequence Modelling</li>
</ul>
<!-- 01rnn -->
</section></section>
<section>
<section id="recurrent-neural-networks" class="title-slide slide level1 center">
<h1>Recurrent Neural Networks</h1>

</section>
<section id="goal-and-overview" class="slide level2">
<h2>Goal and Overview</h2>
<p>Sometimes we’re interested in making predictions about data in the form of <strong>sequences</strong>. Some examples are:</p>
<div class="fragment">
<ul>
<li>Given the price of a stock in the last week, predict whether stock price will go up</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Given a sentence (sequence of chars/words) predict its sentiment</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Given a sentence in English, translate it to French</li>
</ul>
</div>
<div class="fragment">
<p>This last example is a <strong>sequence-to-sequence prediction</strong> task, because both inputs and outputs are sequences.</p>
</div>
</section>
<section id="language-model" class="slide level2">
<h2>Language Model</h2>
<p>We have already seen neural language models that make the <strong>Markov Assumption</strong>:</p>
<p><span class="math display">\[p(w_i | w_1, \ldots, w_{i-1}) = p(w_i | w_{i-3}, w_{i-2}, w_{i-1})\]</span></p>
<div class="fragment">
<p>This means the model is <strong>memoryless</strong>, so it can only use information from its immediate context.</p>
</div>
<div class="fragment">
<center>
<img data-src="imgs/autoregressive.png" height="270">
</center>
<aside class="notes">
<ul>
<li>In this image, the context length is 1.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>
<section id="recurrent-neural-network" class="slide level2">
<h2>Recurrent Neural Network</h2>
<p>But sometimes long-distance context can be important.</p>
<div class="fragment">
<p>If we add connections between the hidden units, it becomes a <strong>recurrent neural network (RNN)</strong>.</p>
</div>
<div class="fragment">
<p>Having a memory lets an RNN use longer-term dependencies:</p>
<center>
<img data-src="imgs/rnn_motivation.png" style="height:30.0%">
</center>
</div>
</section>
<section id="rnn-diagram" class="slide level2">
<h2>RNN Diagram</h2>
<p>We can think of an RNN as a dynamical system with one set of hidden units which feed into themselves. The network’s graph would then have self-loops.</p>
<div class="fragment">
<p>We can <strong>unroll</strong> the RNN’s graph by explicitly representing the units at all time steps. The weights and biases are shared between all time steps</p>
<center>
<img data-src="imgs/unroll.png" width="400">
</center>
</div>
</section>
<section id="simple-rnns" class="slide level2">
<h2>Simple RNNs</h2>
<p>Let’s go through a few examples of very simple RNNs to understand how RNNs compute predictions.</p>
</section>
<section id="simple-rnn-example-sum" class="slide level2">
<h2>Simple RNN Example: Sum</h2>
<p>This simple RNN takes a sequence of numbers as input (scalars), and sums its inputs.</p>
<center>
<img data-src="imgs/rnn_example1_sol.png" height="500">
</center>
</section>
<section id="simple-rnn-example-2-comparison" class="slide level2">
<h2>Simple RNN Example 2: Comparison</h2>
<p>This RNN takes a sequence of <strong>pairs of numbers</strong> as input, and determines if the total values of the first or second input are larger:</p>
<center>
<img data-src="imgs/rnn_example2_sol.png" height="450">
</center>
</section>
<section id="simple-rnn-example-3-parity" class="slide level2">
<h2>Simple RNN Example 3: Parity</h2>
<p>Assume we have a sequence of binary inputs. We’ll consider how to determine the <strong>parity</strong>, i.e.&nbsp;whether the number of 1’s is even or odd. We can compute parity incrementally by keeping track of the parity of the input so far:</p>
<center>
<table class="caption-top">
<colgroup>
<col style="width: 36%">
<col style="width: 27%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th><span style="color:red;">Parity bits:</span></th>
<th></th>
<th> <span style="color:blue;">Input:</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span style="color:red;">0 1 1 0 1 1</span></td>
<td><span class="math inline">\(\longrightarrow\)</span></td>
<td><span style="color:blue;">0 1 0 1 1 0 1 0 1 1</span></td>
</tr>
</tbody>
</table>
</center>
<div class="fragment">
<p>Each parity bit is the XOR of the input and the previous parity bit. Parity is a classic example of a problem that’s hard to solve with a shallow feed-forward net, but easy to solve with an RNN.</p>
</div>
</section>
<section id="parity-approach" class="slide level2">
<h2>Parity Approach</h2>
<p>Let’s find weights and biases for the RNN on the right so that it computes the parity. All hidden and output units are <strong>binary threshold units</strong> (<span class="math inline">\(h(x) = 1\)</span> if <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(h(x) = 0\)</span> otherise).</p>
<div class="fragment">
<p><strong>Strategy</strong></p>
<ul>
<li>The output unit tracks the current parity, which is the XOR of the current input and previous output.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>The hidden units help us compute the XOR.</li>
</ul>
</div>
</section>
<section id="parity-approach-ii" class="slide level2">
<h2>Parity Approach II</h2>
<center>
<img data-src="imgs/parity_rnn_template_new.png" height="500">
</center>
</section>
<section id="unrolling-parity-rnn" class="slide level2">
<h2>Unrolling Parity RNN</h2>
<center>
<img data-src="imgs/parity_rnn_unroll.png" height="500">
</center>
</section>
<section id="parity-computation" class="slide level2">
<h2>Parity Computation</h2>
<p>The output unit should compute the XOR of the current input and previous output:</p>
<center>
<table class="caption-top">
<thead>
<tr class="header">
<th><span class="math inline">\(y^{(t-1)}\)</span></th>
<th><span class="math inline">\(x^{(t)}\)</span></th>
<th><span class="math inline">\(y^{(t)}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
</center>
<aside class="notes">
<ul>
<li><span class="math inline">\(y^{(t)}=0\)</span> meants the number of 1’s in the past input has been even.</li>
<li><span class="math inline">\(y^{(t)}=1\)</span> meants the number of 1’s in the past input has been odd.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="computing-parity" class="slide level2">
<h2>Computing Parity</h2>
<p>Let’s use hidden units to help us compute XOR.</p>
<ul>
<li>Have one unit compute AND, and the other one compute OR.</li>
</ul>
<div class="fragment">
<ul>
<li>Then we can pick weights and biases just like we did for multilayer perceptrons.</li>
</ul>
</div>
</section>
<section id="computing-parity-ii" class="slide level2">
<h2>Computing Parity II</h2>
<div class="columns">
<div class="column" style="width:50%;">
<center>
<table class="caption-top">
<thead>
<tr class="header">
<th><span class="math inline">\(y^{(t-1)}\)</span></th>
<th><span class="math inline">\(x^{(t)}\)</span></th>
<th><span class="math inline">\(h_1^{(t)}\)</span></th>
<th><span class="math inline">\(h_2^{(t)}\)</span></th>
<th><span class="math inline">\(y^{(t)}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
</center>
</div><div class="column" style="width:50%;">
<center>
<img data-src="imgs/parity_rnn_solution.png" height="300">
</center>
</div></div>
<aside class="notes">
<ul>
<li><span class="math inline">\(h_1\)</span> computes <em>and</em> by subtracting -1.5 from the sum of <span class="math inline">\(y\)</span> and the current input <span class="math inline">\(x\)</span> and then applying the binary threshold activation function.</li>
<li><span class="math inline">\(h_2\)</span> computes <em>or</em> by subtracting -0.5 from the sum of <span class="math inline">\(y\)</span> and the current input <span class="math inline">\(x\)</span> and then applying the binary threshold activation function.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="back-propagation-through-time" class="slide level2">
<h2>Back Propagation Through Time</h2>
<p>As you can guess, we don’t usually set RNN weights by hand. Instead, we learn them using backprop.</p>
<div class="fragment">
<p>In particular, we do backprop on the unrolled network. This is known as <strong>backprop through time</strong>.</p>
<div class="columns">
<div class="column" style="width:40%;">
<center>
<img data-src="imgs/bptt_rnn.png" height="290">
</center>
</div><div class="column" style="width:60%;">
<center>
<img data-src="imgs/bptt_rnn_unroll.png" height="300">
</center>
</div></div>
</div>
</section>
<section id="unrolled-bptt" class="slide level2">
<h2>Unrolled BPTT</h2>
<p>Here’s the unrolled computation graph. Notice the weight sharing.</p>
<center>
<img data-src="imgs/unrolled_computation_graph.png" height="470">
</center>
</section>
<section id="what-can-rnns-compute" class="slide level2">
<h2>What can RNNs compute?</h2>
<p>In 2014, Google researchers built an encoder-decoder RNN that learns to execute simple Python programs, one character at a time! <a href="https://arxiv.org/abs/1410.4615">https://arxiv.org/abs/1410.4615</a></p>
<p><img data-src="imgs/code.png" style="height:70.0%"></p>
</section>
<section id="what-can-rnns-compute-1" class="slide level2">
<h2>What can RNNs compute?</h2>
<p>RNNs are good at learning complex syntactic structures: generate Algebraic Geometry LaTex source files that almost compiles:</p>
<center>
<img data-src="imgs/geometry.png" height="350">
</center>
<p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>
</section></section>
<section>
<section id="sentiment-analysis-with-recurrent-neural-networks" class="title-slide slide level1 center">
<h1>Sentiment Analysis with Recurrent Neural Networks</h1>

</section>
<section id="rnn-for-language-modelling" class="slide level2">
<h2>RNN for language modelling</h2>
<p>Usually, the sequence of inputs <span class="math inline">\(x_t\)</span> will be <strong>vectors</strong>. The hidden states <span class="math inline">\(h_t\)</span> are also vectors.</p>
<div class="fragment">
<p>For example, we might use a sequence of one-hot vectors <span class="math inline">\({\bf x}_t\)</span> of words (or characters) to represent a sentence. (What else can we use?)</p>
</div>
<div class="fragment">
<p>How would we use a RNN to determine (say) the sentiment conveyed by the sentence?</p>
</div>
<div class="fragment">
<p>As usual, start with the forward pass…</p>
</div>
</section>
<section id="rnn-initial-hidden-state" class="slide level2">
<h2>RNN: Initial Hidden State</h2>
<center>
<img data-src="imgs/rnn0.png" style="height:70.0%">
</center>
<p>Start with an initial <strong>hidden state</strong> with a blank slate (can be a vector of all zeros, or a parameter that we train)</p>
</section>
<section id="rnn-update-hidden-state" class="slide level2">
<h2>RNN: Update Hidden State</h2>
<center>
<img data-src="imgs/rnn1.png" style="height:70.0%">
</center>
<p>Compute the first hidden state based on the initial hidden state, and the input (the one-hot vector <span class="math inline">\({\bf x}_1\)</span> of the <strong>first word</strong>).</p>
</section>
<section id="rnn-continue-updating-hidden-state" class="slide level2">
<h2>RNN: Continue Updating Hidden State</h2>
<center>
<img data-src="imgs/rnn2.png" style="height:70.0%">
</center>
<p>Update the hidden state based on the subsequent inputs. Note that we are using the <strong>same weights</strong> to perform the update each time.</p>
</section>
<section id="rnn-last-hidden-state" class="slide level2">
<h2>RNN: Last Hidden State</h2>
<center>
<img data-src="imgs/rnn3.png" style="height:70.0%">
</center>
<p>Continue updating the hidden state until we run out of words in our sentence.</p>
</section>
<section id="rnn-compute-prediction" class="slide level2">
<h2>RNN: Compute Prediction</h2>
<center>
<img data-src="imgs/rnn4.png" style="height:60.0%">
</center>
<p>Use the <strong>last hidden state</strong> as input to a prediction network, usually a MLP.</p>
<div class="fragment">
<p>Alternative: take the max-pool and average-pool over all computed hidden states.</p>
</div>
</section>
<section id="sequence-classification" class="slide level2">
<h2>Sequence Classification</h2>
<p>Lab: let’s build this model!</p>
<center>
<img data-src="imgs/rnn4.png" height="500">
</center>
</section>
<section id="sentiment140-data" class="slide level2">
<h2>Sentiment140 Data</h2>
<p>Dataset of tweets with either a positive or negative emoticon, but with the emoticon removed.</p>
<p><strong>Input:</strong> Tweet (sequence of words/characters)</p>
<p><strong>Target</strong>: Positive or negative emoticon?</p>
<p>Example:</p>
<ul>
<li>Negative: “Just going to cry myself to sleep after watching Marley and Me”</li>
</ul>
<div class="fragment">
<ul>
<li>Positive: “WOOOOO! Xbox is back”</li>
</ul>
</div>
</section>
<section id="approach" class="slide level2">
<h2>Approach</h2>
<div class="columns">
<div class="column" style="width:40%;">
<center>
<img data-src="imgs/rnn4.png" height="200">
</center>
</div><div class="column" style="width:60%;">
<ul>
<li>Use GloVe embeddings to represent words as input <span class="math inline">\({\bf x}^{(t)}\)</span> (note: we could have chosen to work at the character level)</li>
</ul>
</div></div>
<div class="fragment">
<ul>
<li>Use a recurrent neural network to get a combined embedding of the <em>entire</em> tweet</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Use a fully-connected layer to make predictions (happy vs sad)</li>
</ul>
<!-- ## Video Demo

[https://play.library.utoronto.ca/watch/a1d297375e8bfe173b48325c37828a75](https://play.library.utoronto.ca/watch/a1d297375e8bfe173b48325c37828a75)
-->
</div>
</section>
<section id="key-considerations" class="slide level2">
<h2>Key Considerations</h2>
<ul>
<li>We’ll be using the PyTorch <code>nn.RNN</code> module, which can be unintuitive</li>
</ul>
<div class="fragment">
<ul>
<li>Batching difficulties: each tweet is a different length, so how can we batch?
<ul>
<li>One way is to <em>pad</em> shorter sequences with a special “padding” token at the end of the sequence</li>
<li>However, we want to minimize this padding due to computational complexity</li>
</ul></li>
</ul>
<!-- 03grad -->
</div>
</section></section>
<section>
<section id="gradient-explosion-and-vanishing" class="title-slide slide level1 center">
<h1>Gradient Explosion and Vanishing</h1>

</section>
<section id="rnn-gradients" class="slide level2">
<h2>RNN Gradients</h2>
<p>Recall the unrolled computation graph for a small RNN:</p>
<center>
<img data-src="imgs/unrolled_computation_graph.png" height="470">
</center>
</section>
<section id="backprop-through-time" class="slide level2">
<h2>Backprop Through Time</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Activations:</strong> <span class="math display">\[\begin{align*}
    \overline{\mathcal{L}} &amp;= 1 \\
    \overline{y^{(t)}} &amp;= \overline{\mathcal{L}} \, \frac{\partial \mathcal{L}}{\partial y^{(t)}} \\
    \overline{r^{(t)}} &amp;= \overline{y^{(t)}} \, \phi^\prime(r^{(t)}) \\
    {\color{magenta} \overline{h^{(t)}}} &amp; {\color{magenta}\, = \overline{r^{(t)}} \, v + \overline{z^{(t+1)}} \, w} \\
    \overline{z^{(t)}} &amp;= \overline{h^{(t)}} \, \phi^\prime(z^{(t)})
\end{align*}\]</span></p>
</div><div class="column" style="width:50%;">
<p><strong>Parameters:</strong> <span class="math display">\[\begin{align*}
    {\color{magenta} \overline{u}} &amp;{\color{magenta} \, = \sum_t \overline{z^{(t)}} \, x^{(t)}} \\
    {\color{magenta} \overline{v}} &amp;{\color{magenta} \,= \sum_t \overline{r^{(t)}} \, h^{(t)}} \\
    {\color{magenta} \overline{w}} &amp;{\color{magenta} \,= \sum_t \overline{z^{(t+1)}} \, h^{(t)}}
\end{align*}\]</span></p>
</div></div>
<p>Key idea: multivariate chain rule!</p>
</section>
<section id="gradient-explosion-and-vanishing-1" class="slide level2">
<h2>Gradient Explosion and Vanishing</h2>
<p>The longer your sequence, the longer gap the time step between when we see potentially important information and when we need it:</p>
<center>
<img data-src="imgs/grad.png" height="300">
</center>
<div class="fragment">
<p>The derivatives need to travel this entire pathway.</p>
</div>
</section>
<section id="why-gradients-explode-or-vanish" class="slide level2">
<h2>Why Gradients Explode or Vanish</h2>
<p>Consider a univariate version of the RNN:</p>
<center>
<img data-src="imgs/encoder_uni.png" height="200">
</center>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Backpropagation updates:</strong> <span class="math display">\[\begin{align*}
    \overline{h^{(t)}} &amp;= \overline{z^{(t+1)}} \, w \\
    \overline{z^{(t)}} &amp;= \overline{h^{(t)}} \, \phi^\prime(z^{(t)})
\end{align*}\]</span></p>
</div><div class="column" style="width:50%;">
<p><strong>Applying this recursively:</strong> <span class="math display">\[\begin{align*}
    \overline{h^{(1)}} = w^{T-1} \phi^\prime(z^{(2)}) \cdots \phi^\prime(z^{(T)}) \overline{h^{(T)}}
\end{align*}\]</span></p>
</div></div>
</section>
<section id="why-gradients-explode-or-vanish-ii" class="slide level2">
<h2>Why Gradients Explode or Vanish II</h2>
<p><strong>With linear activations:</strong> <span class="math display">\[\frac{\partial h^{(T)}}{\partial h^{(1)}} = w^{T-1}\]</span></p>
<p><span class="math display">\[\textbf{Exploding:} \qquad w=1.1, T=50 \Rightarrow \frac{\partial h^{(T)}}{\partial h^{(1)}} = 117.4\]</span></p>
<p><span class="math display">\[\textbf{Vanishing:} \qquad w=0.9, T=50 \Rightarrow \frac{\partial h^{(T)}}{\partial h^{(1)}} = 0.00515\]</span></p>
</section>
<section id="multivariate-hidden-states" class="slide level2">
<h2>Multivariate Hidden States</h2>
<p>More generally, in the multivariate case, the <strong>Jacobians</strong> multiply:</p>
<p><span class="math display">\[\frac{\partial {\bf h}^{(T)}}{\partial {\bf h}^{(1)}} = \prod_{t = 1}^{T - 1} \frac{\partial {\bf h}^{(t + 1)}}{\partial {\bf h}^{(t)}}\]</span></p>
<div class="fragment">
<p>Matrices can “explode” or “vanish” just like scalar values, though it’s slightly harder to make precise.</p>
</div>
</section>
<section id="repeated-application-of-functions" class="slide level2">
<h2>Repeated Application of Functions</h2>
<p>Another way to look at why gradients explode or vanish is that we are applying a function over and over again.</p>
<div class="fragment">
<p>Each hidden layer computes some function of previous hidden layer and the current input: <span class="math inline">\({\bf h}^{(t)} = f({\bf h}^{(t-1)}, {\bf x}^{(t)})\)</span></p>
</div>
<div class="fragment">
<p>This function gets repeatedly applied:</p>
<p><span class="math display">\[\begin{align*}
{\bf h}^{(4)} &amp;= f({\bf h}^{(3)}, {\bf x}^{(4)}) \\
              &amp;= f(f({\bf h}^{(2)}, {\bf x}^{(3)}), {\bf x}^{(4)}) \\
              &amp;= f(f(f({\bf h}^{(1)}, {\bf x}^{(2)}), {\bf x}^{(3)}), {\bf x}^{(4)})
\end{align*}\]</span></p>
</div>
</section>
<section id="iterated-functions-intuition" class="slide level2">
<h2>Iterated Functions (Intuition)</h2>
<p>We get complicated behaviour from iterated functions. Consider <span class="math inline">\(f(x) = 3.5x(1-x)\)</span></p>
<div class="fragment">
<div class="columns">
<div class="column" style="width:25%;">
<p><span class="math inline">\(y = f(x)\)</span></p>
<p><img data-src="imgs/iterated1.png" height="170"></p>
</div><div class="column" style="width:25%;">
<p><span class="math inline">\(y = f(f(x))\)</span></p>
<p><img data-src="imgs/iterated2.png" height="170"></p>
</div><div class="column" style="width:20%;">
<p><span class="math inline">\(y = f^{\circ 3}(x)\)</span></p>
<p><img data-src="imgs/iterated3.png" height="170"></p>
</div><div class="column" style="width:25%;">
<p><span class="math inline">\(y = f^{\circ 6}(x)\)</span></p>
<p><img data-src="imgs/iterated4.png" height="170"></p>
</div></div>
</div>
<div class="fragment">
<p>Note that the function values gravitate towards <strong>fixed points</strong>, and that the derivatives becomes either <strong>very large</strong> or <strong>very small</strong>.</p>
</div>
</section>
<section id="rnn-with-tanh-activation" class="slide level2">
<h2>RNN with tanh activation</h2>
<p>More concretely, consider an RNN with a tanh activation function:</p>
<center>
<img data-src="imgs/tanh_rnn.png" height="400">
</center>
</section>
<section id="rnn-with-tanh-activation-ii" class="slide level2">
<h2>RNN with tanh activation II</h2>
<p>The function computed by the network:</p>
<center>
<img data-src="imgs/tanh_responses.png" height="400">
</center>
</section>
<section id="cliffs" class="slide level2">
<h2>Cliffs</h2>
<p>Repeatedly applying a function adds a new type possible loss landscape: <strong>cliffs</strong>, where the gradient of the loss with respect to a parameter is either close to 0, or very large.</p>
<center>
<img data-src="imgs/cliffs.png" style="height:40.0%">
</center>
</section>
<section id="cliffs-ii" class="slide level2">
<h2>Cliffs II</h2>
<p>Generally, the gradient will explode on some inputs and vanish on others. In expectation, the cost may be fairly smooth.</p>
</section>
<section id="gradient-clipping" class="slide level2">
<h2>Gradient Clipping</h2>
<p>One solution is to “clip” the gradient so that it has a norm of at most <span class="math inline">\(\eta\)</span>. Otherwise, update the gradient <span class="math inline">\({\bf g}\)</span> with <span class="math inline">\({\bf g} \leftarrow \eta\frac{{\bf g}}{||{\bf g}||}\)</span></p>
<div class="fragment">
<p>The gradients are biased, but at least they don’t blow up:</p>
<center>
<img data-src="imgs/clipping.png" height="270">
</center>
</div>
<div class="fragment">
<p>Gradient clipping solves the exploding gradient problem, but not the vanishing gradient problem.</p>
</div>
</section>
<section id="learning-long-term-dependencies" class="slide level2">
<h2>Learning Long-Term Dependencies</h2>
<p><strong>Idea</strong>: Initialization</p>
<p>Hidden units are a kind of memory. Their default behaviour should be to <strong>keep their previous value</strong>.</p>
<div class="fragment">
<p>If the function <span class="math inline">\({\bf h}^{(t)} = f({\bf h}^{(t-1)}, {\bf x}^{(t)})\)</span> is close to the identity, then the gradient computations <span class="math inline">\(\displaystyle \frac{\partial {\bf h}^{(t)}}{\partial {\bf h}^{(t-1)}}\)</span> are stable.</p>
</div>
<div class="fragment">
<p>This initialization allows learning much longer-term dependencies than “vanilla” RNNs</p>
</div>
</section>
<section id="long-term-short-term-memory" class="slide level2">
<h2>Long-Term Short Term Memory</h2>
<p>Change the <strong>architecture</strong> of the recurrent neural network by replacing each single unit in an RNN by a “memory block”:</p>
<center>
<img data-src="imgs/lstm.png" style="height:50.0%">
</center>
</section>
<section id="lstm" class="slide level2">
<h2>LSTM</h2>
<center>
<img data-src="imgs/lstm_jimmy.png" height="550">
</center>
</section>
<section id="lstm-math" class="slide level2">
<h2>LSTM Math</h2>
<p>In each step, we have a vector of memory cells <span class="math inline">\({\bf c}\)</span>, a vector of hidden units <span class="math inline">\({\bf h}\)</span> and vectors of input, output, and forget gates <span class="math inline">\({\bf i}\)</span>, <span class="math inline">\({\bf o}\)</span>, and <span class="math inline">\({\bf f}\)</span>.</p>
<div class="fragment">
<p>There’s a full set of connections from all the inputs and hiddens to the inputs and all of the gates:</p>
<p><span class="math display">\[
\begin{pmatrix}
\bf{i}_t \\
\bf{f}_t \\
\bf{o}_t \\
\bf{g}_t \\
\end{pmatrix} =
\begin{pmatrix}
\sigma \\
\sigma \\
\sigma \\
\tanh
\end{pmatrix} {\bf W}
\begin{pmatrix}
{\bf x}_t \\
{\bf h}_{t - 1}
\end{pmatrix}
\]</span></p>
</div>
</section>
<section id="lstm-math-ii" class="slide level2">
<h2>LSTM Math II</h2>
<p><span class="math display">\[\begin{align*}
{\bf c_t } &amp;= {\bf f_t } \circ {\bf c_{t-1} } + {\bf i_t } \circ {\bf g_t } \\
{\bf h_t } &amp;= {\bf o_t } \circ \tanh({\bf c_t })
\end{align*}\]</span></p>
<div class="fragment">
<p><strong>Exercise:</strong> show that if <span class="math inline">\({\bf f}_{t+1} = 1\)</span>, <span class="math inline">\({\bf i}_{t+1} = 0\)</span>, and <span class="math inline">\({\bf o}_{t} = 0\)</span>, then the gradient of the memory cell gets passed through unmodified, i.e., <span class="math inline">\(\bar{{\bf c}_t} = \bar{{\bf c}_{t+1}}\)</span>.</p>
</div>
</section>
<section id="key-takeaways" class="slide level2">
<h2>Key Takeaways</h2>
<p>You should be able to understand…</p>
<ul>
<li>why learning long-term dependencies is hard for vanilla RNNs</li>
</ul>
<div class="fragment">
<ul>
<li>why gradients vanish/explode in a vanilla RNN</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>what cliffs are and how repeated application of a function generates cliffs</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>what gradient clipping is and when it is useful</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>the mathematics behind why gating works</li>
</ul>
<!-- 04gen -->
</div>
</section></section>
<section>
<section id="text-generation-with-rnn" class="title-slide slide level1 center">
<h1>Text Generation with RNN</h1>

</section>
<section id="rnn-hidden-states" class="slide level2">
<h2>RNN Hidden States</h2>
<p>RNN For Prediction:</p>
<ul>
<li>Process tokens one at a time</li>
</ul>
<div class="fragment">
<ul>
<li>Hidden state is a representation of <strong>all the tokens read thus far</strong></li>
</ul>
</div>
<div class="fragment">
<p>RNN For Generation:</p>
<ul>
<li>Generate tokens one at a time</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Hidden state is a representation of <strong>all the tokens to be generated</strong></li>
</ul>
</div>
</section>
<section id="rnn-hidden-state-updates" class="slide level2">
<h2>RNN Hidden State Updates</h2>
<p>RNN for Prediction:</p>
<ul>
<li>Update hidden state with new input (token)</li>
</ul>
<div class="fragment">
<ul>
<li>Get prediction (e.g.&nbsp;distribution over possible labels)</li>
</ul>
</div>
<div class="fragment">
<p>RNN for Generation:</p>
<ul>
<li>Get prediction distribution of next token</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Generate a token from the distribution</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Update the hidden state with new token</li>
</ul>
</div>
</section>
<section id="text-generation-diagram" class="slide level2">
<h2>Text Generation Diagram</h2>
<center>
<img data-src="imgs/rnn_gen_figure.png">
</center>
<ul>
<li>Get prediction distribution of next token</li>
</ul>
<div class="fragment">
<ul>
<li>Generate a token from the distribution</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Update the hidden state with new token</li>
</ul>
</div>
</section>
<section id="test-time-behaviour-of-generative-rnn" class="slide level2">
<h2>Test Time Behaviour of Generative RNN</h2>
<p>Unlike other models we discussed so far, the training time behaviour of Generative RNNs will be <strong>different</strong> from the test time behaviour</p>
<div class="fragment">
<p>Test time behaviour at each time step:</p>
<ul>
<li>Obtain a <strong>distribution</strong> over possible next tokens</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Sample a token from that distribution</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Update the hidden state based on the sample token</li>
</ul>
</div>
</section>
<section id="training-time-behaviour-of-generative-rnn" class="slide level2">
<h2>Training Time Behaviour of Generative RNN</h2>
<p>During training, we try to get the RNN to generate one particular sequence in the training set. At each time step:</p>
<ul>
<li>Obtain a <strong>distribution</strong> over possible next tokens</li>
</ul>
<div class="fragment">
<ul>
<li>Compare this with the <em>actual</em> next token</li>
</ul>
</div>
<div class="fragment">
<p>Q1: What kind of a problem is this? (regression or classification?)</p>
</div>
<div class="fragment">
<p>Q2: What loss function should we use during training?</p>
<aside class="notes">
<ul>
<li>A1: The problem is classification. The different tokens are the respective classes.</li>
<li>A2: The loss is a cross entropy loss as usual in classification</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>
<section id="text-generation-first-step" class="slide level2">
<h2>Text Generation: First Step</h2>
<center>
<img data-src="imgs/rnn_gen_figure.png" height="150">
</center>
<ul>
<li>Start with an initial hidden state</li>
</ul>
<div class="fragment">
<ul>
<li>Update the hidden state with a “&lt;BOS&gt;” (beginning of string) token to initiate the hidden state</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Get the distribution over the first character</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Compute the cross-entropy loss against the ground truth (R)</li>
</ul>
<aside class="notes">
<ul>
<li>Each token is its own classification problem. This is the first one.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>
<section id="text-generation-with-teacher-forcing" class="slide level2">
<h2>Text Generation with Teacher Forcing</h2>
<center>
<img data-src="imgs/rnn_gen_figure.png" height="150">
</center>
<ul>
<li>Update the hidden state with the <strong>ground truth</strong> token (R) regardless of the prediction from the previous step
<ul>
<li>This technique is called <strong>teaching forcing</strong></li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li>Get the distribution over the second character</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Compute the cross-entropy loss against the ground truth (I)</li>
</ul>
</div>
</section>
<section id="text-generation-later-steps" class="slide level2">
<h2>Text Generation: Later Steps</h2>
<center>
<img data-src="imgs/rnn_gen_figure.png" height="400">
</center>
<p>Continue until we get to the “&lt;EOS&gt;” (end of string) token</p>
</section>
<section id="some-remaining-challenges" class="slide level2">
<h2>Some Remaining Challenges</h2>
<ul>
<li>Vocabularies can be very large once you include people, places, etc.</li>
</ul>
<div class="fragment">
<ul>
<li>It’s computationally difficult to predict distributions over millions of words.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>How do we deal with words we haven’t seen before?</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>In some languages, it’s hard to define what should be considered a word.</li>
</ul>
</div>
</section>
<section id="character-vs-word-level" class="slide level2">
<h2>Character vs Word-level</h2>
<p>Another approach is to model text <em>one character at a time</em></p>
<div class="fragment">
<p>This solves the problem of what to do about previously unseen words.</p>
</div>
<div class="fragment">
<p>Note that long-term memory is essential at the character level!</p>
</div>
</section></section>
<section>
<section id="sequence-to-sequence-architecture" class="title-slide slide level1 center">
<h1>Sequence-to-Sequence Architecture</h1>

</section>
<section id="neural-machine-translation" class="slide level2">
<h2>Neural Machine Translation</h2>
<p>Say we want to translate, e.g.&nbsp;English to French sentences.</p>
<div class="fragment">
<p>We have pairs of translated sentences to train on.</p>
</div>
<div class="fragment">
<p>Here, both the inputs and outputs are sequences!</p>
<p>What can we do?</p>
</div>
</section>
<section id="sequence-to-sequence-architecture-1" class="slide level2">
<h2>Sequence-to-sequence architecture</h2>
<p><img data-src="imgs/nmt.png" style="height:30.0%"></p>
<p>The network first reads and memorizes the sentences.</p>
<div class="fragment">
<p>When it sees the “end token”, it starts outputting the translation.</p>
</div>
<div class="fragment">
<p>The “encoder” and “decoder” are two different networks with different weights.</p>
</div>
</section></section>
<section>
<section id="wrap-up" class="title-slide slide level1 center">
<h1>Wrap Up</h1>

</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">Recurrent Neural Networks can be used for learning sequence data</li>
<li class="fragment">Training RNNs may suffer from gradient explosion and vanishing</li>
<li class="fragment">Important Applications of RNNs are text generation and sequence to sequence modelling</li>
</ul>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/utm-csc413\.github\.io\/2024F-website\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>