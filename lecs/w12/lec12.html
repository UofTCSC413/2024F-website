<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <title>CSC413 - Fall 2024, UTM – CSC413 Neural Networks and Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta property="og:description" content="Lecture 12">
<meta property="og:site_name" content="CSC413 - Fall 2024, UTM">
<meta name="twitter:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta name="twitter:description" content="Lecture 12">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC413 Neural Networks and Deep Learning</h1>
  <p class="subtitle">Lecture 12</p>

<div class="quarto-title-authors">
</div>

</section>
<section id="announcements" class="slide level2">
<h2>Announcements</h2>
<ul>
<li>Book a meeting to discuss your project with an instructor if you haven’t already!</li>
</ul>
<div class="fragment">
<ul>
<li>You should have an overfit version of the model already</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>This is your first step, and should let you know the time/memory requirements of your model</li>
</ul>
</div>
</section>
<section id="review-autoencoders" class="slide level2">
<h2>Review: Autoencoders</h2>
<p>A couple of weeks ago, we discussed the <strong>autoencoders</strong></p>
<ul>
<li>Encoder: maps <span class="math inline">\(x\)</span> to a low-dimensional embedding <span class="math inline">\(z\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>Decoder: uses the low-dimensional embedding <span class="math inline">\(z\)</span> to reconstructs <span class="math inline">\(x\)</span></li>
</ul>
<div class="columns">
<div class="column" style="width:40%;">
<center>
<img data-src="imgs/ae.png" height="300">
</center>
</div><div class="column" style="width:60%;">
<p>Let’s see how much you remember!</p>
</div></div>
</div>
</section>
<section id="review-q1" class="slide level2">
<h2>Review Q1</h2>
<p>What was the objective that we used to train the autoencoder?</p>
<!--
write in:
- minimize the reconstruction error (square loss):
$$\sum_j (\hat{\bf x}_j - {\bf x}_j)^2$$

where $\hat{\bf x} = Decoder(Encoder({\bf x}))$
-->
</section>
<section id="review-q2" class="slide level2">
<h2>Review Q2</h2>
<p>If we train an autoencoder, what tasks can we accomplish with just the encoder portion of the autoencoder?</p>
<!--
write in
- compute distances
- possibly use for transfer learning
- semi-supervised learning?
-->
</section>
<section id="review-q3" class="slide level2">
<h2>Review Q3</h2>
<p>If we train an autoencoder, what tasks can we accomplish with mainly the decoder portion of the autoencoder?</p>
<!--
write in
- generate new data points that look like your training set!
-->
</section>
<section id="review-q4" class="slide level2">
<h2>Review Q4</h2>
<p>What are some limitations of the autoencoder?</p>
</section>
<section id="autoencoder-limitations" class="slide level2">
<h2>Autoencoder Limitations</h2>
<ul>
<li>Images are blurry due to the use of MSE loss</li>
</ul>
<div class="fragment">
<ul>
<li>It’s not certain what good values of embeddings <span class="math inline">\(z\)</span> would be
<ul>
<li>Which part of the embedding space does the encoder maps data to?</li>
<li>This uncertainty means that we can’t generate images without referring back to the encoder</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>It’s not clear what the dimension of the embedding <span class="math inline">\(z\)</span> should be</li>
</ul>
</div>
</section>
<section id="autoencoder-limitations-ii" class="slide level2">
<h2>Autoencoder Limitations II</h2>
<p>Could we resolve (but not all) some of the issues with autoencoder, if we use a more theoretically grounded approach?</p>
<p>Is there a probabilistic version of the autoencoder model?</p>
<!-- 01vae -->
</section>
<section>
<section id="variational-autoencoders" class="title-slide slide level1 center">
<h1>Variational Autoencoders</h1>

</section>
<section id="generative-model" class="slide level2">
<h2>Generative Model</h2>
<p>In CSC311, we learned about <strong>generative models</strong> that describe the distribution that the data comes from</p>
<div class="fragment">
<ul>
<li>i.e.&nbsp;describe the distribution <span class="math inline">\({\bf x} \sim p({\bf x})\)</span>, where <span class="math inline">\({\bf x}\)</span> is a single data point</li>
</ul>
</div>
<div class="fragment">
<p>For example, in the Naive Bayes model for data <span class="math inline">\({\bf x}\)</span> (e.g.&nbsp;bag-of-word encoding of an email, which could be spam or not spam) with <span class="math inline">\({\bf x} \sim p({\bf x})\)</span>, we assumed that <span class="math inline">\(p({\bf x}) = \sum_c p({\bf x}|c)p(c)\)</span>, where <span class="math inline">\(c\)</span> is either spam or not spam. We made further assumptions about <span class="math inline">\(p({\bf x}|c)\)</span>, e.g.&nbsp;that each <span class="math inline">\(x_i\)</span> is an independent Bernoulli.</p>
</div>
</section>
<section id="mathematical-notation-and-assumptions" class="slide level2">
<h2>Mathematical Notation and Assumptions</h2>
<p>Data <span class="math inline">\(x_i \in \mathbb{R}^d\)</span> are:</p>
<ul>
<li>independent, identically distributed (i.i.d)</li>
</ul>
<div class="fragment">
<ul>
<li>generated from the following joint distribution (with the true parameter <span class="math inline">\(\theta^{*}\)</span> unknown)</li>
</ul>
<p><span class="math display">\[p_{\theta^{*}}(\textbf{z}, \textbf{x}) = p_{\theta^{*}}(\textbf{z})p_{\theta^{*}}(\textbf{x} | \textbf{z})\]</span></p>
<p>Where <span class="math inline">\({\bf z}\)</span> is a low-dimensional vector (latent embedding)</p>
</div>
</section>
<section id="mathematical-notation-and-assumptions-ii" class="slide level2">
<h2>Mathematical Notation and Assumptions II</h2>
<ul>
<li>Example <span class="math inline">\({\bf x}\)</span> could be an MNIST digit</li>
</ul>
<div class="fragment">
<ul>
<li>Think of <span class="math inline">\({\bf z}\)</span> as encoding digit features like digit shape, tilt, line thickness, font style, etc…</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>To generate an image, we first sample from the prior distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{z})\)</span> to decide on these digit features, and use <span class="math inline">\(p_{\theta^{*}}(\textbf{x} | \textbf{z})\)</span> to generate an image given those features</li>
</ul>
</div>
</section>
<section id="intractability" class="slide level2">
<h2>Intractability</h2>
<p>Our data set is large, and so the following are <em>intractable</em></p>
<ul>
<li>evidence <span class="math inline">\(p_{\theta^{*}}(\textbf{x})\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>posterior distributions <span class="math inline">\(p_{\theta^{*}}(\textbf{z} | \textbf{x})\)</span></li>
</ul>
<p>In other words, exactly computing the distribution of <span class="math inline">\(p(\textbf{x})\)</span> and <span class="math inline">\(p(\textbf{z} | \textbf{x})\)</span> using our dataset has high runtime complexity.</p>
</div>
</section>
<section id="the-decoder-and-encoder" class="slide level2">
<h2>The Decoder and Encoder</h2>
<div class="columns">
<div class="column" style="width:40%;">
<center>
<img data-src="imgs/ae.png" height="250">
</center>
</div><div class="column" style="width:60%;">
<p>With this assumption, we can think of the autoencoder as doing the following:</p>
</div></div>
<p><strong>Decoder</strong>: A point approximation of the true distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{x}|\textbf{z})\)</span></p>
<p><strong>Encoder</strong>: Making a <strong>point prediction</strong> for the value of the latent vector <span class="math inline">\(z\)</span> that generated the image <span class="math inline">\(x\)</span></p>
</section>
<section id="the-decoder-and-encoder-ii" class="slide level2">
<h2>The Decoder and Encoder II</h2>
<p>Alternative:</p>
<ul>
<li>what if, instead, we try to infer the <strong>distribution</strong> <span class="math inline">\(p_{\theta^{*}}(\textbf{z}|\textbf{x})\)</span>?</li>
</ul>
</section>
<section id="vae-setup-so-far" class="slide level2">
<h2>VAE Setup so far</h2>
<p><strong>Decoder</strong>: An approximation of the true distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{x}|\textbf{z})\)</span></p>
<p><strong>Encoder</strong>: An approximation of the true distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{z}|\textbf{x})\)</span></p>
</section>
<section id="computing-the-encoding-distribution-p_thetatextbfztextbfx" class="slide level2">
<h2>Computing the encoding distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{z}|\textbf{x})\)</span></h2>
<p>Unfortunately, the true distribution <span class="math inline">\(p_{\theta^{*}}({\bf z}|{\bf x})\)</span> is complex (e.g.&nbsp;can be multi-modal).</p>
<p>But can we approximate this distribution with a <strong>simpler distribution</strong>?</p>
<p>Let’s restrict our estimate <span class="math inline">\(q_\phi({\bf z}|{\bf x}) = \mathcal{N}({\bf z}; \boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span> to be a multivariate Gaussian distribution with <span class="math inline">\(\phi = (\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span></p>
</section>
<section id="computing-the-encoding-distribution-p_thetatextbfztextbfx-ii" class="slide level2">
<h2>Computing the encoding distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{z}|\textbf{x})\)</span> II</h2>
<ul>
<li>It suffices to estimate the mean <span class="math inline">\(\boldsymbol{\mu}\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> of <span class="math inline">\(q_\phi({\bf z}|{\bf x})\)</span></li>
<li>Let’s make it simpler and assume that the covariance matrix is diagonal, <span class="math inline">\(\boldsymbol{\Sigma}=\sigma^2 \textbf{I}_{d \times d}\)</span></li>
</ul>
<p>(Note: we don’t have to make this assumption, but it will make computation easier later on)</p>
</section>
<section id="vae-setup-so-far-1" class="slide level2">
<h2>VAE Setup so far</h2>
<p><strong>Decoder</strong>: An approximation of the true distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{x}|\textbf{z})\)</span></p>
<p><strong>Encoder</strong>: Predicts the mean and standard deviations of a distribution <span class="math inline">\(q_\phi({\bf z}|{\bf x})\)</span>, so that the distribution is close to the true distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{z}|\textbf{x})\)</span></p>
<p>We want our estimate distribution to be close to the true distribution. How do we measure the difference between distributions?</p>
</section>
<section id="discrete-entropy" class="slide level2">
<h2>(Discrete) Entropy</h2>
<p><span class="math display">\[H[X] = \sum_x p(X = x) \log \left(\frac{1}{p(X = x)}\right) = \text{E}\left[\log \frac{1}{p(X)}\right]\]</span></p>
<p>Many ways to think about this quantity:</p>
<ul>
<li>The expected number of yes/no questions you would need to ask to correctly predict the next symbol sampled from distribution <span class="math inline">\(p(X)\)</span></li>
</ul>
</section>
<section id="discrete-entropy-ii" class="slide level2">
<h2>(Discrete) Entropy II</h2>
<ul>
<li>The expected “surprise” or “information” in the possible outcomes of random variable <span class="math inline">\(X\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>The minimum number of bits required to compress a symbol <span class="math inline">\(x\)</span> sampled from distribution <span class="math inline">\(p(X)\)</span></li>
</ul>
</div>
</section>
<section id="discrete-entropy-of-a-coin-flip" class="slide level2">
<h2>(Discrete) Entropy of a Coin Flip</h2>
<div class="columns">
<div class="column" style="width:30%;">
<center>
<img data-src="./Binary_entropy_plot.svg" height="250">
</center>
</div><div class="column" style="width:70%;">
<ul>
<li>Entropy of a fair coin flip is <span class="math inline">\(0.5\log(2) + 0.5\log(2) = \log(2) = 1\)</span> bits</li>
<li>Entropy of a fair dice is <span class="math inline">\(\log(6) = 2.58\)</span> bits</li>
</ul>
</div></div>
<div class="fragment">
<ul>
<li>Entropy of characters in English words is about 2.62 bits</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Entropy of characters from the English alphabet selected uniformly at random is <span class="math inline">\(\log(26) = 4.7\)</span> bits</li>
</ul>
</div>
</section>
<section id="kullback-leibler-divergence" class="slide level2">
<h2>Kullback-Leibler Divergence</h2>
<p>Also called: KL Divergence, Relative Entropy</p>
<p>For discrete probability distributions:</p>
<p><span class="math display">\[D_\text{KL}(q(z) ~||~ p(z)) = \sum_z q(z) \log \left(\frac{q(z)}{p(z)}\right)\]</span></p>
<p>For continuous probability distributions:</p>
<p><span class="math display">\[D_\text{KL}(q(z) ~||~ p(z)) = \int q(z) \log \left(\frac{q(z)}{p(z)}\right)\, dz\]</span></p>
</section>
<section id="kl-divergence-example-computation" class="slide level2">
<h2>KL Divergence Example Computation</h2>
<p>Approximating an unfair coin with a fair coin.</p>
<ul>
<li><span class="math inline">\(p(z = 1) = 0.7\)</span> and <span class="math inline">\(p(z = 0) = 0.3\)</span></li>
</ul>
<div class="fragment">
<ul>
<li><span class="math inline">\(q(z = 1) = q(z = 0) = 0.5\)</span></li>
</ul>
</div>
</section>
<section id="kl-divergence-example-computation-ii" class="slide level2">
<h2>KL Divergence Example Computation II</h2>
<p><span class="math display">\[\begin{align*}
D_\text{KL}(q(z) ~||~ p(z)) &amp;= \sum_z q(z) \log \left(\frac{q(z)}{p(z)}\right) \\
&amp;= q(0) \log \left(\frac{q(0)}{p(0)}\right) +  q(1) \log \left(\frac{q(1)}{p(1)}\right) \\
&amp;= 0.5 \log \left(\frac{0.5}{0.3}\right) +  0.5 \log \left(\frac{0.5}{0.7}\right) \\
&amp;= 0.872
\end{align*}\]</span></p>
</section>
<section id="kl-divergence-is-not-symmetric" class="slide level2">
<h2>KL Divergence is not Symmetric!</h2>
<p>Approximating a fair coin with an unfair coin.</p>
<ul>
<li><span class="math inline">\(p(z = 1) = 0.7\)</span> and <span class="math inline">\(p(z = 0) = 0.3\)</span></li>
</ul>
<div class="fragment">
<ul>
<li><span class="math inline">\(q(z = 1) = q(z = 0) = 0.5\)</span></li>
</ul>
</div>
</section>
<section id="kl-divergence-is-not-symmetric-ii" class="slide level2">
<h2>KL Divergence is not Symmetric! II</h2>
<p><span class="math display">\[\begin{align*}
D_\text{KL}(p(z) ~||~ q(z)) &amp;= \sum_z p(z) \log \left(\frac{p(z)}{q(z)}\right) \\
&amp;= p(0) \log \left(\frac{p(0)}{q(0)}\right) +  p(1) \log \left(\frac{p(1)}{q(1)}\right) \\
&amp;= 0.3 \log \left(\frac{0.3}{0.5}\right) +  0.7 \log \left(\frac{0.7}{0.5}\right) \\
&amp;= 0.823 \\
&amp;\neq D_\text{KL}(q(z) ~||~ p(z))
\end{align*}\]</span></p>
</section>
<section id="minimizing-kl-divergence" class="slide level2">
<h2>Minimizing KL Divergence</h2>
<center>
<img data-src="./KL-inclusive-exclusive.png" height="500">
</center>
</section>
<section id="kl-divergence-properties" class="slide level2">
<h2>KL Divergence Properties</h2>
<p>The KL divergence is a measure of the difference between probability distributions.</p>
<p>KL divergence is an asymmetric, nonnegative measure, not a norm. It doesn’t obey the triangle inequality.</p>
<p>KL divergence is always positive. Hint: you can show this using the inequality <span class="math inline">\(\ln(x) \leq x - 1\)</span> for <span class="math inline">\(x &gt; 0\)</span>.</p>
</section>
<section id="kl-divergence-continuous-example" class="slide level2">
<h2>KL Divergence: Continuous Example</h2>
<p>Suppose we have two Gaussian distributions <span class="math inline">\(p(x) \sim N\left(\mu_1, \sigma_1^2\right)\)</span> and <span class="math inline">\(q(x) \sim N\left(\mu_2, \sigma_2^2\right)\)</span>.</p>
<p>What is the KL divergence <span class="math inline">\(D_\text{KL}(p(z) ~||~ q(z))\)</span>?</p>
<p>Recall:</p>
<p><span class="math display">\[p\left(z; \mu_1, \sigma_1^2\right) = \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(z - \mu_1)^2}{2\sigma_1^2}}\]</span></p>
<p><span class="math display">\[\log \left(p\left(z; \mu_1, \sigma_1^2\right)\right) = - \log \sqrt{2 \pi \sigma_1^2} - \frac{(z - \mu_1)^2}{2\sigma_1^2}\]</span></p>
</section>
<section id="kl-divergence-entropy-and-cross-entropy" class="slide level2">
<h2>KL Divergence: Entropy and Cross-Entropy</h2>
<p>We can split the KL divergence into two terms, which we can compute separately:</p>
<p><span class="math display">\[\begin{align*}
D_\text{KL}(p(z) ~||~ q(z)) &amp;= \int p(z) \log \frac{p(z)}{q(z)} dz \\
    &amp;= \int p(z) (\log p(z) - \log q(z)) dz \\
    &amp;= \int p(z) \log p(z) dz - \int p(z) \log q(z) dz \\
    &amp;= -\text{entropy} - \text{cross-entropy}
\end{align*}\]</span></p>
</section>
<section id="kl-divergence-continuous-example-entropy-computation" class="slide level2">
<h2>KL Divergence: Continuous Example, Entropy Computation</h2>
<p><span class="math display">\[\begin{align*}
\int p(z) \log\left(p(z)\right)\, dz \\
&amp;\hspace{-24pt}= \int p(z) \left(-\log\left(\sqrt{2 \pi \sigma_1^2}\right) - \frac{(z - \mu_1)^2}{2\sigma_1^2}\right)\, dz \\
&amp;\hspace{-24pt}=  - \int p(z) \frac{1}{2}\log\left(2 \pi \sigma_1^2\right)\, dz - \int p(z) \frac{(z - \mu_1)^2}{2\sigma_1^2}\, dz \\
&amp;\hspace{-24pt}= \ldots
\end{align*}\]</span></p>
</section>
<section id="kl-divergence-continuous-example-entropy-computation-ii" class="slide level2">
<h2>KL Divergence: Continuous Example, Entropy Computation II</h2>
<p><span class="math display">\[\begin{align*}
\ldots &amp;= -\frac{1}{2}\log\left(2 \pi \sigma_1^2\right) \int p(z)\, dz - \frac{1}{2\sigma_1^2}\int p(z) (z - \mu_1)^2\, dz \\
&amp;= -\frac{1}{2}\log\left(2 \pi \sigma_1^2\right) - \frac{1}{2} \\
&amp;= -\frac{1}{2}\log\left(\sigma_1^2\right) - \frac{1}{2}\log (2 \pi) - \frac{1}{2}
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(\displaystyle \int p(z)\, dz = 1\)</span> and <span class="math inline">\(\displaystyle\int p(z) (z - \mu_1)^2\, dz = \sigma_1^2\)</span></p>
</section>
<section id="kl-divergence-continuous-example-cross-entropy-computation" class="slide level2">
<h2>KL Divergence: Continuous Example, Cross-Entropy Computation</h2>
<p><span class="math display">\[\begin{align*}
\int p(z) \log\left(q(z)\right)\, dz \\
&amp;\hspace{-36pt}= \int p(z) \left(-\log\left(\sqrt{2 \pi \sigma_2^2}\right) - \frac{(z - \mu_2)^2}{2\sigma_2^2}\right)\, dz \\
&amp;\hspace{-36pt}= -\int p(z) \frac{1}{2}\log (2 \pi \sigma_2^2)\, dz - \int p(z) \frac{(z - \mu_2)^2}{2\sigma_2^2}\, dz \\
&amp;\hspace{-36pt}= -\frac{1}{2}\log (2 \pi \sigma_2^2) - \frac{1}{2\sigma_2^2}\int p(z) (z - \mu_2)^2\, dz =  \ldots
\end{align*}\]</span></p>
</section>
<section id="kl-divergence-continuous-example-cross-entropy-computation-ii" class="slide level2">
<h2>KL Divergence: Continuous Example, Cross-Entropy Computation II</h2>
<p><span class="math display">\[\ldots = - \frac{1}{2}\log (2 \pi \sigma_2^2) - \frac{\sigma_1^2 + (\mu_1-\mu_2)^2}{2\sigma_2^2}\]</span></p>
</section>
<section id="back-to-autoencoders-summary-so-far" class="slide level2">
<h2>Back to Autoencoders: Summary so far</h2>
<p>Autoencoder:</p>
<ul>
<li>Decoder: point estimate of <span class="math inline">\(p_{\theta^{*}}(\textbf{x} | \textbf{z})\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>Encoder: point estimate of the value of <span class="math inline">\(\textbf{z}\)</span> that generated the image <span class="math inline">\(\textbf{x}\)</span></li>
</ul>
</div>
</section>
<section id="back-to-autoencoders-summary-so-far-ii" class="slide level2">
<h2>Back to Autoencoders: Summary so far II</h2>
<p>VAE:</p>
<ul>
<li>Decoder: probabilistic estimate of <span class="math inline">\(p_{\theta^{*}}(\textbf{x} | \textbf{z})\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>Encoder: probabilistic estimate of a Gaussian distribution <span class="math inline">\(q_{\phi}(\textbf{z} | \textbf{x})\)</span> that approximates the distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{z} | \textbf{x})\)</span>
<ul>
<li>In particular, our encoder will be a neural network that predicts the mean and standard deviation of <span class="math inline">\(q_{\phi}(\textbf{z} | \textbf{x})\)</span></li>
<li>We can then sample <span class="math inline">\({\bf z}\)</span> from this distribution!</li>
</ul></li>
</ul>
<!--
    - This uses the **reparamaterization trick**
    - Alternative is to sample $z ~ q_{\phi}(\textbf{z} | \textbf{x})$ directly.
      Doable, but gradient estimator will have high variance
-->
</div>
</section>
<section id="vae-objective" class="slide level2">
<h2>VAE Objective</h2>
<p>But how do we train a VAE?</p>
<p>We want to maximize the likelihood of our data:</p>
<p><span class="math display">\[\displaystyle \log(p(x)) = \log\left(\int p(x|z)p(x)\, dz\right)\]</span></p>
<p>And we want to make sure that the distributions <span class="math inline">\(q(z|x)\)</span> and <span class="math inline">\(p(z|x)\)</span> are close:</p>
<ul>
<li>We want to minimize <span class="math inline">\(D_\text{KL}(q({\bf z}|{\bf x}) ~||~ p({\bf z} | {\bf x}))\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>This is a measure of encoder quality</li>
</ul>
</div>
</section>
<section id="vae-objective-ii" class="slide level2">
<h2>VAE Objective II</h2>
<p>In other words, we want to maximize</p>
<p><span class="math display">\[-D_\text{KL}(q({\bf z}|{\bf x}) ~||~ p({\bf z} | {\bf x})) + \log(p(x))\]</span></p>
<p>How can we optimize this quantity in a tractable way?</p>
</section>
<section id="vae-evidence-lower-bound" class="slide level2">
<h2>VAE: Evidence Lower-Bound</h2>
<p><span class="math display">\[\begin{align*}
D_\text{KL}(q({\bf z}|{\bf x}) ~||~ p({\bf z} | {\bf x})) &amp;= \int q({\bf z}|{\bf x}) \log\left(\frac{q({\bf z}|{\bf x})}{p({\bf z}|{\bf x})}\right)\, dz \\
&amp;= \text{E}_q\left(\log\left(\frac{q({\bf z}|{\bf x})}{p({\bf z}|{\bf x})}\right)\right) \\
&amp;= \text{E}_q (\log (q({\bf z}|{\bf x}))) - \text{E}_q(\log(p({\bf z}|{\bf x}))) \\
&amp;= \text{E}_q(\log(q({\bf z}|{\bf x}))) - \text{E}_q(\log(p({\bf z},{\bf x}))) \\
&amp;\hspace{12pt} + \text{E}_q(\log(p({\bf x}))) \\
&amp;= \text{E}_q(\log(q({\bf z}|{\bf x}))) - \text{E}_q(\log(p({\bf z},{\bf x}))) \\
&amp;\hspace{12pt} + \log p({\bf x})
\end{align*}\]</span></p>
</section>
<section id="vae-evidence-lower-bound-ii" class="slide level2">
<h2>VAE: Evidence Lower-Bound II</h2>
<p>We’ll define the <strong>evidence lower-bound</strong>: <span class="math display">\[\text{ELBO}_q({\bf x}) = \text{E}_q(\log(p({\bf z},{\bf x})) - \log(q({\bf z}|{\bf x})))\]</span></p>
<p>So we have <span class="math display">\[\log(p({\bf x})) - D_\text{KL}(q({\bf z}|{\bf x}) ~||~ p({\bf z} | {\bf x})) = \text{ELBO}_q({\bf x})\]</span></p>
</section>
<section id="optimizing-the-elbo" class="slide level2">
<h2>Optimizing the ELBO</h2>
<p>The ELBO gives us a way to estimate the gradients of <span class="math inline">\(\log(p({\bf x})) - D_\text{KL}(q({\bf z}|{\bf x}) ~||~ p({\bf z} | {\bf x}))\)</span></p>
<p>How?</p>
<p><span class="math display">\[\text{ELBO}_q({\bf x}) = \text{E}_q(\log(p({\bf z},{\bf x})) - \log(q({\bf z}|{\bf x})))\]</span></p>
<ul>
<li>The right hand side of this expression is an expectation over <span class="math inline">\(z \sim q(z|x)\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>To estimate the ELBO, we can <strong>sample</strong> from the distribution <span class="math inline">\(z \sim q(z|x)\)</span>, and compute the terms inside.</li>
</ul>
</div>
</section>
<section id="optimizing-the-elbo-ii" class="slide level2">
<h2>Optimizing the ELBO II</h2>
<ul>
<li>We can estimate gradients in the same way—this is called a <strong>Monte-Carlo gradient estimator</strong></li>
</ul>
</section>
<section id="monte-carlo-estimation" class="slide level2">
<h2>Monte Carlo Estimation</h2>
<p>(This notation is unrelated to other slides: <span class="math inline">\(p(z)\)</span> is just a univariate Gaussian distribution, and <span class="math inline">\(f_\phi(z)\)</span> is a function parameterized by <span class="math inline">\(\phi\)</span>)</p>
<p>Suppose we want to optimize an objective <span class="math inline">\(\mathcal{L}(\phi) = \text{E}_{z \sim p(z)}(f_\phi(z))\)</span> where <span class="math inline">\(p(z)\)</span> is a normal distribution.</p>
<p>We can <strong>estimate</strong> <span class="math inline">\(\mathcal{L}(\phi)\)</span> by sampling <span class="math inline">\(z_i \sim p(z)\)</span> and computing</p>
<p><span class="math display">\[\mathcal{L}(\phi) = \text{E}_{z \sim p(z)}(f_\phi(z)) = \int_z p(z)f_\phi(z)\, dz \approx \frac{1}{N} \sum_{i=1}^N f_\phi(z_i)\]</span></p>
</section>
<section id="monte-carlo-gradient-estimation" class="slide level2">
<h2>Monte Carlo Gradient Estimation</h2>
<p>Likewise, if we want to estimate <span class="math inline">\(\nabla_\phi \mathcal{L}\)</span>, we can sample <span class="math inline">\(z_i \sim p(z)\)</span> and compute</p>
<p><span class="math display">\[\begin{align*}
\nabla_\phi \mathcal{L} &amp;= \nabla_\phi \text{E}_{z \sim p(z)}(f_\phi(z)) \\
                        &amp;= \nabla_\phi \int_z p(z)f_\phi(z)\, dz \\
                        &amp;\approx \nabla_\phi  \frac{1}{N} \sum_{i=1}^N f_\phi(z_i) \\
                        &amp;= \frac{1}{N} \sum_{i=1}^N \nabla_\phi  f_\phi(z_i) \\
\end{align*}\]</span></p>
</section>
<section id="the-reparamaterization-trick" class="slide level2">
<h2>The Reparamaterization Trick</h2>
<p><span class="math inline">\(\text{ELBO}_{\theta,\phi}(\textbf{x}) = \text{E}_{q_{\phi}}(\log(p_{\theta}(\textbf{z}, \textbf{x})) - \log(q_{\phi}(\textbf{z}|\textbf{x})))\)</span></p>
<p>Problem: typical Monte-Carlo gradient estimator with samples <span class="math inline">\(\textbf{z} \sim q_{\phi}(\textbf{z}|\textbf{x})\)</span> has very high variance.</p>
<p>Reparameterization trick: instead of sampling <span class="math inline">\(\textbf{z} \sim q_{\phi}(\textbf{z}|\textbf{x})\)</span> express <span class="math inline">\(\textbf{z}=g_{\phi}(\epsilon, \textbf{x})\)</span> where <span class="math inline">\(g\)</span> is deterministic and only <span class="math inline">\(\epsilon\)</span> is stochastic.</p>
</section>
<section id="the-reparamaterization-trick-ii" class="slide level2">
<h2>The Reparamaterization Trick II</h2>
<p>In practise, the reparameterization trick is what makes the VAE encoder deterministic. When running a VAE forward pass:</p>
<ol type="1">
<li>We get the means and standard deviations from the VAE</li>
<li>We sample from <span class="math inline">\(\mathcal{N}({\bf 0}, {\bf I})\)</span></li>
<li>We use the samples from step 2 to get a sample from <span class="math inline">\(q(z)\)</span> obtained from step 1</li>
</ol>
</section>
<section id="vae-summary-so-far" class="slide level2">
<h2>VAE: Summary so far</h2>
<p><strong>Decoder</strong>: estimate of <span class="math inline">\(p_{\theta^{*}}(\textbf{x} | \textbf{z})\)</span>.</p>
<p><strong>Encoder</strong>: estimate of a Gaussian distribution <span class="math inline">\(q_{\phi}(\textbf{z} | \textbf{x})\)</span> that approximates the distribution <span class="math inline">\(p_{\theta^{*}}(\textbf{z} | \textbf{x})\)</span>.</p>
<ul>
<li>Encoder is a NN that predicts the mean and standard deviation of <span class="math inline">\(q_{\phi}(\textbf{z} | \textbf{x})\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>Use the <strong>reparameterization trick</strong> to sample from this distribution</li>
</ul>
</div>
</section>
<section id="vae-summary-so-far-ii" class="slide level2">
<h2>VAE: Summary so far II</h2>
<p>The VAE objective is equal to the evidence lower-bound:</p>
<p><span class="math display">\[\log(p({\bf x})) - D_\text{KL}(q({\bf z}|{\bf x}) ~||~ p({\bf z} | {\bf x})) = \text{ELBO}_q({\bf x})\]</span></p>
<p>Which we can estimate using Monte Carlo</p>
<p><span class="math display">\[\text{ELBO}_q({\bf x}) = \text{E}_q (\log(p({\bf z},{\bf x})) - \log(q({\bf z}|{\bf x})))\]</span></p>
</section>
<section id="vae-summary-so-far-iii" class="slide level2">
<h2>VAE: Summary so far III</h2>
<p>But given a value <span class="math inline">\(z \sim q(z|x)\)</span>, how can we compute</p>
<p><span class="math display">\[\log p({\bf z},{\bf x}) - \log q({\bf z}|{\bf x})\]</span></p>
<p>…or its derivative with respect to the neural network parameters?</p>
<p>We need to do some more math to write this quantity in a form that is easier to estimate.</p>
</section>
<section id="vae-a-simpler-form" class="slide level2">
<h2>VAE: a Simpler Form</h2>
<p><span class="math display">\[\begin{aligned}
    \text{ELBO}_{\theta,\phi}(\textbf{x}) &amp;= \text{E}_{q_{\phi}}(\log(p_{\theta}(\textbf{z}, \textbf{x})) - \log(q_{\phi}(\textbf{z}|\textbf{x}))) \\
    &amp;= \text{E}_{q_{\phi}}(\log(p_{\theta}(\textbf{x} | \textbf{z})) + \log(p_{\theta}(\textbf{z})) - \log(q_{\phi}(\textbf{z}|\textbf{x}))) \\
    &amp;= \text{E}_{q_{\phi}}(\log(p_{\theta}(\textbf{x} | \textbf{z}))) - \text{E}_{q_{\phi}}(\log(p_{\theta}(\textbf{z})) + \log(q_{\phi}(\textbf{z}|\textbf{x}))) \\
    &amp;= \text{E}_{q_{\phi}}(\log(p_{\theta}(\textbf{x} | \textbf{z}))) - D_\text{KL}(q_{\phi}(\textbf{z}|\textbf{x}) ~||~ p_{\theta}(\textbf{z})) \\
    &amp;= \text{decoding quality} - \text{encoding regularization}
\end{aligned}\]</span></p>
<p>Both terms can be computed easily if we make some simplifying assumptions</p>
<p>Let’s see how…</p>
</section>
<section id="computing-decoding-quality" class="slide level2">
<h2>Computing Decoding Quality</h2>
<p>In order to estimate this quantity</p>
<p><span class="math display">\[\text{E}_{q_{\phi}}(\log(p_{\theta}(\textbf{x} | \textbf{z})))\]</span></p>
<p>…we need to make some assumptions about the distribution <span class="math inline">\(p_{\theta}(\textbf{x} | \textbf{z})\)</span>.</p>
</section>
<section id="computing-decoding-quality-ii" class="slide level2">
<h2>Computing Decoding Quality II</h2>
<p>If we make the assumption that <span class="math inline">\(p_{\theta}(\textbf{x} | \textbf{z})\)</span> is a normal distribution centered around some pixel intensity, then optimizing <span class="math inline">\(p_{\theta}(\textbf{x} | \textbf{z})\)</span> is equivalent to optimizing the <em>square loss</em>!</p>
<p>That is, <span class="math inline">\(p_{\theta}(\textbf{x} | \textbf{z})\)</span> tells us how intense a pixel could be, but that pixel could be a bit darker/lighter, following a normal distribution.</p>
<p><strong>Bonus: A traditional autoencoder is optimizing this same quantity!</strong></p>
</section>
<section id="computing-encoding-quality" class="slide level2">
<h2>Computing Encoding Quality</h2>
<p>This KL divergence computes the difference in distribution between two distributions:</p>
<p><span class="math display">\[D_\text{KL}(q_{\phi}(\textbf{z}|\textbf{x})~||~p_{\theta}(\textbf{z}))\]</span></p>
<ul>
<li><span class="math inline">\(q_{\phi}(\textbf{z}|\textbf{x})\)</span> is a normal distribution that approximates <span class="math inline">\(p_\theta(\textbf{z}|\textbf{x})\)</span></li>
</ul>
<div class="fragment">
<ul>
<li><span class="math inline">\(p_{\theta}(\textbf{z})\)</span> is the <strong>prior distribution on</strong> <span class="math inline">\({\bf z}\)</span>
<ul>
<li>distribution of <span class="math inline">\(z\)</span> when we don’t know anything about <span class="math inline">\({\bf x}\)</span> or any other quantity</li>
</ul></li>
</ul>
</div>
</section>
<section id="computing-encoding-quality-ii" class="slide level2">
<h2>Computing Encoding Quality II</h2>
<p>Since <span class="math inline">\({\bf z}\)</span> is a <em>latent</em> variable, not actually observed in the real word, we can choose <span class="math inline">\(p_{\theta}(\textbf{z})\)</span></p>
<ul>
<li>we choose <span class="math inline">\(p_\theta (z) = \mathcal{N}({\bf 0}, {\bf I})\)</span></li>
</ul>
<p>…and we know how to compute the KL divergence of two Gaussian distributions!</p>
</section>
<section id="interpretation" class="slide level2">
<h2>Interpretation</h2>
<p>The VAE objective</p>
<p><span class="math display">\[\text{E}_{q_{\phi}}(\log(p_{\theta}(\textbf{x} | \textbf{z}))) - D_\text{KL}(q_{\phi}(\textbf{z}|\textbf{x}) ~||~ p_{\theta}(\textbf{z}))\]</span></p>
<p>has an extra regularization term that the traditional autoencoder does not.</p>
<p>This extra regularization term pushes the values of <span class="math inline">\({\bf z}\)</span> to be closer to <span class="math inline">\(0\)</span>.</p>
</section>
<section id="mnist-results" class="slide level2">
<h2>MNIST results</h2>
<center>
<img data-src="imgs/aevb.png">
</center>
</section>
<section id="frey-faces-results" class="slide level2">
<h2>Frey Faces results</h2>
<center>
<img data-src="imgs/frey.png">
</center>
</section>
<section id="dimension-of-latent-variables" class="slide level2">
<h2>Dimension of latent variables</h2>
<center>
<img data-src="imgs/highd_latent.png">
</center>
<!-- 02mog -->
</section></section>
<section>
<section id="mixture-of-gaussians" class="title-slide slide level1 center">
<h1>Mixture of Gaussians</h1>

</section>
<section id="variational-inference" class="slide level2">
<h2>Variational Inference</h2>
<p>Variational inference is used in other areas… (TODO)</p>
</section>
<section id="an-example-data-from-mixture-of-gaussians" class="slide level2">
<h2>An example: Data from Mixture of Gaussians</h2>
<ul>
<li>K mixture components, corresponding to normal distributions</li>
</ul>
<div class="fragment">
<ul>
<li>Means <span class="math inline">\(\boldsymbol{\mu}=\{\mu_1,...,\mu_K\} \sim \mathcal{N}(0, \sigma^2\boldsymbol{I})\)</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Mixture selection variable <span class="math inline">\(c_i \sim \text{Categorical}(1/K, ..., 1/K)\)</span></li>
</ul>
</div>
</section>
<section id="an-example-data-from-mixture-of-gaussians-ii" class="slide level2">
<h2>An example: Data from Mixture of Gaussians II</h2>
<ul>
<li>Joint model <span class="math inline">\(\displaystyle p(\boldsymbol{\mu}, c_{1:n}, x_{1:n})=p(\boldsymbol{\mu}) \prod_{i=1}^n p(c_i)p(x_i|c_i,\boldsymbol{\mu})\)</span></li>
</ul>
<div class="fragment">
<div class="columns">
<div class="column" style="width:40%;">
<center>
<img data-src="imgs/mog.jpg">
</center>
</div><div class="column" style="width:60%;">
<ul>
<li>Each <span class="math inline">\(c_i\)</span> has K options, and we have <span class="math inline">\(n\)</span> data points, so <span class="math inline">\(O(K^n)\)</span> to evaluate <span class="math inline">\(\displaystyle p(x_{1:n}) = \int p(\boldsymbol{\mu}, c_{1:n}, x_{1:n})\, d\boldsymbol{\mu}dc_{1:n}\)</span></li>
</ul>
</div></div>
</div>
</section>
<section id="an-example-data-from-mixture-of-gaussians-iii" class="slide level2">
<h2>An example: Data from Mixture of Gaussians III</h2>
<ul>
<li>Each <span class="math inline">\(c_i\)</span> has K options, and we have <span class="math inline">\(n\)</span> data points, so <span class="math inline">\(O(K^n)\)</span> to evaluate <span class="math inline">\(\displaystyle p(x_{1:n}) = \int p(\boldsymbol{\mu}, c_{1:n}, x_{1:n})\, d\boldsymbol{\mu}dc_{1:n}\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>Takeaway message: can’t use direct estimation of the evidence <span class="math inline">\(p(x_{1:n})\)</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>In this particular example we can use EM, but in general it assumes that you know <span class="math inline">\(p(\textbf{z}|\textbf{x})\)</span></li>
</ul>
</div>
</section>
<section id="evidence-lower-bound-elbo" class="slide level2">
<h2>Evidence Lower Bound (ELBO)</h2>
<p><span class="math display">\[\begin{aligned}
    D_\text{KL}(q(\textbf{z})~||~p(\textbf{z}|\textbf{x})) &amp;= \text{E}_{q}\left(\log\left(\frac{q(\textbf{z})}{p(\textbf{z} | \textbf{x})}\right)\right) \\
    &amp;= \text{E}_{q}(\log(q(\textbf{z}))) - \text{E}_{q}(\log(p(\textbf{z} | \textbf{x}))) \\
    &amp;= \text{E}_{q}(\log(q(\textbf{z}))) - \text{E}_{q}(\log(p(\textbf{z},\textbf{x}))) \\
    &amp;\hspace{12pt} + \text{E}_{q}(\log(p(\textbf{x}))) \\
    &amp;= \text{E}_{q}(\log(q(\textbf{z}))) - \text{E}_{q}(\log(p(\textbf{z},\textbf{x}))) \\
    &amp;\hspace{12pt} + \log(p(\textbf{x})) \\
    &amp;= -\text{ELBO}_{q}(\textbf{x}) + \log(p(\textbf{x}))
\end{aligned}\]</span></p>
</section>
<section id="evidence-lower-bound-elbo-ii" class="slide level2">
<h2>Evidence Lower Bound (ELBO) II</h2>
<p>Log-evidence:</p>
<p><span class="math display">\[\log(p(\textbf{x})) = D_\text{KL}(q(\textbf{z}) ~||~ p(\textbf{z} | \textbf{x})) + \text{ELBO}_q(\textbf{x})\]</span></p>
<p>Variational Inference <span class="math inline">\(\rightarrow\)</span> find <span class="math inline">\(q(\textbf{z})\)</span> that maximizes <span class="math inline">\(\text{ELBO}_q\)</span></p>
</section>
<section id="mean-field-approximation" class="slide level2">
<h2>Mean-Field Approximation</h2>
<ul>
<li>Simplification for posterior approximator <span class="math inline">\(q(\textbf{z})\)</span>:</li>
</ul>
<div class="fragment">
<ul>
<li><span class="math inline">\(\displaystyle q(\textbf{z}) = \prod_j q_j(z_j)\)</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>All latent variables <span class="math inline">\(z_j\)</span> are mutually independent</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Each is governed by its own distribution <span class="math inline">\(q_j\)</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>WHY? It makes the optimization easier (analytical gradients)</li>
</ul>
</div>
</section>
<section id="mean-field-approximation-ii" class="slide level2">
<h2>Mean-Field Approximation II</h2>
<ul>
<li>WHY NOT? It fails to model correlations among latent variables, and underestimates variance</li>
</ul>
<center>
<img data-src="imgs/mf_variance.png">
</center>
</section>
<section id="optimization-algorithms" class="slide level2">
<h2>Optimization algorithms</h2>
<ul>
<li>Algo <span class="math inline">\(\#1\)</span>: coordinate ascent along each latent variable of ELBO</li>
</ul>
<div class="fragment">
<ul>
<li>Main problem is that it evaluates ELBO on the entire dataset (not great for big data)</li>
</ul>
</div>
</section>
<section id="optimization-algorithms-ii" class="slide level2">
<h2>Optimization algorithms II</h2>
<ul>
<li>Also susceptible to local minima</li>
</ul>
<center>
<img data-src="imgs/cavi.png" height="450">
</center>
</section>
<section id="optimization-algorithms-iii" class="slide level2">
<h2>Optimization algorithms III</h2>
<ul>
<li>Algo <span class="math inline">\(\#2\)</span>: stochastic optimization over all latent variables</li>
</ul>
<div class="fragment">
<ul>
<li>Uses the natural gradient to account for manifold on which distributions live</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Evaluates ELBO on single data points, or minibatches</li>
</ul>


</div>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/utm-csc413\.github\.io\/2024F-website\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>