<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <title>CSC413 - Fall 2024, UTM – CSC413 Neural Networks and Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta property="og:description" content="Lecture 3">
<meta property="og:site_name" content="CSC413 - Fall 2024, UTM">
<meta name="twitter:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta name="twitter:description" content="Lecture 3">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC413 Neural Networks and Deep Learning</h1>
  <p class="subtitle">Lecture 3</p>

<div class="quarto-title-authors">
</div>

</section>
<section id="lecture-plan" class="slide level2">
<h2>Lecture Plan</h2>
<p>Last week:</p>
<ul>
<li>From linear models to <strong>multilayer perceptrons</strong></li>
</ul>
<div class="fragment">
<ul>
<li>Backpropagation to compute gradients efficiently</li>
</ul>
</div>
</section>
<section id="lecture-plan-ii" class="slide level2">
<h2>Lecture Plan II</h2>
<p>This week:</p>
<ul>
<li>First hour:
<ul>
<li>automatic differentiation</li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li>Second hour:
<ul>
<li>distributed representations</li>
<li>GloVe embeddings</li>
</ul></li>
</ul>
<p>Both will be helpful for Assignment 1</p>
<!-- 01ad -->
</div>
</section>
<section>
<section id="automatic-differentiation-autodiff" class="title-slide slide level1 center">
<h1>Automatic Differentiation (Autodiff)</h1>

</section>
<section id="derivatives-in-machine-learning" class="slide level2">
<h2>Derivatives in Machine Learning</h2>
<p>The machine learning approach requires the minimization of some cost/loss function, which is often done using some variation of <strong>gradient descent</strong>. <span class="math display">\[\theta \leftarrow \theta - \alpha\frac{\partial \mathcal{E}}{\partial \theta}\]</span></p>
</section>
<section id="derivatives-in-machine-learning-1" class="slide level2">
<h2>Derivatives in Machine Learning</h2>
<p><span class="math display">\[\theta \leftarrow \theta - \alpha\frac{\partial \mathcal{E}}{\partial \theta}\]</span></p>
<p>Approaches to computing derivatives:</p>
<ol type="1">
<li>Manually working out derivatives</li>
</ol>
<div class="fragment">
<ol start="2" type="1">
<li>Numeric differentiation (using finite difference approximations)</li>
</ol>
</div>
<div class="fragment">
<ol start="3" type="1">
<li>Symbolic differentiation (using expression manipulation)</li>
</ol>
</div>
</section>
<section id="derivatives-in-machine-learning-2" class="slide level2">
<h2>Derivatives in Machine Learning</h2>
<p><span class="math display">\[\theta \leftarrow \theta - \alpha\frac{\partial \mathcal{E}}{\partial \theta}\]</span></p>
<p>Approaches to computing derivatives:</p>
<ol start="2" type="1">
<li><p>Numeric differentiation (using finite difference approximations)</p></li>
<li><p>Symbolic differentiation (using expression manipulation)</p></li>
</ol>
<div class="fragment">
<ol start="4" type="1">
<li><strong>Automatic differentiation or algorithmic differentiation</strong></li>
</ol>
</div>
</section>
<section id="terminology" class="slide level2">
<h2>Terminology</h2>
<ul>
<li><strong>Automatic differentiation</strong>: convert the program into a sequence of primitive operations, which have specified routines for computing derivatives.Then, we can compute gradients in a mechanical way via the chain rule.
<ul>
<li>Also used in computational fluid dynamics, atmospheric sciences, etc.</li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li><strong>Backpropagation</strong>: special case of autodiff where the <em>program</em> is a neural network forward pass.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Autograd, JAX, PyTorch, TensorFlow are examples of particular implementations of autodiff, i.e.&nbsp;different libraries</li>
</ul>
</div>
</section>
<section id="backpropagation" class="slide level2">
<h2>Backpropagation</h2>
<p>Steps:</p>
<ul>
<li>Convert the computation into a sequence of <strong>primitive operations</strong>
<ul>
<li>Primitive operations have easily computed derivatives</li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li>Build the computation graph</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Perform a forward pass: compute the values of each node</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Perform the backward pass: compute the derivative of the loss with respect to each node</li>
</ul>
</div>
</section>
<section id="autodiff-more-generally" class="slide level2">
<h2>Autodiff, more generally</h2>
<p>We will discuss how an automatic differentiation library could be implemented</p>
<ul>
<li>build the computation graph</li>
</ul>
<div class="fragment">
<ul>
<li><strong>vector-Jacobian products (VJP)</strong> for primitive ops</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>perform the backward pass</li>
</ul>
<p>You will probably never have to implement autodiff yourself but it is good to know its inner workings!</p>
</div>
</section>
<section id="autodiff-more-generally-1" class="slide level2">
<h2>Autodiff, more generally</h2>
<p><strong>Key Insight</strong>: For any new deep learning model that we can come up with, if each step of our computation is differentiable, then we can train that model using gradient descent.</p>
<!--
## Converting to Primitive Operations

**Original Program**
$$y = \sigma(wx + b)$$

**Primitive Operations**
\begin{align*}
t_1 &= wx \\
z &= t_1 + b \\
t_3 &= -z \\
t_4 &= \text{exp}(t_3)\\
\dots
\end{align*}
-->
</section>
<section id="scalar-example" class="slide level2">
<h2>Scalar Example</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="kw">def</span> f(x):</span>
<span id="cb1-2"><a></a>    h <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb1-3"><a></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb1-4"><a></a>        h <span class="op">=</span> x <span class="op">*</span> <span class="fl">1.5</span> <span class="op">+</span> h</span>
<span id="cb1-5"><a></a>    <span class="cf">return</span> x <span class="op">*</span> h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Notation</strong>: <span class="math inline">\(x\)</span> is the input, <span class="math inline">\(y=f(x)\)</span> is the output, we want to compute <span class="math inline">\(\frac{dy}{dx}\)</span></p>
</section>
<section id="scalar-example-1" class="slide level2">
<h2>Scalar Example</h2>
<p><strong>Automatic Differentiation Steps</strong>:</p>
<ul>
<li>convert the computation into a sequence of <strong>primitive operations</strong>
<ul>
<li>we need to be able to compute derivatives for these primitive operations</li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li>build the computation graph</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>perform forward pass</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>perform backward pass</li>
</ul>
</div>
</section>
<section id="scalar-example-primitive-ops" class="slide level2">
<h2>Scalar Example: Primitive Ops</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="kw">def</span> f(x):</span>
<span id="cb2-2"><a></a>    h <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb2-3"><a></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb2-4"><a></a>        h <span class="op">=</span> x <span class="op">*</span> <span class="fl">1.5</span> <span class="op">+</span> h</span>
<span id="cb2-5"><a></a>    <span class="cf">return</span> x <span class="op">*</span> h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Operations:</p>
<div class="fragment">
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a>h0 <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb3-2"><a></a>z1 <span class="op">=</span> x <span class="op">*</span> <span class="fl">1.5</span></span>
<span id="cb3-3"><a></a>h1 <span class="op">=</span> z1 <span class="op">+</span> h0</span>
<span id="cb3-4"><a></a>z2 <span class="op">=</span> x <span class="op">*</span> <span class="fl">1.5</span></span>
<span id="cb3-5"><a></a>h2 <span class="op">=</span> z2 <span class="op">+</span> h1</span>
<span id="cb3-6"><a></a>z3 <span class="op">=</span> x <span class="op">*</span> <span class="fl">1.5</span></span>
<span id="cb3-7"><a></a>h3 <span class="op">=</span> z3 <span class="op">+</span> h2</span>
<span id="cb3-8"><a></a>y  <span class="op">=</span> x <span class="op">*</span> h3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="scalar-example-computation-graph" class="slide level2">
<h2>Scalar Example: Computation Graph</h2>
<p>Exercise: Draw the computation graph:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a>h0 <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb4-2"><a></a>z1 <span class="op">=</span> x <span class="op">*</span> <span class="fl">1.5</span></span>
<span id="cb4-3"><a></a>h1 <span class="op">=</span> z1 <span class="op">+</span> h0</span>
<span id="cb4-4"><a></a>z2 <span class="op">=</span> x <span class="op">*</span> <span class="fl">1.5</span></span>
<span id="cb4-5"><a></a>h2 <span class="op">=</span> z2 <span class="op">+</span> h1</span>
<span id="cb4-6"><a></a>z3 <span class="op">=</span> x <span class="op">*</span> <span class="fl">1.5</span></span>
<span id="cb4-7"><a></a>h3 <span class="op">=</span> z3 <span class="op">+</span> h2</span>
<span id="cb4-8"><a></a>y  <span class="op">=</span> x <span class="op">*</span> h3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Based on the computation graph, we can compute <span class="math inline">\(\frac{dy}{dx}\)</span> via a forward and a backward pass.</p>
</section>
<section id="vector-inputs-and-outputs" class="slide level2">
<h2>Vector Inputs and Outputs</h2>
<p>More generally, input/output to a computation may be <strong>vectors</strong></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="kw">def</span> f(a, w): <span class="co"># a and w are both vectors with size 10</span></span>
<span id="cb5-2"><a></a>    h <span class="op">=</span> a</span>
<span id="cb5-3"><a></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb5-4"><a></a>        h <span class="op">=</span> np.dot(w, h) <span class="op">+</span> h</span>
<span id="cb5-5"><a></a>    z <span class="op">=</span> w <span class="op">*</span> h <span class="co"># element wise multiplication</span></span>
<span id="cb5-6"><a></a>    <span class="cf">return</span> z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So we have <span class="math inline">\(\bf{y} = f(\bf{x})\)</span> (in this example, <span class="math inline">\(\bf{x}\)</span> consists of values in both <code>a</code> and <code>w</code>)</p>
<p><strong>Q</strong>: In our running example, what are the dimensions of <span class="math inline">\({\bf x}\)</span> and <span class="math inline">\({\bf y}\)</span>?</p>
</section>
<section id="the-jacobian-matrix" class="slide level2">
<h2>The Jacobian Matrix</h2>
<p>We wish to compute the gradients <span class="math inline">\(\frac{\partial y_k}{\partial x_i}\)</span> for each <span class="math inline">\(k\)</span> and <span class="math inline">\(i\)</span>, at some <span class="math inline">\(\bf{x}\)</span>.</p>
<p>In other words, we would like the to work with the <strong>Jacobian matrix</strong></p>
<p><span class="math display">\[\begin{align*}
J_f({\bf x}) &amp;= \begin{bmatrix}
\frac{\partial y_1}{\partial x_1}({\bf x}) &amp; \ldots &amp; \frac{\partial y_1}{\partial x_n}({\bf x}) \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_m}{\partial x_1}({\bf x}) &amp; \ldots &amp; \frac{\partial y_m}{\partial x_n}({\bf x})
\end{bmatrix}
\end{align*}\]</span></p>
</section>
<section id="the-jacobian-matrix-1" class="slide level2">
<h2>The Jacobian Matrix</h2>
<p><span class="math display">\[\begin{align*}
J_f({\bf x}) &amp;= \begin{bmatrix}
\frac{\partial y_1}{\partial x_1}({\bf x}) &amp; \ldots &amp; \frac{\partial y_1}{\partial x_n}({\bf x}) \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_m}{\partial x_1}({\bf x}) &amp; \ldots &amp; \frac{\partial y_m}{\partial x_n}({\bf x})
\end{bmatrix}
\end{align*}\]</span></p>
<p>Note that we usually want to avoid explicitly constructing the entries of this Jacobian one by one.</p>
<p>Why? Computing all the partial derivatives one by one is expensive, even with backprop.</p>
</section>
<section id="decomposing-into-primitive-operations" class="slide level2">
<h2>Decomposing Into Primitive Operations</h2>
<p>Suppose <span class="math inline">\(f = f_2 \circ f_1\)</span>, so we have the computations <span class="math inline">\({\bf y} = f_2 \circ f_1(\bf{x})\)</span>, or in other words: <span class="math display">\[\begin{align*}
{\bf z} &amp;= f_1(\bf{x}) \\
{\bf y} &amp;= f_2(\bf{z})
\end{align*}\]</span></p>
<p>If <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> are primitive operations with simple Jacobians, we can apply the <strong>Jacobian chain rule</strong>:</p>
<p><span class="math display">\[J_{f_2 \circ f_1}({\bf x}) = J_{f_2}({\bf z})J_{f_1}({\bf x})\]</span></p>
</section>
<section id="autodiff-more-generally-2" class="slide level2">
<h2>Autodiff, more generally</h2>
<p>This video explains the different ways to automatically compute derivatives:</p>
<p><a href="https://www.youtube.com/watch?v=wG_nF1awSSY" class="uri">https://www.youtube.com/watch?v=wG_nF1awSSY</a></p>
<ul>
<li>manual</li>
</ul>
<div class="fragment">
<ul>
<li>finite differences</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>symbolic differentiation</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>autodiff (forward-mode and reverse-mode differentiation)
<ul>
<li>how to avoid computing Jacobians one by one</li>
</ul></li>
</ul>
</div>
</section>
<section id="avoiding-jacobian-products" class="slide level2">
<h2>Avoiding Jacobian Products</h2>
<p>In practice, computing entries of Jacobians one by one is expensive and we try to avoid it:</p>
<ul>
<li>If the dimension of <span class="math inline">\({\bf y} = f({\bf x})\)</span> is small, use <strong>reverse-mode automatic differentiation</strong></li>
</ul>
<div class="fragment">
<ul>
<li>If the dimension of <span class="math inline">\({\bf x}\)</span> is small, use <strong>forward-mode automatic differentiation</strong></li>
</ul>
<p><strong>Q:</strong> Which of these two cases apply to deep learning most often?</p>
</div>
</section>
<section id="reverse-mode-automatic-differentiation" class="slide level2">
<h2>Reverse-Mode Automatic Differentiation</h2>
<p>Suppose <span class="math inline">\({\bf y}\)</span> is a scalar, and represents the loss <span class="math inline">\(\mathcal{L}\)</span> that we wish to minimize. <span class="math display">\[\begin{align*}
{\bf z} &amp;= f_1(\bf{x}) \\
\mathcal{L} &amp;= f_2(\bf{z}) = {\bf y} \in \mathbb{R}
\end{align*}\]</span></p>
<p>Then we have:</p>
<ul>
<li><span class="math inline">\(\overline{z} = \frac{\partial \mathcal{L}}{\partial z} = J_{f_2}(\bf{z})^\top\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>Since <span class="math inline">\(\overline{x_j} = \sum_i \overline{z_i} \frac{\partial z_i}{\partial x_j}\)</span></li>
</ul>
</div>
</section>
<section id="reverse-mode-automatic-differentiation-1" class="slide level2">
<h2>Reverse-Mode Automatic Differentiation</h2>
<ul>
<li>… we have <span class="math inline">\(\overline{\bf x}^\top = \overline{\bf z}^\top J_{f_1}(\bf{x})\)</span></li>
</ul>
<div class="fragment">
<ul>
<li>… which is a <strong>vector-Jacobian product</strong></li>
</ul>
</div>
<div class="fragment">
<p><strong>Summary:</strong> For each primitive operation, we don’t need to be able to compute entire Jacobian matrix. <strong>We need to be able to compute the vector-Jacobian product.</strong></p>
</div>
</section>
<section id="vector-jacobian-products" class="slide level2">
<h2>Vector Jacobian Products</h2>
<p>For each primitive operation, we must specify the VJPs for each of its arguments</p>
<p>The VJP function should takes in the output gradient (i.e.&nbsp;<span class="math inline">\({\bar y}\)</span>), the answer (<span class="math inline">\(y\)</span>), and the arguments (<span class="math inline">\(x\)</span>), and returns the input gradient (<span class="math inline">\({\bar x}\)</span>). Here are some examples from <a href="https://github.com/mattjj/autodidact/blob/master/autograd/numpy/numpy_vjps.py" class="uri">https://github.com/mattjj/autodidact/blob/master/autograd/numpy/numpy_vjps.py</a></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>defvjp(anp.negative, <span class="kw">lambda</span> g, ans, x: <span class="op">-</span>g)</span>
<span id="cb6-2"><a></a>defvjp(anp.exp,    <span class="kw">lambda</span> g, ans, x: ans <span class="op">*</span> g)</span>
<span id="cb6-3"><a></a>defvjp(anp.log,    <span class="kw">lambda</span> g, ans, x: g <span class="op">/</span> x)</span>
<span id="cb6-4"><a></a></span>
<span id="cb6-5"><a></a>defvjp(anp.add,         <span class="kw">lambda</span> g, ans, x, y : unbroadcast(x, g),</span>
<span id="cb6-6"><a></a>                        <span class="kw">lambda</span> g, ans, x, y : unbroadcast(y, g))</span>
<span id="cb6-7"><a></a>defvjp(anp.multiply,    <span class="kw">lambda</span> g, ans, x, y : unbroadcast(x, y <span class="op">*</span> g),</span>
<span id="cb6-8"><a></a>                        <span class="kw">lambda</span> g, ans, x, y : unbroadcast(y, x <span class="op">*</span> g))</span>
<span id="cb6-9"><a></a>defvjp(anp.subtract,    <span class="kw">lambda</span> g, ans, x, y : unbroadcast(x, g),</span>
<span id="cb6-10"><a></a>                        <span class="kw">lambda</span> g, ans, x, y : unbroadcast(y, <span class="op">-</span>g))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="backprop-as-message-passing" class="slide level2">
<h2>Backprop as Message Passing</h2>

<img data-src="imgs/message_passing.png" style="width:50.0%" class="r-stretch"><ul>
<li>Each node in the computation graph receives <strong>messages</strong> from its children, which it aggregates to compute its error signal</li>
</ul>
<div class="fragment">
<ul>
<li><strong>Messages</strong> then get passed to its parents</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Each message is a VJP</li>
</ul>
</div>
</section>
<section id="backprop-as-message-passing-1" class="slide level2">
<h2>Backprop as Message Passing</h2>
<p>This design provides <strong>modularity!</strong> Each node needs to know how to compute its outgoing messages, i.e.&nbsp;the VJPs corresponding to each of its parents (arguments to the function).</p>
</section>
<section id="differentiable-programming" class="slide level2">
<h2>Differentiable Programming</h2>
<p>Recall the <strong>key insight</strong> from earlier: For any new deep learning model that we can come up with, if each step of our computation is differentiable, then we can train that model using gradient descent.</p>
<p>Example: Learning to learning by gradient descent by gradient descent <a href="https://arxiv.org/pdf/1606.04474.pdf" class="uri">https://arxiv.org/pdf/1606.04474.pdf</a></p>
<p>With AD, any <em>program</em> that has differentiable components can be optimized via gradient descent</p>
<!-- 02lang -->
</section></section>
<section>
<section id="distributed-representations" class="title-slide slide level1 center">
<h1>Distributed Representations</h1>

</section>
<section id="feature-mapping" class="slide level2">
<h2>Feature Mapping</h2>
<ul>
<li>Learning good <em>representations</em> is an important goal in machine learning
<ul>
<li>These representations are also called <em>feature mappings</em>, or <em>embeddings</em></li>
<li>The representations we learn are often <strong>reusable</strong> for other tasks</li>
<li>Finding good representations is an <strong>unsupervised learning</strong> problem!</li>
</ul></li>
</ul>
</section>
<section id="feature-mapping-1" class="slide level2">
<h2>Feature Mapping</h2>
<ul>
<li>Assignment 1:
<ul>
<li>Learn vector representations of <em>words</em>: <strong>unsupervised learning</strong></li>
<li>Sometimes also referred to as <strong>self-supervised learning</strong></li>
</ul></li>
</ul>
</section>
<section id="language-modeling" class="slide level2">
<h2>Language Modeling</h2>
<p>A language model…</p>
<ul>
<li>Models the probability distribution of natural language text.</li>
</ul>
<div class="fragment">
<ul>
<li>Determine the <strong>probability</strong> <span class="math inline">\(p({\bf s})\)</span> that a sequence of words (or a <em>sentence</em>) <span class="math inline">\({\bf s}\)</span> occurs in text.</li>
</ul>
<p>A language model gives us a way to compute <span class="math inline">\(p({\bf s})\)</span></p>
</div>
</section>
<section id="why-language-models-pbf-s" class="slide level2">
<h2>Why Language Models <span class="math inline">\(p({\bf s})\)</span>?</h2>
<ul>
<li>Determine authorship:
<ul>
<li>build a language model <span class="math inline">\(p({\bf s})\)</span> of Shakespeare</li>
<li>determine whether a script is written by Shakespeare</li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li>Generate a machine learning paper (given a <em>corpus</em> of machine learning papers)</li>
</ul>
<!-- $_$ -->
</div>
</section>
<section id="why-language-models-pbf-s-1" class="slide level2">
<h2>Why language models <span class="math inline">\(p({\bf s})\)</span>?</h2>
<ul>
<li>Use as a <em>prior</em> for a speech recognition system <span class="math inline">\(p({\bf s} | {\bf a})\)</span>, where <span class="math inline">\({\bf a}\)</span> represents the observed speech signal.
<ul>
<li>An <strong>observation model</strong>, or likelihood, represented as <span class="math inline">\(p({\bf a} | {\bf s})\)</span>, which tells us how likely the sentence <span class="math inline">\({\bf s}\)</span> is to lead to the acoustic signal <span class="math inline">\({\bf a}\)</span>.</li>
<li>A <strong>prior</strong>, represented as <span class="math inline">\(p({\bf s})\)</span> which tells us how likely a given sentence <span class="math inline">\({\bf s}\)</span> is. For example, “recognize speech” is more likely than “wreck a nice beach”</li>
</ul></li>
</ul>
<!-- $_$ -->
</section>
<section id="why-language-models-pbf-s-2" class="slide level2">
<h2>Why language models <span class="math inline">\(p({\bf s})\)</span>?</h2>
<ul>
<li>Use as a <em>prior</em> for a speech recognition system <span class="math inline">\(p({\bf s} | {\bf a})\)</span>, where <span class="math inline">\({\bf a}\)</span> represents the observed speech signal.
<ul>
<li>Use Bayes rule to infer a <em>posterior distribution</em> over sentences given the speech signal: <span class="math display">\[p({\bf s} | {\bf a}) = \frac{p({\bf s}) p({\bf a} | {\bf s})}{\displaystyle \sum_{{\bf s^\prime}} p({\bf s^\prime})p({\bf a} | {\bf s^\prime})}\]</span></li>
</ul></li>
</ul>
<!-- $_$ -->
</section>
<section id="training-a-language-model" class="slide level2">
<h2>Training a Language Model</h2>
<p>Assume we have a corpus of sentences <span class="math inline">\({\bf s}^{(1)}, \ldots, {\bf s}^{(N)}\)</span></p>
<p>The <strong>maximum likelihood</strong> criterion says we want our model to maximize the probability that our model assigns to the observed sentences. We assume the sentences are independent, so that their probabilities multiply.</p>
</section>
<section id="training-a-language-model-1" class="slide level2">
<h2>Training a Language Model</h2>
<p>In maximum likelihood training, we want to maximize <span class="math display">\[\prod_{i=1}^N p\left({\bf s}^{(i)}\right)\]</span></p>
<!-- $_$ -->
<p>Or minimize: <span class="math display">\[-\sum_{i=1}^N \log p\left({\bf s}^{(i)}\right)\]</span></p>
<p>Since <span class="math inline">\(p({\bf s})\)</span> is usually small, <span class="math inline">\(-\log p({\bf s})\)</span> is reasonably sized, and positive.</p>
</section>
<section id="probability-of-a-sentence" class="slide level2">
<h2>Probability of a sentence</h2>
<p>A sentence is a sequence of words <span class="math inline">\(w_1, w_2, \ldots, w_T\)</span>, so <span class="math display">\[\begin{align*}
p({\bf s}) &amp;= p(w_1, w_2, \ldots, w_T) \\
           &amp;= p(w_1)p(w_2 | w_1) \ldots p(w_T | w_1, w_2, \ldots, w_{T-1})
\end{align*}\]</span> We can make a simplifying <strong>Markov assumption</strong> that the distribution over the next word depends on the preceding few words.</p>
</section>
<section id="probability-of-a-sentence-1" class="slide level2">
<h2>Probability of a sentence</h2>
<p>In assignment 1, we use a context length of 3 and model: <span class="math display">\[\begin{align*}
p(w_t | w_1, w_2, \ldots, w_{t-1}) = p(w_t | w_{t-3}, w_{t-2}, w_{t-1})
\end{align*}\]</span></p>
<p>This is a self-supervised learning problem!</p>
</section>
<section id="n-gram-language-model" class="slide level2">
<h2>N-Gram Language Model</h2>
<p>A simple way of modeling <span class="math inline">\(p(w_t | w_{t-2}, w_{t-1})\)</span> is by constructing a table of conditional probabilities:</p>
<p>Where the probabilities come from the <strong>empirical distribution</strong>: <span class="math display">\[p(w_3 = {\rm cat} | w_1 = {\rm the}, w_2 = {\rm fat}) = \frac{{\rm count}({\rm the\ fat\ cat})}{{\rm count}({\rm the\ fat})}\]</span></p>
<p>The phrases we’re counting are called <em>n-grams</em> (where n is the length), so this is an <strong>n-gram language model</strong>. (Note: the above example is considered a 3-gram model, not a 2-gram model!)</p>
</section>
<section id="example-shakespeare-n-gram-language-model" class="slide level2">
<h2>Example: Shakespeare N-Gram Language Model</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/shakespeare.png"></p>
<figcaption>From <a href="https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf" class="uri">https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf</a></figcaption>
</figure>
</div>
</center>
</section>
<section id="problems-with-n-gram-language-model" class="slide level2">
<h2>Problems with N-Gram Language Model</h2>
<ul>
<li>The number of entries in the conditional probability table is exponential in the context length.</li>
</ul>
<div class="fragment">
<ul>
<li><strong>Data sparsity</strong>: most n-grams never appear in the corpus, even if they are possible.</li>
</ul>
</div>
</section>
<section id="problems-with-n-gram-language-model-1" class="slide level2">
<h2>Problems with N-Gram Language Model</h2>
<p>Ways to deal with data sparsity:</p>
<ul>
<li>Use a short context (but this means the model is less powerful),</li>
</ul>
<div class="fragment">
<ul>
<li>Smooth the probabilities, e.g.&nbsp;by adding imaginary counts,</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Make predictions using an ensemble of n-gram models with different <span class="math inline">\(n\)</span>s.</li>
</ul>
</div>
</section>
<section id="local-vs-distributed-representations" class="slide level2">
<h2>Local vs Distributed Representations</h2>
<p>Conditional probability tables are a kind of <strong>local representation</strong>: all the information about a particular word is stored in one place: a column of the table.</p>
<p>But different words are related, so we ought to be able to share information between them.</p>
</section>
<section id="local-vs-distributed-representations-1" class="slide level2">
<h2>Local vs Distributed Representations</h2>
<center>
<img data-src="imgs/distributed_cartoon.png" height="500">
</center>
</section>
<section id="distributed-representations-word-attributes" class="slide level2">
<h2>Distributed Representations: Word Attributes</h2>
<p>Idea:</p>
<ol type="1">
<li>use the <strong>word attributes</strong> to predict the next word.</li>
<li>learn the <strong>word attributes</strong> using an MLP with backpropagation</li>
</ol>
</section>
<section id="sharing-information" class="slide level2">
<h2>Sharing Information</h2>
<p>Distributed representations allows us to share information between related words. E.g., suppose we’ve seen the sentence</p>
<blockquote>
<p>The cat got squashed in the garden on Friday.</p>
</blockquote>
<p>This should help us predict the words in the sentence</p>
<blockquote>
<p>The dog got flattened in the yard on (???)</p>
</blockquote>
<p>An n-gram model can’t generalize this way, but a distributed representation might let us do so.</p>
</section>
<section id="neural-language-model" class="slide level2">
<h2>Neural Language Model</h2>
<center>
<img data-src="imgs/model2.png" height="450">
</center>
</section>
<section id="word-representations" class="slide level2">
<h2>Word Representations</h2>
<p>Since we are using one-hot encodings for the words, the weight matrix of the word embedding layer acts like a lookup table.</p>
<center>
<img data-src="imgs/lookup_as_linear.png" height="350">
</center>
</section>
<section id="word-representations-1" class="slide level2">
<h2>Word Representations</h2>
<p>Terminology:</p>
<ul>
<li>“Embedding” emphasizes that it’s a location in a high-dimensional space; words that are closer together are more semantically similar.</li>
<li>“Feature vector” emphasizes that it’s a vector that can be used for making predictions, just like other feature mappings we’ve looked at (e.g.&nbsp;polynomials).</li>
</ul>
</section>
<section id="what-do-word-embeddings-look-like" class="slide level2">
<h2>What do Word Embeddings look like?</h2>
<p>It’s hard to visualize an <span class="math inline">\(n\)</span>-dimensional space, but there are algorithms for mapping the embeddings to two dimensions.</p>
<div class="columns">
<div class="column" style="width:50%;">
<center>
<img data-src="imgs/p2.png">
</center>
</div><div class="column" style="width:50%;">
<p>In assignment 1, we use algorithm called tSNE, which tries to make distances in the 2-D embedding match the original high-dimensional distances as closely as possible.</p>
</div></div>
</section>
<section id="a-note-about-these-visualizations" class="slide level2">
<h2>A note about these visualizations</h2>
<ul>
<li>Thinking about high-dimensional embeddings
<ul>
<li>Most vectors are nearly orthogonal (i.e.&nbsp;dot product is close to 0)</li>
<li>Most points are far away from each other</li>
<li>“In a 30-dimensional grocery store, anchovies can be next to fish and next to pizza toppings” - Geoff Hinton</li>
</ul></li>
</ul>
</section>
<section id="a-note-about-these-visualizations-1" class="slide level2">
<h2>A note about these visualizations</h2>
<ul>
<li>The 2D embeddings might be fairly misleading, since they can’t preserve the distance relationship from a higher-dimensional embedding. (Unrelated words might be close together in 2D but far apart in 3D)</li>
</ul>
</section></section>
<section>
<section id="glove-embeddings" class="title-slide slide level1 center">
<h1>GloVe Embeddings</h1>

</section>
<section id="glove" class="slide level2">
<h2>GloVe</h2>
<ul>
<li>Fitting language models is really hard
<ul>
<li>It’s really important to make good predictions about relative probabilities of rare words</li>
<li>Computing the predictive distribution requires a large softmax</li>
</ul></li>
</ul>
</section>
<section id="glove-1" class="slide level2">
<h2>GloVe</h2>
<ul>
<li>Maybe this is overkill if all you want is word representations</li>
</ul>
<div class="fragment">
<ul>
<li>Global Vector (GloVe) embeddings are a simpler and faster approach based on a matrix factorization similar to principal component analysis (PCA)</li>
</ul>
<p>Idea: First fit the distributed word representations using GloVe, then plug these embeddings into a neural net that does some other task (e.g.&nbsp;translation)</p>
</div>
</section>
<section id="the-distributional-hypothesis" class="slide level2">
<h2>The Distributional Hypothesis</h2>
<p><strong>Distributional Hypothesis</strong>: Words with similar distributions have similar meaning</p>
<p>Consider a <strong>co-occurrence matrix</strong> <span class="math inline">\(X\)</span>, which counts the number of times the words appear nearby (say, less than 5 positions apart)</p>
<p>This is a <span class="math inline">\(V \times V\)</span> matrix, where <span class="math inline">\(V\)</span> is the vocabulary size.</p>
</section>
<section id="co-occurrence-matrix" class="slide level2">
<h2>Co-occurrence Matrix</h2>
<p>Sentence:</p>
<blockquote>
<p>The cat got squashed in the garden on Friday. The dog got flattened in the yard on Thursday.</p>
</blockquote>
</section>
<section id="co-occurrence-matrix-1" class="slide level2">
<h2>Co-occurrence Matrix</h2>
<p>Part of the co-occurrence matrix:</p>
<table class="caption-top">
<thead>
<tr class="header">
<th></th>
<th>the</th>
<th>cat</th>
<th>dog</th>
<th>got</th>
<th>squashed</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>the</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>cat</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>dog</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>got</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>squashed</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="glove-embedding-training" class="slide level2">
<h2>GloVe Embedding Training</h2>
<p><strong>Key insight</strong>: The co-occurrence matrix of words contain information about the semantic information (meaning) of words</p>
<p><img data-src="imgs/occurence.png" style="height:50.0%"></p>
<p>In particular, the <em>ratio</em> of co-occurrences encodes semantic information!</p>
</section>
<section id="intuition-pump" class="slide level2">
<h2>Intuition Pump</h2>
<p>Suppose we fit a rank-K approximation</p>
<p><span class="math display">\[{\bf X} \approx {\bf R}{\bf \hat{R}}^\top\]</span></p>
<p>Where <span class="math inline">\({\bf R}\)</span> and <span class="math inline">\({\bf \hat{R}}\)</span> are <span class="math inline">\(V \times K\)</span> matrices</p>
<ul>
<li>Each row <span class="math inline">\({\bf r}_i\)</span> of <span class="math inline">\({\bf R}\)</span> is the K-dim representation of a word.</li>
<li>Each entry of <span class="math inline">\({\bf X}\)</span> is approximated as <span class="math inline">\(x_{ij} \approx {\bf r}_i^T {\bf \hat{r}}_j\)</span> <!-- - Hence, more similar words are more likely to co-occur (Lisa: I think this interpretation is wrong)--></li>
<li>Minimizing the squared Frobenius norm of the <span class="math inline">\(\left|\left|{\bf X} - {\bf R}{\bf \hat{R}}^\top\right|\right|_F^2\)</span> is basically PCA</li>
</ul>
<div class="fragment">
<ul>
<li>There are some other tricks to make the optimization work</li>
</ul>
<!--
## GloVe -- (do we keep?)

**Problem**: ${\bf X}$ is extremely large, so fitting the factorization using least squares is infeasible

. . .

- **Solution**: Reweight the entries so that only nonzero counts matter

. . .

**Problem**: word counts are heavy-tail distributed (some words *very* frequently used, lots of infrequent words). The most common words will dominate the cost function.

. . .

- **Solution**: Approximate $\log x_{ij}$ instead of $x_{ij}$


## GloVe Cost function -- (do we keep?)

$$\mathcal{J}({\bf R}) = \sum_{i,j} f(x_{ij})({\bf r}_i^\top {\bf \hat{r}}_j + b_i + \hat{b}_j - \log x_{ij})^2$$

...where $f(x{ij}) = (\frac{x_{ij}}{100})^{\frac{3}{4}}$ if $x_{ij} < 100$ and 1 otherwise.

We only need to consider nonzero entries of ${\bf X}$!

Q: What are the tunable parameters in this model?
-->
</div>
</section>
<section id="glove-embeddings-1" class="slide level2">
<h2>GloVe Embeddings</h2>
<p>Pre-trained models are available for download:</p>
<p><a href="https://nlp.stanford.edu/projects/glove/" class="uri">https://nlp.stanford.edu/projects/glove/</a></p>
<p>Practitioners often use these embeddings to do other language modeling tasks.</p>
</section>
<section id="glove-embedding-demo" class="slide level2">
<h2>GloVe Embedding Demo</h2>
<p>Demo on Google Colab</p>
<p><a href="https://colab.research.google.com/drive/1aNbE6HcawVF67RV0hWi4qK33Um7cKykr?usp=sharing" class="uri">https://colab.research.google.com/drive/1aNbE6HcawVF67RV0hWi4qK33Um7cKykr?usp=sharing</a></p>
</section>
<section id="key-idea-from-the-demo" class="slide level2">
<h2>Key idea from the Demo</h2>
<ul>
<li>Distances are somewhat meaningful, and are based on <strong>word co-occurrences</strong>
<ul>
<li>the words “black” and “white” will have similar embeddings because they co-occur with similar other words.</li>
<li>“cat” and “dog” is more similar to each other than “cat” and “kitten” because the latter two words occur in <em>different contexts</em>!</li>
</ul></li>
</ul>
</section>
<section id="key-idea-from-the-demo-1" class="slide level2">
<h2>Key idea from the Demo</h2>
<ul>
<li>Word Analogies: Directions in the embedding space can be meaningful
<ul>
<li>“king” - “man” + “woman” <span class="math inline">\(\approx\)</span> “queen”</li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li>Bias in Word Embeddings (and Neural Networks in General)
<ul>
<li>neural networks pick up pattern in the data</li>
<li>these patterns can be biased and discriminatory</li>
</ul></li>
</ul>
</div>
</section>
<section id="bias-and-fairness" class="slide level2">
<h2>Bias and Fairness</h2>
<p>Word embeddings are inherently biased because there is bias in the training data.</p>
<p>Neural networks learn patterns in the training data, so if the training data contains human biases, then so will the trained model! This effect was seen in:</p>
<ul>
<li><a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">criminal sentencing</a></li>
</ul>
<div class="fragment">
<ul>
<li><a href="https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/">predictive policing</a></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G">resume filtering</a></li>
</ul>


</div>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"src":"chalkboard.json","boardmarkerWidth":2,"chalkWidth":2,"chalkEffect":1},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/utm-csc413\.github\.io\/2024F-website\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>