<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <title>CSC413 - Fall 2024, UTM – CSC413 Neural Networks and Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta property="og:description" content="Lecture 5">
<meta property="og:site_name" content="CSC413 - Fall 2024, UTM">
<meta name="twitter:title" content="CSC413 Neural Networks and Deep Learning – CSC413 - Fall 2024, UTM">
<meta name="twitter:description" content="Lecture 5">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC413 Neural Networks and Deep Learning</h1>
  <p class="subtitle">Lecture 5</p>

<div class="quarto-title-authors">
</div>

</section>
<section id="lecture-plan" class="slide level2">
<h2>Lecture Plan</h2>
<ul>
<li>Optimization</li>
</ul>
</section>
<section>
<section id="the-optimization-landscape" class="title-slide slide level1 center">
<h1>The Optimization Landscape</h1>

</section>
<section id="optimization-problem" class="slide level2">
<h2>Optimization Problem</h2>
<p>Let’s group all the parameters (weights and biases) of a network into a single vector <span class="math inline">\({\bf \theta}\)</span></p>
<p>We wish to find the minima of a function <span class="math inline">\(f({\theta}): \mathbb{R}^D \rightarrow \mathbb{R}\)</span>.</p>
<p>We already discussed gradient descent, but …</p>
<ul>
<li>What property does <span class="math inline">\(f\)</span> need to have for gradient descent to work well?</li>
<li>Are there techniques that can work better than (vanilla) gradient descent?</li>
</ul>
</section>
<section id="optimization-problem-contd" class="slide level2">
<h2>Optimization Problem (cont’d)</h2>
<p>We wish to find the minima of a function <span class="math inline">\(f({\theta}): \mathbb{R}^D \rightarrow \mathbb{R}\)</span>.</p>
<p>We already discussed gradient descent, but …</p>
<ul>
<li>What property does <span class="math inline">\(f\)</span> need to have for gradient descent to work well?</li>
<li>Are there techniques that can work better than (vanilla) gradient descent?</li>
<li>Are there cases where gradient descent (and related) optimization methods fail?</li>
<li>How can deep learning practitioners diagnose and fix optimization issues?</li>
</ul>
</section>
<section id="visualizing-optimization-problems" class="slide level2">
<h2>Visualizing Optimization Problems</h2>
<p>Visualizing optimization problems in high dimensions is challenging. Intuitions that we get from 1D and 2D optimization problems can be helpful.</p>
<p>In 1D and 2D, we can visualize <span class="math inline">\(f\)</span> by drawing plots, e.g.&nbsp;surface plots and contour plots</p>
<center>
<img data-src="imgs/fig_error_surface.png" height="350">
</center>
<p>Q: Sketch a contour plot that represents the same function as the figure above.</p>
</section>
<section id="visualizing-optimization-problems-contd" class="slide level2">
<h2>Visualizing Optimization Problems (cont’d)</h2>
<center>
<img data-src="imgs/fig_error_surface.png" height="400">
</center>
<p>Q: Sketch a contour plot that represents the same function as the figure above.</p>
</section>
<section id="review-contour-plots" class="slide level2">
<h2>Review: Contour Plots</h2>

<img data-src="imgs/HimmelblauFunction.png" style="width:80.0%" class="r-stretch"><p>Q: Where are the 4 local minima in this contour plot?</p>
</section>
<section id="minimization-intuition-in-1d" class="slide level2">
<h2>Minimization Intuition in 1D</h2>
<p>Suppose we have a function <span class="math inline">\(f({\theta}): \mathbb{R}^1 \rightarrow \mathbb{R}\)</span> that we wish to minimize. How do we go about doing this?</p>
<div class="fragment">
<p><span class="math inline">\(\theta \rightarrow \theta - \alpha f'(\theta)\)</span></p>
</div>
<div class="fragment">
<p>Gradient descent and other techniques all are founded on approximating <span class="math inline">\(f\)</span> with its <strong>Taylor series expansion</strong>:</p>
<p><span class="math display">\[\begin{align*}
f(\theta) \approx f(\theta_0) + f'(\theta_0)^T(\theta - \theta_0) + \frac{1}{2} (\theta - \theta_0)^2 f''(\theta_0) + \ldots
\end{align*}\]</span></p>
<p>Understanding this use of the Taylor series approximation helps us understand more involved optimization techniques that use higher-order derivatives.</p>
</div>
</section>
<section id="taylor-series-expansion-in-high-dimensions" class="slide level2">
<h2>Taylor Series Expansion in High Dimensions</h2>
<p>Suppose we have a function <span class="math inline">\(f({\theta}): \mathbb{R}^D \rightarrow \mathbb{R}\)</span> that we wish to minimize.</p>
<p>Again, we can explore approximating <span class="math inline">\(f\)</span> with its <strong>Taylor series expansion</strong>:</p>
<p><span class="math display">\[\begin{align*}
f(\theta) \approx f(\theta_0) + \nabla f(\theta_0)^T(\theta - \theta_0) + \frac{1}{2} (\theta - \theta_0)^T H(\theta_0) (\theta - \theta_0) + \ldots
\end{align*}\]</span></p>
<p>Gradient descent uses first-order information, but other optimization algorithms use some information about the <strong>Hessian</strong>.</p>
</section>
<section id="recap-gradient" class="slide level2">
<h2>Recap: Gradient</h2>
<p>The <strong>gradient</strong> of a function <span class="math inline">\(f({\theta}): \mathbb{R}^D \rightarrow \mathbb{R}\)</span> is the vector of partial derivatives:</p>
<p><span class="math display">\[\begin{align*}
\nabla_{\theta} f = \frac{\partial f}{\partial {\theta}} = \begin{bmatrix}
\frac{\partial f}{\partial \theta_1}  \\
\frac{\partial f}{\partial \theta_2}  \\
\vdots \\
\frac{\partial f}{\partial \theta_D}  \\
\end{bmatrix}
\end{align*}\]</span></p>
</section>
<section id="recap-hessian" class="slide level2">
<h2>Recap: Hessian</h2>
<p>The <strong>Hessian Matrix</strong>, denoted <span class="math inline">\({\bf H}\)</span> or <span class="math inline">\(\nabla^2 f\)</span> is the matrix of second derivatives</p>
<p><span class="math display">\[\begin{align*}
H = \nabla^2 f = \begin{bmatrix}
\frac{\partial^2 f}{\partial \theta_1^2}  &amp; \frac{\partial^2 f}{\partial \theta_1 \partial \theta_2} &amp; \dots  &amp;
     &amp; \frac{\partial^2 f}{\partial \theta_1 \partial \theta_D} \\
\frac{\partial^2 f}{\partial \theta_1 \partial \theta_2}  &amp; \frac{\partial^2 f}{\partial \theta_2^2} &amp; \dots  &amp;
     &amp; \frac{\partial^2 f}{\partial \theta_2 \partial \theta_D} \\
\vdots
\frac{\partial^2 f}{\partial \theta_n \partial \theta_2}  &amp; \frac{\partial^2 f}{\partial \theta_n \partial \theta_2} &amp; \dots  &amp;
     &amp; \frac{\partial^2 f}{\partial \theta_D^2} \\
\end{bmatrix}
\end{align*}\]</span></p>
<p>The Hessian is symmetric because <span class="math inline">\(\frac{\partial^2 f}{\partial \theta_i \partial \theta_j} = \frac{\partial^2 f}{\partial \theta_j \partial \theta_i}\)</span></p>
</section>
<section id="multivariate-taylor-series" class="slide level2">
<h2>Multivariate Taylor Series</h2>
<p>Recall the second-order <strong>Taylor series expansion</strong> of <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[\begin{align*}
f(\theta) \approx f(\theta_0) + \nabla f(\theta_0)^T(\theta - \theta_0) + \frac{1}{2} (\theta - \theta_0)^T H(\theta_0) (\theta - \theta_0)
\end{align*}\]</span></p>
<p>A <strong>critical point</strong> of <span class="math inline">\(f\)</span> is a point where the gradient is zero, so that</p>
<p><span class="math display">\[\begin{align*}
f(\theta) \approx f(\theta_0) + \frac{1}{2} (\theta - \theta_0)^T H(\theta_0) (\theta - \theta_0)
\end{align*}\]</span></p>
</section>
<section id="multivariate-taylor-series-contd" class="slide level2">
<h2>Multivariate Taylor Series (cont’d)</h2>
<p>A <strong>critical point</strong> of <span class="math inline">\(f\)</span> is a point where the gradient is zero, so that</p>
<p><span class="math display">\[\begin{align*}
f(\theta) \approx f(\theta_0) + \frac{1}{2} (\theta - \theta_0)^T H(\theta_0) (\theta - \theta_0)
\end{align*}\]</span></p>
<p>How do we know if the critical point is a maximum, minimum, or something else?</p>
<div class="fragment">
<ul>
<li><strong>Minimum</strong>: The Hessian is positive definite.</li>
<li><strong>Maximum</strong>: The Hessian is negative definite.</li>
</ul>
</div>
</section>
<section id="spectral-decomposition-of-h" class="slide level2">
<h2>Spectral Decomposition of <span class="math inline">\(H\)</span></h2>
<p><span class="math display">\[\begin{align*}
f(\theta) \approx f(\theta_0) + f'(\theta_0)^T(\theta - \theta_0) + \frac{1}{2} (\theta - \theta_0)^2 f''(\theta_0) + \ldots
\end{align*}\]</span></p>
<p>We won’t go into details in this course, but…</p>
<ul>
<li>A lot of important features of the optimization landscape can be characterized by the eigenvalues of the Hessian <span class="math inline">\(H\)</span>.</li>
<li>Recall that a symmetric matrix (such as <span class="math inline">\(H\)</span>) has only real eigenvalues, and there is an orthogonal basis of eigenvectors.</li>
</ul>
</section>
<section id="spectral-decomposition-of-h-contd" class="slide level2">
<h2>Spectral Decomposition of <span class="math inline">\(H\)</span> (cont’d)</h2>
<ul>
<li>A lot of important features of the optimization landscape can be characterized by the eigenvalues of the Hessian <span class="math inline">\(H\)</span>.</li>
<li>Recall that a symmetric matrix (such as <span class="math inline">\(H\)</span>) has only real eigenvalues, and there is an orthogonal basis of eigenvectors.</li>
<li>This can be expressed in terms of the <strong>spectral decomposition</strong>: <span class="math inline">\(H = Q\Lambda Q^T\)</span>, where <span class="math inline">\(Q\)</span> is an orthogonal matrix (whose columns are the eigenvectors) and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix (whose diagonal entries are the eigenvalues).</li>
</ul>
</section>
<section id="first--vs-second-order-information" class="slide level2">
<h2>First- vs Second-Order Information</h2>
<p>In Gradient Descent, we approximate <span class="math inline">\(f\)</span> with its <strong>first-order Taylor approximation</strong>. In other words, we are using <strong>first-order</strong> information in our optimization procedure.</p>
<p>An area of research known as <strong>second-order optimization</strong> develops algorithms which explicitly use curvature information (the Hessian <span class="math inline">\(H\)</span>), but these are complicated and difficult to scale to large neural nets and large datasets.</p>
<p>But before we get there…</p>
</section>
<section id="features-of-the-optimization-landscape" class="slide level2">
<h2>Features of the Optimization Landscape</h2>
<center>
<img data-src="imgs/convex_function.png" height="200"> <img data-src="imgs/local_minimum.png" height="200"> <img data-src="imgs/Saddle_point.png" height="200">
</center>
<center>
<img data-src="imgs/plateau.png" height="200"> <img data-src="imgs/rosenbrock.png" height="200">
</center>
</section>
<section id="feature-1-convexity-of-linear-models" class="slide level2">
<h2>Feature 1: Convexity of Linear Models</h2>
<p>Linear regression and logistic regressions are <strong>convex</strong> problems—i.e.&nbsp;its loss function is convex.</p>
<p>A function <span class="math inline">\(f\)</span> is convex if for any <span class="math inline">\(a \in (0, 1)\)</span></p>
<p><span class="math display">\[f(ax + (1 - a) y) &lt;  af(x) + (1-a)f(y)\]</span></p>
<ul>
<li>The cost function only has one minima.</li>
<li>There are no local minima that is not global minima.</li>
<li>Intuitively: the cost function is “bowl-shaped”.</li>
</ul>

<img data-src="imgs/convexity.png" style="width:40.0%" class="r-stretch"></section>
<section id="q-are-these-loss-surfaces-convex" class="slide level2">
<h2>Q: Are these loss surfaces convex?</h2>

<img data-src="imgs/q_convex.png" style="width:80.0%" class="r-stretch"></section>
<section id="convexity-in-1d" class="slide level2">
<h2>Convexity in 1D</h2>
<p>How do we know if a function <span class="math inline">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span> is convex?</p>
<div class="fragment">
<p>When <span class="math inline">\(f''(x)\)</span> is positive everywhere!</p>
<p>Likewise, analyzing the Hessian matrix <span class="math inline">\(H\)</span> tells us whether a function <span class="math inline">\(f: \mathbb{R}^D \rightarrow \mathbb{R}\)</span> is convex. (Hint: <span class="math inline">\(H\)</span> needs to have only positive eigenvalues)</p>
</div>
</section>
<section id="neural-networks-are-not-convex" class="slide level2">
<h2>Neural Networks are Not Convex</h2>
<p>In general, neural networks are <strong>not convex</strong>.</p>
<p>One way to see this is that neural networks have <strong>weight space symmetry</strong>:</p>
<p><img data-src="imgs/permutation_symmetry.png" style="height:30.0%"></p>
</section>
<section id="neural-networks-are-not-convex-contd" class="slide level2">
<h2>Neural Networks are Not Convex (cont’d)</h2>
<ul>
<li>Suppose you are at a local minima <span class="math inline">\({\bf \theta}\)</span>.</li>
<li>You can swap two hidden units, and therefore swap the corresponding weights/biases, and get <span class="math inline">\({\bf \theta}^\prime\)</span>,</li>
<li>then <span class="math inline">\({\bf \theta}^\prime\)</span> must also be a local minima!</li>
</ul>
<p>Video: <a href="https://play.library.utoronto.ca/watch/78367fbca4c4a42a30ec5862cdd0c756">Convexity of MLP</a></p>
</section>
<section id="feature-2-local-minima-in-neural-networks" class="slide level2">
<h2>Feature 2: Local Minima in Neural Networks</h2>
<p>Even though any multilayer neural net can have local optima, we usually don’t worry too much about them.</p>
<p>It’s possible to construct arbitrarily bad local minima even for ordinary classification MLPs. It’s poorly understood why these don’t arise in practice.</p>
<p>Over the past 5 years or so, CS theorists have made lots of progress proving gradient descent converges to global minima for some non-convex problems, including some specific neural net architectures.</p>
</section>
<section id="feature-3-saddle-points" class="slide level2">
<h2>Feature 3: Saddle Points</h2>
<center>
<img data-src="imgs/Saddle_point.png" height="200">
</center>
<p>A saddle point has <span class="math inline">\(\nabla_\theta \mathcal{E} = {\bf 0}\)</span>, even though we are not at a minimum.</p>
<p>Minima with respect to some directions, maxima with respect to others. In other words, <span class="math inline">\(H\)</span> has some positive and some negative eigenvalues.</p>
</section>
<section id="feature-3-saddle-points-contd" class="slide level2">
<h2>Feature 3: Saddle Points (cont’d)</h2>
<p>A saddle point has <span class="math inline">\(\nabla_\theta \mathcal{E} = {\bf 0}\)</span>, even though we are not at a minimum.</p>
<p>Minima with respect to some directions, maxima with respect to others. In other words, <span class="math inline">\(H\)</span> has some positive and some negative eigenvalues.</p>
<p>When would saddle points be a problem?</p>
<ul>
<li>If we’re exactly on the saddle point, then we’re stuck.</li>
<li>If we’re slightly to the side, then we can get unstuck.</li>
</ul>
</section>
<section id="initialization" class="slide level2">
<h2>Initialization</h2>
<ul>
<li>If we initialize all weights/biases to the same value, (e.g.&nbsp;0)</li>
<li>…then all the hidden states in the same layer will have the same value, (e.g.&nbsp;<span class="math inline">\({\bf h}\)</span> will be a vector containing the same value repeated)</li>
<li>…then all of the error signals for weights in the same layer are the same. (e.g.&nbsp;each row of <span class="math inline">\(\overline{W^{(2)}}\)</span> will be identical)</li>
</ul>
</section>
<section id="initialization-contd" class="slide level2">
<h2>Initialization (cont’d)</h2>
<ul>
<li>…then all of the error signals for weights in the same layer are the same. (e.g.&nbsp;each row of <span class="math inline">\(\overline{W^{(2)}}\)</span> will be identical)</li>
</ul>
<p><span class="math display">\[\begin{align*}
\overline{{\bf y}} &amp;= \overline{\mathcal{L}}({\bf y} - {\bf t}) \\
\overline{W^{(2)}} &amp;= \overline{{\bf y}}{\bf h}^T \\
\overline{{\bf h}} &amp;= {W^{(2)}}^T \overline{{\bf y}} \\
\overline{{\bf z}} &amp;= \overline{{\bf h}} \circ \sigma^\prime({\bf z}) \\
\overline{W^{(1)}} &amp;= \overline{{\bf z}} {\bf x}^T
\end{align*}\]</span></p>
</section>
<section id="random-initialization" class="slide level2">
<h2>Random Initialization</h2>
<p><strong>Solution</strong>: don’t initialize all your weights to zero!</p>
<p>Instead, <em>break the symmetry</em> by using small random values.</p>
<p>We initialize the weights by sampling from a random normal distribution with:</p>
<ul>
<li>Mean = 0</li>
<li>Variance = <span class="math inline">\(\frac{2}{{\rm fan\_in}}\)</span> where <code>fan_in</code> is the number of input neurons that feed into this feature. (He et al.&nbsp;2015)</li>
</ul>
</section>
<section id="feature-4-plateaux" class="slide level2">
<h2>Feature 4: Plateaux</h2>
<p>A flat region in the cost is called a <strong>plateau</strong>. (Plural: plateaux)</p>
<p><img data-src="imgs/plateau.png" style="height:50.0%"></p>
<p>Can you think of examples?</p>
<ul>
<li>logistic activation with least squares</li>
<li>0-1 loss</li>
<li>ReLU activation (potentially)</li>
</ul>
</section>
<section id="plateaux-and-saturated-units" class="slide level2">
<h2>Plateaux and Saturated Units</h2>
<p>An important example of a plateau is a <strong>saturated unit</strong>. This is when activations always end up in the flat region of its activation function. Recall the backprop equation for the weight derivative:</p>
<ul>
<li>If <span class="math inline">\(\phi^{\prime}(z)\)</span> is always close to zero, then the weights will get stuck.</li>
<li>If there is a ReLU unit whose input <span class="math inline">\(z_i\)</span> is always negative, the weight derivatives will be <em>exactly</em> 0. We call this neuron a <strong>dead unit</strong>.</li>
</ul>
</section>
<section id="ravines" class="slide level2">
<h2>Ravines</h2>
<p>Lots of sloshing around the walls, only a small derivative along the slope of the ravine’s floor.</p>
</section>
<section id="ravines-2d-intuition" class="slide level2">
<h2>Ravines (2D Intuition)</h2>

<img data-src="imgs/scale_problem.png" style="width:50.0%" class="r-stretch"><ul>
<li>Gradient component <span class="math inline">\(\frac{\partial \mathcal{E}}{\partial w_1}\)</span> is large</li>
<li>Gradient component <span class="math inline">\(\frac{\partial \mathcal{E}}{\partial w_2}\)</span> is small</li>
</ul>
</section>
<section id="ravines-example" class="slide level2">
<h2>Ravines Example</h2>
<p>Suppose we have the following dataset for linear regression.</p>
<ul>
<li>Which weight, <span class="math inline">\(w_1\)</span> or <span class="math inline">\(w_2\)</span>, will receive a larger gradient descent update?</li>
<li>Which one do you want to receive a larger update?</li>
<li>Note: the figure vastly <em>understates</em> the narrowness of the ravine!</li>
</ul>
</section>
<section id="ravines-another-examples" class="slide level2">
<h2>Ravines: another examples</h2>
<!-- \begin{columns}
\begin{column}{0.4 \textwidth}
  \begin{small}
  \begin{tabular}{rr|r}
    $x_1$ & $x_2$ & $t$ \\
    \hline
    1003.2 & 1005.1 & 3.3 \\
    1001.1 & 1008.2 & 4.8 \\
    998.3 & 1003.4 & 2.9 \\
    \vdots \hspace{1em} & \vdots \hspace{1.5em} & \vdots \hspace{0.5em}
  \end{tabular}
  \end{small}
\end{column}
\begin{column}{0.4 \textwidth}
  \includegraphics[width=\linewidth]{imgs/offset_problem.png}
\end{column}
\end{columns} -->
<table class="caption-top">
<thead>
<tr class="header">
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(t\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1003.2</td>
<td>1005.1</td>
<td>3.3</td>
</tr>
<tr class="even">
<td>1001.1</td>
<td>1008.2</td>
<td>4.8</td>
</tr>
<tr class="odd">
<td>998.3</td>
<td>1003.4</td>
<td>2.9</td>
</tr>
</tbody>
</table>
<center>
<img data-src="imgs/offset_problem.png" height="300">
</center>
</section>
<section id="avoiding-ravines" class="slide level2">
<h2>Avoiding Ravines</h2>
<p>To help avoid these problems, it’s a good idea to <strong>center</strong> or <strong>normalize</strong> your inputs to zero mean and unit variance, especially when they’re in arbitrary units (feet, seconds, etc.).</p>
<p>Hidden units may have non-centered activations, and this is harder to deal with.</p>
<p>A recent method called <strong>batch normalization</strong> explicitly centers each hidden activation. It often speeds up training by 1.5-2x.</p>
</section>
<section id="method-batch-normalization" class="slide level2">
<h2>Method: Batch Normalization</h2>
<p><strong>Idea</strong>: Normalize the activations <strong>per batch</strong> during training, so that the activations have zero mean and unit variance.</p>
<p>What about during test time (i.e.&nbsp;during model evaluation)?</p>
<ul>
<li>Keep track of the activation mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span> during training.</li>
<li>Use that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> at test time: <span class="math inline">\(z^\prime = \frac{z - \mu}{\sigma}\)</span>.</li>
</ul>
<!--
## Batch Normalization in PyTorch

```python
class PyTorchWordEmb(nn.Module):
  def __init__(self, emb_size=100, ...)
    super(PyTorchWordEmb, self).__init__()
    self.word_emb_layer = nn.Linear(vocab_size, ...)
    self.fc_layer1 = nn.Linear(emb_size * 3, num_hidden)
    self.bn = nn.BatchNorm1d(num_hidden)         # <----
    self.fc_layer2 = nn.Linear(num_hidden, 250)
    self.num_hidden = num_hidden
    self.emb_size = emb_size
  def forward(self, inp):
    embeddings = torch.relu(self.word_emb_layer(inp))
    embeddings = embeddings.reshape([-1, ...])
    hidden = torch.relu(self.fc_layer1(embeddings))
    hidden = self.bn(hidden)                    # <----
    return self.fc_layer2(hidden)
```

## Differentiating model training vs evaluation

Since model behaviour is different during training vs evaluation,
we need to annotate

```python
model.train()
```

...before a forward pass during training, and...

```python
model.eval()
```

...before a forward pass during evaluation.

-->
</section>
<section id="batch-normalization-video" class="slide level2">
<h2>Batch Normalization Video</h2>
<p><a href="https://play.library.utoronto.ca/watch/3e2b87ac8e5730f404893ce9270b4b75" class="uri">https://play.library.utoronto.ca/watch/3e2b87ac8e5730f404893ce9270b4b75</a></p>
</section>
<section id="method-momentum" class="slide level2">
<h2>Method: Momentum</h2>
<p><strong>Momentum</strong> is a simple and highly effective method to deal with narrow ravines. Imagine a hockey puck on a frictionless surface (representing the cost function). It will accumulate momentum in the downhill direction:</p>
<p><span class="math display">\[\begin{align*}
{\bf p} &amp;\gets \mu {\bf p} - \alpha \frac{\partial \mathcal{E}}{\partial \theta} \\
\theta &amp;\gets \theta + {\bf p}
\end{align*}\]</span></p>
</section>
<section id="method-momentum-contd" class="slide level2">
<h2>Method: Momentum (cont’d)</h2>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the learning rate, just like in gradient descent.</li>
<li><span class="math inline">\(\mu\)</span> is a damping parameter. It should be slightly less than 1 (e.g.&nbsp;0.9 or 0.99).
<ul>
<li>If <span class="math inline">\(\mu = 1\)</span>, conservation of energy implies it will never settle down.</li>
</ul></li>
</ul>
</section>
<section id="why-momentum-works" class="slide level2">
<h2>Why Momentum Works</h2>
<ul>
<li>In the high curvature directions, the gradients cancel each other out, so momentum dampens the oscillations.</li>
<li>In the low curvature directions, the gradients point in the same direction, allowing the parameters to pick up speed.</li>
</ul>
<center>
<img data-src="imgs/momentum.png" height="250">
</center>
</section>
<section id="why-momentum-works-contd" class="slide level2">
<h2>Why Momentum Works (cont’d)</h2>
<center>
<img data-src="imgs/momentum.png" height="250">
</center>
<ul>
<li>If the gradient is constant (i.e.&nbsp;the cost surface is a plane), the parameters will reach a terminal velocity of <span class="math inline">\(-\frac{\alpha}{1 - \mu} \cdot \frac{\partial \mathcal{E}}{\partial \theta}\)</span> This suggests if you increase <span class="math inline">\(\mu\)</span>, you should lower <span class="math inline">\(\alpha\)</span> to compensate.</li>
<li>Momentum sometimes helps a lot, and almost never hurts.</li>
</ul>
</section>
<section id="gradient-descent-with-momentum" class="slide level2">
<h2>Gradient Descent with Momentum</h2>
<p>Q: Which trajectory has the highest/lowest momentum setting?</p>
<center>
<p><img data-src="imgs/Himmelblau_Momentum05.png" height="250"> <img data-src="imgs/Himmelblau_Momentum95.png" height="250"></p>
<p><img data-src="imgs/Himmelblau_Momentum99.png" height="250"> <img data-src="imgs/Himmelblau_Momentum90.png" height="250"></p>
</center>
</section>
<section id="second-order-information" class="slide level2">
<h2>Second-Order Information</h2>
<p>An area of research known as <strong>second-order optimization</strong> develops algorithms which explicitly use curvature information (the Hessian <span class="math inline">\(H\)</span>), but these are complicated and difficult to scale to large neural nets and large datasets.</p>
<p>But can we use just a bit of second-order information?</p>
</section>
<section id="method-rmsprop" class="slide level2">
<h2>Method: RMSProp</h2>
<p>SGD takes large steps in directions of high curvature and small steps in directions of low curvature.</p>
<p><strong>RMSprop</strong> is a variant of SGD which rescales each coordinate of the gradient to have norm 1 on average. It does this by keeping an exponential moving average <span class="math inline">\(s_j\)</span> of the squared gradients.</p>
</section>
<section id="method-rmsprop-contd" class="slide level2">
<h2>Method: RMSProp (cont’d)</h2>
<p>The following update is applied to each coordinate j independently: <span class="math display">\[\begin{align*}
s_j &amp;\leftarrow  (1-\gamma)s_j + \gamma \left(\frac{\partial \mathcal{J}}{\partial \theta_j}\right)^2 \\
\theta_j &amp;\leftarrow \theta_j - \frac{\alpha}{\sqrt{s_j + \epsilon}} \frac{\partial \mathcal{J}}{\partial \theta_j}
\end{align*}\]</span></p>
<p>If the eigenvectors of the Hessian are axis-aligned (dubious assumption) then RMSprop can correct for the curvature. In practice, it typically works slightly better than SGD.</p>
</section>
<section id="method-adam" class="slide level2">
<h2>Method: Adam</h2>
<p>Adam = RMSprop + momentum</p>
<p>Adam is the most commonly used optimizer for neural network</p>
<center>
<img data-src="imgs/adamcite.png" height="200">
</center>
<!-- 02optim -->
</section></section>
<section>
<section id="gradient-descent-and-sgd" class="title-slide slide level1 center">
<h1>Gradient Descent and SGD</h1>

</section>
<section id="learning-rate" class="slide level2">
<h2>Learning Rate</h2>
<p>The learning rate <span class="math inline">\(\alpha\)</span> is a hyperparameter we need to tune. Here are the things that can go wrong in batch mode:</p>
<center>
<p><img data-src="imgs/slow_progress.png" height="250"> <img data-src="imgs/oscillations.png" height="250"> <img data-src="imgs/instability.png" height="250"></p>
<table class="caption-top">
<thead>
<tr class="header">
<th><span class="math inline">\(\alpha\)</span> too small</th>
<th><span class="math inline">\(\alpha\)</span> too large</th>
<th><span class="math inline">\(\alpha\)</span> much too large</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>slow progress</td>
<td>oscillations</td>
<td>instability</td>
</tr>
</tbody>
</table>
</center>
</section>
<section id="stochastic-gradient-descent" class="slide level2">
<h2>Stochastic Gradient Descent</h2>
<p>Batch gradient descent moves directly downhill. SGD takes steps in a noisy direction, but moves downhill on average.</p>
<center>
<table class="caption-top">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>batch gradient descent</th>
<th>stochastic gradient descent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img data-src="imgs/batch_gradient_descent.png" height="350"></td>
<td><img data-src="imgs/sgd.png" height="350"></td>
</tr>
</tbody>
</table>
</center>
</section>
<section id="sgd-learning-rate" class="slide level2">
<h2>SGD Learning Rate</h2>
<p>In stochastic training, the learning rate also influences the <em>fluctuations</em> due to the stochasticity of the gradients.</p>
<center>
<img data-src="imgs/fluctuations.png" height="250">
</center>
<ul>
<li>Use a large learning rate early in training so you can get close to the optimum</li>
<li>Gradually decay the learning rate to reduce the fluctuations</li>
</ul>
</section>
<section id="sgd-batch-size" class="slide level2">
<h2>SGD Batch Size</h2>
<p>The tradeoff between smaller vs larger batch size</p>
<p><span class="math display">\[\begin{align*}
Var\left[\frac{1}{S} \sum_{i=1}^S \frac{\partial \mathcal{L}^{(i)}}{\partial \theta_j}\right]
&amp;=
\frac{1}{S^2} Var \left[\sum_{i=1}^S \frac{\partial \mathcal{L}^{(i)}}{\partial \theta_j} \right] \\
&amp;=
\frac{1}{S} Var \left[\frac{\partial \mathcal{L}^{(i)}}{\partial \theta_j} \right]
\end{align*}\]</span></p>
<p>Larger batch size implies smaller variance, but at what cost?</p>
</section>
<section id="training-curve-or-learning-curve" class="slide level2">
<h2>Training Curve (or Learning Curve)</h2>
<p>To diagnose optimization problems, it’s useful to look at <strong>learning curves</strong>: plot the training cost (or other metrics) as a function of iteration.</p>
<center>
<img data-src="imgs/training_curve.png" height="350">
</center>
</section>
<section id="training-curve-or-learning-curve-contd" class="slide level2">
<h2>Training Curve (or Learning Curve) (cont’d)</h2>
<ul>
<li><strong>Note</strong>: use a fixed subset of the training data to monitor the training error. Evaluating on a different batch (e.g.&nbsp;the current one) in each iteration adds a <em>lot</em> of noise to the curve!</li>
<li><strong>Note</strong>: it’s very hard to tell from the training curves whether an optimizer has converged. They can reveal major problems, but they can’t guarantee convergence.</li>
</ul>
</section>
<section id="visualizing-optimization-algorithms" class="slide level2">
<h2>Visualizing Optimization Algorithms</h2>
<p>You might want to check out these links.</p>
<p>An overview of gradient descent algorithms:</p>
<ul>
<li><a href="https://www.ruder.io/optimizing-gradient-descent/" class="uri">https://www.ruder.io/optimizing-gradient-descent/</a></li>
</ul>
<p>CS231n:</p>
<ul>
<li><a href="https://cs231n.github.io/neural-networks-3/" class="uri">https://cs231n.github.io/neural-networks-3/</a></li>
</ul>
<p>Why momentum really works:</p>
<ul>
<li><a href="https://distill.pub/2017/momentum/" class="uri">https://distill.pub/2017/momentum/</a></li>
</ul>
<!-- next -->
</section></section>
<section id="what-to-do-this-week" class="title-slide slide level1 center">
<h1>What to do this week?</h1>
<ul>
<li>Work on Assignment 1</li>
<li>Study the practice materials before the tutorial</li>
<li>Continue readings and using the weekly overviews/checklist</li>
</ul>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://utm-csc413.github.io/2024F-website/" target="_blank">↩︎ Back to Course Website</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"src":"chalkboard.json","boardmarkerWidth":2,"chalkWidth":2,"chalkEffect":1},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/utm-csc413\.github\.io\/2024F-website\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>