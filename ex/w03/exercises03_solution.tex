% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\usepackage{../latex_packages/abbreviations}
\usepackage{fancyhdr}
\pagestyle{fancy}
\let\headrule\empty
\let\footrule\empty
\lhead{{\bfseries CSC\,413}}
\chead{{\bfseries Exercises - Week 3}}
\rhead{{\bfseries Shkurti / Gilitschenski}}
\lfoot{{}}
\cfoot{{\thepage}}
\rfoot{{}}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\author{}
\date{}
\begin{document}


\subsection{Exercise 1 - Maximum Likelihood Estimation
Refresher}\label{exercise-1---maximum-likelihood-estimation-refresher}

Assume you are given datapoints \((\fx_i)_{i=1}^N\) with
\(\fx_i\in\R^n\) coming from a Gaussian distribution. Derive the maximum
likelihood estimator of its mean.

\subsubsection{Solution}\label{solution}

First, let's quickly remember that the maximum likelihood estimator
(MLE) of a probability distribution from dataapoints
\(\fx_1, \ldots, \fx_N\) is given by \[
\hte_{\mathrm{MLE}} = \argmax_{\te \in \Te} \prod_{i=1}^N f(\fx_i | \te),
\] where \(f\) is the probability density function of the considered
probability distribution family, \(\te\) are the parameters of the
distribution, and \(\Te\) is the parameter space (a set containing all
possible parameters). The product on the right hand side is also known
as the likelihood function. In practice, we usually work with the
log-likelihood function instead. Because the logarithm is monotonously
increasing, the resulting estimators are the same, i.e. \[
\hte_{\mathrm{MLE}} 
 = \argmax_{\te \in \Te} \prod_{i=1}^N f(\fx_i | \te),
 = \argmax_{\te \in \Te} \sum_{i=1}^N \log \li( f(\fx_i | \te)\ri).
\]

Second, let's remember that the probability density function of a
multivariate Gaussian distribution is given by \[
f(\fx_i | \mu, \Si) 
 = \frac{1}{(2 \pi)^{n/2} |\Si|^{1/2}} 
   \exp \li( 
     - \frac{1}{2} 
     (\fx_i - \mu)^\top
     \Si^{-1} 
     (\fx_i - \mu) 
   \ri) 
\] with parameters \(\te=(\mu, \Si)\), where \(\mu\in\R^n\) is the mean
vector and \(\Si\in\R^{n\times n}\) is the covariance matrix. Moreover,
the notation \(|\Si|\) denotes the determinant of \(\Si\).

Our goal is to maximize the log-likelihood function which in this case
is \[\begin{aligned}
l(\mu, \Si| \fx_1, \ldots, \fx_N) 
  & := \sum_{i=1}^N \log f(\fx_i | \mu, \Si) \\
  & = \sum_{i=1}^N \li( 
    - \frac{n}{2} \log (2 \pi) 
    - \frac{1}{2} \log |\Si|  
    - \frac{1}{2}  (\fx_i - \mu)^\top \Si^{-1} (\fx_i - \mu) 
  \ri).
\end{aligned}\] For obtaining the MLE of \(\mu\), we can simply take the
gradient of \(l\) with respect to \(\mu\) and set it to zero resulting
in: \[
\nabla_\mu l(\mu, \Si| \fx_1, \ldots, \fx_N) 
 = \sum_{i=1}^N  \Si^{-1} ( \fx_i - \mu )
 = 0 
\] Since \(\Si\) is a covariance matrix, it is positive definite. Thus,
we can multiply both sides of the equation by \(\Si\) and obtain \[
\begin{aligned}
0 & = N \mu - \sum_{i=1}^N  \fx_i,
\\
\Rightarrow \hmu_{\mathrm{MLE}} &=  \frac{1}{N} \sum_{i=1}^N \fx_i .
\end{aligned}
\] Conveniently, \(\Si\) disappears and thus we do not have to worry
about it.

\subsection{Exercise 2 - More
Gradients}\label{exercise-2---more-gradients}

You are an ML Engineer at Googlezon where you are working on an internal
ML framework called TorchsorFlow. You are tasked with implementing a new
layer known as BatchNormalization. The idea of this layer is as follows:

During training, consider the outputs of the previous layer
\(\fa_i=(a_i^{(1)}, \ldots, a_i^{(N)})\) for each element
\(i\in \{1, \ldots, M\}\) of the input batch. Compute the mean \(\mu_j\)
and variance \(\si_j^2\) over each input dimension \(j\). Use the
resulting statistics to normalize the output of the previous layer.
Finally, rescale the resulting vector with a learned constant \(\gm\)
and shift it by another learned constant \(\be\).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Write down the mathematical expression for the BatchNormalization
  layer. What are its learnable parameters?
\item
  Compute the gradient of the loss \(\mcL\) with respect to the input of
  the BatchNormalization \(\fa_i\) layer.
\item
  At test time, the batch size is usually 1. So, it is not meaningful
  (or even possible) to compute mean / variance. How would you implement
  a layer like this?
\end{enumerate}

\subsubsection{Solution}\label{solution-1}

For the purpose of batch normalization, we can consider each output
neuron individually. Thus, we will simplify our notation and write
\(a_i\), \(y_i\), \ldots{} instead of \(a_i^{(j)}\), \(y_i^{(j)}\),
\ldots{} respectively.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  The forward pass is given by the following equations \[
  \begin{aligned}
  \mu_B 
    &:= \frac{1}{M}\sum_{i=1}^m a_i, 
    &\text{(mini-batch mean)}  \\
  \sigma_B^2 
    &:=\frac{1}{M}\sum_{i=1}^M (a_i-\mu_B)^2,
    &\text{(mini-batch variance)}\\
  \ha_i 
    &:= \frac{a_i-\mu_B}{\sqrt{\si_B^2+\epsilon}},   
  &\text{(normalize)}\\
    y_i
    &:= BN_{\gamma, \beta}((a_i)_{i=1}^M)
    := \gamma\ha_i + \beta.
    &\text{(scale and shift)}
  \end{aligned}
  \] The entire layer is defined as \[
  BN(\fa_1, \ldots \fa_M)=\big(
    BN_{\gm^{(1)}, \be^{(1)}}\big((a_i^{(1)})_{i=1}^M\big),
    \ldots, 
    BN_{\gm^{(N)}, \be^{(N)}}\big((a_i^{(N)})_{i=1}^M\big)
    \big)
  \] where \(\gm^{(1)}, \ldots, \gm^{(N)}\) and
  \(\be^{(1)}, \ldots, \be^{(N)}\) are learnable parameters.
\item
  The derivatives can be expressed using the chain rule where we obtain
  \(\mu_B\), \(\si_B\), \(\ha_i\), and \(y_i\) during the forward pass
  while \(\partial \mcL/\partial y_i\) is obtained from earlier steps of
  the backward pass. The remaining derivatives are: \[
  \begin{aligned}
  \frac{\partial\mcL}{\partial \ha_i} &= \fr{\partial \mcL}{y_i}\cdot \gm, \\ 
  \frac{\partial\mcL}{\partial \si_B^2}
    &= 
    \sum_{i=1}^M \fr{\partial\mcL}{\partial\ha_i}\cdot(a_i-\mu_B)\cdot
    \frac{-1}{2}(\si_B^2+\epsilon)^{-3/2}, \\ 
  \fr{\partial\mcL}{\partial\mu_B} 
    &=
    \bigg(\sum_{i=1}^M \fr{\partial\mcL}{\partial\ha_i}
    \cdot\frac{-1}{\sqrt{\sigma_B^2+\epsilon}}\bigg) 
    +\fr{\partial\mcL}{ \partial \si_B^2}
    \cdot\frac{\sum_{i=1}^M  -2(a_i-\mu_B)}{M},\\
  \fr{\partial\mcL}{\partial a_i}
    &=
    \fr{\partial\mcL}{\partial\ha_i}
    \cdot \frac{1}{\sqrt{\sigma_B^2+\epsilon}}  + \fr{\partial \mcL}{\partial \sigma_B^2} \cdot \frac{2(a_i-\mu_B)}{M} + \fr{\partial \mcL}{\partial \mu_B}\cdot \frac{1}{M},\\
  \fr{\partial\mcL}{\partial\gamma}&= \sum_{i=1}^M \fr{\partial\mcL}{\partial y_i} \cdot \ha_i, \\ 
  \fr{\partial\mcL}{\partial\beta} &= \sum_{i=1}^M \fr{\partial\mcL}{\partial y_i}.
  \end{aligned}
  \] Here \(\epsilon\) is a small constant which is added in practice to
  the variance to avoid division by zero. It is actually not part of the
  derivative.
\end{enumerate}

\subsection{Exercise 3 - Autodiff
Modes}\label{exercise-3---autodiff-modes}

Consider the function \(F(x) = f_3(f_2(f_1(x))\) and assume you also
know the derivatives \(f_i'\) for all \(f_i\).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Apply the chain rule to express \(F'(x)\) in terms of \(f_i'\)s and
  \(f_i\).
\item
  Write down the pseudocode for computing \(F'(x)\) using the forward
  mode and the reverse mode respectively. Assume all functions to be
  scalar functions of a scalar variable,
  i.e.~\(f_i: \R \rightarrow \R\).
\item
  If you simply ask your interpreter / compiler to evaluate the
  expression in (a), will the computation be in forward mode, reverse
  mode, or neither of the modes? Why? You can assume that your
  interpreter / compiler does not do any caching or optimization and
  simply evaluates the expression from left to right. Does anything
  change if you assume that your interpreter caches results that have
  been computed before?
\end{enumerate}

\subsubsection{Solution}\label{solution-2}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  For better readability we will write \((g\circ h)(x)\) for
  \(g(h(x))\). By applying the chain rule, we obtain \[
  \begin{aligned}
  F'(x) 
   &= (f_3 \circ f_2 \circ f_1)' (x) \\
   &= (f_2 \circ f_1)'(x) 
  \cdot (f_3' \circ f_2 \circ f_1)(x)\\
  &= f_1'(x) \cdot (f_2' \circ f_1)(x) 
  \cdot (f_3' \circ f_2 \circ f_1)(x)
  \end{aligned}
  \]
\item
  First, let's start with the forward mode

\begin{verbatim}
d = f1'(x)
v = f1(x)
d = f2'(x) * d
v = f2(v)
d = f3'(v) * d
\end{verbatim}

  Now, for the reverse mode, we first do a ``forward pass'' before
  computing gradients:

\begin{verbatim}
v1 = f1(x)
v2 = f2(v1)
d = f3'(v2)
d = d*f2'(v1)
d = d*f1'(x)
\end{verbatim}
\item
  Simply evaluating the expression in (a) is not in line with any of the
  modes. It also involves repeated computations because \(f_1(x)\) will
  be computed twice. Now, if we allow for caching of ingtermediate
  results, this doubling of compugtaiton disappears. The order written
  above will then be in line with forward move automatic
  differentiation. However, this is specific to our example and in
  general not true.
\end{enumerate}

\subsection{Exercise 4 - GloVe
Embeddings}\label{exercise-4---glove-embeddings}

Open the notebook presented in class and work through it by trying some
of the ideas presented therein for different word combinations.




\end{document}
